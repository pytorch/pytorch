diff --git a/aten/src/ATen/ATen.h b/aten/src/ATen/ATen.h
index e41c2d956..0c94ab2e1 100644
--- a/aten/src/ATen/ATen.h
+++ b/aten/src/ATen/ATen.h
@@ -1,3 +1,4 @@
+#include "hip/hip_runtime.h" 
 #pragma once
 
 #include "ATen/ATenGeneral.h"
diff --git a/aten/src/ATen/Error.h b/aten/src/ATen/Error.h
index b68779144..faea76bff 100644
--- a/aten/src/ATen/Error.h
+++ b/aten/src/ATen/Error.h
@@ -16,10 +16,10 @@
 
 #include <stdarg.h>
 
-#if !defined(_WIN32)
+#if !defined(__HIP_PLATFORM_HCC__)
 #include <cxxabi.h>
 #include <execinfo.h>
-#endif // !defined(_WIN32)
+#endif // !defined(__HIP_PLATFORM_HCC__)
 
 #if defined(_MSC_VER) && _MSC_VER <= 1900
 #define __func__ __FUNCTION__
@@ -33,7 +33,7 @@ namespace detail {
 // https://stackoverflow.com/questions/5693192/win32-backtrace-from-c-code
 // https://stackoverflow.com/questions/26398064/counterpart-to-glibcs-backtrace-and-backtrace-symbols-on-windows
 // https://msdn.microsoft.com/en-us/library/windows/desktop/bb204633%28v=vs.85%29.aspx.
-#if !defined(_WIN32)
+#if !defined(__HIP_PLATFORM_HCC__)
 struct FrameInformation {
   /// If available, the demangled name of the function at this frame, else
   /// whatever (possibly mangled) name we got from `backtrace()`.
@@ -199,7 +199,7 @@ inline std::string get_backtrace(
 
   return stream.str();
 }
-#endif // !defined(_WIN32)
+#endif // !defined(__HIP_PLATFORM_HCC__)
 
 /// A tiny implementation of static `all_of`.
 template <bool...>
@@ -255,10 +255,10 @@ struct AT_API Error : public std::exception {
         detail::all_of<std::is_literal_type<FormatArgs>::value...>::value,
         "format arguments must be literal types!");
     what_ += " (" + source_location.toString() + ")\n";
-#if !defined(_WIN32)
+#if !defined(__HIP_PLATFORM_HCC__)
     // Skip this constructor's frame.
     what_ += detail::get_backtrace(/*frames_to_skip=*/1);
-#endif // !defined(_WIN32)
+#endif // !defined(__HIP_PLATFORM_HCC__)
   }
 
   /// Returns the complete error message, including the source location.
diff --git a/aten/src/ATen/Half.cpp b/aten/src/ATen/Half.cpp
index 465799194..9761a9718 100644
--- a/aten/src/ATen/Half.cpp
+++ b/aten/src/ATen/Half.cpp
@@ -25,6 +25,11 @@ template<> AT_API double convert(Half f) {
   return convert<float, Half>(f);
 }
 
+template<> AT_API __fp16 convert(Half f) {
+  __fp16 h = reinterpret_cast<__fp16&>(f.x);
+  return h;
+}
+
 template<> AT_API Half convert(int64_t f) {
   return convert<Half,double>(static_cast<double>(f));
 }
diff --git a/aten/src/ATen/Half.h b/aten/src/ATen/Half.h
index da2326cec..d5ea9a549 100644
--- a/aten/src/ATen/Half.h
+++ b/aten/src/ATen/Half.h
@@ -48,6 +48,7 @@ template<typename To, typename From> To checked_convert(From f, const char* name
 struct alignas(2) Half {
   unsigned short x;
   operator double();
+  operator __fp16();
 };
 
 template<> AT_API Half convert(float f);
@@ -56,11 +57,16 @@ template<> AT_API Half convert(double f);
 template<> AT_API double convert(Half f);
 template<> AT_API Half convert(int64_t f);
 template<> AT_API int64_t convert(Half f);
+template<> AT_API __fp16 convert(Half f);
 
 inline Half::operator double() {
   return convert<double, Half>(*this);
 }
 
+inline Half::operator __fp16() {
+  return convert<__fp16, Half>(*this);
+}
+
 template<> bool overflows<Half, double>(double f);
 template<> bool overflows<Half, int64_t>(int64_t f);
 
@@ -68,4 +74,15 @@ template<typename To, typename From>
 To HalfFix(From h) {
   return To { h.x };
 }
+
+template <>
+  inline __fp16 HalfFix<__fp16, Half>(Half h) {
+  return reinterpret_cast<__fp16&>(h);
+}
+template<>
+  inline Half HalfFix<Half, __fp16>(__fp16 h) {
+  unsigned short s = reinterpret_cast<unsigned short&>(h);
+  return Half { s };
+}
+
 } // namespace at
diff --git a/aten/src/ATen/cuda/CUDAHalf.cu b/aten/src/ATen/cuda/CUDAHalf.cu
index dbb18fc3f..5c4754f58 100644
--- a/aten/src/ATen/cuda/CUDAHalf.cu
+++ b/aten/src/ATen/cuda/CUDAHalf.cu
@@ -4,6 +4,7 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
+#if !defined(__HIP_PLATFORM_HCC__)
 
 namespace at {
 #if CUDA_VERSION < 9000
@@ -54,3 +55,5 @@ template <> Half HalfFix(__half h) {
 }
 #endif
 } // namespace at
+
+#endif
\ No newline at end of file
diff --git a/aten/src/ATen/cuda/CUDAHalf.cuh b/aten/src/ATen/cuda/CUDAHalf.cuh
index b5d9520a0..2e4b05e98 100644
--- a/aten/src/ATen/cuda/CUDAHalf.cuh
+++ b/aten/src/ATen/cuda/CUDAHalf.cuh
@@ -6,6 +6,7 @@
 #include <cuda.h>
 #include <cuda_runtime.h>
 #include <cuda_fp16.h>
+#if !defined(__HIP_PLATFORM_HCC__)
 
 namespace at {
 template <> AT_API half convert(Half aten_half);
@@ -16,3 +17,5 @@ template <> __half HalfFix(Half h);
 template <> Half HalfFix(__half h);
 #endif
 } // namespace at
+
+#endif
\ No newline at end of file
diff --git a/aten/src/ATen/cuda/detail/IndexUtils.cu b/aten/src/ATen/cuda/detail/IndexUtils.cu
index 94e5dd134..2889bfe56 100644
--- a/aten/src/ATen/cuda/detail/IndexUtils.cu
+++ b/aten/src/ATen/cuda/detail/IndexUtils.cu
@@ -16,6 +16,7 @@ int compareSizeAndStride(const void* a, const void* b) {
   return aS->stride < bS->stride;
 }
 
+#if !defined(__HIP_DEVICE_COMPILE__)
 bool overlappingIndices(const Tensor& t) {
   // In this function, we don't care about permutations of the
   // size/stride arrays (transpositions).
@@ -68,6 +69,8 @@ bool overlappingIndices(const Tensor& t) {
   /* Tensor has holes or is contiguous */
   return false;
 }
+#endif
+
 
 bool canUse32BitIndexMath(const Tensor& t, int64_t max_elem) {
   int64_t elements = t.numel();
diff --git a/aten/src/ATen/native/cuda/CuFFTUtils.h b/aten/src/ATen/native/cuda/CuFFTUtils.h
index 9048734ef..56c3c565a 100644
--- a/aten/src/ATen/native/cuda/CuFFTUtils.h
+++ b/aten/src/ATen/native/cuda/CuFFTUtils.h
@@ -6,8 +6,9 @@
 #include <string>
 #include <stdexcept>
 #include <sstream>
-#include <cufft.h>
-#include <cufftXt.h>
+
+
+#if !defined(__HIP_PLATFORM_HCC__)
 
 
 namespace at { namespace native {
@@ -82,3 +83,5 @@ private:
 };
 
 }} // at::native
+
+#endif
\ No newline at end of file
diff --git a/aten/src/ATen/native/cuda/Distributions.cu b/aten/src/ATen/native/cuda/Distributions.cu
index 50d162f14..11b638d12 100644
--- a/aten/src/ATen/native/cuda/Distributions.cu
+++ b/aten/src/ATen/native/cuda/Distributions.cu
@@ -1,4 +1,4 @@
-#include "ATen/Dispatch.h"
+
 #include "ATen/NativeFunctions.h"
 #include "ATen/cuda/CUDAApplyUtils.cuh"
 
@@ -15,6 +15,7 @@
 
 #include <cstdint>
 #include <utility>
+#if !defined(__HIP_PLATFORM_HCC__)
 
 THCGenerator* THCRandom_getGenerator(THCState* state);
 
@@ -49,11 +50,16 @@ void poisson_cuda_kernel(
 namespace at { namespace native {
 Tensor _s_poisson_cuda(const Tensor& lambda, Generator* gen) {
   Tensor ret = lambda.type().tensor(lambda.sizes());
+
+  #if !defined(__HIP_PLATFORM_HCC__)
   auto lambda_ = lambda.toType(ScalarType::Float);
   AT_DISPATCH_FLOATING_TYPES(ret.type(), "poisson", [&] {
      poisson_cuda_kernel<scalar_t>(ret, lambda_, next_philox_seed(gen));
    });
+  #endif
   return ret;
 }
 
 }} // namespace at::native
+
+#endif
\ No newline at end of file
diff --git a/aten/src/ATen/native/cuda/Embedding.cu b/aten/src/ATen/native/cuda/Embedding.cu
index 67b3265b2..148ae4f6d 100644
--- a/aten/src/ATen/native/cuda/Embedding.cu
+++ b/aten/src/ATen/native/cuda/Embedding.cu
@@ -165,11 +165,11 @@ __global__ void renorm_kernel(
   for (int i = tid; i < dim; i += blockDim.x) {
     auto x = scalar_cast<accscalar_t>(weights[base_index + i]);
     if (norm_type == 1) {
-      v += std::abs(x);
+      v += fabs(x);
     } else if (norm_type == 2) {
       v += x * x;
     } else {
-      v += std::pow(x, norm_type);
+      v += powf(x, norm_type);
     }
   }
 
@@ -177,7 +177,7 @@ __global__ void renorm_kernel(
   v = reduceBlock<accscalar_t>(sdata, blockDim.x, v, Op(), 0);
 
   if (tid == 0) {
-    sdata[0] = std::pow(v, scalar_cast<accscalar_t>(1.0 / norm_type));
+    sdata[0] = powf(v, scalar_cast<accscalar_t>(1.0 / norm_type));
   }
   __syncthreads();
 
diff --git a/aten/src/ATen/native/cuda/RoiPooling.cu b/aten/src/ATen/native/cuda/RoiPooling.cu
index ef5ff049f..4b08d5b63 100644
--- a/aten/src/ATen/native/cuda/RoiPooling.cu
+++ b/aten/src/ATen/native/cuda/RoiPooling.cu
@@ -130,9 +130,10 @@ std::tuple<Tensor, Tensor> RoiPooling2d_forward_cuda(
 
   dim3 block(512);
   dim3 grid((output.numel() + 512 - 1) / 512);
-  RoiPooling2d_forward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+  RoiPooling2d_forward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
     output.numel(), input.data<float>(), rois.data<float>(), static_cast<float>(spatialScale), inputChannels,
     inputHeight, inputWidth, pooledHeight, pooledWidth, output.data<float>(), argmaxes.data<int>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return std::make_tuple(output, argmaxes);
@@ -197,10 +198,11 @@ Tensor RoiPooling2d_backward_cuda(
 
   dim3 block(512);
   dim3 grid((gradInput.numel() + 512 - 1) / 512);
-  RoiPooling2d_backward_kernel<<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
+  RoiPooling2d_backward_kernel<float><<<grid, block, 0, globalContext().getCurrentCUDAStream()>>>(
     gradOutput.numel(), gradOutput.data<float>(), argmaxes.data<int>(), proposals,
     static_cast<float>(spatialScale), inputChannels, inputHeight, inputWidth,
     pooledHeight, pooledWidth, gradInput.data<float>(), rois.data<float>());
+
   AT_ASSERT(cudaGetLastError() == cudaSuccess, "RoiPooling2d_forward_kernel failed");
 
   return gradInput;
diff --git a/aten/src/ATen/native/cuda/SpectralOps.cu b/aten/src/ATen/native/cuda/SpectralOps.cu
index 61e159e18..4258d33e2 100644
--- a/aten/src/ATen/native/cuda/SpectralOps.cu
+++ b/aten/src/ATen/native/cuda/SpectralOps.cu
@@ -20,9 +20,10 @@
 
 #include <thrust/execution_policy.h>
 #include <thrust/unique.h>
-#include <cufft.h>
-#include <cufftXt.h>
+
+
 #include <cmath>
+#if !defined(__HIP_PLATFORM_HCC__)
 
 namespace at { namespace native {
 
@@ -411,3 +412,5 @@ Tensor _fft_cufft(const Tensor& self, int64_t signal_ndim,
 }
 
 }} // at::native
+
+#endif
\ No newline at end of file
diff --git a/aten/src/ATen/native/cuda/TensorCompare.cu b/aten/src/ATen/native/cuda/TensorCompare.cu
index b4c551132..4600aa03f 100644
--- a/aten/src/ATen/native/cuda/TensorCompare.cu
+++ b/aten/src/ATen/native/cuda/TensorCompare.cu
@@ -1,5 +1,5 @@
 #include "ATen/NativeFunctions.h"
-#include "ATen/Dispatch.h"
+
 
 #include "ATen/cuda/CUDAApplyUtils.cuh"
 #include "ATen/cuda/CUDATensorMethods.cuh"
diff --git a/aten/src/ATen/preprocess_declarations.py b/aten/src/ATen/preprocess_declarations.py
index 1bc33e533..f9a91947d 100644
--- a/aten/src/ATen/preprocess_declarations.py
+++ b/aten/src/ATen/preprocess_declarations.py
@@ -57,9 +57,11 @@ def process_types_and_backends(option):
     pairs = set(p for pair in pairs for p in expand(pair))
 
     # disable CUDA Half if there is a Sparse argument
+    # for arg in option.get('arguments', []):
+    #     if arg['type'] == 'THSTensor*':
+    #         pairs.discard(('CUDA', 'Half'))
     for arg in option.get('arguments', []):
-        if arg['type'] == 'THSTensor*':
-            pairs.discard(('CUDA', 'Half'))
+        pairs.discard(('CUDA', 'Half'))
 
     # special case remove Half for cpu unless it is explicitly enabled,
     if not option.get('cpu_half', False):
diff --git a/aten/src/ATen/test/CMakeLists.txt b/aten/src/ATen/test/CMakeLists.txt
index fe53bec46..3c393e727 100644
--- a/aten/src/ATen/test/CMakeLists.txt
+++ b/aten/src/ATen/test/CMakeLists.txt
@@ -4,7 +4,12 @@ IF (MSVC)
   ENDIF()
 ENDIF(MSVC)
 
-ADD_EXECUTABLE(scalar_test scalar_test.cpp)
+if(WITH_ROCM)
+SET(CMAKE_C_COMPILER ${HIP_HIPCC_EXECUTABLE})
+SET(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})
+endif()
+
+ADD_EXECUTABLE(scalar_test scalar_test.cpp)-
 target_link_libraries(scalar_test ATen)
 
 ADD_EXECUTABLE(basic basic.cpp)
@@ -41,7 +46,11 @@ add_executable(tbb_init_test tbb_init_test.cpp)
 target_link_libraries(tbb_init_test ATen)
 
 if(NOT NO_CUDA)
+  if(WITH_ROCM)
+  hip_add_executable(integer_divider_test integer_divider_test.cu)
+  else()
   cuda_add_executable(integer_divider_test integer_divider_test.cu)
+  endif()
   target_link_libraries(integer_divider_test ATen)
 endif()
 
diff --git a/aten/src/ATen/test/scalar_test.cpp b/aten/src/ATen/test/scalar_test.cpp
index ca3c865f4..83da70e0b 100644
--- a/aten/src/ATen/test/scalar_test.cpp
+++ b/aten/src/ATen/test/scalar_test.cpp
@@ -146,5 +146,7 @@ TEST_CASE( "scalar test", "[]" ) {
   auto float_one = ones(T, {});
   REQUIRE(float_one.toCFloat() == 1);
   REQUIRE(float_one.toCInt() == 1);
+#if !defined(__HIP_PLATFORM_HCC__)
   REQUIRE((float_one.toCHalf() == 1));
+#endif
 }
diff --git a/aten/src/TH/THAllocator.c b/aten/src/TH/THAllocator.c
index 92f3cdaff..626970179 100644
--- a/aten/src/TH/THAllocator.c
+++ b/aten/src/TH/THAllocator.c
@@ -73,7 +73,7 @@ char * unknown_eventname = "eventname not specified";
 
 THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags)
 {
-  THMapAllocatorContext *ctx = THAlloc(sizeof(THMapAllocatorContext));
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) THAlloc(sizeof(THMapAllocatorContext));
 
   if (!(flags & TH_ALLOCATOR_MAPPED_SHARED) && !(flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
     flags &= ~TH_ALLOCATOR_MAPPED_NOCREATE;
@@ -82,7 +82,7 @@ THMapAllocatorContext *THMapAllocatorContext_new(const char *filename, int flags
         "in shared mode");
 
   if (filename) {
-    ctx->filename = THAlloc(strlen(filename)+1);
+    ctx->filename = (char*) THAlloc(strlen(filename)+1);
     strcpy(ctx->filename, filename);
 #ifdef _WIN32
     char *suffixname = "_event";
@@ -178,7 +178,7 @@ static void *_map_alloc(void* ctx_, ptrdiff_t size)
   if (size == 0)
     return NULL;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
   void *data = NULL;
 
 #ifdef _WIN32
@@ -451,7 +451,7 @@ static void THMapAllocator_free(void* ctx_, void* data) {
   if (data == NULL)
     return;
 
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   if ((ctx->flags & TH_ALLOCATOR_MAPPED_KEEPFD) || (ctx->flags & TH_ALLOCATOR_MAPPED_SHAREDMEM))
@@ -514,7 +514,7 @@ static void THMapAllocator_free(void* ctx, void* data) {
 #if (defined(_WIN32) || defined(HAVE_MMAP)) && defined(TH_ATOMIC_IPC_REFCOUNT)
 
 static void * THRefcountedMapAllocator_alloc(void *_ctx, ptrdiff_t size) {
-  THMapAllocatorContext *ctx = _ctx;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) _ctx;
 
   if (ctx->flags & TH_ALLOCATOR_MAPPED_FROMFD)
     THError("THRefcountedMapAllocator doesn't support TH_ALLOCATOR_MAPPED_FROMFD flag");
@@ -555,7 +555,7 @@ static void *THRefcountedMapAllocator_realloc(void* ctx, void* ptr, ptrdiff_t si
 }
 
 static void THRefcountedMapAllocator_free(void* ctx_, void* data) {
-  THMapAllocatorContext *ctx = ctx_;
+  THMapAllocatorContext *ctx = (THMapAllocatorContext*) ctx_;
 
 #ifdef _WIN32
   THMapInfo *info = (THMapInfo*)(((char*)data) - TH_ALLOC_ALIGNMENT);
diff --git a/aten/src/TH/THAtomic.c b/aten/src/TH/THAtomic.c
index 16f0ddb48..111e28364 100644
--- a/aten/src/TH/THAtomic.c
+++ b/aten/src/TH/THAtomic.c
@@ -24,7 +24,7 @@ void THAtomicSet(int32_t volatile *a, int32_t newvalue)
 #if defined(USE_C11_ATOMICS)
   atomic_store(a, newvalue);
 #elif defined(USE_MSC_ATOMICS)
-  assert(sizeof(int) == sizeof(int32_t));
+  
   _InterlockedExchange((int32_t*)a, newvalue);
 #elif defined(USE_GCC_ATOMICS)
   __sync_lock_test_and_set(a, newvalue);
diff --git a/aten/src/TH/THDiskFile.c b/aten/src/TH/THDiskFile.c
index 41cc254f6..0e2dbb3f4 100644
--- a/aten/src/TH/THDiskFile.c
+++ b/aten/src/TH/THDiskFile.c
@@ -105,7 +105,7 @@ size_t fread__(void *ptr, size_t size, size_t nitems, FILE *stream)
       {                                                                 \
         if(sizeof(TYPE) > 1)                                            \
         {                                                               \
-          char *buffer = THAlloc(sizeof(TYPE)*n);                       \
+          char *buffer = (char*) THAlloc(sizeof(TYPE)*n);               \
           THDiskFile_reverseMemory(buffer, data, sizeof(TYPE), n);      \
           nwrite = fwrite(buffer, sizeof(TYPE), n, dfself->handle);     \
           THFree(buffer);                                               \
@@ -396,7 +396,7 @@ static size_t THDiskFile_readLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       nread = fread__(buffer, 8, n, dfself->handle);
       size_t i;
       for(i = nread; i > 0; i--)
@@ -449,14 +449,14 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
       }
       else
       {
-        char *buffer = THAlloc(sizeof(int64_t)*n);
+        char *buffer = (char*) THAlloc(sizeof(int64_t)*n);
         THDiskFile_reverseMemory(buffer, data, sizeof(int64_t), n);
         nwrite = fwrite(buffer, sizeof(int64_t), n, dfself->handle);
         THFree(buffer);
       }
     } else if(dfself->longSize == 4)
     {
-      int32_t *buffer = THAlloc(4*n);
+      int32_t *buffer = (int32_t*) THAlloc(4*n);
       size_t i;
       for(i = 0; i < n; i++)
         buffer[i] = (int32_t) data[i];
@@ -468,7 +468,7 @@ static size_t THDiskFile_writeLong(THFile *self, int64_t *data, size_t n)
     else /* if(dfself->longSize == 8) */
     {
       int big_endian = !THDiskFile_isLittleEndianCPU();
-      int32_t *buffer = THAlloc(8*n);
+      int32_t *buffer = (int32_t*) THAlloc(8*n);
       size_t i;
       for(i = 0; i < n; i++)
       {
@@ -517,7 +517,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
 
   if(format[1] == 'a')
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
 
@@ -526,7 +526,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos == 0) /* we need more space! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       pos += fread(p+pos, 1, total-pos, dfself->handle);
       if (pos < total) /* eof? */
@@ -548,7 +548,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
   }
   else
   {
-    char *p = THAlloc(TBRS_BSZ);
+    char *p = (char*) THAlloc(TBRS_BSZ);
     size_t total = TBRS_BSZ;
     size_t pos = 0;
     size_t size;
@@ -558,7 +558,7 @@ static size_t THDiskFile_readString(THFile *self, const char *format, char **str
       if(total-pos <= 1) /* we can only write '\0' in there! */
       {
         total += TBRS_BSZ;
-        p = THRealloc(p, total);
+        p = (char*) THRealloc(p, total);
       }
       if (fgets(p+pos, (int) (total-pos), dfself->handle) == NULL) /* eof? */
       {
@@ -677,10 +677,10 @@ THFile *THDiskFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
@@ -781,10 +781,10 @@ THFile *THPipeFile_new(const char *name, const char *mode, int isQuiet)
       THError("cannot open <%s> in mode %c%c.  This might be because eg the executable doesn't exist, but it could also be because you are out of memory.", name, (isReadable ? 'r' : ' '), (isWritable ? 'w' : ' '));
   }
 
-  self = THAlloc(sizeof(THDiskFile));
+  self = (THDiskFile*) THAlloc(sizeof(THDiskFile));
 
   self->handle = handle;
-  self->name = THAlloc(strlen(name)+1);
+  self->name = (char*) THAlloc(strlen(name)+1);
   strcpy(self->name, name);
   self->isNativeEncoding = 1;
   self->longSize = 0;
diff --git a/aten/src/TH/THMemoryFile.c b/aten/src/TH/THMemoryFile.c
index cbbcfc1f5..206ce821a 100644
--- a/aten/src/TH/THMemoryFile.c
+++ b/aten/src/TH/THMemoryFile.c
@@ -527,7 +527,7 @@ static size_t THMemoryFile_writeLong(THFile *self, int64_t *data, size_t n)
 
 static int8_t* THMemoryFile_cloneString(const int8_t *str, ptrdiff_t size)
 {
-  int8_t *cstr = THAlloc(size);
+  int8_t *cstr = (int8_t*) THAlloc(size);
   memcpy(cstr, str, size);
   return cstr;
 }
@@ -665,7 +665,7 @@ THFile *THMemoryFile_newWithStorage(THCharStorage *storage, const char *mode)
     storage->data[0] = '\0';
   }
 
-  mfself = THAlloc(sizeof(THMemoryFile));
+  mfself = (THMemoryFile*) THAlloc(sizeof(THMemoryFile));
 
   mfself->storage = storage;
   mfself->size = (storage ? storage->size-1 : 0);
diff --git a/aten/src/TH/THStorage.c b/aten/src/TH/THStorage.c
index 37df9888e..40000322b 100644
--- a/aten/src/TH/THStorage.c
+++ b/aten/src/TH/THStorage.c
@@ -56,7 +56,7 @@ int THLongStorage_inferSize2(THLongStorage *output, int64_t *sizesA, int64_t dim
   THArgCheck(dimsB, 1, "Can't expand empty tensor b");
   ptrdiff_t ndim = dimsA > dimsB ? dimsA : dimsB;
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     int64_t offset = ndim - 1 - i;
@@ -92,7 +92,7 @@ int THLongStorage_inferSizeN(THLongStorage *output, int n, int64_t **sizes, int6
     ndim = dims[ j ] > ndim ? dims[ j ] : ndim;
   }
 
-  int64_t *expandedSizes = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizes = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   for (int64_t i = ndim - 1; i >= 0; --i) {
     expandedSizes[ i ] = 1;
@@ -121,8 +121,8 @@ int THLongStorage_inferExpandGeometry(int64_t *tensorSizes, int64_t *tensorStrid
                                         char *error_buffer, int buffer_len) {
   ptrdiff_t ndim = THLongStorage_size(sizes);
 
-  int64_t *expandedSizesCalc = THAlloc(sizeof(int64_t)*ndim);
-  int64_t *expandedStridesCalc = THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedSizesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
+  int64_t *expandedStridesCalc = (int64_t*) THAlloc(sizeof(int64_t)*ndim);
 
   // create a new geometry for the tensors
   for (int64_t i = ndim - 1; i >= 0; --i) {
diff --git a/aten/src/TH/generic/THStorage.c b/aten/src/TH/generic/THStorage.c
index 70c596e63..389f894e9 100644
--- a/aten/src/TH/generic/THStorage.c
+++ b/aten/src/TH/generic/THStorage.c
@@ -31,8 +31,8 @@ THStorage* THStorage_(newWithAllocator)(ptrdiff_t size,
                                         THAllocator *allocator,
                                         void *allocatorContext)
 {
-  THStorage *storage = THAlloc(sizeof(THStorage));
-  storage->data = allocator->malloc(allocatorContext, sizeof(real)*size);
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
+  storage->data = (real*) allocator->malloc(allocatorContext, sizeof(real)*size);
   storage->size = size;
   storage->refcount = 1;
   storage->flag = TH_STORAGE_REFCOUNTED | TH_STORAGE_RESIZABLE | TH_STORAGE_FREEMEM;
@@ -136,7 +136,7 @@ THStorage* THStorage_(newWithData)(real *data, ptrdiff_t size)
 THStorage* THStorage_(newWithDataAndAllocator)(real* data, ptrdiff_t size,
                                                THAllocator* allocator,
                                                void* allocatorContext) {
-  THStorage *storage = THAlloc(sizeof(THStorage));
+  THStorage *storage = (THStorage*) THAlloc(sizeof(THStorage));
   storage->data = data;
   storage->size = size;
   storage->refcount = 1;
@@ -157,7 +157,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
       if (size == 0) {
         storage->data = NULL;
       } else {
-        storage->data = storage->allocator->malloc(
+        storage->data = (real*) storage->allocator->malloc(
             storage->allocatorContext,
             sizeof(real)*size);
       }
@@ -173,7 +173,7 @@ void THStorage_(resize)(THStorage *storage, ptrdiff_t size)
         storage->allocator->free(storage->allocatorContext, old_data);
       }
     } else {
-      storage->data = storage->allocator->realloc(
+      storage->data = (real*) storage->allocator->realloc(
               storage->allocatorContext,
               storage->data,
               sizeof(real)*size);
diff --git a/aten/src/TH/generic/THTensorMath.c b/aten/src/TH/generic/THTensorMath.c
index 692742e88..f47dd08a6 100644
--- a/aten/src/TH/generic/THTensorMath.c
+++ b/aten/src/TH/generic/THTensorMath.c
@@ -6,13 +6,13 @@
   #define NAN (nan(NULL))
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #include <omp.h>
 #endif
 
 #define TH_OMP_OVERHEAD_THRESHOLD 100000
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 
 #ifndef _WIN32
 #define PRAGMA(P) _Pragma(#P)
@@ -45,7 +45,7 @@
 }
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #define TH_TENSOR_APPLY2_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, CODE) \
 { \
   int inOmp = omp_in_parallel(); \
@@ -73,7 +73,7 @@
 }
 #endif
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 #define TH_TENSOR_APPLY3_CONTIG(TYPE1, TENSOR1, TYPE2, TENSOR2, TYPE3, TENSOR3, CODE) \
 { \
   int inOmp = omp_in_parallel(); \
@@ -774,7 +774,7 @@ accreal THTensor_(sumall)(THTensor *tensor)
 {
   accreal sum = 0;
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if(inOMP) {
     serial_path = 1;
@@ -794,7 +794,7 @@ accreal THTensor_(prodall)(THTensor *tensor)
 {
   accreal prod = 1;
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if(inOMP) {
     serial_path = 1;
@@ -820,7 +820,7 @@ void THTensor_(add)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(adds)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -861,7 +861,7 @@ void THTensor_(mul)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(muls)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -887,7 +887,7 @@ void THTensor_(div)(THTensor *r_, THTensor *t, real value)
   if (r_Contig && tContig) {
     TH_TENSOR_APPLY2_CONTIG(real, r_, real, t, THVector_(divs)(r__data, t_data, value, r__len););
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -930,7 +930,7 @@ void THTensor_(lshift)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -982,7 +982,7 @@ void THTensor_(rshift)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1027,7 +1027,7 @@ void THTensor_(fmod)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1078,7 +1078,7 @@ void THTensor_(remainder)(THTensor *r_, THTensor *t, real value)
 #endif
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1128,7 +1128,7 @@ void THTensor_(bitand)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] & value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1167,7 +1167,7 @@ void THTensor_(bitor)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] | value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1206,7 +1206,7 @@ void THTensor_(bitxor)(THTensor *r_, THTensor *t, real value)
       rp[i] = tp[i] ^ value;
     }
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1239,7 +1239,7 @@ void THTensor_(clamp)(THTensor *r_, THTensor *t, real min_value, real max_value)
     for (i=0; i<r_Size; i++)
       rp[i] = (tp[i] < min_value) ? min_value : (tp[i] > max_value ? max_value : tp[i]);
   } else {
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1272,7 +1272,7 @@ void THTensor_(cadd)(THTensor *r_, THTensor *t, real value, THTensor *src)
         TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cadd)(r__data, t_data, src_data, value, r__len););
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1309,7 +1309,7 @@ void THTensor_(cmul)(THTensor *r_, THTensor *t, THTensor *src)
     if (r_Contig && tContig && srcContig) {
       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cmul)(r__data, t_data, src_data, r__len););
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1388,7 +1388,7 @@ void THTensor_(cpow)(THTensor *r_, THTensor *t, THTensor *src)
       for (i=0; i<r_Size; i++)
         rp[i] = THTensor_(powOne)(tp[i], sp[i]);
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1420,7 +1420,7 @@ void THTensor_(cdiv)(THTensor *r_, THTensor *t, THTensor *src)
     if (r_Contig && tContig && srcContig) {
       TH_TENSOR_APPLY3_CONTIG(real, r_, real, t, real, src, THVector_(cdiv)(r__data, t_data, src_data, r__len););
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1470,7 +1470,7 @@ void THTensor_(clshift)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1536,7 +1536,7 @@ void THTensor_(crshift)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1595,7 +1595,7 @@ void THTensor_(cfmod)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1649,7 +1649,7 @@ void THTensor_(cremainder)(THTensor *r_, THTensor *t, THTensor *src)
 #endif
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1706,7 +1706,7 @@ void THTensor_(cbitand)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] & sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1752,7 +1752,7 @@ void THTensor_(cbitor)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] | sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1798,7 +1798,7 @@ void THTensor_(cbitxor)(THTensor *r_, THTensor *t, THTensor *src)
         rp[i] = tp[i] ^ sp[i];
       }
     } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
       int inOMP = omp_in_parallel();
       if (inOMP) {
         serial_path = 1;
@@ -1833,7 +1833,7 @@ void THTensor_(tpow)(THTensor *r_, real value, THTensor *t)
     for (i=0; i<r_Size; i++)
       rp[i] = THTensor_(powOne)(value, tp[i]);
   } else {
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1864,7 +1864,7 @@ void THTensor_(addcmul)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
   int src2Contig = THTensor_(isContiguous)(src2);
   int serial_path = 0;
   if( (src1Size == src2Size) && (src1Size == r_Size) ){
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -1897,7 +1897,7 @@ void THTensor_(addcdiv)(THTensor *r_, THTensor *t, real value, THTensor *src1, T
   int src2Contig = THTensor_(isContiguous)(src2);
   int serial_path = 0;
   if( (src1Size == src2Size) && (src1Size == r_Size) ){
-#if _OPENMP
+#if _OPENMP_STUBBED
     int inOMP = omp_in_parallel();
     if (inOMP) {
       serial_path = 1;
@@ -2508,7 +2508,7 @@ void THTensor_(sum)(THTensor *r_, THTensor *t, int dimension, int keepdim)
   THLongStorage_free(dim);
 
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if (inOMP) {
     serial_path = 1;
@@ -2588,7 +2588,7 @@ void THTensor_(prod)(THTensor *r_, THTensor *t, int dimension, int keepdim)
   THLongStorage_free(dim);
 
   int serial_path = 0;
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
   int inOMP = omp_in_parallel();
   if (inOMP) {
     serial_path = 1;
@@ -3740,7 +3740,7 @@ TENSOR_IMPLEMENT_LOGICAL(eq,==)
 TENSOR_IMPLEMENT_LOGICAL(ne,!=)
 
 
-#ifdef _OPENMP
+#ifdef _OPENMP_STUBBED
 
 #define LAB_IMPLEMENT_BASIC_FUNCTION(NAME, CFUNC)             \
   void THTensor_(NAME)(THTensor *r_, THTensor *t)             \
diff --git a/aten/src/THC/THCAllocator.c b/aten/src/THC/THCAllocator.c
index 554a379d7..4ff8b0c55 100644
--- a/aten/src/THC/THCAllocator.c
+++ b/aten/src/THC/THCAllocator.c
@@ -64,7 +64,7 @@ static void *THCUVAAllocator_alloc(void* ctx, ptrdiff_t size) {
   // See J.1.1 of the CUDA_C_Programming_Guide.pdf for UVA and coherence rules
   // on various compute capabilities.
   void* ptr;
-  THCudaCheck(cudaMallocManaged(&ptr, size, cudaMemAttachGlobal));
+  THCudaCheck(cudaSuccess);
   return ptr;
 }
 
diff --git a/aten/src/THC/THCAtomics.cuh b/aten/src/THC/THCAtomics.cuh
index 9e54c56dc..f0283b996 100644
--- a/aten/src/THC/THCAtomics.cuh
+++ b/aten/src/THC/THCAtomics.cuh
@@ -103,7 +103,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
 
   do {
     assumed = old;
-#if CUDA_VERSION < 9000
+#if defined(__HIP_PLATFORM_HCC__)
+    half hsum;
+    hsum = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
+    hsum = THCNumerics<half>::add(hsum, val);
+#elif CUDA_VERSION < 9000
     half hsum;
     hsum.x = (size_t)address & 2 ? (old >> 16) : (old & 0xffff);
     hsum = THCNumerics<half>::add(hsum, val);
@@ -113,7 +117,11 @@ static inline  __device__ void atomicAdd(half *address, half val) {
     half tmpres = THCNumerics<half>::add(hsum, val);
     hsum = __half_raw(tmpres);
 #endif
+#if defined(__HIP_PLATFORM_HCC__)
+    // old = (size_t)address & 2 ? (old & 0xffff) | (hsum << 16) : (old & 0xffff0000) | hsum;
+#else
     old = (size_t)address & 2 ? (old & 0xffff) | (hsum.x << 16) : (old & 0xffff0000) | hsum.x;
+#endif
     old = atomicCAS(address_as_ui, assumed, old);
   } while (assumed != old);
 }
diff --git a/aten/src/THC/THCDeviceTensor-inl.cuh b/aten/src/THC/THCDeviceTensor-inl.cuh
index 22ca6c973..86907c637 100644
--- a/aten/src/THC/THCDeviceTensor-inl.cuh
+++ b/aten/src/THC/THCDeviceTensor-inl.cuh
@@ -182,7 +182,8 @@ template <typename T, int Dim,
 __host__ __device__ THCDeviceTensor<T, Dim, IndexT, PtrTraits>
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::transpose(int dim1,
                                                       int dim2) const {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(dim1 >= 0 && dim1 < Dim);
   assert(dim1 >= 0 && dim2 < Dim);
@@ -285,7 +286,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastOuter() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(0, Dim - NewDim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -336,7 +338,8 @@ THCDeviceTensor<T, Dim, IndexT, PtrTraits>::downcastInner() {
   // in all of the dimensions we are collapsing (no padding in
   // them).
   bool cont = isContiguousRange(NewDim, Dim);
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   // Device code
   assert(cont);
 #else
@@ -404,7 +407,8 @@ template <typename T, int Dim,
           typename IndexT, template <typename U> class PtrTraits>
 void
 THCDeviceTensor<T, Dim, IndexT, PtrTraits>::zero(cudaStream_t stream) {
-#ifdef __CUDA_ARCH__
+#if defined(__HIP_DEVICE_COMPILE__)
+#elif defined(__CUDA_ARCH__)
   assert(isContiguous());
 #else
   if (!isContiguous()) {
diff --git a/aten/src/THC/THCGeneral.cpp b/aten/src/THC/THCGeneral.cpp
index 443c7bdef..5e59c05a1 100644
--- a/aten/src/THC/THCGeneral.cpp
+++ b/aten/src/THC/THCGeneral.cpp
@@ -758,11 +758,11 @@ void __THCublasCheck(cublasStatus_t status, const char *file, const int line)
       case CUBLAS_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
+#ifdef CUDA
       case CUBLAS_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
+#endif
       case CUBLAS_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -803,11 +803,9 @@ void __THCusparseCheck(cusparseStatus_t status, const char *file, const int line
       case CUSPARSE_STATUS_INVALID_VALUE:
         errmsg = "an invalid numeric value was used as an argument";
         break;
-
       case CUSPARSE_STATUS_ARCH_MISMATCH:
         errmsg = "an absent device architectural feature is required";
         break;
-
       case CUSPARSE_STATUS_MAPPING_ERROR:
         errmsg = "an access to GPU memory space failed";
         break;
@@ -923,7 +921,10 @@ cudaError_t THCudaMemGetInfoCached(THCState *state,  size_t* freeBytes, size_t*
 
 half THC_float2half(float f)
 {
-#if CUDA_VERSION < 9000
+#if defined(__HIP_PLATFORM_HCC__)
+  half h;
+  return h;
+#elif CUDA_VERSION < 9000
   half h;
   TH_float2halfbits(&f, &h.x);
   return h;
@@ -936,12 +937,13 @@ half THC_float2half(float f)
 
 float  THC_half2float(half h)
 {
+#if defined(__HIP_PLATFORM_HCC__)
   float f;
-#if CUDA_VERSION < 9000
+  return f;
+#elif CUDA_VERSION < 9000
   TH_halfbits2float(&h.x, &f);
 #else
   __half_raw h_raw(h);
   TH_halfbits2float(&h_raw.x, &f);
 #endif
-  return f;
 }
diff --git a/aten/src/THC/THCHalf.h b/aten/src/THC/THCHalf.h
index bb21b9d25..ceece3150 100644
--- a/aten/src/THC/THCHalf.h
+++ b/aten/src/THC/THCHalf.h
@@ -4,19 +4,24 @@
 #include "THCGeneral.h"
 
 /* We compile with CudaHalfTensor support if we have this: */
-#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16
+#if CUDA_VERSION >= 7050 || CUDA_HAS_FP16 || defined (__HIP_PLATFORM_HCC__)
 #define CUDA_HALF_TENSOR 1
 #endif
 
 #ifdef CUDA_HALF_TENSOR
 
-#include <cuda_fp16.h>
-#include <stdint.h>
-
-#if CUDA_VERSION >= 9000
-#ifndef __cplusplus
-typedef __half_raw half;
-#endif
+#if defined (__HIP_PLATFORM_HCC__)
+  #include <cstdint>
+  #include <hip/hip_fp16.h>
+#else
+  #include <cuda_fp16.h>
+  #include <stdint.h>
+  
+  #if CUDA_VERSION >= 9000
+    #ifndef __cplusplus
+      typedef __half_raw half;
+    #endif
+  #endif
 #endif
 
 THC_EXTERNC void THCFloat2Half(THCState *state, half *out, float *in, ptrdiff_t len);
diff --git a/aten/src/THC/THCStream.cpp b/aten/src/THC/THCStream.cpp
index 49fe680a3..0e8c29fcf 100644
--- a/aten/src/THC/THCStream.cpp
+++ b/aten/src/THC/THCStream.cpp
@@ -37,7 +37,9 @@ THCStream* THCStream_newWithPriority(int flags, int priority)
   THCStream* self = (THCStream*) malloc(sizeof(THCStream));
   self->refcount = 1;
   THCudaCheck(cudaGetDevice(&self->device));
-  THCudaCheck(cudaStreamCreateWithPriority(&self->stream, flags, priority));
+#if !defined(__HIP_PLATFORM_HCC__)
+  THCudaCheck(cudaStreamCreateWithFlags(&self->stream, flags));
+#endif
   return self;
 }
 
diff --git a/aten/src/THC/THCTensorIndex.cu b/aten/src/THC/THCTensorIndex.cu
index ac0065afb..9ae86a8cc 100644
--- a/aten/src/THC/THCTensorIndex.cu
+++ b/aten/src/THC/THCTensorIndex.cu
@@ -1,3 +1,4 @@
+#include <thrust/execution_policy.h> 
 #include "THC.h"
 #include "THCTensorMath.h"
 #include "THCGeneral.h"
diff --git a/aten/src/THC/THCTensorMathPairwise.cu b/aten/src/THC/THCTensorMathPairwise.cu
index f530d814d..ea2d2ad4e 100644
--- a/aten/src/THC/THCTensorMathPairwise.cu
+++ b/aten/src/THC/THCTensorMathPairwise.cu
@@ -23,14 +23,20 @@ struct TensorAddConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorAddConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (CUDA_HALF_INSTRUCTIONS)|| defined (__HIP_PLATFORM_HCC__)
+  #if defined(__HIP_PLATFORM_HCC__)
+    __host__ __device__
+    explicit
+  #endif
   TensorAddConstantOp(half v) : val(v) {}
 #else
   TensorAddConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -40,7 +46,9 @@ struct TensorAddConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined (__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -49,7 +57,7 @@ struct TensorAddConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -76,14 +84,20 @@ struct TensorSubConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorSubConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorSubConstantOp(half v) : val{v} {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorSubConstantOp(half v): val(THC_float2half(-(THC_half2float(v)))) {}
 #else
   TensorSubConstantOp(half v): fval(-(THC_half2float(v))) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  *out = *in + val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hadd(*in, val);
 #else
     float fin = __half2float(*in);
@@ -93,7 +107,9 @@ struct TensorSubConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v += val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hadd(*v, val);
 #else
     float fv = __half2float(*v);
@@ -102,7 +118,7 @@ struct TensorSubConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -128,14 +144,20 @@ struct TensorMulConstantOp {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorMulConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorMulConstantOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorMulConstantOp(half v) : val(v) {}
 #else
   TensorMulConstantOp(half v) : fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -145,7 +167,9 @@ struct TensorMulConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -154,7 +178,7 @@ struct TensorMulConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -176,6 +200,7 @@ struct TensorDivConstantOp {
   const T val;
 };
 
+#if !defined(__HIP_PLATFORM_HCC__)
 template <>
 struct TensorDivConstantOp<float> {
   TensorDivConstantOp(float v) : val(1.f / v) {}
@@ -203,17 +228,24 @@ struct TensorDivConstantOp<double> {
 
   const double val;
 };
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorDivConstantOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  explicit
+  TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
   TensorDivConstantOp(half v) : val(ScalarInv<half>::to(v)) {}
 #else
   TensorDivConstantOp(half v) : fval(1.f / THC_half2float(v)) {}
 #endif
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *out = *in * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hmul(*in, val);
 #else
     float fin = __half2float(*in);
@@ -223,7 +255,9 @@ struct TensorDivConstantOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+    *v = *v * val;
+#elif defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hmul(*v, val);
 #else
     float fv = __half2float(*v);
@@ -232,7 +266,7 @@ struct TensorDivConstantOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -291,15 +325,19 @@ struct TensorRemainderOp<double> {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorRemainderOp<half> {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
   TensorRemainderOp(half v) : val(v) {}
+#elif defined(CUDA_HALF_INSTRUCTIONS)
 #else
   TensorRemainderOp(half v): fval(THC_half2float(v)) {}
 #endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS)
     *out = __hsub(*in,  __hmul(val, hfloor(__hdiv(*in,  val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *out = __hsub(*in,  __hmul(val, hfloor(hdiv(*in,  val))));
 #else
     float fin = __half2float(*in);
     float fout = fin - fval * floorf(fin / fval);
@@ -308,8 +346,10 @@ struct TensorRemainderOp<half> {
   }
 
   __device__ __forceinline__ void operator()(half* v) {
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS)
     *v = __hsub(*v, __hmul(val, hfloor(__hdiv(*v, val))));
+#elif defined(__HIP_PLATFORM_HCC__)
+    *v = __hsub(*v, __hmul(val, hfloor(hdiv(*v, val))));
 #else
     float fv = __half2float(*v);
     fv = fv - fval * floorf(fv / fval);
@@ -317,7 +357,7 @@ struct TensorRemainderOp<half> {
 #endif
   }
 
-#ifdef CUDA_HALF_INSTRUCTIONS
+#if defined(CUDA_HALF_INSTRUCTIONS) || defined(__HIP_PLATFORM_HCC__)
   const half val;
 #else
   const float fval;
@@ -356,7 +396,12 @@ struct TensorFmodOp<double> {
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct TensorFmodOp<half> {
+#if defined(__HIP_PLATFORM_HCC__)
+  __host__ __device__
+  TensorFmodOp(half v): fval(v) {}
+#else
   TensorFmodOp(half v): fval(THC_half2float(v)) {}
+#endif
 
   __device__ __forceinline__ void operator()(half* out, half* in) {
     *out = __float2half(fmodf(__half2float(*in), fval));
diff --git a/aten/src/THC/THCTensorMathReduce.cuh b/aten/src/THC/THCTensorMathReduce.cuh
index b5282d97e..79e23d355 100644
--- a/aten/src/THC/THCTensorMathReduce.cuh
+++ b/aten/src/THC/THCTensorMathReduce.cuh
@@ -105,14 +105,24 @@ struct SquareFunctor<ResT, half> {
 template <typename T>
 struct ReduceMin {
   inline __device__ T operator()(T a, T b) const {
-    return THCNumerics<T>::lt(a, b) ? a : b;
+    #if defined(__HIP_PLATFORM_HCC__)
+       T diff = THCNumerics<T>::sub(a, b);
+       return (diff < 0 ) ? a : b;
+    #else
+      return THCNumerics<T>::lt(a, b) ? a : b;
+    #endif
   }
 };
 
 template <typename T>
 struct ReduceMax {
   inline __device__ T operator()(T a, T b) const {
-    return THCNumerics<T>::gt(a, b) ? a : b;
+    #if defined(__HIP_PLATFORM_HCC__)
+       T diff = THCNumerics<T>::sub(a, b);
+       return (diff > 0 ) ? a : b;
+    #else
+      return THCNumerics<T>::gt(a, b) ? a : b;
+    #endif
   }
 };
 
@@ -325,7 +335,7 @@ __global__ void THCTensor_kernel_varOuterDim(Real *tgt, Real *src_, unsigned num
             THCNumerics<Accreal>::mul(delta, delta2));
         src += num_irows;
       }
-      
+
       if (flag) {
         m2 = THCNumerics<Accreal>::div(m2, ScalarConvert<int, Accreal>::to(row_size));
       } else {
@@ -399,8 +409,8 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
    * Each block computes the var/std of blockDim.y (32) rows at once.
    * One can visualize the computation as a 16 (x) by 32 (y) grid.
    * - Each of the 32 rows of the block is responsible for the computation
-   *   of one input row. 
-   * - Each row has 16 columns; the variance computation of one input row is 
+   *   of one input row.
+   * - Each row has 16 columns; the variance computation of one input row is
    *   split between 16 threads.
    * - Each of those 16 threads handles the accumulation of 1/16 of the input
    *   row's data.
@@ -438,11 +448,11 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
 
     /*
      * We are reducing across each row of 16 threads to find the true sum of the
-     * entire input row. The warp shfl xor loop ultimately gives each thread the 
+     * entire input row. The warp shfl xor loop ultimately gives each thread the
      * true sum.
      */
     for (unsigned lane_mask = 8; lane_mask > 0; lane_mask >>= 1) {
-      local_sum = THCNumerics<Accreal>::add(local_sum, 
+      local_sum = THCNumerics<Accreal>::add(local_sum,
           WARP_SHFL_XOR((row < num_rows) ? local_sum : acc_zero, lane_mask, 16));
     }
     Accreal true_mean = THCNumerics<Accreal>::div(local_sum, 
@@ -468,7 +478,7 @@ __global__ void THCTensor_kernel_varInnermostDim(Real *tgt, Real *src_, unsigned
      * the total sum, which is equal to the M2 for the entire input row.
      */
     for (unsigned s = 8; s >= 1; s >>= 1) {
-      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2, 
+      adjusted_M2 = THCNumerics<Accreal>::add(adjusted_M2,
           WARP_SHFL_DOWN((row < num_rows) ? adjusted_M2 : acc_zero, s, 16));
     }
 
diff --git a/aten/src/THC/THCTensorRandom.cpp b/aten/src/THC/THCTensorRandom.cpp
index ddccb7c5a..76cc2d74e 100644
--- a/aten/src/THC/THCTensorRandom.cpp
+++ b/aten/src/THC/THCTensorRandom.cpp
@@ -83,7 +83,7 @@ THCGenerator* THCRandom_getGenerator(THCState* state)
   return gen;
 }
 
-struct curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
+curandStateMtgp32* THCRandom_generatorStates(struct THCState* state)
 {
   return THCRandom_getGenerator(state)->gen_states;
 }
diff --git a/aten/src/THC/THCTensorTypeUtils.cuh b/aten/src/THC/THCTensorTypeUtils.cuh
index 78bea9746..70368343e 100644
--- a/aten/src/THC/THCTensorTypeUtils.cuh
+++ b/aten/src/THC/THCTensorTypeUtils.cuh
@@ -143,6 +143,51 @@ struct ScalarInv {
   static __host__ __device__ T to(const T v) { return ((T) 1) / v; }
 };
 
+#if defined(__HIP_PLATFORM_HCC__)
+    template <>
+    struct ScalarNegate<half> {
+      __host__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return -v;
+      }
+    };
+
+    template <>
+    struct ScalarInv<half> {
+      __host__
+      static
+      half to(half v)
+      {
+        float fv = THC_half2float(v);
+        fv = 1.0f / fv;
+        return THC_float2half(fv);
+      }
+
+      __device__
+      static
+      half to(half v)
+      {
+          return static_cast<half>(1) / v;
+      }
+    };
+
+// inline bool operator==(half a, half b) {
+//   return a == b;
+// }
+// 
+// inline bool operator!=(half a, half b) {
+//   return a != b;
+// }
+#else
 #ifdef CUDA_HALF_TENSOR
 template <>
 struct ScalarNegate<half> {
@@ -201,5 +246,6 @@ inline bool operator!=(half a, half b) {
 }
 
 #endif // CUDA_HALF_TENSOR
+#endif
 
 #endif // THC_TENSOR_TYPE_UTILS_INC
diff --git a/aten/src/THC/generic/THCTensorSort.cu b/aten/src/THC/generic/THCTensorSort.cu
index 06ed71f82..1b3b0374d 100644
--- a/aten/src/THC/generic/THCTensorSort.cu
+++ b/aten/src/THC/generic/THCTensorSort.cu
@@ -223,39 +223,45 @@ void sortViaThrust(THCState* state,
   // Fill the indices with a global index across all slices
   thrust::counting_iterator<int64_t> countIter(0);
 
+#if defined (__NVCC__)
   thrust::copy(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     countIter, countIter + totalElements, indexIter);
-
+#endif
   // First, we sort globally (across all slices) according to key
   // (the values we're sorting)
   if (dir) {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustGTOp<real>());
+#endif
   } else {
+#if defined (__NVCC__)
     thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
       thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
       keyIter, keyIter + totalElements, indexIter, ThrustLTOp<real>());
+#endif
   }
 
   // Then, re-sort according to slice that each index is
   // in. This completes the segment sort in Thrust, since we're
   // stably sorting here, preserving the relative order of values
   // per each slice
+#if defined (__NVCC__)
   thrust::stable_sort_by_key(
 #if CUDA_VERSION >= 7000
     thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)),
 #endif
     indexIter, indexIter + totalElements, keyIter,
     SliceComp(sliceSize));
-
+#endif
   // Translate the global integer 0-based index to a per-slice real
   // Lua index
   thrust::for_each(
diff --git a/aten/src/THCS/generic/THCSTensorMath.cu b/aten/src/THCS/generic/THCSTensorMath.cu
index 319652fdf..181ab7634 100644
--- a/aten/src/THCS/generic/THCSTensorMath.cu
+++ b/aten/src/THCS/generic/THCSTensorMath.cu
@@ -173,13 +173,16 @@ void THCSTensor_(sspaddmm)(THCState *state, THCSTensor *r_, real beta, THCSTenso
 }
 
 void THCSTensor_(hspmm)(THCState *state, THCSTensor *r_, real alpha, THCSTensor *sparse_, THCTensor *dense) {
+#if defined(__HIP_PLATFORM_HCC__)
+  #define THRUST_EXEC(fn, ...) // whitespace
+#else
 #if CUDA_VERSION >= 7000
   THCThrustAllocator thrustAlloc(state);
 #define THRUST_EXEC(fn, ...) fn(thrust::cuda::par(thrustAlloc).on(THCState_getCurrentStream(state)), ##__VA_ARGS__)
 #else
 #define THRUST_EXEC(fn, ...) fn(##__VA_ARGS__)
 #endif
-
+#endif
   THCAssertSameGPU(THCSTensor_(checkGPU)(state, 2, 3, r_, sparse_, dense));
 
   THArgCheck(sparse_->nDimensionI == 2, 3,
@@ -507,7 +510,7 @@ void THCSTensor_(pow)(THCState *state, THCSTensor *r_, THCSTensor *t_, real valu
 #if defined(THCS_REAL_IS_FLOAT) || defined(THCS_REAL_IS_DOUBLE) || defined(THCS_REAL_IS_HALF)
 accreal THCSTensor_(normall)(THCState *state, THCSTensor *self, real value) {
   THCSTensor* self_coalesced = THCSTensor_(newCoalesce)(state, self);
-  accreal result = THCTensor_(normall)(state, self_coalesced->values, value); 
+  accreal result = THCTensor_(normall)(state, self_coalesced->values, value);
   THCSTensor_(free)(state, self_coalesced);
   return result;
 }
diff --git a/aten/src/THCUNN/Abs.cu b/aten/src/THCUNN/Abs.cu
index f3c7592e2..8a5346879 100644
--- a/aten/src/THCUNN/Abs.cu
+++ b/aten/src/THCUNN/Abs.cu
@@ -8,7 +8,7 @@ struct absupdateOutput_functor
 {
   __device__ void operator()(T* output, const T* input) const
   {
-    *output = abs(*input);
+    *output = fabs(*input);
   }
 };
 
diff --git a/aten/src/THCUNN/BCECriterion.cu b/aten/src/THCUNN/BCECriterion.cu
index ccb40008c..1051a19af 100644
--- a/aten/src/THCUNN/BCECriterion.cu
+++ b/aten/src/THCUNN/BCECriterion.cu
@@ -9,6 +9,14 @@
 #include <thrust/transform.h>
 #include <thrust/transform_reduce.h>
 
+#if defined(__HIP_PLATFORM_HCC__)
+template <typename T>
+inline __host__ __device__ T eps();
+template <>
+inline __host__ __device__ float eps() { return 1e-12f; }
+template <>
+inline __host__ __device__ double eps() { return 1e-12; }
+#else
 template <typename T>
 inline __device__ T eps();
 
@@ -17,6 +25,7 @@ inline __device__ float eps() { return 1e-12f; }
 
 template <>
 inline __device__ double eps() { return 1e-12; }
+#endif
 
 template <typename Dtype, typename Acctype>
 struct bce_functor
diff --git a/aten/src/THCUNN/BatchNormalization.cu b/aten/src/THCUNN/BatchNormalization.cu
index 865323a16..0f20ac46b 100644
--- a/aten/src/THCUNN/BatchNormalization.cu
+++ b/aten/src/THCUNN/BatchNormalization.cu
@@ -6,15 +6,16 @@
 #include "THCDeviceTensor.cuh"
 #include "THCDeviceTensorUtils.cuh"
 #include "THCDeviceUtils.cuh"
-const int WARP_SIZE = 32;
+
+const int WARP_SIZE = 64;
 
 // The maximum number of threads in a block
-const int MAX_BLOCK_SIZE = 512;
+const int MAX_BLOCK_SIZE = 256;
 
 // Number of threads in a block given an input size up to MAX_BLOCK_SIZE
 static int getNumThreads(int nElem) {
-  int threadSizes[5] = { 32, 64, 128, 256, MAX_BLOCK_SIZE };
-  for (int i = 0; i != 5; ++i) {
+  int threadSizes[3] = { 64, 128, MAX_BLOCK_SIZE };
+  for (int i = 0; i != 3; ++i) {
     if (nElem <= threadSizes[i]) {
       return threadSizes[i];
     }
@@ -67,7 +68,7 @@ struct GradOp {
     : mean(m), input(i), gradOutput(g) {}
   __device__ __forceinline__ Float2<Dtype, Acctype> operator()(int batch, int plane, int n) {
     Dtype g = gradOutput[batch][plane][n];
-    Dtype c = ScalarConvert<Acctype, Dtype>::to(input[batch][plane][n] - mean);
+    Dtype c = ScalarConvert<Acctype, Dtype>::to((input[batch][plane][n]).template as<Acctype>() - mean);
     return Float2<Dtype, Acctype>(g, g * c);
   }
   const Acctype mean;
@@ -196,11 +197,12 @@ __global__ void BatchNormalizationUpdateOutput_kernel(
     Acctype unbiasedVar = varN / (N - 1);
     saveMean[plane] = ScalarConvert<Acctype, Dtype>::to(mean);
     saveStd[plane] = ScalarConvert<Acctype, Dtype>::to(invStd);
+
     if (runningMean.data() != NULL) {
-      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningMean[plane] + momentum * mean);
+      runningMean[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningMean[plane]).template as<Acctype>() + momentum * mean);
     }
     if (runningVar.data() != NULL) {
-      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * runningVar[plane] + momentum * unbiasedVar);
+      runningVar[plane] = ScalarConvert<Acctype, Dtype>::to((1 - momentum) * (runningVar[plane]).template as<Acctype>() + momentum * unbiasedVar);
     }
   }
 
@@ -240,7 +242,7 @@ __global__ void BatchNormalizationBackward_kernel(
     stdVal = ScalarConvert<Dtype, Acctype>::to(saveStd[plane]);
   } else {
     mean = ScalarConvert<Dtype, Acctype>::to(runningMean[plane]);
-    stdVal = 1 / sqrt(runningVar[plane] + eps);
+    stdVal = 1 / sqrt((runningVar[plane]).template as<Acctype>() + eps);
   }
 
   Acctype weightVal = weight.numElements() > 0 ? ScalarConvert<Dtype, Acctype>::to(weight[plane]) : Acctype(1);
@@ -275,16 +277,15 @@ __global__ void BatchNormalizationBackward_kernel(
 
   if (gradWeight.numElements() > 0) {
     if (threadIdx.x == 0) {
-      gradWeight[plane] += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
+      (gradWeight[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * dotP * stdVal);
     }
   }
 
   if (gradBias.numElements() > 0) {
     if (threadIdx.x == 0) {
-      gradBias[plane] += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
+      (gradBias[plane]).template as<Dtype>() += ScalarConvert<Acctype, Dtype>::to(scale * gradOutputSum);
     }
   }
 }
-
 #include "generic/BatchNormalization.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/aten/src/THCUNN/ClassNLLCriterion.cu b/aten/src/THCUNN/ClassNLLCriterion.cu
index 1043454ff..cedd36021 100644
--- a/aten/src/THCUNN/ClassNLLCriterion.cu
+++ b/aten/src/THCUNN/ClassNLLCriterion.cu
@@ -56,7 +56,7 @@ __global__ void ClassNLLCriterion_updateOutput_no_reduce_kernel(
     assert(cur_target  >= 0 && cur_target  < n_classes);
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
-    output[index] = -weight * input[index][cur_target];
+    output[index] = -weight * input[index][cur_target].template as<Dtype>();
   }
 }
 
@@ -78,7 +78,7 @@ __global__ void ClassNLLCriterion_updateGradInput_no_reduce_kernel(
     assert(cur_target  >= 0 && cur_target  < n_classes);
     Dtype weight =
        weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1);
-    gradInput[index][cur_target] = -weight * gradOutput[index];
+    gradInput[index][cur_target] = -weight * gradOutput[index].template as<Dtype>();
   }
 }
 
diff --git a/aten/src/THCUNN/FeatureLPPooling.cu b/aten/src/THCUNN/FeatureLPPooling.cu
index 4ad190fbe..08c277064 100644
--- a/aten/src/THCUNN/FeatureLPPooling.cu
+++ b/aten/src/THCUNN/FeatureLPPooling.cu
@@ -468,6 +468,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -505,6 +506,7 @@ runFeatureLPPoolingUpdateOutput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
@@ -602,6 +604,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_STRIDE_CASE(4, WIDTH);                 \
     }
 
+#if !defined(__HIP_PLATFORM_HCC__)
   if (power == 2.0f) {
     switch (width) {
       L2_WIDTH_CASE(2);
@@ -639,6 +642,7 @@ runFeatureLPPoolingUpdateGradInput(THCState* state,
       LP_WIDTH_CASE(16);
     }
   }
+#endif
 
   // Otherwise, we have an unhandled width and/or stride.
   return false;
diff --git a/aten/src/THCUNN/IndexLinear.cu b/aten/src/THCUNN/IndexLinear.cu
index 2729f9277..98ac09ece 100644
--- a/aten/src/THCUNN/IndexLinear.cu
+++ b/aten/src/THCUNN/IndexLinear.cu
@@ -22,7 +22,11 @@ __device__ double atomicExch(double *address, double val) {
 }
 
 template<typename Ty, bool train>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateOutput(
     Ty *output,
     Ty *normalizedValues,
@@ -141,7 +145,11 @@ void updateOutput(
 // to generate gradWeight of size [keysSize x outDim]
 // nth block along y dimension computes on the non zero elements from the nth batch.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradWeight(
     Ty *gradWeight,
     const Ty *gradOutput,
@@ -213,7 +221,11 @@ void accGradWeight(
 // The gradBias is just a reduction of gradOutput along the batches.
 // There is only one block along y dimension performing the reduction.
 template<typename Ty, bool update>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accGradBias(
     Ty *buffer,
     const Ty *gradOutput,
@@ -267,7 +279,11 @@ void accGradBias(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void updateWeight(
     Ty *weight,
     const Ty *gradWeight,
@@ -336,7 +352,11 @@ void updateWeight(
 // This kernel is launched batchSize number of times.
 // At each step in the iteration, the weights are updated in place in a sparse manner.
 template<typename Ty>
+#if defined(__HIP_PLATFORM_HCC__)
+__global__ 
+#else
 __global__ static
+#endif
 void accUpdateWeight(
     Ty *weight,
     const int64_t weightStride,
diff --git a/aten/src/THCUNN/LogSigmoid.cu b/aten/src/THCUNN/LogSigmoid.cu
index 8facb5bf4..357b7bf95 100644
--- a/aten/src/THCUNN/LogSigmoid.cu
+++ b/aten/src/THCUNN/LogSigmoid.cu
@@ -3,7 +3,7 @@
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
 
-#ifdef _MSC_VER
+#if defined(_MSC_VER) || defined(__HIP_PLATFORM_HCC__)
 #define ZERO_MACRO zero<T>()
 template <typename T>
 inline __device__ typename std::enable_if<std::is_same<T, double>::value, T>::type zero() {
diff --git a/aten/src/THCUNN/LookupTable.cu b/aten/src/THCUNN/LookupTable.cu
index 854230913..d4ed68e43 100644
--- a/aten/src/THCUNN/LookupTable.cu
+++ b/aten/src/THCUNN/LookupTable.cu
@@ -7,6 +7,7 @@
 #include "THCTensorSort.cuh"
 #include "../THC/THCTensorMathReduce.cuh"
 
+#if defined(__NVCC__)
 const int WARP_SIZE = 32;
 
 __device__ __forceinline__ bool warpHasCollision(int val)
@@ -184,14 +185,15 @@ struct FastPow<DType, AccType, 2>
     return xA * xA;
   }
 };
+#endif
 
 /* Calculate norms of the rows of weight_ptr given by idx_ptr and capture them in norms */
 template <typename DType, typename AccType, typename IndexType, int Norm>
 __global__
-void calculate_norms_and_renorm(DType *weights, 
-                                THCIndex_t *indices, 
+void calculate_norms_and_renorm(DType *weights,
+                                THCIndex_t *indices,
                                 AccType normType,
-                                AccType maxNorm, 
+                                AccType maxNorm,
                                 IndexType dim)
 {
   // Some casting hacks since dynamic shared memory and templates don't work together:
@@ -211,7 +213,7 @@ void calculate_norms_and_renorm(DType *weights,
         (sdata, blockDim.x, v, ReduceAdd<AccType, AccType>(), accZero);
 
   if (tid == 0) {
-    sdata[0] = std::pow(v, 
+    sdata[0] = std::pow(v,
         THCNumerics<AccType>::div(ScalarConvert<int, AccType>::to(1), normType)
     );
   }
diff --git a/aten/src/THCUNN/RReLU.cu b/aten/src/THCUNN/RReLU.cu
index bf4503515..4af16463a 100644
--- a/aten/src/THCUNN/RReLU.cu
+++ b/aten/src/THCUNN/RReLU.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 #include <THC/THCApply.cuh>
+#if defined(__NVCC__)
 #include "common.h"
 #include <curand.h>
 #include <curand_kernel.h>
@@ -119,6 +120,7 @@ struct RReLUupdateGradInputEvalIP_functor
     }
   }
 };
+#endif
 
 #include "generic/RReLU.cu"
 #include "THCGenerateFloatTypes.h"
diff --git a/aten/src/THCUNN/SmoothL1Criterion.cu b/aten/src/THCUNN/SmoothL1Criterion.cu
index 8e35599ba..8b32c9246 100644
--- a/aten/src/THCUNN/SmoothL1Criterion.cu
+++ b/aten/src/THCUNN/SmoothL1Criterion.cu
@@ -70,6 +70,7 @@ struct smoothl1_updateGradInput_functor
   const Dtype norm;
   const Dtype gradOutput;
 
+  __host__ __device__
   smoothl1_updateGradInput_functor(Dtype norm_, Dtype gradOutput_)
     : norm(norm_), gradOutput(gradOutput_)
   {}
diff --git a/aten/src/THCUNN/SoftMaxCommon.cuh b/aten/src/THCUNN/SoftMaxCommon.cuh
index 692a08079..f96fee941 100644
--- a/aten/src/THCUNN/SoftMaxCommon.cuh
+++ b/aten/src/THCUNN/SoftMaxCommon.cuh
@@ -50,9 +50,13 @@ void SpatialSoftMax_getLaunchSizes(
   block = SpatialSoftMax_getBlockSize(outer_size, dim_size, inner_size);
   uint32_t block_threads = block.x * block.y;
   smem_size = block.x == 1 ? 0 : block_threads * sizeof(AccumT);
+#if defined(__HIP_PLATFORM_HCC__)
+  int max_active_blocks = 8;
+#else
   int max_active_blocks;
   cudaOccupancyMaxActiveBlocksPerMultiprocessor(&max_active_blocks,
                                                 k, block_threads, smem_size);
+#endif
   max_active_blocks *= THCState_getCurrentDeviceProperties(state)->multiProcessorCount;
   grid = SpatialSoftMax_getGridSize(block, max_active_blocks, outer_size, dim_size, inner_size);
 }
diff --git a/aten/src/THCUNN/SparseLinear.cu b/aten/src/THCUNN/SparseLinear.cu
index 9110bbcac..596c970f7 100644
--- a/aten/src/THCUNN/SparseLinear.cu
+++ b/aten/src/THCUNN/SparseLinear.cu
@@ -2,6 +2,7 @@
 #include "THCHalf.h"
 #include "THCHalfAutoNumerics.cuh"
 
+#if defined(__NVCC__)
 #include <cusparse.h>
 
 static cusparseHandle_t cusparse_handle = 0;
@@ -14,6 +15,7 @@ static void init_cusparse() {
     }
   }
 }
+#endif
 
 #ifdef CUDA_HALF_TENSOR
 void THNN_CudaHalfSparseLinear_updateOutput(
diff --git a/aten/src/THCUNN/SpatialClassNLLCriterion.cu b/aten/src/THCUNN/SpatialClassNLLCriterion.cu
index 83addd09a..80cdfb753 100644
--- a/aten/src/THCUNN/SpatialClassNLLCriterion.cu
+++ b/aten/src/THCUNN/SpatialClassNLLCriterion.cu
@@ -62,7 +62,7 @@ __global__ void SpatialClassNLLCriterion_updateGradInput_no_reduce_kernel(
     }
     Dtype value =
         -(weights ? weights[cur_target] : ScalarConvert<int, Dtype>::to(1));
-    gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w];
+    gradInput[b][cur_target][h][w] = value * gradOutput[b][h][w].template as<Dtype>();
   }
 }
 
diff --git a/aten/src/THCUNN/SpatialGridSamplerBilinear.cu b/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
index 3bbc10557..2e4b80864 100644
--- a/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
+++ b/aten/src/THCUNN/SpatialGridSamplerBilinear.cu
@@ -88,16 +88,16 @@ __global__ void SpatialGridSamplerBilinear_updateOutput_kernel(
     for (c = 0; c < C; ++c) {
       out_val = ScalarConvert<int,Dtype>::to(0);
       if (WITHIN_BOUNDS(ix_nw, iy_nw, IH, IW)) {
-        out_val += input[n][c][iy_nw][ix_nw] * nw;
+        out_val += (input[n][c][iy_nw][ix_nw]).template as<Dtype>() * nw;
       }
       if (WITHIN_BOUNDS(ix_ne, iy_ne, IH, IW)) {
-        out_val += input[n][c][iy_ne][ix_ne] * ne;
+        out_val += (input[n][c][iy_ne][ix_ne]).template as<Dtype>() * ne;
       }
       if (WITHIN_BOUNDS(ix_sw, iy_sw, IH, IW)) {
-        out_val += input[n][c][iy_sw][ix_sw] * sw;
+        out_val += (input[n][c][iy_sw][ix_sw]).template as<Dtype>() * sw;
       }
       if (WITHIN_BOUNDS(ix_se, iy_se, IH, IW)) {
-        out_val += input[n][c][iy_se][ix_se] * se;
+        out_val += (input[n][c][iy_se][ix_se]).template as<Dtype>() * se;
       }
       output[n][c][h][w] = out_val;
     }
diff --git a/aten/src/THCUNN/SpatialUpSamplingBilinear.cu b/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
index 11f37b465..71fb09b37 100644
--- a/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
+++ b/aten/src/THCUNN/SpatialUpSamplingBilinear.cu
@@ -52,10 +52,10 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = h0lambda * (w0lambda * data1[n][c][h1][w1]
-                            + w1lambda * data1[n][c][h1][w1+w1p])
-                            + h1lambda * (w0lambda * data1[n][c][h1+h1p][w1]
-                            + w1lambda * data1[n][c][h1+h1p][w1+w1p]);
+        const Acctype val = h0lambda * (w0lambda * (data1[n][c][h1][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1][w1+w1p]).template as<Acctype>())
+                            + h1lambda * (w0lambda * (data1[n][c][h1+h1p][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][h1+h1p][w1+w1p]).template as<Acctype>());
         data2[n][c][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -84,7 +84,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][h1][w1];
-          data1[n][c][h2][w2] += val;
+          (data1[n][c][h2][w2]).template as<Dtype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/THCHalfAutoNumerics.cuh b/aten/src/THCUNN/THCHalfAutoNumerics.cuh
index 2653fed0b..532242996 100644
--- a/aten/src/THCUNN/THCHalfAutoNumerics.cuh
+++ b/aten/src/THCUNN/THCHalfAutoNumerics.cuh
@@ -31,6 +31,8 @@ inline __host__ __device__ double fmaxType(double x, double y) {
 
 // arithmetic functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ half operator+(half a, half b) {
   return THCNumerics<half>::add(a, b);
 }
@@ -159,6 +161,7 @@ inline __host__ __device__ half& operator/=(half &lhs, const half &rhs) {
   lhs = lhs / rhs;
   return lhs;
 }
+#endif
 
 inline __host__ __device__ half abs(half a) {
   return THCNumerics<half>::abs(a);
@@ -212,6 +215,8 @@ inline __host__ __device__ half operator*(half a, bool b) {
 
 // comparison functions
 
+#if defined(__HIP_PLATFORM_HCC__)
+#else
 inline __host__ __device__ bool operator<(half a, half b) {
   return THCNumerics<half>::lt(a, b);
 }
@@ -243,6 +248,7 @@ inline __host__ __device__ bool operator>=(half a, half b) {
 inline __host__ __device__ bool operator>=(half a, int b) {
   return THCNumerics<half>::ge(a, ScalarConvert<int ,half>::to(b));
 }
+#endif
 
 #endif
 #endif
diff --git a/aten/src/THCUNN/TemporalUpSamplingLinear.cu b/aten/src/THCUNN/TemporalUpSamplingLinear.cu
index 98e4f2833..69ddb32fd 100644
--- a/aten/src/THCUNN/TemporalUpSamplingLinear.cu
+++ b/aten/src/THCUNN/TemporalUpSamplingLinear.cu
@@ -42,8 +42,8 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = w0lambda * data1[n][c][w1]
-                            + w1lambda * data1[n][c][w1+w1p];
+        const Acctype val = w0lambda * (data1[n][c][w1]).template as<Acctype>()
+                            + w1lambda * (data1[n][c][w1+w1p]).template as<Acctype>();
         data2[n][c][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -68,7 +68,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][w1];
-          data1[n][c][w2] += val;
+          (data1[n][c][w2]).template as<Dtype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu b/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
index 6fa8ae1fd..4b9e95939 100644
--- a/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
+++ b/aten/src/THCUNN/VolumetricGridSamplerBilinear.cu
@@ -66,7 +66,7 @@ __global__ void VolumetricGridSamplerBilinear_updateOutput_kernel(
     int ix_tnw = floor(ScalarConvert<Dtype,float>::to(ix));
     int iy_tnw = floor(ScalarConvert<Dtype,float>::to(iy));
     int iz_tnw = floor(ScalarConvert<Dtype,float>::to(iz));
-    
+
     int ix_tne = ix_tnw + 1;
     int iy_tne = iy_tnw;
     int iz_tne = iz_tnw;
@@ -138,28 +138,28 @@ __global__ void VolumetricGridSamplerBilinear_updateOutput_kernel(
     for (c = 0; c < C; ++c) {
       out_val = ScalarConvert<int,Dtype>::to(0);
       if (WITHIN_BOUNDS(ix_tnw, iy_tnw, iz_tnw, ID, IH, IW)) {
-        out_val += input[n][c][iz_tnw][iy_tnw][ix_tnw] * tnw;
+        out_val += (input[n][c][iz_tnw][iy_tnw][ix_tnw]).template as<Dtype>() * tnw;
       }
       if (WITHIN_BOUNDS(ix_tne, iy_tne, iz_tne, ID, IH, IW)) {
-        out_val += input[n][c][iz_tne][iy_tne][ix_tne] * tne;
+        out_val += (input[n][c][iz_tne][iy_tne][ix_tne]).template as<Dtype>() * tne;
       }
       if (WITHIN_BOUNDS(ix_tsw, iy_tsw, iz_tsw, ID, IH, IW)) {
-        out_val += input[n][c][iz_tsw][iy_tsw][ix_tsw] * tsw;
+        out_val += (input[n][c][iz_tsw][iy_tsw][ix_tsw]).template as<Dtype>() * tsw;
       }
       if (WITHIN_BOUNDS(ix_tse, iy_tse, iz_tse, ID, IH, IW)) {
-        out_val += input[n][c][iz_tse][iy_tse][ix_tse] * tse;
+        out_val += (input[n][c][iz_tse][iy_tse][ix_tse]).template as<Dtype>() * tse;
       }
       if (WITHIN_BOUNDS(ix_bnw, iy_bnw, iz_bnw, ID, IH, IW)) {
-        out_val += input[n][c][iz_bnw][iy_bnw][ix_bnw] * bnw;
+        out_val += (input[n][c][iz_bnw][iy_bnw][ix_bnw]).template as<Dtype>() * bnw;
       }
       if (WITHIN_BOUNDS(ix_bne, iy_bne, iz_bne, ID, IH, IW)) {
-        out_val += input[n][c][iz_bne][iy_bne][ix_bne] * bne;
+        out_val += (input[n][c][iz_bne][iy_bne][ix_bne]).template as<Dtype>() * bne;
       }
       if (WITHIN_BOUNDS(ix_bsw, iy_bsw, iz_bsw, ID, IH, IW)) {
-        out_val += input[n][c][iz_bsw][iy_bsw][ix_bsw] * bsw;
+        out_val += (input[n][c][iz_bsw][iy_bsw][ix_bsw]).template as<Dtype>() * bsw;
       }
       if (WITHIN_BOUNDS(ix_bse, iy_bse, iz_bse, ID, IH, IW)) {
-        out_val += input[n][c][iz_bse][iy_bse][ix_bse] * bse;
+        out_val += (input[n][c][iz_bse][iy_bse][ix_bse]).template as<Dtype>() * bse;
       }
       output[n][c][d][h][w] = out_val;
     }
@@ -211,7 +211,7 @@ __global__ void VolumetricGridSamplerBilinear_updateGradInput_kernel(
     int ix_tnw = floor(ScalarConvert<Dtype,float>::to(ix));
     int iy_tnw = floor(ScalarConvert<Dtype,float>::to(iy));
     int iz_tnw = floor(ScalarConvert<Dtype,float>::to(iz));
-    
+
     int ix_tne = ix_tnw + 1;
     int iy_tne = iy_tnw;
     int iz_tne = iz_tnw;
@@ -259,7 +259,7 @@ __global__ void VolumetricGridSamplerBilinear_updateGradInput_kernel(
     Dtype bne_val;
     Dtype bsw_val;
     Dtype bse_val;
-    
+
     int ix_tnw_cl, iy_tnw_cl, iz_tnw_cl, ix_tne_cl, iy_tne_cl, iz_tne_cl;
     int ix_tsw_cl, iy_tsw_cl, iz_tsw_cl, ix_tse_cl, iy_tse_cl, iz_tse_cl;
     int ix_bnw_cl, iy_bnw_cl, iz_bnw_cl, ix_bne_cl, iy_bne_cl, iz_bne_cl;
diff --git a/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu b/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
index 03d506ac5..b99423bbc 100644
--- a/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
+++ b/aten/src/THCUNN/VolumetricUpSamplingTrilinear.cu
@@ -63,14 +63,14 @@ __global__ void caffe_gpu_interp2_kernel(const int n,
     //
     for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
-        const Acctype val = t0lambda * (h0lambda * (w0lambda * data1[n][c][t1][h1][w1]
-                                                  + w1lambda * data1[n][c][t1][h1][w1+w1p])
-                                      + h1lambda * (w0lambda * data1[n][c][t1][h1+h1p][w1]
-                                                  + w1lambda * data1[n][c][t1][h1+h1p][w1+w1p]))
-                          + t1lambda * (h0lambda * (w0lambda * data1[n][c][t1+t1p][h1][w1]
-                                                  + w1lambda * data1[n][c][t1+t1p][h1][w1+w1p])
-                                      + h1lambda * (w0lambda * data1[n][c][t1+t1p][h1+h1p][w1]
-                                                  + w1lambda * data1[n][c][t1+t1p][h1+h1p][w1+w1p]));
+        const Acctype val = t0lambda * (h0lambda * (w0lambda * (data1[n][c][t1][h1][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1][h1+h1p][w1+w1p]).template as<Acctype>()))
+                          + t1lambda * (h0lambda * (w0lambda * (data1[n][c][t1+t1p][h1][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1][w1+w1p]).template as<Acctype>())
+                                      + h1lambda * (w0lambda * (data1[n][c][t1+t1p][h1+h1p][w1]).template as<Acctype>()
+                                                  + w1lambda * (data1[n][c][t1+t1p][h1+h1p][w1+w1p]).template as<Acctype>()));
         data2[n][c][t2][h2][w2] = ScalarConvert<Acctype, Dtype>::to(val);
       }
     }
@@ -104,7 +104,7 @@ __global__ void caffe_gpu_interp2_kernel_backward(const int n,
       for (int n = 0; n < batchsize ; n++){
         for (int c = 0; c < channels; ++c) {
           const Dtype val = data2[n][c][t1][h1][w1];
-          data1[n][c][t2][h2][w2] += val;
+          (data1[n][c][t2][h2][w2]).template as<Acctype>() += val;
         }
       }
       return;
diff --git a/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu b/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
index 61a1c7046..222ec9b87 100644
--- a/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
+++ b/aten/src/THCUNN/generic/SpatialCrossMapLRN.cu
@@ -80,12 +80,14 @@ void LRNbackward(THCState* state, THCTensor* input, THCTensor* output,
   gradOutput = THCTensor_(newContiguous)(state, gradOutput);
 
   int n_threads = batchSize * imsize_h * imsize_w;
+#if defined(__NVCC__)
   LRNComputeDiff<real, accreal> <<<GET_BLOCKS(n_threads), CUDA_NUM_THREADS, 0, THCState_getCurrentStream(state)>>>(
       n_threads, THCTensor_(data)(state, input), THCTensor_(data)(state, output),
       THCTensor_(data)(state, scale), THCTensor_(data)(state, gradOutput), batchSize, nInputPlane, imsize_h, imsize_w,
       local_size, -beta, ScalarConvert<int, real>::to(2) * alpha * beta / local_size,
       THCTensor_(data)(state, gradInput));
   THCudaCheck(cudaGetLastError());
+#endif
 
   THCTensor_(free)(state, input);
   THCTensor_(free)(state, gradOutput);
diff --git a/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu b/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
index 6eb74bb43..b02aef610 100644
--- a/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
+++ b/aten/src/THCUNN/generic/VolumetricDilatedMaxPooling.cu
@@ -2,9 +2,9 @@
 #define THC_GENERIC_FILE "generic/VolumetricDilatedMaxPooling.cu"
 #else
 
-#define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                         \
-  cuda_VolumetricDilatedMaxPooling_updateOutput<KW><<<grid, block,             \
-    0, THCState_getCurrentStream(state)>>>(                             \
+#define UPDATE_OUTPUT_KERNEL_WIDTH(KW) case KW:                  \
+  cuda_VolumetricDilatedMaxPooling_updateOutput<KW>              \
+    <<<grid, block, 0, THCState_getCurrentStream(state)>>>(      \
     inputData, inputTime, inputHeight, inputWidth, \
     cudaIndices, cudaOutput, kT, kH, dT, dH, dW, padT, padH, padW,\
     dilationT, dilationH, dilationW, offsetZ); \
@@ -234,7 +234,7 @@ void THNN_(VolumetricDilatedMaxPooling_updateOutput)(
   } else {
     THCTensor_(retain)(state, output);
   }
-  
+
   real* inputData = THCTensor_(data)(state, input);
 
   THCDeviceTensor<real, 4> cudaOutput;
diff --git a/caffe2/core/common_gpu.h b/caffe2/core/common_gpu.h
index de2b4f814..32623becc 100644
--- a/caffe2/core/common_gpu.h
+++ b/caffe2/core/common_gpu.h
@@ -294,6 +294,7 @@ class DeviceGuard {
 template <typename T, int N>
 struct SimpleArray {
   T data[N];
+  int size;
 };
 
 }  // namespace caffe2
diff --git a/caffe2/utils/math_gpu.cu b/caffe2/utils/math_gpu.cu
index 1b45108ef..a7c73c352 100644
--- a/caffe2/utils/math_gpu.cu
+++ b/caffe2/utils/math_gpu.cu
@@ -2013,7 +2013,6 @@ void Im2colNd<float, CUDAContext, StorageOrder::NCHW>(
       break;
     case 4:
       IM2COL_ND_KERNEL(4);
-      break;
     case 5:
       IM2COL_ND_KERNEL(5);
       break;
@@ -2418,6 +2417,38 @@ namespace {
 
 constexpr int kCUDATransposeMaxDims = 8;
 
+__device__ int GetXIndex(
+    const int ndim,
+    const int* X_strides,
+    const int* Y_dims,
+    int Y_index) {
+  int X_index = 0;
+#pragma unroll
+  for (int i = ndim - 1; i >= 0 && Y_index > 0; --i) {
+    X_index += (Y_index % Y_dims[i]) * X_strides[i];
+    Y_index /= Y_dims[i];
+  }
+  return X_index;
+}
+
+template <typename T, int D>
+__global__ void TransposeCUDAKernel(
+    const int size,
+    const SimpleArray<int, D> X_strides,
+    const SimpleArray<int, D> Y_dims,
+    const T* X,
+    T* Y) {
+  const int ndim = X_strides.size;
+  CUDA_1D_KERNEL_LOOP(Y_index, size) {
+    const int X_index = GetXIndex(ndim, X_strides.data, Y_dims.data, Y_index);
+#if __CUDA_ARCH__ >= 350
+    Y[Y_index] = __ldg(X + X_index);
+#else
+    Y[Y_index] = X[X_index];
+#endif
+  }
+}
+
 template <typename T, int D>
 void EigenTransposeCUDAImpl(
     const int* X_dims,
@@ -2429,6 +2460,7 @@ void EigenTransposeCUDAImpl(
   Eigen::DSizes<Eigen::DenseIndex, D> X_dims_array;
   Eigen::DSizes<Eigen::DenseIndex, D> Y_dims_array;
   Eigen::array<Eigen::DenseIndex, D> axes_array;
+#pragma unroll
   for (int i = 0; i < D; ++i) {
     X_dims_array[i] = static_cast<Eigen::DenseIndex>(X_dims[i]);
     Y_dims_array[i] = static_cast<Eigen::DenseIndex>(
@@ -2492,41 +2524,19 @@ bool EigenTransposeCUDA(
   return false;
 }
 
-template <typename T, int D>
-__global__ void TransposeCUDAKernel(
-    const int size,
-    const SimpleArray<int, D> X_strides,
-    const SimpleArray<int, D> Y_dims,
-    const T* X,
-    T* Y) {
-  CUDA_1D_KERNEL_LOOP(Y_index, size) {
-    int X_index = 0;
-    int Y_index_val = Y_index;
-#pragma unroll
-    for (int i = D - 1; i >= 0; --i) {
-      X_index += (Y_index_val % Y_dims.data[i]) * X_strides.data[i];
-      Y_index_val /= Y_dims.data[i];
-    }
-#if __CUDA_ARCH__ >= 350
-    Y[Y_index] = __ldg(X + X_index);
-#else
-    Y[Y_index] = X[X_index];
-#endif
-  }
-}
-
 template <int D>
 void ComputeXStride(
+    const int ndim,
     const int* X_dims,
     const int* axes,
     int* X_strides) {
   int buff[D];
   int cur_stride = 1;
-  for (int i = D - 1; i >= 0; --i) {
+  for (int i = ndim - 1; i >= 0; --i) {
     buff[i] = cur_stride;
     cur_stride *= X_dims[i];
   }
-  for (int i = 0; i < D; ++i) {
+  for (int i = 0; i < ndim; ++i) {
     X_strides[i] = buff[axes[i]];
   }
 }
@@ -2534,6 +2544,7 @@ void ComputeXStride(
 template <typename T, int D>
 void TransposeCUDAImpl(
     const int size,
+    const int ndim,
     const int* X_dims,
     const int* Y_dims,
     const int* axes,
@@ -2542,8 +2553,10 @@ void TransposeCUDAImpl(
     CUDAContext* context) {
   SimpleArray<int, D> X_strides_array;
   SimpleArray<int, D> Y_dims_array;
-  ComputeXStride<D>(X_dims, axes, X_strides_array.data);
-  for (int i = 0; i < D; ++i) {
+  X_strides_array.size = ndim;
+  ComputeXStride<D>(ndim, X_dims, axes, X_strides_array.data);
+  Y_dims_array.size = ndim;
+  for (int i = 0; i < ndim; ++i) {
     Y_dims_array.data[i] = Y_dims == nullptr ? X_dims[axes[i]] : Y_dims[i];
   }
   TransposeCUDAKernel<T, D>
@@ -2553,88 +2566,52 @@ void TransposeCUDAImpl(
          context->cuda_stream()>>>(size, X_strides_array, Y_dims_array, X, Y);
 }
 
-template <typename T>
-void TransposeCUDA(
-    const int size,
-    const int ndim,
-    const int* X_dims,
-    const int* Y_dims,
-    const int* axes,
-    const T* X,
-    T* Y,
-    CUDAContext* context) {
-  switch (ndim) {
-    case 1: {
-      TransposeCUDAImpl<T, 1>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 2: {
-      TransposeCUDAImpl<T, 2>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 3: {
-      TransposeCUDAImpl<T, 3>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 4: {
-      TransposeCUDAImpl<T, 4>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 5: {
-      TransposeCUDAImpl<T, 5>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 6: {
-      TransposeCUDAImpl<T, 6>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 7: {
-      TransposeCUDAImpl<T, 7>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    case 8: {
-      TransposeCUDAImpl<T, 8>(size, X_dims, Y_dims, axes, X, Y, context);
-      break;
-    }
-    default: { break; }
-  }
-}
-
 } // namespace
 
-#define CAFFE2_SPECIALIZED_CUDA_TRANSPOSE(T)                             \
-  template <>                                                            \
-  void Transpose<T, CUDAContext>(                                        \
-      const int size,                                                    \
-      const int ndim,                                                    \
-      const int* dims,                                                   \
-      const int* axes,                                                   \
-      const T* X,                                                        \
-      T* Y,                                                              \
-      CUDAContext* context) {                                            \
-    CAFFE_ENFORCE_LE(                                                    \
-        ndim, kCUDATransposeMaxDims, "ndim exceeds compile time max.");  \
-    if (EigenTransposeCUDA(ndim, dims, nullptr, axes, X, Y, context)) {  \
-      return;                                                            \
-    }                                                                    \
-    TransposeCUDA<T>(size, ndim, dims, nullptr, axes, X, Y, context);    \
-  }                                                                      \
-  template <>                                                            \
-  void Transpose<T, CUDAContext>(                                        \
-      const int size,                                                    \
-      const int ndim,                                                    \
-      const int* X_dims,                                                 \
-      const int* Y_dims,                                                 \
-      const int* axes,                                                   \
-      const T* X,                                                        \
-      T* Y,                                                              \
-      CUDAContext* context) {                                            \
-    CAFFE_ENFORCE_LE(                                                    \
-        ndim, kCUDATransposeMaxDims, "ndim exceeds compile time max.");  \
-    if (EigenTransposeCUDA(ndim, X_dims, Y_dims, axes, X, Y, context)) { \
-      return;                                                            \
-    }                                                                    \
-    TransposeCUDA<T>(size, ndim, X_dims, Y_dims, axes, X, Y, context);   \
+#define CAFFE2_SPECIALIZED_CUDA_TRANSPOSE(T)                                   \
+  template <>                                                                  \
+  void Transpose<T, CUDAContext>(                                              \
+      const int size,                                                          \
+      const int ndim,                                                          \
+      const int* dims,                                                         \
+      const int* axes,                                                         \
+      const T* X,                                                              \
+      T* Y,                                                                    \
+      CUDAContext* context) {                                                  \
+    CAFFE_ENFORCE_LE(                                                          \
+        ndim, kCUDATransposeMaxDims, "ndim exceeds compile time max.");        \
+    if (EigenTransposeCUDA(ndim, dims, nullptr, axes, X, Y, context)) {        \
+      return;                                                                  \
+    }                                                                          \
+    if (ndim <= 4) {                                                           \
+      TransposeCUDAImpl<T, 4>(size, ndim, dims, nullptr, axes, X, Y, context); \
+    } else {                                                                   \
+      TransposeCUDAImpl<T, kCUDATransposeMaxDims>(                             \
+          size, ndim, dims, nullptr, axes, X, Y, context);                     \
+    }                                                                          \
+  }                                                                            \
+  template <>                                                                  \
+  void Transpose<T, CUDAContext>(                                              \
+      const int size,                                                          \
+      const int ndim,                                                          \
+      const int* X_dims,                                                       \
+      const int* Y_dims,                                                       \
+      const int* axes,                                                         \
+      const T* X,                                                              \
+      T* Y,                                                                    \
+      CUDAContext* context) {                                                  \
+    CAFFE_ENFORCE_LE(                                                          \
+        ndim, kCUDATransposeMaxDims, "ndim exceeds compile time max.");        \
+    if (EigenTransposeCUDA(ndim, X_dims, Y_dims, axes, X, Y, context)) {       \
+      return;                                                                  \
+    }                                                                          \
+    if (ndim <= 4) {                                                           \
+      TransposeCUDAImpl<T, 4>(                                                 \
+          size, ndim, X_dims, Y_dims, axes, X, Y, context);                    \
+    } else {                                                                   \
+      TransposeCUDAImpl<T, kCUDATransposeMaxDims>(                             \
+          size, ndim, X_dims, Y_dims, axes, X, Y, context);                    \
+    }                                                                          \
   }
 CAFFE2_SPECIALIZED_CUDA_TRANSPOSE(float)
 CAFFE2_SPECIALIZED_CUDA_TRANSPOSE(double)
diff --git a/docs/source/distributed.rst b/docs/source/distributed.rst
index 23846f18b..27decd0f9 100644
--- a/docs/source/distributed.rst
+++ b/docs/source/distributed.rst
@@ -7,35 +7,35 @@ Distributed communication package - torch.distributed
 .. automodule:: torch.distributed
 .. currentmodule:: torch.distributed
 
-Currently torch.distributed supports four backends, each with
+Currently torch.distributed supports three backends, each with
 different capabilities. The table below shows which functions are available
 for use with CPU / CUDA tensors.
 MPI supports cuda only if the implementation used to build PyTorch supports it.
 
 
-+------------+-----------+-----------+-----------+-----------+
-| Backend    | ``tcp``   | ``gloo``  | ``mpi``   | ``nccl``  |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| Device     | CPU | GPU | CPU | GPU | CPU | GPU | CPU | GPU |
-+============+=====+=====+=====+=====+=====+=====+=====+=====+
-| send       |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| recv       |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| broadcast  |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| all_reduce |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| reduce     |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| all_gather |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| gather     |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| scatter    |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
-| barrier    |    |    |    |    |    | ?   |    |    |
-+------------+-----+-----+-----+-----+-----+-----+-----+-----+
++------------+-----------+-----------+-----------+
+| Backend    | ``tcp``   | ``gloo``  | ``mpi``   |
++------------+-----+-----+-----+-----+-----+-----+
+| Device     | CPU | GPU | CPU | GPU | CPU | GPU |
++============+=====+=====+=====+=====+=====+=====+
+| send       |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| recv       |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| broadcast  |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| all_reduce |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| reduce     |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| all_gather |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| gather     |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| scatter    |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
+| barrier    |    |    |    |    |    | ?   |
++------------+-----+-----+-----+-----+-----+-----+
 
 .. _distributed-basics:
 
@@ -173,7 +173,7 @@ as they should never be created manually, but they are guaranteed to support two
 * ``is_completed()`` - returns True if the operation has finished
 * ``wait()`` - will block the process until the operation is finished.
   ``is_completed()`` is guaranteed to return True once it returns.
-
+  
 When using the MPI backend, :func:`~torch.distributed.isend` and :func:`~torch.distributed.irecv`
 support non-overtaking, which has some guarantees on supporting message order. For more detail, see
 http://mpi-forum.org/docs/mpi-2.2/mpi22-report/node54.htm#Node54
@@ -199,76 +199,3 @@ Collective functions
 
 .. autofunction:: barrier
 
-Multi-GPU collective functions
-------------------------------
-
-If you have more than one GPU on each node, when using the NCCL backend,
-:func:`~torch.distributed.broadcast_multigpu`
-:func:`~torch.distributed.all_reduce_multigpu`
-:func:`~torch.distributed.reduce_multigpu` and
-:func:`~torch.distributed.all_gather_multigpu` support distributed collective
-operations among multiple GPUs within each node. These functions can potentially
-improve the overall distributed training performance and be easily used by
-passing a list of tensors. Each Tensor in the passed tensor list needs
-to be on a separate GPU device of the host where the function is called. Note
-that the length of the tensor list needs to be identical among all the
-distributed processes. Also note that currently the multi-GPU collective
-functions are only supported by the NCCL backend.
-
-For example, if the system we use for distributed training has 2 nodes, each
-of which has 8 GPUs. On each of the 16 GPUs, there is a tensor that we would
-like to all-reduce. The following code can serve as a reference:
-
-Code running on Node 0
-
-::
-
-    import torch
-    import torch.distributed as dist
-
-    dist.init_process_group(backend="nccl",
-                            init_method="file:///distributed_test",
-                            world_size=2,
-                            rank=0)
-    tensor_list = []
-    for dev_idx in range(torch.cuda.device_count()):
-        tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))
-
-    dist.all_reduce_multigpu(tensor_list)
-
-Code running on Node 1
-
-::
-
-    import torch
-    import torch.distributed as dist
-
-    dist.init_process_group(backend="nccl",
-                            init_method="file:///distributed_test",
-                            world_size=2,
-                            rank=1)
-    tensor_list = []
-    for dev_idx in range(torch.cuda.device_count()):
-        tensor_list.append(torch.FloatTensor([1]).cuda(dev_idx))
-
-    dist.all_reduce_multigpu(tensor_list)
-
-After the call, all 16 tensors on the two nodes will have the all-reduced value
-of 16
-
-.. autofunction:: broadcast_multigpu
-
-.. autofunction:: all_reduce_multigpu
-
-.. autofunction:: reduce_multigpu
-
-.. autofunction:: all_gather_multigpu
-
-
-Launch utility
---------------
-
-The `torch.distributed` package also provides a launch utility in
-`torch.distributed.launch`.
-
-.. automodule:: torch.distributed.launch
diff --git a/scripts/get_python_cmake_flags.py b/scripts/get_python_cmake_flags.py
index 90badacaf..0220d1e5a 100644
--- a/scripts/get_python_cmake_flags.py
+++ b/scripts/get_python_cmake_flags.py
@@ -8,7 +8,7 @@
 # if your installation is not being properly detected by CMake.
 #
 #   mkdir -p build && cd build
-#   cmake $(python ../scripts/get_python_cmake_flags.py) ..
+#   cmake $(python ../scripts/get_python_libs.py) ..
 #   make
 #
 
diff --git a/setup.py b/setup.py
index 7e68bbb93..ea27d171b 100644
--- a/setup.py
+++ b/setup.py
@@ -126,6 +126,10 @@ IS_WINDOWS = (platform.system() == 'Windows')
 IS_DARWIN = (platform.system() == 'Darwin')
 IS_LINUX = (platform.system() == 'Linux')
 
+# If using ROCM stack disable distributed for now
+if WITH_ROCM:
+  WITH_DISTRIBUTED=False
+
 NUM_JOBS = multiprocessing.cpu_count()
 max_jobs = os.getenv("MAX_JOBS")
 if max_jobs is not None:
@@ -222,6 +226,8 @@ def build_libs(libs):
     if WITH_CUDA:
         my_env["CUDA_BIN_PATH"] = CUDA_HOME
         build_libs_cmd += ['--with-cuda']
+    if WITH_ROCM:
+        build_libs_cmd += ['--with-rocm']
     if WITH_NNPACK:
         build_libs_cmd += ['--with-nnpack']
     if WITH_CUDNN:
@@ -315,6 +321,9 @@ class build_deps(Command):
                        'torch/lib/include/pybind11')
         self.copy_file('torch/torch.h', 'torch/lib/include/torch/torch.h')
 
+        if WITH_ROCM:
+            os.environ["CC"] = 'hipcc'
+            os.environ["CXX"] = 'hipcc'
 
 build_dep_cmds = {}
 
@@ -753,6 +762,41 @@ if WITH_CUDA:
         "torch/csrc/nn/THCUNN.cpp",
     ]
 
+elif WITH_ROCM:
+    rocm_include_path = '/opt/rocm/include'
+    hcc_include_path = '/opt/rocm/hcc/include'
+    hipblas_include_path = '/opt/rocm/hipblas/include'
+    hipsparse_include_path = '/opt/rocm/hcsparse/include'
+    print(rocm_include_path)
+    print(hcc_include_path)
+    print(hipblas_include_path)
+    print(hipsparse_include_path)
+    hip_lib_path = '/opt/rocm/hip/lib'
+    hcc_lib_path = '/opt/rocm/hcc/lib'
+    include_dirs.append(rocm_include_path)
+    include_dirs.append(hcc_include_path)
+    include_dirs.append(hipblas_include_path)
+    include_dirs.append(hipsparse_include_path)
+    include_dirs.append(tmp_install_path + "/include/THCUNN")
+    extra_link_args.append('-L' + hip_lib_path)
+    extra_link_args.append('-Wl,-rpath,' + hip_lib_path)
+    extra_link_args.append('-shared')
+    extra_compile_args += ['-DWITH_ROCM']
+    extra_compile_args += ['-D__HIP_PLATFORM_HCC__']
+
+    os.environ["LDSHARED"] = 'gcc'
+
+    main_sources += [
+        "torch/csrc/cuda/Module.cpp",
+        "torch/csrc/cuda/Storage.cpp",
+        "torch/csrc/cuda/Stream.cpp",
+        "torch/csrc/cuda/utils.cpp",
+        "torch/csrc/cuda/comm.cpp",
+        "torch/csrc/cuda/python_comm.cpp",
+        "torch/csrc/cuda/serialization.cpp",
+        "torch/csrc/nn/THCUNN.cpp",
+    ]
+
 if WITH_NCCL:
     if WITH_SYSTEM_NCCL:
         main_link_args += [NCCL_SYSTEM_LIB]
@@ -822,6 +866,7 @@ if not IS_WINDOWS:
     DL = Extension("torch._dl",
                    sources=["torch/csrc/dl.c"],
                    language='c',
+                   extra_link_args=['-shared']
                    )
     extensions.append(DL)
 
diff --git a/test/run_nn_tests_amd.sh b/test/run_nn_tests_amd.sh
new file mode 100644
index 000000000..5c34bf88d
--- /dev/null
+++ b/test/run_nn_tests_amd.sh
@@ -0,0 +1,175 @@
+python test_nn.py TestNN.test_module_backcompat
+python test_nn.py TestNN.test_hooks
+python test_nn.py TestNN.test_hook_cpp
+python test_nn.py TestNN.test_hook_fail
+python test_nn.py TestNN.test_hook_writeable
+python test_nn.py TestNN.test_zero_grad
+python test_nn.py TestNN.test_volatile
+python test_nn.py TestNN._test_dropout
+python test_nn.py TestNN.test_parameters
+python test_nn.py TestNN.test_named_parameters
+python test_nn.py TestNN.test_children
+python test_nn.py TestNN.test_dir
+python test_nn.py TestNN.test_named_children
+python test_nn.py TestNN.test_modules
+python test_nn.py TestNN.test_named_modules
+python test_nn.py TestNN.test_register_buffer_raises_error_if_attr_exists
+python test_nn.py TestNN.test_register_buffer_allows_overwriting_with_same_name
+python test_nn.py TestNN.test_register_parameter_raises_error_if_attr_exists
+python test_nn.py TestNN.test_register_parameter_allows_overwriting_with_same_name
+python test_nn.py TestNN.test_add_module_raises_error_if_attr_exists
+python test_nn.py TestNN.test_Sequential_getitem
+python test_nn.py TestNN.test_ListModule
+python test_nn.py TestNN.test_ParameterList
+python test_nn.py TestNN.test_add_module
+python test_nn.py TestNN.test_type
+python test_nn.py TestNN.test_non_leaf_parameters
+python test_nn.py TestNN.test_clip_grad_norm
+python test_nn.py TestNN.test_parameters_to_vector
+python test_nn.py TestNN.test_vector_to_parameters
+python test_nn.py TestNN.test_weight_norm
+python test_nn.py TestNN.test_weight_norm_pickle
+python test_nn.py TestNN.test_embedding_padding_idx
+python test_nn.py TestNN.test_embedding_max_norm
+python test_nn.py TestNN.test_embedding_max_norm_cuda
+python test_nn.py TestNN.test_embedding_functional
+python test_nn.py TestNN._test_EmbeddingBag
+python test_nn.py TestNN.test_EmbeddingBag
+python test_nn.py TestNN.test_EmbeddingBag_cuda
+python test_nn.py TestNN.test_Dropout
+python test_nn.py TestNN.test_Dropout2d
+python test_nn.py TestNN.test_Dropout3d
+python test_nn.py TestNN.test_AlphaDropout
+python test_nn.py TestNN._test_InstanceNorm
+python test_nn.py TestNN.test_InstanceNorm2d
+python test_nn.py TestNN.test_InstanceNorm1d
+python test_nn.py TestNN.test_InstanceNorm3d
+python test_nn.py TestNN.test_pad
+python test_nn.py TestNN.test_normalize
+python test_nn.py TestNN._test_maxpool_indices
+python test_nn.py TestNN.test_Conv2d_naive_groups
+python test_nn.py TestNN.test_Conv2d_naive_groups_cuda
+python test_nn.py TestNN.test_batchnorm_eval
+python test_nn.py TestNN.test_batchnorm_eval_cuda
+python test_nn.py TestNN.test_MaxPool1d_indices
+python test_nn.py TestNN.test_MaxPool1d_indices_cuda
+python test_nn.py TestNN.test_MaxPool2d_indices
+python test_nn.py TestNN.test_MaxPool2d_indices_cuda
+python test_nn.py TestNN.test_MaxPool3d_indices
+python test_nn.py TestNN.test_MaxPool3d_indices_cuda
+python test_nn.py TestNN.test_AdaptiveMaxPool1d_indices
+python test_nn.py TestNN.test_AdaptiveMaxPool1d_indices_cuda
+python test_nn.py TestNN.test_AdaptiveMaxPool2d_indices
+python test_nn.py TestNN.test_AdaptiveMaxPool2d_indices_cuda
+python test_nn.py TestNN._test_scatter
+python test_nn.py TestNN.test_scatter_cpu
+python test_nn.py TestNN.test_scatter_gpu
+python test_nn.py TestNN._test_gather
+python test_nn.py TestNN.test_gather_cpu
+python test_nn.py TestNN.test_gather_gpu
+python test_nn.py TestNN._test_broadcast_double_backwards
+python test_nn.py TestNN.test_broadcast_double_backwards_gpu
+python test_nn.py TestNN.test_replicate
+python test_nn.py TestNN.test_replicate_buffers
+python test_nn.py TestNN.test_parallel_apply
+python test_nn.py TestNN.test_data_parallel_multiple_input
+python test_nn.py TestNN.test_data_parallel_small_back
+python test_nn.py TestNN.test_data_parallel
+python test_nn.py TestNN.test_data_parallel_nested_output
+python test_nn.py TestNN.test_data_parallel_nested_input
+python test_nn.py TestNN.test_data_parallel_module
+python test_nn.py TestNN.test_data_parallel_module_kwargs_only
+python test_nn.py TestNN.test_state_dict
+python test_nn.py TestNN.test_load_state_dict
+python test_nn.py TestNN.test_parameter_assignment
+python test_nn.py TestNN.test_assignment
+python test_nn.py TestNN.def test_assignments
+python test_nn.py TestNN.test_Conv2d_inconsistent_types
+python test_nn.py TestNN.test_Conv2d_inconsistent_types_on_GPU_without_cudnn
+python test_nn.py TestNN.test_Conv2d_inconsistent_types_on_GPU_with_cudnn
+python test_nn.py TestNN.test_Conv2d_missing_argument
+python test_nn.py TestNN.test_Conv2d_backward_twice
+python test_nn.py TestNN.test_Conv2d_large_workspace
+python test_nn.py TestNN.test_conv_modules_raise_error_on_incorrect_input_size
+python test_nn.py TestNN.test_ConvTranspose2d_output_size
+python test_nn.py TestNN._test_Conv2d_naive_groups
+python test_nn.py TestNN.test_Conv2d_groups_nobias
+python test_nn.py TestNN.test_MaxUnpool2d_output_size
+python test_nn.py TestNN.test_container_copy
+python test_nn.py TestNN.test_RNN_cell
+python test_nn.py TestNN.test_invalid_dropout_p
+python test_nn.py TestNN.test_pack_padded_sequence
+python test_nn.py TestNN._test_variable_sequence
+python test_nn.py TestNN.test_variable_sequence
+python test_nn.py TestNN.test_variable_sequence_cuda
+python test_nn.py TestNN.test_LSTM_cell
+python test_nn.py TestNN.test_cudnn_weight_format
+python test_nn.py TestNN.test_cuda_rnn_fused
+python test_nn.py TestNN.test_rnn_initial_hidden_state
+python test_nn.py TestNN._test_rnn_retain_variables
+python test_nn.py TestNN.test_rnn_retain_variables
+python test_nn.py TestNN.test_rnn_retain_variables_cuda
+python test_nn.py TestNN._test_RNN_cpu_vs_cudnn
+python test_nn.py TestNN.test_RNN_cpu_vs_cudnn_no_dropout
+python test_nn.py TestNN.test_RNN_cpu_vs_cudnn_with_dropout
+python test_nn.py TestNN.test_RNN_dropout
+python test_nn.py TestNN.test_RNN_dropout_state
+python test_nn.py TestNN.test_RNN_change_dropout
+python test_nn.py TestNN.test_inplace_thnn
+python test_nn.py TestNN.test_noncontig_conv_grad
+python test_nn.py TestNN.test_pixel_shuffle
+python test_nn.py TestNN.test_bce_with_logits_raises_if_target_and_input_are_different_size
+python test_nn.py TestNN.test_bce_with_logits_gives_same_result_as_sigmoid_and_bce_loss
+python test_nn.py TestNN.test_bce_with_logits_has_correct_grad_at_zero
+python test_nn.py TestNN.test_bce_with_logits_broadcasts_weights
+python test_nn.py TestNN.test_bce_loss_broadcasts_weights
+python test_nn.py TestNN.test_batchnorm_raises_error_if_running_mean_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_running_var_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_weight_is_not_same_size_as_input
+python test_nn.py TestNN.test_batchnorm_raises_error_if_bias_is_not_same_size_as_input
+python test_nn.py TestNN._test_batchnorm_eval
+python test_nn.py TestNN.test_pairwise_distance
+python test_nn.py TestNN.test_triplet_margin_loss
+python test_nn.py TestNN.test_triplet_margin_swap_loss
+python test_nn.py TestNN.test_cosine_similarity
+python test_nn.py TestNN.test_grid_sample
+python test_nn.py TestNN.def test_cpu_against_cuda
+python test_nn.py TestNN.test_shape
+python test_nn.py TestNN.test_affine_grid
+python test_nn.py TestNN.test_upsamplingNearest1d
+python test_nn.py TestNN.test_upsamplingLinear1d
+python test_nn.py TestNN.test_upsamplingNearest2d
+python test_nn.py TestNN.test_upsamplingBilinear2d
+python test_nn.py TestNN.test_upsamplingNearest3d
+python test_nn.py TestNN.test_upsamplingTrilinear3d
+python test_nn.py TestNN.test_linear_broadcasting
+python test_nn.py TestNN.test_bilinear
+python test_nn.py TestNN.test_conv_double_backward
+python test_nn.py TestNN.test_conv_double_backward_no_bias
+python test_nn.py TestNN.test_conv_double_backward_groups
+python test_nn.py TestNN.test_conv_double_backward_stride
+python test_nn.py TestNN.test_conv_double_backward_cuda
+python test_nn.py TestNN.test_calculate_gain_linear
+python test_nn.py TestNN.test_calculate_gain_nonlinear
+python test_nn.py TestNN.test_calculate_gain_leaky_relu
+python test_nn.py TestNN.test_calculate_gain_leaky_relu_only_accepts_numbers
+python test_nn.py TestNN.test_calculate_gain_only_accepts_valid_nonlinearities
+python test_nn.py TestNN.test_uniform
+python test_nn.py TestNN.test_normal
+python test_nn.py TestNN.test_constant
+python test_nn.py TestNN.test_eye
+python test_nn.py TestNN.test_eye_only_works_on_2d_inputs
+python test_nn.py TestNN.test_dirac_properties
+python test_nn.py TestNN.test_dirac_identity
+python test_nn.py TestNN.test_dirac_only_works_on_3_4_5d_inputs
+python test_nn.py TestNN.test_xavier_uniform_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_xavier_normal_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_xavier_uniform
+python test_nn.py TestNN.test_xavier_normal
+python test_nn.py TestNN.test_kaiming_uniform_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_kaiming_normal_errors_on_inputs_smaller_than_2d
+python test_nn.py TestNN.test_kaiming_uniform
+python test_nn.py TestNN.test_kaiming_normal
+python test_nn.py TestNN.test_sparse_only_works_on_2d_inputs
+python test_nn.py TestNN.test_sparse_default_std
+python test_nn.py TestNN.test_orthogonal
diff --git a/tools/build_pytorch_libs.sh b/tools/build_pytorch_libs.sh
index cf856421a..cd85e05b2 100755
--- a/tools/build_pytorch_libs.sh
+++ b/tools/build_pytorch_libs.sh
@@ -12,9 +12,13 @@ set -ex
 
 # Options for building only a subset of the libraries
 WITH_CUDA=0
+WITH_ROCM=0
 if [[ "$1" == "--with-cuda" ]]; then
   WITH_CUDA=1
   shift
+elif [[ "$1" == "--with-rocm" ]]; then
+  WITH_ROCM=1
+  shift
 fi
 
 WITH_NNPACK=0
@@ -74,13 +78,13 @@ C_FLAGS="${C_FLAGS} -DOMPI_SKIP_MPICXX=1"
 LDFLAGS="-L\"$INSTALL_DIR/lib\" "
 LD_POSTFIX=".so.1"
 LD_POSTFIX_UNVERSIONED=".so"
-if [[ $(uname) == 'Darwin' ]]; then
-    LDFLAGS="$LDFLAGS -Wl,-rpath,@loader_path"
-    LD_POSTFIX=".1.dylib"
-    LD_POSTFIX_UNVERSIONED=".dylib"
-else
-    LDFLAGS="$LDFLAGS -Wl,-rpath,\$ORIGIN"
-fi
+# if [[ $(uname) == 'Darwin' ]]; then
+#     LDFLAGS="$LDFLAGS -Wl,-rpath,@loader_path"
+#     LD_POSTFIX=".1.dylib"
+#     LD_POSTFIX_UNVERSIONED=".dylib"
+# else
+#     LDFLAGS="$LDFLAGS -Wl,-rpath,\$ORIGIN"
+# fi
 CPP_FLAGS=" -std=c++11 "
 GLOO_FLAGS=""
 THD_FLAGS=""
@@ -179,6 +183,36 @@ function build() {
   fi
 }
 
+function build_rocm_aten() {
+  mkdir -p build
+  pushd build
+  ${CMAKE_VERSION} .. \
+  ${CMAKE_GENERATOR} \
+      -DCMAKE_BUILD_TYPE=$BUILD_TYPE \
+      -DNO_CUDA=$((1-$WITH_CUDA)) \
+      -DNO_NNPACK=$((1-$WITH_NNPACK)) \
+      -DCUDNN_INCLUDE_DIR=$CUDNN_INCLUDE_DIR \
+      -DCUDNN_LIB_DIR=$CUDNN_LIB_DIR \
+      -DCUDNN_LIBRARY=$CUDNN_LIBRARY \
+      -DNO_MKLDNN=$((1-$WITH_MKLDNN)) \
+      -DMKLDNN_INCLUDE_DIR=$MKLDNN_INCLUDE_DIR \
+      -DMKLDNN_LIB_DIR=$MKLDNN_LIB_DIR \
+      -DMKLDNN_LIBRARY=$MKLDNN_LIBRARY \
+      -DATEN_NO_CONTRIB=1 \
+      -DCMAKE_INSTALL_PREFIX="$INSTALL_DIR" \
+      -DCMAKE_EXPORT_COMPILE_COMMANDS=1 \
+      -DCMAKE_C_FLAGS="$USER_CFLAGS" \
+      -DCMAKE_CXX_FLAGS="$USER_CFLAGS" \
+      -DCMAKE_EXE_LINKER_FLAGS="$USER_LDFLAGS" \
+      -DWITH_ROCM=1 \
+      -DCMAKE_SHARED_LINKER_FLAGS="$USER_LDFLAGS"
+      # STOP!!! Are you trying to add a C or CXX flag?  Add it
+      # to aten/CMakeLists.txt, not here.  We need the vanilla
+      # cmake build to work.
+  ${CMAKE_INSTALL} -j"$NUM_JOBS"
+  popd
+}
+
 function build_nccl() {
   mkdir -p build/nccl
   pushd build/nccl
@@ -250,7 +284,11 @@ for arg in "$@"; do
         popd
     elif [[ "$arg" == "ATen" ]]; then
         pushd "$BASE_DIR/aten"
-        build_aten
+        if [[ $WITH_ROCM -eq 1 ]]; then
+          build_rocm_aten
+        else
+          build_aten
+        fi
         popd
     elif [[ "$arg" == "THD" ]]; then
         pushd "$TORCH_LIB_DIR"
diff --git a/torch/csrc/DynamicTypes.cpp b/torch/csrc/DynamicTypes.cpp
index 08702be83..4500e8ec6 100644
--- a/torch/csrc/DynamicTypes.cpp
+++ b/torch/csrc/DynamicTypes.cpp
@@ -10,7 +10,7 @@
 #include <unordered_map>
 #include <sstream>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #include <THCS/THCS.h>
 #endif
diff --git a/torch/csrc/Module.cpp b/torch/csrc/Module.cpp
index 167ca46b9..1183fb478 100644
--- a/torch/csrc/Module.cpp
+++ b/torch/csrc/Module.cpp
@@ -376,7 +376,7 @@ bool THCPByteStorage_init(PyObject *module);
 
 bool THCPStream_init(PyObject *module);
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 PyMethodDef* THCPModule_methods();
 namespace torch { namespace cuda {
 
@@ -388,7 +388,7 @@ void initModule(PyObject *module);
 namespace torch { namespace nn {
 
 void init__THNN(PyObject*);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 void init__THCUNN(PyObject*);
 #endif
 
@@ -435,7 +435,7 @@ static PyObject* initModule() {
   THPUtils_addPyMethodDefs(methods, TorchMethods);
   THPUtils_addPyMethodDefs(methods, DataLoaderMethods);
   THPUtils_addPyMethodDefs(methods, torch::autograd::python_functions());
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THPUtils_addPyMethodDefs(methods, THCPModule_methods());
 #endif
 #ifdef WITH_CUDNN
@@ -471,7 +471,7 @@ static PyObject* initModule() {
   torch::jit::initJITBindings(module);
   torch::autograd::initNNFunctions(module);
   torch::autograd::init_legacy_variable(module);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   torch::cuda::initModule(module);
 #endif
   ASSERT_TRUE(THPDoubleStorage_init(module));
@@ -483,7 +483,7 @@ static PyObject* initModule() {
   ASSERT_TRUE(THPCharStorage_init(module));
   ASSERT_TRUE(THPByteStorage_init(module));
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   // This will only initialise base classes and attach them to library namespace
   // They won't be ready for real usage until importing cuda module, that will
   // complete the process (but it defines Python classes before calling back into
@@ -536,7 +536,7 @@ static PyObject* initModule() {
 #endif
 
   torch::nn::init__THNN(module);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   torch::nn::init__THCUNN(module);
 #endif
 
diff --git a/torch/csrc/THP_API.h b/torch/csrc/THP_API.h
index fbb9ce06c..f0e5ac291 100644
--- a/torch/csrc/THP_API.h
+++ b/torch/csrc/THP_API.h
@@ -6,7 +6,7 @@
     be defined only when compiling the core torch package.
 #endif
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "cuda/THCP.h"
 #include "cuda/undef_macros.h"
 #endif
diff --git a/torch/csrc/allocators.cpp b/torch/csrc/allocators.cpp
index 15e4f8712..fa1d0173f 100644
--- a/torch/csrc/allocators.cpp
+++ b/torch/csrc/allocators.cpp
@@ -59,7 +59,7 @@ THAllocator THStorageWeakRefAllocator = {
   free_wrapper<StorageWeakRefAllocator>,
 };
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 cudaError_t CudaStorageWeakRefAllocator::malloc(void** ptr, size_t size, cudaStream_t stream) {
   THError("CudaStorageWeakRefAllocator: malloc not supported");
   return cudaSuccess;
diff --git a/torch/csrc/allocators.h b/torch/csrc/allocators.h
index 25b2ac986..b6c431427 100644
--- a/torch/csrc/allocators.h
+++ b/torch/csrc/allocators.h
@@ -5,7 +5,7 @@
 #include <memory>
 
 #include <TH/TH.h>
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -41,7 +41,7 @@ public:
   void free(void* ptr);
 };
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 class CudaStorageWeakRefAllocator {
 public:
   CudaStorageWeakRefAllocator(PyObject *wrapped_object, THCDeviceAllocator *alloc, void *ctx) {
@@ -63,6 +63,6 @@ public:
 
 extern THAllocator THObjectPtrAllocator;
 extern THAllocator THStorageWeakRefAllocator;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern THCDeviceAllocator THCStorageWeakRefAllocator;
 #endif
diff --git a/torch/csrc/autograd/engine.cpp b/torch/csrc/autograd/engine.cpp
index 4eb52df3b..af8ee14d5 100644
--- a/torch/csrc/autograd/engine.cpp
+++ b/torch/csrc/autograd/engine.cpp
@@ -22,7 +22,7 @@
 #include <queue>
 #include <TH/TH.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <THC/THC.h>
 #endif
@@ -498,7 +498,7 @@ auto Engine::ready_queue(int device) -> ReadyQueue& {
 
 auto Engine::start_threads() -> void {
   int num_devices = 0;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   // check for case of compiled with CUDA but no available devices
   if (cudaGetDeviceCount(&num_devices) != cudaSuccess) {
     cudaGetLastError();
diff --git a/torch/csrc/cuda/Module.cpp b/torch/csrc/cuda/Module.cpp
index ff8dea1f9..cca6e80c9 100644
--- a/torch/csrc/cuda/Module.cpp
+++ b/torch/csrc/cuda/Module.cpp
@@ -65,7 +65,6 @@ PyObject * THCPModule_getDeviceCount_wrap(PyObject *self)
   END_HANDLE_TH_ERRORS
 }
 
-
 PyObject * THCPModule_getCurrentStream_wrap(PyObject *self)
 {
   HANDLE_TH_ERRORS
@@ -109,7 +108,7 @@ PyObject * THCPModule_getDriverVersion(PyObject *self)
 
 PyObject * THCPModule_getCompiledVersion(PyObject *self)
 {
-  return PyLong_FromLong((long) CUDA_VERSION);
+  return PyLong_FromLong((long) 0);
 }
 
 PyObject * THCPModule_getRNGState(PyObject *_unused)
@@ -299,7 +298,9 @@ static void bindCudaDeviceProperties(PyObject* module) {
     .def_readonly("major", &cudaDeviceProp::major)
     .def_readonly("minor", &cudaDeviceProp::minor)
     .def_readonly("is_multi_gpu_board", &cudaDeviceProp::isMultiGpuBoard)
+#if defined(__NVCC__)
     .def_readonly("is_integrated", &cudaDeviceProp::integrated)
+#endif
     .def_readonly("multi_processor_count", &cudaDeviceProp::multiProcessorCount)
     .def_readonly("total_memory", &cudaDeviceProp::totalGlobalMem)
     .def("__repr__", [](const cudaDeviceProp &prop) {
diff --git a/torch/csrc/distributed/Module.cpp b/torch/csrc/distributed/Module.cpp
index a04fdf1bc..2c8b71d49 100644
--- a/torch/csrc/distributed/Module.cpp
+++ b/torch/csrc/distributed/Module.cpp
@@ -9,7 +9,7 @@
 #include "torch/csrc/PythonTypes.h"
 #include "torch/csrc/autograd/python_variable.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "torch/csrc/cuda/Stream.h"
 #endif
 
@@ -82,7 +82,7 @@ static bool THDPModule_assignStateless(PyObject *self)
 static std::unordered_map<PyObject*, THDReduceOp> obj2reduceop;
 static std::unordered_map<PyObject*, THDGroup> obj2group;
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern THCState* state;
 #endif
 
@@ -109,7 +109,7 @@ PyObject* THDPModule_initProcessGroup(PyObject *_unused, PyObject *args)
     AutoNoGIL nogil;
     THDProcessGroupInit(channel_type, init_method, world_size, group_name, rank);
   }
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THDSetCudaStatePtr(&state);
 #endif
   Py_RETURN_NONE;
@@ -149,14 +149,14 @@ PyObject* THDPModule_initMasterWorker(PyObject *_unused, PyObject *args)
     AutoNoGIL nogil;
     THDMasterWorkerInit(channel_type, init_method, world_size, group_name, rank);
   }
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   THDSetCudaStatePtr(&state);
 #endif
   Py_RETURN_NONE;
   END_HANDLE_TH_ERRORS
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 PyObject* THDPModule_registerStream(PyObject *_unused, PyObject *_stream)
 {
   HANDLE_TH_ERRORS
@@ -183,7 +183,7 @@ PyObject* THDPModule_getNumProcesses(PyObject *_unused)
   END_HANDLE_TH_ERRORS
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 extern PyObject* THCPDoubleTensorClass;
 extern PyObject* THCPFloatTensorClass;
 extern PyObject* THCPHalfTensorClass;
@@ -981,7 +981,7 @@ static struct PyMethodDef _THDPModule_methods[] = {
   {"_dist_destroy_process_group", (PyCFunction)THDPModule_destroyProcessGroup, METH_NOARGS, NULL},
   {"_dist_clear_group_cache", (PyCFunction)THDPModule_clearGroupCache, METH_VARARGS, NULL},
   {"_dist_init_master_worker", (PyCFunction)THDPModule_initMasterWorker, METH_VARARGS, NULL},
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   {"_dist_register_stream", (PyCFunction)THDPModule_registerStream, METH_O, NULL},
 #endif
   {"_dist_get_rank", (PyCFunction)THDPModule_getRank, METH_NOARGS, NULL},
diff --git a/torch/csrc/generic/StorageMethods.cpp b/torch/csrc/generic/StorageMethods.cpp
index 00c8c063c..99b34f792 100644
--- a/torch/csrc/generic/StorageMethods.cpp
+++ b/torch/csrc/generic/StorageMethods.cpp
@@ -1,4 +1,4 @@
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda_runtime.h>
 #endif
 
@@ -29,7 +29,7 @@ static PyObject * THPStorage_(copy_)(PyObject *self, PyObject *args, PyObject *k
 static PyObject * THPStorage_(isPinned)(THPStorage *self)
 {
   HANDLE_TH_ERRORS
-#if defined(WITH_CUDA)
+#if defined(WITH_ROCM)
   cudaPointerAttributes attr;
   cudaError_t err = cudaPointerGetAttributes(&attr, self->cdata->data);
   if (err != cudaSuccess) {
diff --git a/torch/csrc/generic/StorageSharing.cpp b/torch/csrc/generic/StorageSharing.cpp
index 5de9e7e4d..333bc803a 100644
--- a/torch/csrc/generic/StorageSharing.cpp
+++ b/torch/csrc/generic/StorageSharing.cpp
@@ -1,8 +1,14 @@
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <cuda_runtime.h>
 #endif
 
+#if defined(__HIP_PLATFORM_HCC__)
+  #undef PyBytes_AS_STRING(op)
+  #undef PyBytes_GET_SIZE(op)
+  #define PyBytes_AS_STRING(op) (((PyBytesObject *)(op))->ob_sval)
+  #define PyBytes_GET_SIZE(op)  Py_SIZE(op)
+#endif
 
 static PyObject * THPStorage_(sharedDecref)(THPStorage *self)
 {
@@ -253,6 +259,7 @@ static PyObject * THPStorage_(shareCuda)(THPStorage *self)
     THCudaCheck(cudaIpcGetMemHandle(&handle, base_ptr));
 
     _handle = PyBytes_FromStringAndSize((char *)&handle, CUDA_IPC_HANDLE_SIZE);
+
     _offset = PyLong_FromSsize_t((Py_ssize_t)offset);
     size = PyLong_FromSize_t(base_size / sizeof(real));
   }
diff --git a/torch/csrc/nn/type_checks.h b/torch/csrc/nn/type_checks.h
index b0463c309..a975342ef 100644
--- a/torch/csrc/nn/type_checks.h
+++ b/torch/csrc/nn/type_checks.h
@@ -68,7 +68,7 @@ static inline THIntTensor* THNN_IntTensor_Unpack(PyObject* obj) {
   return torch::nn::unpack<THIntTensor>(obj);
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 
 static inline bool THNN_CudaHalfTensor_Check(PyObject* obj) {
   return torch::nn::check_type(obj, at::TypeID::CUDAHalf);
@@ -102,4 +102,4 @@ static inline THCudaLongTensor* THNN_CudaLongTensor_Unpack(PyObject* obj) {
   return torch::nn::unpack<THCudaLongTensor>(obj);
 }
 
-#endif  // WITH_CUDA
+#endif  // WITH_ROCM
diff --git a/torch/csrc/utils.cpp b/torch/csrc/utils.cpp
index 48d04ac14..3517ecab1 100644
--- a/torch/csrc/utils.cpp
+++ b/torch/csrc/utils.cpp
@@ -17,7 +17,7 @@
 #include "generic/utils.cpp"
 #include <TH/THGenerateHalfType.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "torch/csrc/cuda/THCP.h"
 #endif
 
@@ -232,7 +232,7 @@ bool maybeThrowBackCompatKeepdimWarn(char *func) {
   return true;
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 std::vector <THCStream*> THPUtils_PySequence_to_THCStreamList(PyObject *obj) {
   if (!PySequence_Check(obj)) {
     throw std::runtime_error("Expected a sequence in THPUtils_PySequence_to_THCStreamList");
diff --git a/torch/csrc/utils.h b/torch/csrc/utils.h
index 5756954bc..b3d3b0cd6 100644
--- a/torch/csrc/utils.h
+++ b/torch/csrc/utils.h
@@ -9,7 +9,7 @@
 #include "torch/csrc/utils/python_numbers.h"
 #include "torch/csrc/utils/python_compat.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -177,7 +177,7 @@ void setBackCompatKeepdimWarn(bool warn);
 bool getBackCompatKeepdimWarn();
 bool maybeThrowBackCompatKeepdimWarn(char *func);
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 std::vector <THCStream*> THPUtils_PySequence_to_THCStreamList(PyObject *obj);
 #endif
 
diff --git a/torch/csrc/utils/auto_gpu.h b/torch/csrc/utils/auto_gpu.h
index 093709745..fa53c2f1b 100644
--- a/torch/csrc/utils/auto_gpu.h
+++ b/torch/csrc/utils/auto_gpu.h
@@ -7,7 +7,7 @@
 
 #include <ATen/ATen.h>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <cuda_runtime.h>
 #endif
@@ -29,7 +29,7 @@ struct AutoGPU {
   }
 
   ~AutoGPU() {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     if (original_device != -1) {
       cudaSetDevice(original_device);
     }
@@ -37,7 +37,7 @@ struct AutoGPU {
   }
 
   inline void setDevice(int device) {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     if (device == -1) {
       return;
     }
@@ -55,7 +55,7 @@ struct AutoGPU {
   int original_device = -1;
 
 private:
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   static void cudaCheck(cudaError_t err) {
     if (err != cudaSuccess) {
       std::string msg = "CUDA error (";
diff --git a/torch/csrc/utils/auto_stream.h b/torch/csrc/utils/auto_stream.h
index b18f7edcb..616313d20 100644
--- a/torch/csrc/utils/auto_stream.h
+++ b/torch/csrc/utils/auto_stream.h
@@ -2,13 +2,13 @@
 
 // RAII structs to set CUDA stream
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 extern THCState* state;
 #endif
 
 struct AutoStream {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   explicit AutoStream(THCStream* stream)
     : original_stream(THCState_getStream(state))
   {
diff --git a/torch/csrc/utils/cuda_enabled.h b/torch/csrc/utils/cuda_enabled.h
index 5ec2aba3b..880b74c08 100644
--- a/torch/csrc/utils/cuda_enabled.h
+++ b/torch/csrc/utils/cuda_enabled.h
@@ -4,7 +4,7 @@ namespace torch {
 namespace utils {
 
 static inline bool cuda_enabled() {
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
   return true;
 #else
   return false;
diff --git a/torch/csrc/utils/python_strings.h b/torch/csrc/utils/python_strings.h
index 36dce84e0..55737944d 100644
--- a/torch/csrc/utils/python_strings.h
+++ b/torch/csrc/utils/python_strings.h
@@ -5,6 +5,13 @@
 #include <string>
 #include "object_ptr.h"
 
+#if defined(__HIP_PLATFORM_HCC__)
+  #undef PyBytes_AS_STRING(op)
+  #undef PyBytes_GET_SIZE(op)
+  #define PyBytes_AS_STRING(op) (((PyBytesObject *)(op))->ob_sval)
+  #define PyBytes_GET_SIZE(op)  Py_SIZE(op)
+#endif
+
 // Utilities for handling Python strings. Note that PyString, when defined, is
 // the same as PyBytes.
 
diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py
index f52ab04f1..4e3f63c4b 100644
--- a/torch/cuda/__init__.py
+++ b/torch/cuda/__init__.py
@@ -123,7 +123,7 @@ def _lazy_call(callable):
         # Don't store the actual traceback to avoid memory cycle
         _queued_calls.append((callable, traceback.format_stack()))
 
-_lazy_call(_check_capability)
+#_lazy_call(_check_capability)
 
 
 class DeferredCudaCallError(Exception):
@@ -159,9 +159,9 @@ def _lazy_init():
             "Cannot re-initialize CUDA in forked subprocess. " + msg)
     _check_driver()
     torch._C._cuda_init()
-    _cudart = _load_cudart()
-    _cudart.cudaGetErrorName.restype = ctypes.c_char_p
-    _cudart.cudaGetErrorString.restype = ctypes.c_char_p
+    # _cudart = _load_cudart()
+    #_cudart.cudaGetErrorName.restype = ctypes.c_char_p
+    #_cudart.cudaGetErrorString.restype = ctypes.c_char_p
     _original_pid = os.getpid()
     _initialized = True
     # Important to do this after _initialized, since some queued calls
diff --git a/torch/distributed/__init__.py b/torch/distributed/__init__.py
index f8b26b121..b3c55cc44 100644
--- a/torch/distributed/__init__.py
+++ b/torch/distributed/__init__.py
@@ -244,11 +244,12 @@ def broadcast_multigpu(tensor_list, src, group=group.WORLD):
     Arguments:
         tensor_list (List[Tensor]): Tensors that participate in the collective
             operation. if ``src`` is the rank, then the first element of
-            ``tensor_list`` (``tensor_list[0]``) will be broadcasted to all
-            other tensors (on different GPUs) in the src process and all tensors
-            in ``tensor_list`` of other non-src processes. You also need to make
-            sure that ``len(tensor_list)`` is the same for all the distributed
-            processes calling this function.
+            tensor_list (tensor_list[0]) will be broadcasted to all other
+            tensors (on different GPUs) in the src process and all tensors in
+            tensor_list of other non-src processes.
+
+            You also need to make sure that len(tensor_list) is the same for
+            all the distributed processes calling this function.
 
         src (int): Source rank.
         group (optional): Group of the collective.
@@ -279,11 +280,11 @@ def broadcast(tensor, src, group=group.WORLD):
 def all_reduce_multigpu(tensor_list, op=reduce_op.SUM, group=group.WORLD):
     """Reduces the tensor data across all machines in such a way that all get
     the final result. This function reduces a number of tensors on every node,
-    while each tensor resides on different GPUs.
+    while each tensor resides on different GPUs
     Therefore, the input tensor in the tensor list needs to be GPU tensors.
     Also, each tensor in the tensor list needs to reside on a different GPU.
 
-    After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise
+    After the call, all ``tensor``s in the tensor list  is going to be bitwise
     identical in all processes.
 
     Only nccl backend is currently supported
@@ -293,7 +294,8 @@ def all_reduce_multigpu(tensor_list, op=reduce_op.SUM, group=group.WORLD):
         tensor list (List[Tensor]): List of input and output tensors of
             the collective. The function operates in-place and requires that
             each tensor to be a GPU tensor on different GPUs.
-            You also need to make sure that ``len(tensor_list)`` is the same for
+
+            You also need to make sure that len(tensor_list) is the same for
             all the distributed processes calling this function.
 
         op (optional): One of the values from ``torch.distributed.reduce_op``
@@ -326,9 +328,9 @@ def all_reduce(tensor, op=reduce_op.SUM, group=group.WORLD):
 
 def reduce_multigpu(tensor_list, dst, op=reduce_op.SUM, group=group.WORLD):
     """Reduces the tensor data on multiple GPUs across all machines. Each tensor
-    in ``tensor_list`` should reside on a separate GPU
+    in tensor_list should reside on a separate GPU
 
-    Only the GPU of ``tensor_list[0]`` on the process with rank ``dst`` is
+    Only the GPU of tensor_list[0] on the process with rank ``dst`` is
     going to receive the final result.
 
     Only nccl backend is currently supported
@@ -336,8 +338,9 @@ def reduce_multigpu(tensor_list, dst, op=reduce_op.SUM, group=group.WORLD):
 
     Arguments:
         tensor_list (List[Tensor]): Input and output GPU tensors of the
-            collective. The function operates in-place.
-            You also need to make sure that ``len(tensor_list)`` is the same for
+            collective . The function operates in-place.
+
+            You also need to make sure that len(tensor_list) is the same for
             all the distributed processes calling this function.
 
         dst (int): Destination rank
@@ -373,7 +376,7 @@ def all_gather_multigpu(output_tensor_lists,
                         input_tensor_list,
                         group=group.WORLD):
     """Gathers tensors from the whole group in a list.
-    Each tensor in ``tensor_list`` should reside on a separate GPU
+    Each tensor in tensor_list should reside on a separate GPU
 
     Only nccl backend is currently supported
     tensors should only be GPU tensors
@@ -382,22 +385,25 @@ def all_gather_multigpu(output_tensor_lists,
         output_tensor_lists (List[List[Tensor]]): Output lists. It should
             contain correctly-sized tensors on each GPU to be used for output of
             the collective.
-            e.g. ``output_tensor_lists[i]`` contains the all_gather
-            result that resides on the GPU of ``input_tensor_list[i]``.
-            Note that each element of ``output_tensor_lists[i]`` has the size of
-            ``world_size * len(input_tensor_list)``, since the function all
-            gathers the result from every single GPU in the group. To interpret
-            each element of ``output_tensor_list[i]``, note that
-            ``input_tensor_list[j]`` of rank k will be appear in
-            ``output_tensor_list[i][rank * world_size + j]``
-            Also note that ``len(output_tensor_lists)``, and the size of each
-            element in ``output_tensor_lists`` (each element is a list,
-            therefore ``len(output_tensor_lists[i])``) need to be the same
+
+            e.g. output_tensor_lists[i] contains the all_gather
+            result that resides on the GPU of input_tensor_list[i].
+
+            Note that each element of output_tensor_lists[i] has the size of
+            world_size * len(input_tensor_list), since the function all gathers
+            the result from every single GPU in the group. To interpret each
+            element of output_tensor_list[i], note that input_tensor_list[j] of
+            rank k will be appear in
+            output_tensor_list[i][rank * world_size + j]
+
+            Also note that len(output_tensor_lists), and the size of each
+            element in output_tensor_lists (each element is a list,
+            therefore len(output_tensor_lists[i])), need to be the same
             for all the distributed processes calling this function.
 
         input_tensor_list (List[Tensor]): List of tensors(on different GPUs) to
             be broadcast from current process.
-            Note that ``len(input_tensor_list)`` needs to be the same for
+            Note that len(input_tensor_list) needs to be the same for
             all the distributed processes calling this function.
         group (optional): Group of the collective.
     """
diff --git a/torch/distributed/launch.py b/torch/distributed/launch.py
index fb8c48968..89f733f99 100644
--- a/torch/distributed/launch.py
+++ b/torch/distributed/launch.py
@@ -1,5 +1,5 @@
 r"""
-`torch.distributed.launch` is a module that spawns up multiple distributed
+torch.distributed.launch is a helper module that spawns up multiple distributed
 training processes on each of the training nodes.
 
 The utility can be used for single-node distributed training, in which one or
@@ -24,39 +24,26 @@ GPU (nproc_per_node - 1)*.
 
 1. Single-Node multi-process distributed training
 
-::
-
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other
-               arguments of your training script)
+  ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
+  YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and all other arguments of
+  your training script)``
 
 2. Multi-Node multi-process distributed training: (e.g. two nodes)
 
-
-Node 1: *(IP: 192.168.1.1, and has a free port: 1234)*
-
-::
-
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"
-               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
-               and all other arguments of your training script)
-
-Node 2:
-
-::
-
-    >>> python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
-               --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"
-               --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3
-               and all other arguments of your training script)
+    **Node 1**: *(IP: 192.168.1.1, and has a free port: 1234)*
+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
+        --nnodes=2 --node_rank=0 --master_addr="192.168.1.1"
+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and
+        all other arguments of your training script)``
+    **Node 2**:
+        ``python -m torch.distributed.launch --nproc_per_node=NUM_GPUS_YOU_HAVE
+        --nnodes=2 --node_rank=1 --master_addr="192.168.1.1"
+        --master_port=1234 YOUR_TRAINING_SCRIPT.py (--arg1 --arg2 --arg3 and
+        all other arguments of your training script)``
 
 3. To look up what optional arguments this module offers:
 
-::
-
-    >>> python -m torch.distributed.launch --help
-
+        >>> python -m torch.distributed.launch --help
 
 **Important Notices:**
 
@@ -66,22 +53,18 @@ the NCCL distributed backend. Thus NCCL backend is the recommended backend to
 use for GPU training.
 
 2. In your training program, you must parse the command-line argument:
-``--local_rank=LOCAL_PROCESS_RANK``, which will be provided by this module.
+``--local_rank=LOCAL_PROCESS_RANK, which will be provided by this module.
 If your training program uses GPUs, you should ensure that your code only
 runs on the GPU device of LOCAL_PROCESS_RANK. This can be done by:
 
-Parsing the local_rank argument
-
-::
+    Parsing the local_rank argument
 
     >>> import argparse
     >>> parser = argparse.ArgumentParser()
     >>> parser.add_argument("--local_rank", type=int)
     >>> args = parser.parse_args()
 
-Set your device to local rank using either
-
-::
+    Set your device to local rank using either
 
     >>> torch.cuda.set_device(arg.local_rank)  # before your code runs
 
@@ -93,24 +76,18 @@ Set your device to local rank using either
 3. In your training program, you are supposed to call the following function
 at the beginning to start the distributed backend. You need to make sure that
 the init_method uses ``env://``, which is the only supported ``init_method``
-by this module.
-
-::
+by this module:
 
-    torch.distributed.init_process_group(backend='YOUR BACKEND',
-                                         init_method='env://')
+    ``torch.distributed.init_process_group(backend='YOUR BACKEND',
+    init_method='env://')``
 
 4. In your training program, you can either use regular distributed functions
-or use :func:`torch.nn.parallel.DistributedDataParallel` module. If your
-training program uses GPUs for training and you would like to use
-:func:`torch.nn.parallel.DistributedDataParallel` module,
-here is how to configure it.
+or use DistributedDataParallel module. If your training program uses GPUs for
+training and you would like to use DistributedDataParallel module, here is how
+to configure it.
 
-::
-
-    model = torch.nn.parallel.DistributedDataParallel(model,
-                                                      device_ids=[arg.local_rank],
-                                                      output_device=arg.local_rank)
+    ``model = torch.nn.parallel.DistributedDataParallel(model,
+    device_ids=[arg.local_rank], output_device=arg.local_rank)``
 
 Please ensure that ``device_ids`` argument is set to be the only GPU device id
 that your code will be operating on. This is generally the local rank of the
@@ -172,36 +149,31 @@ def parse_args():
     return parser.parse_args()
 
 
-def main():
-    args = parse_args()
-
-    # world size in terms of number of processes
-    dist_world_size = args.nproc_per_node * args.nnodes
-
-    # set PyTorch distributed related environmental variables
-    current_env = os.environ.copy()
-    current_env["MASTER_ADDR"] = args.master_addr
-    current_env["MASTER_PORT"] = str(args.master_port)
-    current_env["WORLD_SIZE"] = str(dist_world_size)
+args = parse_args()
 
-    processes = []
+# world size in terms of number of processes
+dist_world_size = args.nproc_per_node * args.nnodes
 
-    for local_rank in range(0, args.nproc_per_node):
-        # each process's rank
-        dist_rank = args.nproc_per_node * args.node_rank + local_rank
-        current_env["RANK"] = str(dist_rank)
+# set PyTorch distributed related environmental variables
+current_env = os.environ.copy()
+current_env["MASTER_ADDR"] = args.master_addr
+current_env["MASTER_PORT"] = str(args.master_port)
+current_env["WORLD_SIZE"] = str(dist_world_size)
 
-        # spawn the processes
-        cmd = ["python",
-               args.training_script,
-               "--local_rank={}".format(local_rank)] + args.training_script_args
+processes = []
 
-        process = subprocess.Popen(cmd, env=current_env)
-        processes.append(process)
+for local_rank in range(0, args.nproc_per_node):
+    # each process's rank
+    dist_rank = args.nproc_per_node * args.node_rank + local_rank
+    current_env["RANK"] = str(dist_rank)
 
-    for process in processes:
-        process.wait()
+    # spawn the processes
+    cmd = ["python",
+           args.training_script,
+           "--local_rank={}".format(local_rank)] + args.training_script_args
 
+    process = subprocess.Popen(cmd, env=current_env)
+    processes.append(process)
 
-if __name__ == "__main__":
-    main()
+for process in processes:
+    process.wait()
diff --git a/torch/lib/THD/base/Cuda.cpp b/torch/lib/THD/base/Cuda.cpp
index 5e83aaffb..879240896 100644
--- a/torch/lib/THD/base/Cuda.cpp
+++ b/torch/lib/THD/base/Cuda.cpp
@@ -1,7 +1,7 @@
 #include "Cuda.hpp"
 #include <unordered_map>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 THCState** _THDCudaState;
 
 void THDSetCudaStatePtr(THCState **state) {
diff --git a/torch/lib/THD/base/Cuda.h b/torch/lib/THD/base/Cuda.h
index 1cc3fd5c3..3bd5469c2 100644
--- a/torch/lib/THD/base/Cuda.h
+++ b/torch/lib/THD/base/Cuda.h
@@ -1,6 +1,6 @@
 #pragma once
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "../THD.h"
 
 #include <THC/THC.h>
diff --git a/torch/lib/THD/base/Cuda.hpp b/torch/lib/THD/base/Cuda.hpp
index 260eea69f..3c7516ff8 100644
--- a/torch/lib/THD/base/Cuda.hpp
+++ b/torch/lib/THD/base/Cuda.hpp
@@ -1,6 +1,6 @@
 #pragma once
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #include "Cuda.h"
 
diff --git a/torch/lib/THD/base/DataChannel.cpp b/torch/lib/THD/base/DataChannel.cpp
index 2a6f783e1..993d91e34 100644
--- a/torch/lib/THD/base/DataChannel.cpp
+++ b/torch/lib/THD/base/DataChannel.cpp
@@ -5,7 +5,7 @@
 #ifdef WITH_MPI
 #include "data_channels/DataChannelMPI.hpp"
 #endif // WITH_MPI
-#if defined(WITH_CUDA) && defined(WITH_DISTRIBUTED_NCCL)
+#if defined(WITH_ROCM) && defined(WITH_DISTRIBUTED_NCCL)
 #include "data_channels/DataChannelNccl.hpp"
 #endif // WITH_DISTRIBUTED_NCCL
 #include "data_channels/DataChannelTCP.hpp"
@@ -43,7 +43,7 @@ DataChannel* DataChannel::newChannel(THDChannelType type, std::string init_metho
       );
 
     case THDChannelNccl:
-#if defined(WITH_CUDA) && defined(WITH_DISTRIBUTED_NCCL)
+#if defined(WITH_ROCM) && defined(WITH_DISTRIBUTED_NCCL)
       return new DataChannelNccl(GET_CONFIG);
 #endif
       throw std::runtime_error(
diff --git a/torch/lib/THD/base/TensorDescriptor.cpp b/torch/lib/THD/base/TensorDescriptor.cpp
index d87a65242..7be7937ff 100644
--- a/torch/lib/THD/base/TensorDescriptor.cpp
+++ b/torch/lib/THD/base/TensorDescriptor.cpp
@@ -29,7 +29,7 @@ THDTensorDescriptor THDTensorDescriptor_newFromTHByteTensor(THByteTensor *tensor
   return at::getType(at::Backend::CPU, at::ScalarType::Byte).unsafeTensorFromTH((void*)tensor, true);
 }
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaDoubleTensor(THCudaDoubleTensor *tensor) {
   return at::getType(at::Backend::CUDA, at::ScalarType::Double).unsafeTensorFromTH((void*)tensor, true);
diff --git a/torch/lib/THD/base/TensorDescriptor.h b/torch/lib/THD/base/TensorDescriptor.h
index 66ef02ca3..46b6badbd 100644
--- a/torch/lib/THD/base/TensorDescriptor.h
+++ b/torch/lib/THD/base/TensorDescriptor.h
@@ -2,7 +2,7 @@
 
 #include "../THD.h"
 #include <TH/TH.h>
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <THC/THC.h>
 #endif
 
@@ -18,7 +18,7 @@ THDTensorDescriptor THDTensorDescriptor_newFromTHIntTensor(THIntTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHShortTensor(THShortTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCharTensor(THCharTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHByteTensor(THByteTensor *tensor);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaDoubleTensor(THCudaDoubleTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaFloatTensor(THCudaTensor *tensor);
 THDTensorDescriptor THDTensorDescriptor_newFromTHCudaHalfTensor(THCudaHalfTensor *tensor);
diff --git a/torch/lib/THD/base/data_channels/DataChannelMPI.cpp b/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
index 56e300eb1..62fa6e7c7 100644
--- a/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
+++ b/torch/lib/THD/base/data_channels/DataChannelMPI.cpp
@@ -10,7 +10,7 @@
 #include <unordered_map>
 #include <iostream>
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda_runtime.h>
 #endif
 
@@ -140,7 +140,7 @@ rank_type DataChannelMPI::getNumProcesses() {
 struct AutoGPU {
   AutoGPU(int new_device) {
     if (new_device == -1) return;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     cudaGetDevice(&device_);
     cudaSetDevice(new_device);
 #endif
@@ -148,7 +148,7 @@ struct AutoGPU {
 
   ~AutoGPU() {
     if (device_ == -1) return;
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     cudaSetDevice(device_);
 #endif
   }
diff --git a/torch/lib/THD/base/data_channels/GlooCache.hpp b/torch/lib/THD/base/data_channels/GlooCache.hpp
index eaed54aff..0f0577e68 100644
--- a/torch/lib/THD/base/data_channels/GlooCache.hpp
+++ b/torch/lib/THD/base/data_channels/GlooCache.hpp
@@ -9,7 +9,7 @@
 #include "gloo/allreduce_ring.h"
 #include "gloo/barrier_all_to_all.h"
 #include "gloo/broadcast_one_to_all.h"
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include "gloo/cuda_allreduce_ring.h"
 #include "gloo/cuda_allreduce_halving_doubling.h"
 #include "gloo/cuda_allreduce_halving_doubling_pipelined.h"
@@ -19,7 +19,7 @@
 #include "gloo/rendezvous/store.h"
 #include "gloo/rendezvous/prefix_store.h"
 
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
 #include <cuda.h>
 #include <THC/THC.h>
 #endif
@@ -141,7 +141,7 @@ struct GlooCache {
     if (device == DeviceType::CPU) {
       return std::shared_ptr<buffer_type>(new char[bytes],
                                           std::default_delete<char[]>());
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       buffer_type *buf;
       THCudaCheck(THCudaMalloc(THDGetCudaState(), (void**)&buf, bytes));
@@ -184,7 +184,7 @@ struct GlooCache {
 
     if (t_dev == DeviceType::CPU) {
       std::memcpy(input_buffer, t.data_ptr(), tensor_bytes);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (t_dev == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
       THCudaCheck(cudaMemcpyAsync(input_buffer, t.data_ptr(), tensor_bytes,
@@ -202,7 +202,7 @@ struct GlooCache {
 
     if (t_dev == DeviceType::CPU) {
       std::memcpy(t.data_ptr(), output_buffer, tensor_bytes);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (t_dev == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
       THCudaCheck(cudaMemcpyAsync(t.data_ptr(), output_buffer, tensor_bytes,
@@ -318,7 +318,7 @@ struct algorithm_spec<CollectiveType::ALL_REDUCE, T> {
         std::initializer_list<T*>{reinterpret_cast<T*>(input_buffer.get())},
         count,
         THDToGlooReduceOp<T>(op));
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       if (op != THDReduceSUM) {
         throw std::runtime_error("Gloo backend only supports sum op for CUDA all reduce");
@@ -388,7 +388,7 @@ struct algorithm_spec<CollectiveType::BROADCAST, T> {
         std::initializer_list<T*>{reinterpret_cast<T*>(input_buffer.get())},
         count,
         src_rank);
-#ifdef WITH_CUDA
+#ifdef WITH_ROCM
     } else if (device == DeviceType::CUDA) {
       auto stream = THCState_getCurrentStream(THDGetCudaState());
 
