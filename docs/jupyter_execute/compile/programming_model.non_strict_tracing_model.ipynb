{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22020c07",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import header_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e089f8e",
   "metadata": {},
   "source": [
    "# Non-strict Tracing Programming Model\n",
    "\n",
    "**Summary:**\n",
    "- **Non-strict tracing** is a way to trace Python code that is less strict than Dynamo, but may result in silent incorrectness.\n",
    "- Non-strict tracing runs a Python function and uses Python and PyTorch’s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\n",
    "- A function is **non-strict traceable** if it complies with some constraints, namely, that the function is **pure** and does not directly manipulate Tensor.data_ptr().\n",
    "- Non-strict tracing may **specialize** on certain variables and treat them as **constants**, baking the values of the variables into the trace.\n",
    "\n",
    "`torch.compile` internals (`make_fx`, AOTDispatcher) use **non-strict tracing**. [`torch._dynamo.nonstrict_trace`](programming_model.dynamo_nonstrict_trace) can also be used in `torch.compile`d code to mark sections of code to be traced with non-strict tracing.\n",
    "Non-strict tracing runs a Python function and uses Python and PyTorch’s operator overloading capabilities to record what Tensor operations occurred during execution into a trace.\n",
    "\n",
    "**`make_fx`** is the main entrypoint for non-strict tracing. For the following function, only the top branch is taken during execution of the inputs, so it captures a graph with only that branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108b1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class f(torch.nn.Module):\n",
      "    def forward(self, x_1: \"f32[3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n",
      "        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n",
      "        return div\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.fx.experimental.proxy_tensor import make_fx\n",
    "def f(x):\n",
    "    if x.shape[0] > 2:\n",
    "        return x ** 2 / 6\n",
    "    else:\n",
    "        return x * 3\n",
    "x = torch.randn(3)\n",
    "gm = make_fx(f, tracing_mode=\"fake\")(x)\n",
    "gm.print_readable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ff330",
   "metadata": {},
   "source": [
    "Non-strict tracing differs from Dynamo (strict) tracing in that **it is unsafe**, that is, given a function, it captures a graph of Tensor operations that may have different semantics than the original function.\n",
    "Given a Python function, Dynamo Tracing captures a graph of Tensor operations and residual bytecode that when combined give the same semantics as the Python function.\n",
    "\n",
    "(programming_model.non_strict_tracing_model.pure_functions)=\n",
    "\n",
    "## Pure Functions\n",
    "\n",
    "Non-strict tracing is sound only on **pure functions**, and thus only pure functions should be non-strict traced.\n",
    "\n",
    "A pure function is a function with the following properties:\n",
    "\n",
    "- **Determinism.** Given the same inputs, the pure function will always return the same output.\n",
    "- **No side effects.** A pure function does not have any side effects such as modifying external state or performing I/O operations.\n",
    "- **Explicit input/output.** All the input data must be passed through the function parameters and all of the outputs are returned from the function.\n",
    "\n",
    "Here are some examples of impure functions for which the captured graph behaves differently from the original function.\n",
    "\n",
    "### Example 1: No explicit input (e.g. accesses global tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d675df74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. call function tensor([1, 2, 3])\n",
      "1. call graph tensor([1, 2, 3])\n",
      "2. call function tensor([2, 3, 4])\n",
      "2. call graph tensor([1, 2, 3])\n",
      "3. call function tensor([3, 4, 5])\n",
      "3. call graph tensor([3, 4, 5])\n",
      "4. call function tensor([4, 5, 6])\n",
      "4. call graph tensor([4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "var = torch.tensor(1)\n",
    "def function_with_global_access(y):\n",
    "    return y + var\n",
    "x = torch.tensor([0, 1, 2])\n",
    "# _allow_non_fake_inputs=True is needed to capture the global variable\n",
    "# for demonstration purposes.\n",
    "gm = make_fx(\n",
    "    function_with_global_access, tracing_mode=\"fake\", _allow_non_fake_inputs=True\n",
    ")(x)\n",
    "# Non-strict Tracing captures the value of the global (1.)\n",
    "print(\"1. call function\", function_with_global_access(x))\n",
    "print(\"1. call graph\", gm(x))\n",
    "# However, after changing the global, the captured graph\n",
    "# produces a different result from the original function\n",
    "var = torch.tensor(2)\n",
    "print(\"2. call function\", function_with_global_access(x))\n",
    "print(\"2. call graph\", gm(x))\n",
    "# To capture a graph that can have a varying `var` tensor,\n",
    "# it must be an explicit input:\n",
    "def function_fixed(y, var):\n",
    "    return y + var\n",
    "var = torch.tensor(3)\n",
    "gm = make_fx(function_fixed, tracing_mode=\"fake\")(x, var)\n",
    "print(\"3. call function\", function_fixed(x, var))\n",
    "print(\"3. call graph\", gm(x, var))\n",
    "var = torch.tensor(4)\n",
    "print(\"4. call function\", function_fixed(x, var))\n",
    "print(\"4. call graph\", gm(x, var))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af67b969",
   "metadata": {},
   "source": [
    "See [Specialization and Constants](specialization-and-constants) for an explanation of why.\n",
    "\n",
    "### Example 2: Side effect (printing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e2212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "def function_with_side_effect(y):\n",
    "    print(y)\n",
    "x = torch.tensor([0, 1, 2])\n",
    "_ = function_with_side_effect(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e851e9f",
   "metadata": {},
   "source": [
    "Running `f` in Python prints a Tensor as a side effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ff870df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeTensor(..., size=(3,), dtype=torch.int64)\n"
     ]
    }
   ],
   "source": [
    "gm = make_fx(function_with_side_effect, tracing_mode=\"fake\")(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4460ef4",
   "metadata": {},
   "source": [
    "During non-strict tracing, this print occurs during the graph capture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2e21b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = gm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac818ee2",
   "metadata": {},
   "source": [
    "The graph does not store a call to the `print` statement, so executing the graph doesn’t print anything.\n",
    "\n",
    "### Example 3: Side effect (input list mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1466480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lst) after one call 1\n",
      "len(lst) after two calls 0\n",
      "len(lst) after graph capture 2\n",
      "len(lst) after one call to graph 2\n",
      "len(lst) after two calls to graph 2\n"
     ]
    }
   ],
   "source": [
    "lst = []\n",
    "def function_with_input_list_mutation(lst):\n",
    "    val = lst.pop()\n",
    "    return val\n",
    "x = torch.tensor([0, 1, 2])\n",
    "y = torch.tensor([0, 1, 2])\n",
    "# Each time the function is executed, the list shrinks in size\n",
    "lst = [x, y]\n",
    "function_with_input_list_mutation(lst)\n",
    "print(\"len(lst) after one call\", len(lst))\n",
    "function_with_input_list_mutation(lst)\n",
    "print(\"len(lst) after two calls\", len(lst))\n",
    "# With Non-strict Tracing, the length of the list shrinks during\n",
    "# the graph capture but not in invocations of the graph.\n",
    "lst = [x, y]\n",
    "gm = make_fx(function_with_input_list_mutation, tracing_mode=\"fake\")(lst)\n",
    "print(\"len(lst) after graph capture\", len(lst))\n",
    "gm(lst)\n",
    "print(\"len(lst) after one call to graph\", len(lst))\n",
    "gm(lst)\n",
    "print(\"len(lst) after two calls to graph\", len(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdb9ae",
   "metadata": {},
   "source": [
    "### No direct data_ptr manipulation\n",
    "Directly manipulating `Tensor.data_ptr` is not non-strict traceable. The intuition behind this is that PyTorch is unable to tell *how* you manipulated the `data_ptr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d04089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cannot access data pointer of Tensor (e.g. FakeTensor, FunctionalTensor). If you're using torch.compile/export/fx, it is likely that we are erroneously tracing into a custom kernel. To fix this, please wrap the custom kernel into an opaque custom op. Please see the following for details: https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "# Create a tensor with a single element\n",
    "tensor = torch.tensor([42], dtype=torch.int32)  # Using int32 for simplicity\n",
    "def function_with_data_ptr(tensor):\n",
    "    # Get the data pointer\n",
    "    ptr = tensor.data_ptr()\n",
    "    # Cast the pointer to a ctypes pointer\n",
    "    ctypes_ptr = ctypes.cast(ptr, ctypes.POINTER(ctypes.c_int32))\n",
    "    # Increment the value at the pointer\n",
    "    ctypes_ptr.contents.value += 1\n",
    "    return tensor\n",
    "try:\n",
    "    make_fx(function_with_data_ptr, tracing_mode=\"fake\")(tensor)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678ddd2",
   "metadata": {},
   "source": [
    "(specialization-and-constants)=\n",
    "## Specialization and Constants\n",
    "\n",
    "Non-strict tracing captures a graph that may be specialized on some values. What this means is the captured graph is only valid for these values. We say the graph treats those values as **constant**.\n",
    "\n",
    "All non-Tensor variables are treated as constant during Non-strict Tracing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c997a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class f(torch.nn.Module):\n",
      "    def forward(self, x_1: \"i64[3]\", y_1):\n",
      "        # No stacktrace found for following nodes\n",
      "        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n",
      "        return add\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class f(torch.nn.Module):\\n    def forward(self, x_1: \"i64[3]\", y_1):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    return x + y\n",
    "x = torch.tensor([0, 1, 2])\n",
    "y = 3.14\n",
    "gm = make_fx(f, tracing_mode=\"fake\")(x, y)\n",
    "gm.print_readable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef911dd",
   "metadata": {},
   "source": [
    "3.14 is a constant in the graph.\n",
    "\n",
    "Non-strict tracing will also specialize on properties of the input Tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f901f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class f(torch.nn.Module):\n",
      "    def forward(self, x_1: \"f32[3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\n",
      "        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\n",
      "        return div\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        pow_1: \"f32[3]\" = torch.ops.aten.pow.Tensor_Scalar(x_1, 2);  x_1 = None\\n        div: \"f32[3]\" = torch.ops.aten.div.Tensor(pow_1, 6);  pow_1 = None\\n        return div\\n        '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x):\n",
    "    if x.shape[0] > 2:\n",
    "        return x ** 2 / 6\n",
    "    else:\n",
    "        return x * 3\n",
    "x = torch.randn(3)\n",
    "gm = make_fx(f, tracing_mode=\"fake\")(x)\n",
    "gm.print_readable()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889fad8",
   "metadata": {},
   "source": [
    "And it will also specialize on any variables not directly passed into the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78419465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class f(torch.nn.Module):\n",
      "    def forward(self, x_1: \"f32[3]\"):\n",
      "        # No stacktrace found for following nodes\n",
      "        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\n",
      "        return add\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class f(torch.nn.Module):\\n    def forward(self, x_1: \"f32[3]\"):\\n        # No stacktrace found for following nodes\\n        add: \"f32[3]\" = torch.ops.aten.add.Tensor(x_1, 3.14);  x_1 = None\\n        return add\\n        '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = torch.tensor(1)\n",
    "def f(x):\n",
    "    return x + y\n",
    "x = torch.randn(3)\n",
    "gm = make_fx(f, tracing_mode=\"fake\")(x)\n",
    "gm.print_readable()"
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "mystnb": {
   "execution_show_tb": true,
   "execution_timeout": 30,
   "merge_streams": true
  },
  "source_map": [
   11,
   16,
   31,
   41,
   61,
   90,
   96,
   101,
   105,
   107,
   111,
   113,
   119,
   141,
   146,
   162,
   171,
   178,
   184,
   193,
   197
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}