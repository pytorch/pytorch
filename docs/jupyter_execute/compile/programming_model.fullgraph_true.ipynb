{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18b1b8a",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import header_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef24984",
   "metadata": {},
   "source": [
    "# Use `fullgraph=True` to Identify and Eliminate Graph Breaks\n",
    "\n",
    "Using `torch.compile(fullgraph=False)` (the default) is a good way to get started with `torch.compile`: it supports all Python programs out-of-the-box via the ability to graph break and gives good performance on common cases.\n",
    "\n",
    "However, if you're trying to get more performance out of your model, you should explicitly think about what regions of code should be compiled:\n",
    "- We recommend using `torch.compile(fullgraph=True)` to find and eliminate graph breaks in your code.\n",
    "- If you're a library developer (or testing if your code \"works\" with `torch.compile`), we recommend testing using `torch.compile(fullgraph=True)`.\n",
    "\n",
    "`torch.compile(fullgraph=True)` offers stronger guarantees over `fullgraph=False`:\n",
    "we will always capture a single FX graph to be compiled (or error if we cannot due to a graph break).\n",
    "**In particular, you are forced to resolve every graph break that is encountered.**\n",
    "\n",
    "There are a number of strategies for resolving a graph break.\n",
    "\n",
    "## Strategy 1:  Rewrite the unsupported code to use features supported by Dynamo\n",
    "\n",
    "Many graph break error messages will give some suggestions on how to rewrite code to avoid the graph break.\n",
    "If the graph break is still difficult to resolve, then please move on to the next strategy\n",
    "or submit an issue to the [PyTorch GitHub repo](https://github.com/pytorch/pytorch/issues).\n",
    "\n",
    "More graph break examples and how to resolve them can be found in [Common Graph Breaks](programming_model.common_graph_breaks).\n",
    "\n",
    "Example: Dynamo does not support calling `next` on a `list_iterator` object that was an input to the function being compiled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38fc17bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsupported method call\n",
      "  Explanation: Dynamo does not know how to trace method `__next__` of class `list_iterator`\n",
      "  Hint: Avoid calling `list_iterator.__next__` in your code.\n",
      "  Hint: Please report an issue to PyTorch.\n",
      "  Hint: Dynamo does not fully support tracing builtin iterators (e.g. `map`, `zip`, `enumerate`) passed in from uncompiled to compiled regions (e.g. `torch.compile(fn)(enumerate(...))`). This can happen unintentionally if a previous graph break happens with a builtin iterator in the local scope.\n",
      "  Hint: List/dict comprehensions in Python <= 3.11 result in implicit function calls, which Dynamo cannot trace as a top level frame. Possible workarounds are (1) use a loop instead of a comprehension, (2) fix any graph breaks in the function above the comprehension, (3) wrap the comprehension in a function, or (4) use Python 3.12+.\n",
      "\n",
      "  Developer debug context: call_method UserDefinedObjectVariable(list_iterator) __next__ [] {}\n",
      "\n",
      " For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0156.html\n",
      "\n",
      "from user code:\n",
      "   File \"C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_3548\\1195637716.py\", line 3, in f\n",
      "    a = next(xs)\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f(xs):\n",
    "    a = next(xs)\n",
    "    b = next(xs)\n",
    "    return a + b\n",
    "\n",
    "xs = [torch.tensor(1.), torch.tensor(2.)]\n",
    "try:\n",
    "    out = f(iter(xs))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec23827",
   "metadata": {},
   "source": [
    "Instead, rewrite the compiled function to accept a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e875cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f_rewritten(xs):\n",
    "    it = iter(xs)\n",
    "    a = next(it)\n",
    "    b = next(it)\n",
    "    return a + b\n",
    "\n",
    "f_rewritten(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d9edb2",
   "metadata": {},
   "source": [
    "## Strategy 2: Pure functions can always be compiled via an escape hatch.\n",
    "\n",
    "**Summary**: The space of all Python functions is vast and thus it is impractical for Dynamo to be able to trace\n",
    "through every Python function without graph breaks. For Python functions considered to be \"pure\"\n",
    "that Dynamo cannot trace through without graph breaks, we provide some escape hatches to attempt\n",
    "to trace through these functions anyway:\n",
    "\n",
    "1. Use `custom_op` or `triton_op` on pure triton kernels.\n",
    "2. Use `nonstrict_trace` for pure functions that only use PyTorch Tensor ops.\n",
    "3. Use `custom_op` for all other pure functions.\n",
    "\n",
    "A \"pure function\" is a function with the following properties:\n",
    "\n",
    "- Determinism. Given the same inputs, the pure function will always return the same output\n",
    "- No external side effects. A pure function does not have any externally-visible side effects,\n",
    "  such as modifying external state or performing I/O operations.\n",
    "  Side effects that remain internal to the function are allowed (e.g. mutating intermediate tensors).\n",
    "  One notable exception is that mutating `torch.*` ops on function input Tensors are generally allowed.\n",
    "- Explicit input/output. All the input data must be passed through the function parameters and all of the outputs are returned from the function.\n",
    "\n",
    "See [Pure Functions](programming_model.non_strict_tracing_model.pure_functions) for examples.\n",
    "\n",
    "Dynamo is theoretically able to handle a wide variety of impure functions, but may be lacking coverage for specific\n",
    "Python language features. However, pure functions can always be compiled via an escape hatch.\n",
    "\n",
    "If you have a graph break it may be possible to refactor the code around it into a pure function and use an escape hatch that bypasses Dynamo tracing:\n",
    "\n",
    "1. Use `torch._dynamo.nonstrict_trace` if you want the Tensor operations in the function to show up in the Dynamo output graph (and therefore be optimizable). `nonstrict_trace` tells Dynamo to use **non-strict tracing**.\n",
    "2. Use custom operators if you want the function to be opaque w.r.t. to `torch.compile` (both the frontend Dynamo and the backend).\n",
    "\n",
    "Note that there is nothing preventing these escape hatches from being applied to impure functions,\n",
    "but **we do not provide any soundness guarantees**.\n",
    "\n",
    "Example: If Dynamo doesn't support some Python feature or API that is non-strict traceable (e.g. it uses PyTorch operations), [use `torch._dynamo.nonstrict_trace` to capture it instead](programming_model.dynamo_nonstrict_trace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f642e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call to `torch._dynamo.graph_break()`\n",
      "  Explanation: User-inserted graph break. Message: None\n",
      "  Hint: Remove the `torch._dynamo.graph_break()` call.\n",
      "\n",
      "  Developer debug context: Called `torch._dynamo.graph_break()` with args `[]`, kwargs `{}`\n",
      "\n",
      " For more details about this graph break, please visit: https://meta-pytorch.github.io/compile-graph-break-site/gb/gb0025.html\n",
      "\n",
      "from user code:\n",
      "   File \"C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_3548\\2422769198.py\", line 11, in f\n",
      "    return g(w)\n",
      "  File \"C:\\Users\\Aditya\\AppData\\Local\\Temp\\ipykernel_3548\\2422769198.py\", line 4, in g\n",
      "    torch._dynamo.graph_break()\n",
      "\n",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0.5847, -0.7274,  0.1944])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a function that Dynamo doesn't support (due to the graph_break() call).\n",
    "def g(x):\n",
    "    y = x.sin()\n",
    "    torch._dynamo.graph_break()\n",
    "    z = y.sin()\n",
    "    return z\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def f(x):\n",
    "    w = x.sin()\n",
    "    return g(w)\n",
    "\n",
    "x = torch.randn(3)\n",
    "try:\n",
    "    f(x)  # Graph Break: there was a call to torch._dynamo.graph_break()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def f_rewritten(x):\n",
    "    w = x.sin()\n",
    "    return torch._dynamo.nonstrict_trace(g)(w)\n",
    "f_rewritten(x)  # works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be40a3c",
   "metadata": {},
   "source": [
    "Example: use [custom operators](programming_model.custom_ops) to create opaque functions w.r.t. to `torch.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e648b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error checking compiler version for cl: [WinError 2] The system cannot find the file specified\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['where', 'cl']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m      4\u001b[39m cpp_source = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[33mtorch::Tensor square_cpu(torch::Tensor input) \u001b[39m\u001b[33m{\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[33m    // Check that input is a CPU tensor\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[33m}\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Load the extension inline\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m square_module = \u001b[43mload_inline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquare_cpu_kernel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcpp_sources\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpp_source\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msquare_cpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msquare\u001b[39m(x):\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m square_module.square_cpu(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\dev\\pytorch\\venv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2051\u001b[39m, in \u001b[36mload_inline\u001b[39m\u001b[34m(name, cpp_sources, cuda_sources, sycl_sources, functions, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, with_pytorch_error_handling, keep_intermediates, use_pch, no_implicit_headers)\u001b[39m\n\u001b[32m   2047\u001b[39m     _maybe_write(sycl_source_path, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(sycl_sources))\n\u001b[32m   2049\u001b[39m     sources.append(sycl_source_path)\n\u001b[32m-> \u001b[39m\u001b[32m2051\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_jit_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2058\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2059\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2060\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2061\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2062\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2063\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_python_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2064\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2065\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_intermediates\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\dev\\pytorch\\venv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2134\u001b[39m, in \u001b[36m_jit_compile\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_python_module, is_standalone, keep_intermediates)\u001b[39m\n\u001b[32m   2130\u001b[39m                 hipified_sources.add(hipify_result[s_abs].hipified_path \u001b[38;5;28;01mif\u001b[39;00m s_abs \u001b[38;5;129;01min\u001b[39;00m hipify_result \u001b[38;5;28;01melse\u001b[39;00m s_abs)\n\u001b[32m   2132\u001b[39m             sources = \u001b[38;5;28mlist\u001b[39m(hipified_sources)\n\u001b[32m-> \u001b[39m\u001b[32m2134\u001b[39m         \u001b[43m_write_ninja_file_and_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m            \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verbose:\n\u001b[32m   2148\u001b[39m     logger.debug(\u001b[33m'\u001b[39m\u001b[33mNo modifications detected for re-loaded extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m, skipping build step...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\dev\\pytorch\\venv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2271\u001b[39m, in \u001b[36m_write_ninja_file_and_build_library\u001b[39m\u001b[34m(name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, build_directory, verbose, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2267\u001b[39m     os.makedirs(build_directory, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2269\u001b[39m \u001b[38;5;66;03m# NOTE: Emitting a new ninja build file does not cause re-compilation if\u001b[39;00m\n\u001b[32m   2270\u001b[39m \u001b[38;5;66;03m# the sources did not change, so it's ok to re-emit (and it's fast).\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2271\u001b[39m \u001b[43m_write_ninja_file_to_build_library\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuild_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2274\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_cuda_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_sycl_cflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_ldflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_include_paths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_standalone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[32m   2285\u001b[39m     logger.info(\u001b[33m'\u001b[39m\u001b[33mBuilding extension module \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m, name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\dev\\pytorch\\venv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2764\u001b[39m, in \u001b[36m_write_ninja_file_to_build_library\u001b[39m\u001b[34m(path, name, sources, extra_cflags, extra_cuda_cflags, extra_sycl_cflags, extra_ldflags, extra_include_paths, with_cuda, with_sycl, is_standalone)\u001b[39m\n\u001b[32m   2761\u001b[39m ext = EXEC_EXT \u001b[38;5;28;01mif\u001b[39;00m is_standalone \u001b[38;5;28;01melse\u001b[39;00m LIB_EXT\n\u001b[32m   2762\u001b[39m library_target = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2764\u001b[39m \u001b[43m_write_ninja_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2765\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2766\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2767\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpost_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2768\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcuda_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2769\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2770\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2771\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2772\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2773\u001b[39m \u001b[43m    \u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43msycl_dlink_post_cflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2774\u001b[39m \u001b[43m    \u001b[49m\u001b[43msources\u001b[49m\u001b[43m=\u001b[49m\u001b[43msources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobjects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mldflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2778\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_cuda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2779\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwith_sycl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\dev\\pytorch\\venv\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:2950\u001b[39m, in \u001b[36m_write_ninja_file\u001b[39m\u001b[34m(path, cflags, post_cflags, cuda_cflags, cuda_post_cflags, cuda_dlink_post_cflags, sycl_cflags, sycl_post_cflags, sycl_dlink_post_cflags, sources, objects, ldflags, library_target, with_cuda, with_sycl)\u001b[39m\n\u001b[32m   2948\u001b[39m link_rule = [\u001b[33m'\u001b[39m\u001b[33mrule link\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m   2949\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IS_WINDOWS:\n\u001b[32m-> \u001b[39m\u001b[32m2950\u001b[39m     cl_paths = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwhere\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2951\u001b[39m \u001b[43m                                        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.decode(*SUBPROCESS_DECODE_ARGS).split(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m   2952\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cl_paths) >= \u001b[32m1\u001b[39m:\n\u001b[32m   2953\u001b[39m         cl_path = os.path.dirname(cl_paths[\u001b[32m0\u001b[39m]).replace(\u001b[33m'\u001b[39m\u001b[33m:\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m$:\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:472\u001b[39m, in \u001b[36mcheck_output\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    469\u001b[39m         empty = \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    470\u001b[39m     kwargs[\u001b[33m'\u001b[39m\u001b[33minput\u001b[39m\u001b[33m'\u001b[39m] = empty\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m           \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m.stdout\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['where', 'cl']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load_inline\n",
    "\n",
    "# C++ source code for the square operation\n",
    "cpp_source = \"\"\"\n",
    "torch::Tensor square_cpu(torch::Tensor input) {\n",
    "    // Check that input is a CPU tensor\n",
    "    TORCH_CHECK(input.device().is_cpu(), \"Input must be a CPU tensor\");\n",
    "\n",
    "    // Create output tensor with same shape and dtype as input\n",
    "    torch::Tensor output = torch::empty_like(input);\n",
    "\n",
    "    // Get data pointers\n",
    "    float* input_data = input.data_ptr<float>();\n",
    "    float* output_data = output.data_ptr<float>();\n",
    "\n",
    "    // Get total number of elements\n",
    "    int64_t numel = input.numel();\n",
    "\n",
    "    // For loop to compute square of each element\n",
    "    for (int64_t i = 0; i < numel; i++) {\n",
    "        output_data[i] = input_data[i] * input_data[i];\n",
    "    }\n",
    "\n",
    "    return output;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Load the extension inline\n",
    "square_module = load_inline(\n",
    "    name=\"square_cpu_kernel\",\n",
    "    cpp_sources=cpp_source,\n",
    "    functions=[\"square_cpu\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "def square(x):\n",
    "    return square_module.square_cpu(x)\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def f(x):\n",
    "    return square(x)\n",
    "\n",
    "try:\n",
    "    f(torch.randn(3, 3))  # graph break\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fa9ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use torch.library.custom_op to define a new custom operator.\n",
    "# Custom operators are opaque with respect to torch.compile:\n",
    "# that is, torch.compile does not peek into them.\n",
    "\n",
    "@torch.library.custom_op(\"mylib::square\", mutates_args=())\n",
    "def square(x: torch.Tensor) -> torch.Tensor:\n",
    "    return square_module.square_cpu(x)\n",
    "\n",
    "# Use register_fake to add a ``FakeTensor`` kernel for the operator\n",
    "@square.register_fake\n",
    "def _(x):\n",
    "    return x.new_empty(x.size())\n",
    "\n",
    "print(f(torch.randn(3, 3)))  # no graph break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af502e46",
   "metadata": {},
   "source": [
    "For more information on `triton_op` for custom triton kernels, see the\n",
    "[user-defined triton kernel tutorial](https://docs.pytorch.org/tutorials/recipes/torch_compile_user_defined_triton_kernel_tutorial.html).\n",
    "\n",
    "\n",
    "## Strategy 3: Don't compile the code\n",
    "\n",
    "Not all code is amenable to being compiled. `torch.compile` is a compiler for Tensor computation;\n",
    "it will not be able to optimize things like disk IO. Try to refactor the code such that the unsupported\n",
    "code is not called in the compiled region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa10c13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph=True)\n",
    "def f(x):\n",
    "   y = x ** 2  / 2\n",
    "   torch.save(y, \"foo.pt\")\n",
    "   z = y ** 3 / 6\n",
    "   return z\n",
    "\n",
    "x = torch.randn(3)\n",
    "try:\n",
    "    f(x)  # Graph Break: torch.save not supported\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_rewritten(x):\n",
    "   y = g(x)\n",
    "   torch.save(y, \"foo.pt\")\n",
    "   z = h(y)\n",
    "   return z\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def g(x):\n",
    "   y = x ** 2  / 2\n",
    "   return y\n",
    "\n",
    "@torch.compile(fullgraph=True)\n",
    "def h(y):\n",
    "   z = y ** 3 / 6\n",
    "   return z\n",
    "\n",
    "f_rewritten(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f9eec",
   "metadata": {
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.remove(\"foo.pt\")"
   ]
  }
 ],
 "metadata": {
  "file_format": "mystnb",
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "mystnb": {
   "execution_show_tb": true,
   "execution_timeout": 30,
   "merge_streams": true
  },
  "source_map": [
   11,
   16,
   42,
   54,
   58,
   67,
   104,
   128,
   132,
   181,
   196,
   208,
   223,
   243
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}