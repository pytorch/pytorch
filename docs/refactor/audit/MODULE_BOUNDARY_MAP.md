# PyTorch Module Boundary Map

**Purpose**: Document the current module structure, dependency directions, and coupling hotspots to guide safe refactoring.

**Audit Date**: 2026-02-08  
**Baseline Commit**: c5f1d40

---

## 1. High-Level Module Structure

```
pytorch/
â”œâ”€â”€ c10/              # Core abstractions (device, dtype, tensor basics)
â”œâ”€â”€ aten/             # Tensor operators (native implementations)
â”œâ”€â”€ torch/            # Python frontend
â”‚   â”œâ”€â”€ csrc/        # C++ implementation (Python bindings, autograd engine)
â”‚   â”œâ”€â”€ nn/          # Neural network modules
â”‚   â”œâ”€â”€ optim/       # Optimizers
â”‚   â”œâ”€â”€ distributed/ # Multi-node training
â”‚   â”œâ”€â”€ _dynamo/     # Compiler: graph capture
â”‚   â”œâ”€â”€ _inductor/   # Compiler: code generation
â”‚   â””â”€â”€ fx/          # Graph IR
â”œâ”€â”€ torchgen/        # Code generation for operators
â”œâ”€â”€ functorch/       # Function transforms (merging into torch)
â”œâ”€â”€ caffe2/          # Legacy (mostly deprecated)
â””â”€â”€ test/            # Test suite
```

---

## 2. Module Dependency Directions

### 2.1 Ideal Layering (Bottom-Up)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          User Code (Scripts)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     torch (Python API)                  â”‚  â† Public surface
â”‚  - torch.nn, torch.optim, torch.cuda    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     torch._C (Python bindings)          â”‚  â† C++ <-> Python bridge
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     torch/csrc/ (C++ implementation)    â”‚  â† Autograd, JIT, etc.
â”‚  - autograd, jit, distributed           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     aten (Tensor operators)             â”‚  â† Core math ops
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     c10 (Core abstractions)             â”‚  â† Device, Tensor, Storage
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Actual Dependency Graph (Simplified)

```mermaid
graph TD
    User[User Code] --> Torch[torch/]
    Torch --> TorchC[torch/_C]
    Torch --> TorchNN[torch/nn]
    Torch --> TorchOptim[torch/optim]
    Torch --> TorchDist[torch/distributed]
    Torch --> TorchDynamo[torch/_dynamo]
    Torch --> TorchInductor[torch/_inductor]
    Torch --> TorchFX[torch/fx]
    
    TorchC --> TorchCSRC[torch/csrc/]
    TorchCSRC --> ATen[aten/]
    ATen --> C10[c10/]
    
    TorchDist --> TorchCSRC
    TorchDynamo --> TorchFX
    TorchInductor --> TorchFX
    
    TorchNN --> TorchC
    TorchOptim --> TorchC
    
    TorchGen[torchgen/] -.generates.-> ATen
    TorchGen -.generates.-> TorchC
```

**Legend**:
- Solid arrows: Runtime dependency
- Dotted arrows: Build-time dependency (code generation)

---

## 3. Module Descriptions

### 3.1 `c10/` (Core Abstractions)

**Purpose**: Device-agnostic tensor abstractions, memory management, type system

**Key Components**:
- `c10/core/`: `ScalarType`, `Device`, `Allocator`, `TensorImpl`
- `c10/util/`: `intrusive_ptr`, `half`, `complex`, `ArrayRef`
- `c10/cuda/`, `c10/xpu/`: Device-specific backends

**Dependencies**:
- **Depends on**: Standard library only (no PyTorch deps)
- **Depended by**: Everything (c10 is the foundation)

**Coupling**: ğŸŸ¢ **LOW** (well-isolated, stable)

**Refactor Risk**: ğŸ”´ **VERY HIGH** (changes ripple through entire codebase)

---

### 3.2 `aten/` (ATen Tensor Library)

**Purpose**: Operator implementations (CPU, CUDA, vectorized, etc.)

**Key Components**:
- `aten/src/ATen/`: Tensor class, operator declarations
- `aten/src/ATen/native/`: Native operator implementations (700+ ops)
- `aten/src/ATen/core/`: Operator registration, dispatch

**Dependencies**:
- **Depends on**: `c10/`, `third_party/` (BLAS, cuDNN, etc.)
- **Depended by**: `torch/csrc/`, Python bindings

**Coupling**: ğŸŸ¡ **MEDIUM** (large, but operator-level isolation)

**Refactor Risk**: ğŸŸ  **HIGH** (operator changes affect all users, but dispatch system provides isolation)

**Code Generation**:
- ~80% of ATen is auto-generated by `torchgen/`
- Manual edits: `aten/src/ATen/native/*.cpp` (native implementations)

---

### 3.3 `torch/csrc/` (C++ Implementation)

**Purpose**: Python bindings, autograd engine, JIT compiler, distributed runtime

**Key Components**:
- `torch/csrc/autograd/`: Automatic differentiation engine
- `torch/csrc/jit/`: TorchScript JIT compiler
- `torch/csrc/distributed/`: C++ distributed backend (c10d)
- `torch/csrc/api/`: C++ frontend (torch::nn)

**Dependencies**:
- **Depends on**: `aten/`, `c10/`, `pybind11`
- **Depended by**: `torch/_C` (Python bindings)

**Coupling**: ğŸŸ  **HIGH** (tightly coupled to Python runtime via pybind11)

**Refactor Risk**: ğŸŸ  **HIGH** (changes can break Python bindings or autograd)

---

### 3.4 `torch/` (Python Frontend)

**Purpose**: User-facing Python API

**Key Components**:
- `torch/__init__.py`: Main API entry point (~2,000 lines of imports)
- `torch/nn/`: Neural network modules
- `torch/optim/`: Optimizers
- `torch/distributed/`: Distributed training (Python layer)
- `torch/_dynamo/`: Compiler (graph capture via bytecode introspection)
- `torch/_inductor/`: Compiler (code generation backend)
- `torch/fx/`: Graph IR

**Dependencies**:
- **Depends on**: `torch/_C` (C++ bindings), `torch/csrc/`
- **Depended by**: User code (entire ecosystem)

**Coupling**: ğŸ”´ **VERY HIGH** (monolithic `__init__.py`, many cross-module imports)

**Refactor Risk**: ğŸ”´ **VERY HIGH** (any change affects public API)

---

### 3.5 `torchgen/` (Code Generation)

**Purpose**: Generate C++/Python code from operator schema definitions

**Key Components**:
- `torchgen/api/`: API definition parsing
- `torchgen/gen.py`: Code generation entry point
- `torchgen/dest/`: Code generation templates

**Dependencies**:
- **Depends on**: `pyyaml` (parses `native_functions.yaml`)
- **Depended by**: `aten/`, `torch/_C` (build-time)

**Coupling**: ğŸŸ¢ **LOW** (isolated, runs at build time)

**Refactor Risk**: ğŸŸ  **MEDIUM** (changes affect hundreds of generated files, but templated)

---

### 3.6 `functorch/` (Function Transforms)

**Purpose**: `vmap`, `grad`, `jvp` (function transforms for functional programming)

**Key Components**:
- `functorch/api.py`: Public API
- `functorch/_src/`: Internal implementations

**Dependencies**:
- **Depends on**: `torch/`, `torch/_C`
- **Depended by**: `torch.func` (re-exported)

**Coupling**: ğŸŸ¡ **MEDIUM** (being merged into `torch/`)

**Refactor Risk**: ğŸŸ¡ **MEDIUM** (transition period, some users use legacy `functorch` imports)

**Status**: **Being merged into `torch.func`** (expect deprecation of standalone `functorch` package)

---

### 3.7 `caffe2/` (Legacy)

**Purpose**: Legacy Caffe2 code (mostly deprecated)

**Key Components**:
- `caffe2/core/`, `caffe2/operators/`: Old Caffe2 operators

**Dependencies**:
- **Depends on**: `c10/`, `aten/` (partially)
- **Depended by**: Almost nothing (legacy mobile code only)

**Coupling**: ğŸŸ¢ **LOW** (isolated, deprecated)

**Refactor Risk**: ğŸŸ¢ **LOW** (can be removed incrementally)

**Status**: **Deprecated** (no new code should depend on `caffe2`)

---

## 4. Dependency Hotspots & Cycles

### 4.1 God Module: `torch/__init__.py`

**Issue**: 
- Imports ~50+ submodules
- Any refactor touching `torch/` requires careful import ordering

**Example**:
```python
# torch/__init__.py (simplified)
from torch._C import *
from torch.tensor import Tensor
from torch import nn
from torch import optim
from torch import distributed
# ... 40+ more imports
```

**Refactor Risk**: ğŸ”´ **VERY HIGH**

**Recommendation**: 
- Use lazy imports where possible (e.g., `torch.distributed` already lazy-loads)
- Document import order dependencies

---

### 4.2 Circular Dependency: `torch.nn` â†” `torch.autograd`

**Issue**:
- `torch.nn.Module` uses `torch.autograd.grad()`
- `torch.autograd.Function` (custom ops) often used in `nn.Module` subclasses

**Resolution**: 
- Not a hard cycle (API boundary is clean)
- Both live in `torch/` namespace, so tolerable

**Refactor Risk**: ğŸŸ¡ **MEDIUM**

---

### 4.3 Circular Dependency: `torch._dynamo` â†” `torch.fx`

**Issue**:
- `torch._dynamo` captures graphs using `torch.fx`
- `torch.fx` transformations may invoke `_dynamo` for re-tracing

**Resolution**:
- Managed via explicit API boundaries
- Both are compiler internals (not public-facing)

**Refactor Risk**: ğŸŸ¡ **MEDIUM**

---

### 4.4 Distributed <-> Autograd Coupling

**Issue**:
- `torch.distributed.autograd` hooks into autograd engine
- `torch.distributed` RPC uses autograd for distributed backward

**Resolution**:
- Necessary coupling (distributed autograd requires both)
- Well-documented API boundary

**Refactor Risk**: ğŸŸ  **HIGH** (multi-process testing required for any changes)

---

## 5. Coupling Metrics

| Module | Incoming Deps | Outgoing Deps | Coupling Score |
|--------|---------------|---------------|----------------|
| `c10/` | All modules | None (stdlib only) | ğŸŸ¢ 1/10 (foundation) |
| `aten/` | `torch/`, `torchgen/` | `c10/`, `third_party/` | ğŸŸ¡ 3/10 (operator layer) |
| `torch/csrc/` | `torch/` (bindings) | `aten/`, `c10/`, `pybind11` | ğŸŸ  5/10 (bridge layer) |
| `torch/` | User code, tests | Everything below | ğŸ”´ 9/10 (monolithic) |
| `torch/nn/` | User models | `torch/`, `torch/_C` | ğŸŸ  6/10 (high usage) |
| `torch/distributed/` | Training scripts | `torch/`, `torch/csrc/distributed/` | ğŸŸ  7/10 (complex) |
| `torch/_dynamo/` | `torch.compile` users | `torch/fx`, `torch/` | ğŸŸ¡ 5/10 (compiler internal) |
| `torch/_inductor/` | `torch.compile` | `torch/fx`, `torch/` | ğŸŸ¡ 5/10 (compiler internal) |
| `torchgen/` | Build system | `pyyaml` | ğŸŸ¢ 2/10 (isolated) |
| `functorch/` | `torch.func` users | `torch/` | ğŸŸ¡ 4/10 (being merged) |
| `caffe2/` | Legacy code | `c10/`, `aten/` (partial) | ğŸŸ¢ 2/10 (deprecated) |

**Coupling Score**: 1 (low) - 10 (high)

---

## 6. Suggested Future Boundaries

### 6.1 Separate `torch.distributed` into Standalone Repo (Optional)

**Rationale**:
- Distributed training is complex, evolving independently
- Large surface area (370 files)
- Could be versioned separately

**Challenges**:
- Tight integration with autograd
- Would require stable ABI boundary

**Recommendation**: **Defer** (not urgent, high complexity)

---

### 6.2 Consolidate Compiler Stack (`_dynamo` + `_inductor` + `fx`)

**Rationale**:
- All three are part of `torch.compile` pipeline
- Shared abstractions (graph IR)

**Challenges**:
- Currently separate for historical reasons
- Merging may not provide clear benefits

**Recommendation**: **Monitor** (revisit if compiler stack grows further)

---

### 6.3 Extract `torchgen` as Separate Package

**Rationale**:
- Code generation is build-time only
- Could be reused by third-party projects

**Challenges**:
- Tightly coupled to PyTorch operator schema
-Versionin versioning would be complex

**Recommendation**: **Not recommended** (tight coupling, low reuse value)

---

### 6.4 Remove `caffe2` Entirely

**Rationale**:
- Deprecated, minimal usage
- Adds maintenance burden

**Challenges**:
- Some legacy mobile code may still depend on it
- Need to verify no production users

**Recommendation**: **M25** (include in Phase 4 structural refactors)

---

## 7. Dependency Diagram (Detailed)

```
User Code
    â”‚
    â”œâ”€â†’ torch/              (Python API)
    â”‚   â”œâ”€â†’ torch/_C        (C++ bindings)
    â”‚   â”œâ”€â†’ torch/nn        (NN modules)
    â”‚   â”œâ”€â†’ torch/optim     (Optimizers)
    â”‚   â”œâ”€â†’ torch/distributed (Distributed training)
    â”‚   â”œâ”€â†’ torch/_dynamo   (Compiler: capture)
    â”‚   â”œâ”€â†’ torch/_inductor (Compiler: codegen)
    â”‚   â””â”€â†’ torch/fx        (Graph IR)
    â”‚
    torch/_C
    â”‚
    â”œâ”€â†’ torch/csrc/         (C++ implementation)
    â”‚   â”œâ”€â†’ torch/csrc/autograd (Autograd engine)
    â”‚   â”œâ”€â†’ torch/csrc/jit      (TorchScript JIT)
    â”‚   â””â”€â†’ torch/csrc/distributed (C10d backend)
    â”‚
    torch/csrc/
    â”‚
    â”œâ”€â†’ aten/               (Tensor operators)
    â”‚   â”œâ”€â†’ aten/src/ATen/native/ (Op implementations)
    â”‚   â””â”€â†’ aten/src/ATen/core/   (Dispatch)
    â”‚
    aten/
    â”‚
    â”œâ”€â†’ c10/                (Core abstractions)
    â”‚   â”œâ”€â†’ c10/core/       (Tensor, Device, Allocator)
    â”‚   â”œâ”€â†’ c10/cuda/       (CUDA backend)
    â”‚   â””â”€â†’ c10/util/       (Utilities)
    â”‚
    c10/
    â”‚
    â””â”€â†’ stdlib              (C++ standard library)
```

---

## 8. Refactor Safety Checklist

Before refactoring any module, verify:

- [ ] **Identify Dependents**: Run `grep -r "from <module> import"` to find all importers
- [ ] **Check Coupling Score**: High coupling (>6/10) = higher risk
- [ ] **Review Invariants**: Check `INVARIANTS_CATALOG.md` for affected invariants
- [ ] **Test Coverage**: Verify module has strong test coverage
- [ ] **Dependency Direction**: Ensure refactor doesn't introduce reverse dependencies (e.g., `c10/` should never depend on `torch/`)
- [ ] **API Stability**: If public API, requires deprecation cycle
- [ ] **Build System**: Check if `torchgen/` generates code for this module
- [ ] **CI Impact**: Will refactor require new CI workflows?

---

## 9. Summary: Module Refactor Risk Matrix

| Module | Coupling | Public API | Refactor Risk | Recommended Approach |
|--------|----------|-----------|---------------|---------------------|
| `c10/` | Low | Yes (C++) | ğŸ”´ Very High | Avoid breaking changes; extend only |
| `aten/` | Medium | Yes (C++) | ğŸŸ  High | Use operator versioning; test extensively |
| `torch/csrc/` | High | Partial | ğŸŸ  High | Verify Python bindings; test autograd |
| `torch/` | Very High | Yes (Python) | ğŸ”´ Very High | Deprecation cycle required; extensive testing |
| `torch/nn/` | High | Yes | ğŸ”´ Very High | State dict compatibility critical |
| `torch/optim/` | Medium | Yes | ğŸŸ¡ Medium | Numeric correctness tests required |
| `torch/distributed/` | High | Yes | ğŸŸ  High | Multi-process tests; protocol version |
| `torch/_dynamo/` | Medium | Partial (beta) | ğŸŸ¡ Medium | Compiler tests; perf benchmarks |
| `torch/_inductor/` | Medium | Partial (beta) | ğŸŸ¡ Medium | Compiler tests; perf benchmarks |
| `torch/fx/` | Medium | Yes | ğŸŸ¡ Medium | Graph IR stability |
| `torchgen/` | Low | No | ğŸŸ¡ Medium | Affects generated code; build tests |
| `functorch/` | Medium | Yes (merging) | ğŸŸ¡ Medium | Deprecation plan for standalone package |
| `caffe2/` | Low | Deprecated | ğŸŸ¢ Low | Can remove incrementally |

---

**End of Module Boundary Map**

