name: Build Flash Attention 3 wheels (Windows)

on:
  workflow_dispatch:
  pull_request:
    paths:
      - .github/workflows/_binary-build-flash-attention-wheel-windows.yml

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}
  cancel-in-progress: true

jobs:
  get-label-type:
    if: github.repository_owner == 'pytorch'
    name: get-label-type
    uses: pytorch/pytorch/.github/workflows/_runner-determinator.yml@main
    with:
      triggering_actor: ${{ github.triggering_actor }}
      issue_owner: ${{ github.event.pull_request.user.login || github.event.issue.user.login }}
      curr_branch: ${{ github.head_ref || github.ref_name }}
      curr_ref_type: ${{ github.ref_type }}

  build-wheel:
    name: "Build FA3 Windows ${{ matrix.cuda_version }}"
    needs: get-label-type
    runs-on: "${{ needs.get-label-type.outputs.label-type }}${{ matrix.runner }}"
    strategy:
      fail-fast: false
      matrix:
        include:
          - cuda_version: "12.8.1"
            cuda_short: "128"
            torch_cuda_arch_list: "8.0;8.6;9.0"
            runner: "windows.12xlarge"
    timeout-minutes: 1440
    defaults:
      run:
        shell: bash
    env:
      CUDA_VERSION: ${{ matrix.cuda_short }}
      TORCH_CUDA_ARCH_LIST: ${{ matrix.torch_cuda_arch_list }}
      PYTHON_VERSION: "3.12"
      VC_YEAR: "2022"
      DISTUTILS_USE_SDK: 1
    steps:
      - name: Enable git long paths and symlinks on Windows
        run: |
          git config --global core.longpaths true
          git config --global core.symlinks true
          git config --global core.ignorecase false
          git config --global core.fsmonitor false
      - name: Setup SSH (Click me for login details)
        uses: pytorch/test-infra/.github/actions/setup-ssh@main
        with:
          github-secret: ${{ secrets.GITHUB_TOKEN }}

      - name: Checkout PyTorch
        uses: pytorch/pytorch/.github/actions/checkout-pytorch@main
        with:
          no-sudo: true
          submodules: false

      - name: Checkout Flash-Attention
        uses: actions/checkout@v4
        with:
          repository: Dao-AILab/flash-attention
          ref: bc0e4ac01484ffb61ddc694724826bec4d9cf1c2
          submodules: recursive
          path: flash-attention

      - name: Set CUDA environment
        shell: bash
        run: |
          CUDA_VER="${{ matrix.cuda_version }}"
          CUDA_VER_SHORT="${CUDA_VER%.*}"
          CUDA_PATH="/c/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v${CUDA_VER_SHORT}"
          echo "CUDA_HOME=${CUDA_PATH}" >> "${GITHUB_ENV}"
          echo "${CUDA_PATH}/bin" >> "${GITHUB_PATH}"
          echo "Using CUDA ${CUDA_VER_SHORT} at ${CUDA_PATH}"
      - name: Setup MSVC
        uses: ilammy/msvc-dev-cmd@v1

      - name: Remove link.exe conflict
        shell: bash
        run: rm -f /usr/bin/link

      - name: Setup Windows
        uses: ./.github/actions/setup-win
        with:
          cuda-version: ${{ matrix.cuda_version }}
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Apply Flash Attention patches for Windows
        shell: bash
        run: |
          cd flash-attention/hopper
          sed -i 's/if bare_metal_version >= Version("12.3") and bare_metal_version < Version("13.0") and bare_metal_version != Version("12.8"):/if bare_metal_version >= Version("12.3") and bare_metal_version < Version("13.0") and bare_metal_version != Version("12.8") and not is_offline_build():/' setup.py
          sed -i '/flags.append(f'"'"'ldflags = /i\    ldflags.remove("/LTCG")' setup.py

          BUILD_DATE=$(date +%Y%m%d)
          sed -i "s/version=\"3.0.0.b1\"/version=\"3.0.0b1+cu${CUDA_VERSION}.${BUILD_DATE}\"/" setup.py

          grep -n "is_offline_build" setup.py || echo "WARNING: offline build patch may not have applied"
          grep -n 'ldflags.remove' setup.py || echo "WARNING: LTCG patch may not have applied"
          grep -n "version=" setup.py | head -1

      - name: Build Flash Attention 3 wheel
        shell: bash
        env:
          FA_FINAL_PACKAGE_DIR: ${{ runner.temp }}/artifacts
          PYTORCH_ROOT: ${{ github.workspace }}
          FLASH_ATTENTION_FORCE_BUILD: "TRUE"
          FLASH_ATTENTION_OFFLINE_BUILD: "TRUE"
          NVCC_THREADS: "1"
          MAX_JOBS: "4"
          FLASH_ATTENTION_DISABLE_SOFTCAP: "TRUE"
        run: |
          set -x
          mkdir -p "${FA_FINAL_PACKAGE_DIR}"
          echo "CUDA_HOME=${CUDA_HOME}"
          nvcc --version
          python -m pip install torch --index-url "https://download.pytorch.org/whl/cu${CUDA_VERSION}"
          python -m pip install einops==0.8.2 ninja==1.13.0 numpy==2.4.1
          cd flash-attention/hopper
          python setup.py bdist_wheel -d "${FA_FINAL_PACKAGE_DIR}" -k --plat-name win_amd64
          echo "Wheel built:"
          ls -la "${FA_FINAL_PACKAGE_DIR}"/*.whl || true
      - uses: actions/upload-artifact@50769540e7f4bd5e21e526ee35c689e35e0d6874 # v4.4.0
        with:
          name: flash-attn-3-wheel-cu${{ matrix.cuda_short }}-windows_amd64
          if-no-files-found: error
          path: ${{ runner.temp }}/artifacts/*.whl

  upload-wheel:
      name: "Upload FA3 ${{ matrix.cuda_short }} windows_amd64"
      needs: build-wheel
      runs-on: ubuntu-latest
      if: ${{ github.event_name == 'workflow_dispatch' }}
      strategy:
        fail-fast: false
        matrix:
          include:
            - cuda_short: "128"
      permissions:
        id-token: write
        contents: read
      container:
        image: continuumio/miniconda3:4.12.0
      environment: pytorchbot-env
      steps:
        - uses: actions/checkout@11bd71901bbe5b1630ceea73d27597364c9af683 # v4.2.2

        - name: Configure AWS credentials(PyTorch account) for test
          uses: aws-actions/configure-aws-credentials@ececac1a45f3b08a01d2dd070d28d111c5fe6722 # v4.1.0
          with:
            role-to-assume: arn:aws:iam::749337293305:role/gha_workflow_test_build_wheels
            aws-region: us-east-1

        - name: Download Build Artifacts
          uses: actions/download-artifact@65a9edc5881444af0b9093a5e628f2fe47ea3b2e # v4.1.7
          with:
            name: flash-attn-3-wheel-cu${{ matrix.cuda_short }}-windows_amd64
            path: ${{ runner.temp }}/artifacts

        - name: Upload binaries to test index
          env:
            PACKAGE_TYPE: wheel
            UPLOAD_CHANNEL: test
            UPLOAD_SUBFOLDER: cu${{ matrix.cuda_short }}
            PKG_DIR: ${{ runner.temp }}/artifacts
            DRY_RUN: disabled
          shell: bash
          run: |
            set -ex
            bash .circleci/scripts/binary_upload.sh
