name: linux-binary-test

on:
  workflow_call:
    inputs:
      build_name:
        required: true
        type: string
        description: The build's name
      build_environment:
        required: true
        type: string
        description: The build environment
      PACKAGE_TYPE:
        required: true
        type: string
        description: Package type
      DESIRED_CUDA:
        required: true
        type: string
        description: Desired Cuda version
      GPU_ARCH_VERSION:
        required: false
        type: string
        description: GPU Arch version
      GPU_ARCH_TYPE:
        required: true
        type: string
        description: GPU Arch type
      DOCKER_IMAGE:
        required: true
        type: string
        description: Docker image to use
      LIBTORCH_CONFIG:
        required: false
        type: string
        description: Desired libtorch config (for libtorch builds only)
      LIBTORCH_VARIANT:
        required: false
        type: string
        description: Desired libtorch variant (for libtorch builds only)
      DESIRED_DEVTOOLSET:
        required: false
        type: string
        description: Desired dev toolset
      DESIRED_PYTHON:
        required: true
        type: string
        description: Desired python version
      runs_on:
        required: true
        type: string
        description: Hardware to run this job on. Valid values are linux.4xlarge, linux.4xlarge.nvidia.gpu, and linux.rocm.gpu

jobs:
  build:
    runs-on: linux.4xlarge
    timeout-minutes: 240
    env:
      PYTORCH_ROOT: /pytorch
      BUILDER_ROOT: /builder
      PACKAGE_TYPE: ${{ inputs.PACKAGE_TYPE }}
      # TODO: This is a legacy variable that we eventually want to get rid of in
      #       favor of GPU_ARCH_VERSION
      DESIRED_CUDA: ${{ inputs.DESIRED_CUDA }}
      GPU_ARCH_VERSION: ${{ inputs.GPU_ARCH_VERSION }}
      GPU_ARCH_TYPE: ${{ inputs.GPU_ARCH_TYPE }}
      DOCKER_IMAGE: ${{ inputs.DOCKER_IMAGE }}
      SKIP_ALL_TESTS: 1
      LIBTORCH_CONFIG: ${{ inputs.LIBTORCH_CONFIG }}
      LIBTORCH_VARIANT: ${{ inputs.LIBTORCH_VARIANT }}
      DESIRED_DEVTOOLSET: ${{ inputs.DESIRED_DEVTOOLSET }}
      DESIRED_PYTHON: ${{ inputs.DESIRED_PYTHON }}
      # Needed for conda builds
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      ANACONDA_USER: pytorch
      AWS_DEFAULT_REGION: us-east-1
      BINARY_ENV_FILE: /tmp/env
      BUILD_ENVIRONMENT: ${{ inputs.build_environment }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      PR_NUMBER: ${{ github.event.pull_request.number }}
      PYTORCH_FINAL_PACKAGE_DIR: /artifacts
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
    steps:
      # Setup the environment for non-ROCm builds
      - name: Checkout PyTorch
        if: ${{ inputs.GPU_ARCH_TYPE != 'rocm' }}
        uses: pytorch/pytorch/.github/actions/checkout-pytorch@master
      - name: Setup Linux
        if: ${{ inputs.GPU_ARCH_TYPE != 'rocm' }}
        uses: ./.github/actions/setup-linux
      - name: Chown workspace
        if: ${{ inputs.GPU_ARCH_TYPE != 'rocm' }}
        uses: ./.github/actions/chown-workspace
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        if: ${{ inputs.GPU_ARCH_TYPE != 'rocm' }}
        uses: ./.github/actions/setup-ssh
        with:
          github-secret: ${{ secrets.GITHUB_TOKEN }}
      - name: Clean workspace
        if: ${{ inputs.GPU_ARCH_TYPE != 'rocm' }}
        shell: bash
        run: |
          rm -rf "${GITHUB_WORKSPACE}"
          mkdir "${GITHUB_WORKSPACE}"
      # End of non-ROCm setup

      # Setup the environment for ROCm builds
      - name: Clean workspace
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' }}
        run: |
          rm -rf "${GITHUB_WORKSPACE}"
          mkdir "${GITHUB_WORKSPACE}"
      - name: Set DOCKER_HOST
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' }}
        run: echo "DOCKER_HOST=unix:///run/user/$(id -u)/docker.sock" >> "${GITHUB_ENV}"
      - name: Runner health check system info
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' && always() }}
        run: |
          cat /etc/os-release || true
          cat /etc/apt/sources.list.d/rocm.list || true
          cat /opt/rocm/.info/version || true
          whoami
      - name: Runner health check rocm-smi
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm'  && always()}}
        run: |
          rocm-smi
      - name: Runner health check rocminfo
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' && always()}}
        run: |
          rocminfo
      - name: Runner health check GPU count
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' && always() }}
        run: |
          ngpu=$(rocminfo | grep -c -E 'Name:.*\sgfx')
          if [[ "x$ngpu" != "x2" && "x$ngpu" != "x4" ]]; then
              echo "Failed to detect GPUs on the runner"
              exit 1
          fi
      - name: Runner health check disconnect on failure
        if: ${{ failure() && inputs.GPU_ARCH_TYPE == 'rocm' }}
        run: |
          killall runsvc.sh
      - name: Preserve github env variables for use in docker
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' }}
        run: |
          env | grep '^GITHUB' >> "/tmp/github_env_${GITHUB_RUN_ID}"
          env | grep '^CI' >> "/tmp/github_env_${GITHUB_RUN_ID}"
      # End of ROCm setup

      - uses: seemethere/download-artifact-s3@v4
        name: Download Build Artifacts
        with:
          name: ${{ inputs.build_name }}
          path: "${{ runner.temp }}/artifacts/"


      - name: Checkout PyTorch to pytorch dir
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
          submodules: recursive
          path: pytorch
      - name: Clean PyTorch checkout
        run: |
          # Remove any artifacts from the previous checkouts
          git clean -fxd
        working-directory: pytorch

      - name: Checkout pytorch/builder to builder dir
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          ref: main
          submodules: recursive
          repository: pytorch/builder
          path: builder
      - name: Clean pytorch/builder checkout
        run: |
          # Remove any artifacts from the previous checkouts
          git clean -fxd
        working-directory: builder

      - name: ROCm set GPU_FLAG
        if: ${{ inputs.GPU_ARCH_TYPE == 'rocm' }}
        run: |
          echo "GPU_FLAG=--device=/dev/mem --device=/dev/kfd --device=/dev/dri --group-add video --group-add daemon" >> "${GITHUB_ENV}"

      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        uses: nick-fields/retry@71062288b76e2b6214ebde0e673ce0de1755740a
        if: ${{ inputs.GPU_ARCH_TYPE == 'cuda' }}
        with:
          timeout_minutes: 10
          max_attempts: 3
          command: |
            set -ex
            pushd pytorch
            bash .github/scripts/install_nvidia_utils_linux.sh
            echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
            popd

      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Test PyTorch binary
        run: |
          set -x
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BINARY_ENV_FILE \
            -e BUILDER_ROOT \
            -e BUILD_ENVIRONMENT \
            -e BUILD_SPLIT_CUDA \
            -e DESIRED_CUDA \
            -e DESIRED_DEVTOOLSET \
            -e DESIRED_PYTHON \
            -e GITHUB_ACTIONS \
            -e GPU_ARCH_TYPE \
            -e GPU_ARCH_VERSION \
            -e LIBTORCH_VARIANT \
            -e PACKAGE_TYPE \
            -e PYTORCH_FINAL_PACKAGE_DIR \
            -e PYTORCH_ROOT \
            -e SKIP_ALL_TESTS \
            --tty \
            --detach \
            -v "${GITHUB_WORKSPACE}/pytorch:/pytorch" \
            -v "${GITHUB_WORKSPACE}/builder:/builder" \
            -v "${RUNNER_TEMP}/artifacts:/final_pkgs" \
            -w / \
            "${DOCKER_IMAGE}"
          )
          docker exec -t -w "${PYTORCH_ROOT}" "${container_name}" bash -c "bash .circleci/scripts/binary_populate_env.sh"
          # Generate test script
          docker exec -t -w "${PYTORCH_ROOT}" -e OUTPUT_SCRIPT="/run.sh" "${container_name}" bash -c "bash .circleci/scripts/binary_linux_test.sh"
          docker exec -t "${container_name}" bash -c "source ${BINARY_ENV_FILE} && bash -x /run.sh"

      - name: Hold runner for 2 hours or until ssh sessions have drained
        working-directory: pytorch/
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
