# Template is at:    .github/templates/pull.yml.j2
# Generation script: .github/scripts/generate_ci_workflows.py
name: pull

on:
  pull_request:
  push:
    branches:
      - master
      - release/*
      - fbsync

concurrency:
  group: pull-${{ github.event.pull_request.number || (startsWith(github.ref_name, 'ciflow') && github.ref_name || github.sha) }}-${{ github.event_name == 'workflow_dispatch' }}
  cancel-in-progress: true

jobs:
  test-tools:
    if: ${{ github.repository == 'pytorch/pytorch' }}
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.6 - 3.9'
          architecture: x64
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          fetch-depth: 0 # deep clone, to allow us to use git log
      - name: Install dependencies
        # mypy and boto3 versions copied from
        # .circleci/docker/common/install_conda.sh
        run: |
          set -eux
          pip install -r requirements.txt
          pip install boto3==1.16.34
          make setup_lint
      - name: Test tools
        run: python -m unittest discover -vs tools/test -p 'test_*.py'

  # LINT JOBS
  lint-quick-checks:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x
          architecture: x64
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Install requirements
        id: requirements
        run: pip3 install -r requirements.txt --user
      - name: Ensure consistent CircleCI YAML config
        if: ${{ always() && steps.requirements.outcome == 'success' }}
        run: cd .circleci && ./ensure-consistency.py
      - name: lint native_functions.yaml
        if: ${{ always() && steps.requirements.outcome == 'success' }}
        run: |
          pip3 install ruamel.yaml==0.17.4 --user
          .github/scripts/lint_native_functions.py
      - name: Ensure correct trailing newlines
        if: ${{ always() && steps.requirements.outcome == 'success' }}
        run: |
          (! git --no-pager grep -Il '' -- . ':(exclude)**/contrib/**' ':(exclude)third_party' ':(exclude)**.expect' ':(exclude)**.ipynb' ':(exclude)tools/clang_format_hash' | tools/linter/trailing_newlines.py || (echo "The above files do not have correct trailing newlines; please normalize them"; false))
      - name: Ensure no trailing spaces
        if: always()
        run: |
          (! git --no-pager grep -In '[[:blank:]]$' -- . ':(exclude)**/contrib/**' ':(exclude)**.diff' ':(exclude)third_party' || (echo "The above lines have trailing spaces; please remove them"; false))
      - name: Ensure no tabs
        if: always()
        run: |
          (! git --no-pager grep -In $'\t' -- . ':(exclude)*.svg' ':(exclude)**Makefile' ':(exclude)**/contrib/**' ':(exclude)third_party' ':(exclude).gitattributes' ':(exclude).gitmodules' || (echo "The above lines have tabs; please convert them to spaces"; false))
      - name: Ensure no non-breaking spaces
        if: always()
        run: |
          # NB: We use 'printf' below rather than '\u000a' since bash pre-4.2
          # does not support the '\u000a' syntax (which is relevant for local linters)
          (! git --no-pager grep -In "$(printf '\xC2\xA0')" -- . || (echo "The above lines have non-breaking spaces (U+00A0); please convert them to spaces (U+0020)"; false))
      - name: Ensure canonical include
        if: always()
        run: |
          (! git --no-pager grep -In $'#include "' -- ./c10 ./aten ./torch/csrc ':(exclude)aten/src/ATen/native/quantized/cpu/qnnpack/**' || (echo "The above lines have include with quotes; please convert them to #include <xxxx>"; false))
      - name: Ensure no versionless Python shebangs
        if: always()
        run: |
          (! git --no-pager grep -In '#!.*python$' -- . || (echo "The above lines have versionless Python shebangs; please specify either python2 or python3"; false))
      - name: Ensure no unqualified noqa
        if: always()
        run: |
          # shellcheck disable=SC2016
          (! git --no-pager grep -InP '# noqa(?!: [A-Z]+\d{3})' -- '**.py' '**.pyi' ':(exclude)caffe2' || (echo 'The above lines have unqualified `noqa`; please convert them to `noqa: XXXX`'; false))
      - name: Ensure no unqualified type ignore
        if: always()
        run: |
          # shellcheck disable=SC2016
          (! git --no-pager grep -InP '# type:\s*ignore(?!\[)' -- '**.py' '**.pyi' ':(exclude)test/test_jit.py' || (echo 'The above lines have unqualified `type: ignore`; please convert them to `type: ignore[xxxx]`'; false))
      - name: Ensure GitHub PyPi dependencies are pinned
        if: always()
        run: |
          (! git --no-pager grep --color=always -InP \
                '(pip|pip3|python -m pip|python3 -m pip|python3 -mpip|python -mpip) install ([a-z][\.a-z-0-9]*+(?!(=|.*\.whl))([[:blank:]]|))+' \
                -- .github \
                ':(exclude)**.rst' \
                ':(exclude)**.py' \
                ':(exclude)**.md' \
                ':(exclude)**.diff' \
                ':(exclude)third_party' ||
            (echo "The above lines have unpinned PyPi installs; please pin them to a specific version: e.g. 'thepackage==1.2'"; false))
      # note that this next step depends on a clean checkout;
      # if you run it locally then it will likely to complain
      # about all the generated files in torch/test
      - name: Ensure C++ source files are not executable
        if: always()
        run: |
          # shellcheck disable=SC2016
          (! find . \( -path ./third_party -o -path ./.git -o -path ./torch/bin -o -path ./build \) -prune -o -type f -executable -regextype posix-egrep -not -regex '.+(\.(bash|sh|py|so)|git-pre-commit|git-clang-format|gradlew)$' -print | grep . || (echo 'The above files have executable permission; please remove their executable permission by using `chmod -x`'; false))
      - name: C++ docs check
        if: ${{ always() && steps.requirements.outcome == 'success' }}
        run: |
          sudo apt-get install -y doxygen
          cd docs/cpp/source && ./check-doxygen.sh
      - name: CUDA kernel launch check
        if: ${{ always() && steps.requirements.outcome == 'success' }}
        run: |
          set -eux
          python torch/testing/_check_kernel_launches.py |& tee "${GITHUB_WORKSPACE}"/cuda_kernel_launch_checks.txt
      - name: Ensure no direct cub include
        if: always()
        run: |
          (! git --no-pager grep -I -no $'#include <cub/' --  ./aten  ':(exclude)aten/src/ATen/cuda/cub*.cuh' || (echo "The above files have direct cub include; please include ATen/cuda/cub.cuh instead and wrap your cub calls in at::native namespace if necessary"; false))
      - name: Ensure no raw cuda api calls
        if: always()
        run: |
          (! git --no-pager grep -I -no $'cudaStreamSynchronize' --  ./aten ./c10 ':(exclude)aten/src/ATen/test' ':(exclude)c10/cuda/CUDAFunctions.h' || (echo "The above files call raw cuda APIs directly; please use at::cuda wrappers instead"; false))
      - name: Ensure all test files have header containing ownership information
        if: always()
        run: |
          python3 -m pip install boto3==1.19.12
          .github/scripts/lint_test_ownership.py

  lint-clang-format:
    runs-on: ubuntu-18.04
    if: ${{ github.event_name == 'pull_request' }}
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x
          architecture: x64
      - name: Fetch PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          fetch-depth: 0 # deep clone, to allow us to use git merge-base
      - name: Run clang-format
        env:
          BASE_SHA: ${{ github.event.pull_request.base.sha }}
        run: |
          set -eu
          # This is necessary to get the same results regardless of whether the
          # PR was opened directly or from a forked repo. See: `9f890a92` for more info.
          git remote add upstream https://github.com/pytorch/pytorch
          git fetch upstream "$GITHUB_BASE_REF"

          # only run clang-format on allowlisted files
          echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
          echo "| clang-format failures found! Run: "
          echo "|    tools/linter/clang_format_ci.sh ${BASE_SHA} "
          echo "| to fix this error. "
          echo "| For more info, see: https://github.com/pytorch/pytorch/wiki/clang-format "
          echo "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"

          tools/linter/clang_format_ci.sh "${BASE_SHA}"

          GIT_DIFF=$(git diff)
          if [[ -z $GIT_DIFF ]]; then
            exit 0
          fi
          echo "$GIT_DIFF"
          exit 1

  lint-py2-setup-validate-errormsg:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 2.x
          architecture: x64
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Attempt to run setup.py
        run: |
          if ! python2 setup.py | grep -q "Python 2 has reached end-of-life and is no longer supported by PyTorch."; then
            echo 'Running setup.py with Python 2 did not give the expected error message.'
            false
          fi
      - name: Keep torch.utils.collect_env python2 compliant
        run: python2 -m py_compile torch/utils/collect_env.py

  lint-shellcheck:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x
          architecture: x64
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Install requirements
        id: requirements
        run: |
          pip3 install -r requirements.txt --user
      - name: Install Jinja2
        run: |
          pip3 install Jinja2==3.0.1 --user
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Regenerate workflows
        id: generate_workflows
        run: .github/scripts/generate_ci_workflows.py
      - name: Assert that regenerating the workflows didn't change them
        run: |
          if ! .github/scripts/report_git_status.sh .github/workflows; then
            echo
            echo 'As shown by the above diff, the committed .github/workflows'
            echo 'are not up to date according to .github/templates.'
            echo 'Please run this command, commit, and push again to your PR:'
            echo
            echo '    .github/scripts/generate_ci_workflows.py'
            echo
            echo 'If running that command does nothing, you may need to rebase'
            echo 'onto a more recent commit from the PyTorch master branch.'
            false
          fi
      - name: Install ShellCheck
        id: install_shellcheck
        if: always()
        # https://github.com/koalaman/shellcheck/tree/v0.7.2#installing-a-pre-compiled-binary
        run: |
          set -x
          scversion="v0.7.2"
          wget -qO- "https://github.com/koalaman/shellcheck/releases/download/${scversion?}/shellcheck-${scversion?}.linux.x86_64.tar.xz" | tar -xJv
          mkdir -p ~/.local/bin
          cp "shellcheck-${scversion}/shellcheck" ~/.local/bin/
          rm -r "shellcheck-${scversion}"
          ~/.local/bin/shellcheck --version
      - name: Extract scripts from GitHub Actions workflows
        if: ${{ always() && steps.install_shellcheck.outcome == 'success' }}
        run: |
          # For local lints, remove the .extracted_scripts folder if it was already there
          rm -rf .extracted_scripts
          tools/extract_scripts.py --out=.extracted_scripts
      - name: Run ShellCheck
        if: ${{ always() && steps.install_shellcheck.outcome == 'success' }}
        run: |
          if ! tools/linter/run_shellcheck.sh .extracted_scripts .jenkins/pytorch; then
            echo
            echo 'ShellCheck gave a nonzero exit code. Please fix the warnings'
            echo 'listed above. Note that if a path in one of the above warning'
            echo 'messages starts with .extracted_scripts/ then that means it'
            echo 'is referring to a shell script embedded within another file,'
            echo 'whose path is given by the path components immediately'
            echo 'following the .extracted_scripts/ prefix.'
            false
          fi
      - name: Check that jobs will be cancelled
        if: ${{ always() && steps.generate_workflows.outcome == 'success' }}
        run: |
          .github/scripts/ensure_actions_will_cancel.py
      - name: Run actionlint
        shell: bash
        run: |
          set -eux
          bash <(curl https://raw.githubusercontent.com/rhysd/actionlint/main/scripts/download-actionlint.bash)
          ./actionlint --color
          rm actionlint

  lint-toc:
    runs-on: ubuntu-18.04
    # https://github.com/actions/virtual-environments/issues/599#issuecomment-602754687
    env:
      NPM_CONFIG_PREFIX: ~/.npm-global
    steps:
      - name: Setup Node
        uses: actions/setup-node@v2
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Install markdown-toc
        run: npm install -g markdown-toc
      - name: Regenerate ToCs and check that they didn't change
        run: |
          set -eu
          export PATH=~/.npm-global/bin:"$PATH"
          for FILE in $(git grep -Il '<!-- toc -->' -- '**.md'); do
            markdown-toc --bullets='-' -i "$FILE"
          done

          if ! .github/scripts/report_git_status.sh .; then
            echo
            echo 'As shown by the above diff, the table of contents in one or'
            echo 'more Markdown files is not up to date with the file contents.'
            echo 'You can either apply that Git diff directly to correct the'
            echo 'table of contents, or if you have npm installed, you can'
            echo 'install the npm package markdown-toc and run the following'
            # shellcheck disable=SC2016
            echo 'command (replacing $FILE with the filename for which you want'
            echo 'to regenerate the table of contents):'
            echo
            # shellcheck disable=SC2016
            echo "    markdown-toc --bullets='-' -i \"\$FILE\""
            false
          fi

  lint-flake8-py3:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x
          architecture: x64
      - name: Fetch PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          fetch-depth: 2 # to allow us to use github.event.pull_request.head.sha
      - name: Prepare output dir with HEAD commit SHA
        env:
          HEAD_SHA: ${{ github.event.pull_request.head.sha }}
        run: |
          mkdir flake8-output
          cd flake8-output
          echo "$HEAD_SHA" > commit-sha.txt
      - name: Install dependencies
        run: |
          set -eux
          pip3 install typing-extensions==3.10 --user # for tools/linter/translate_annotations.py
          pip3 install -r requirements-flake8.txt --user
          flake8 --version
      - name: Run flake8
        run: |
          set -eux
          flake8 | tee "${GITHUB_WORKSPACE}"/flake8-output.txt
      - name: Translate annotations
        if: ${{ github.event_name == 'pull_request' }}
        env:
          HEAD_SHA: ${{ github.event.pull_request.head.sha }}
        run: |
          tools/linter/translate_annotations.py \
            --file="${GITHUB_WORKSPACE}"/flake8-output.txt \
            --regex='^(?P<filename>.*?):(?P<lineNumber>\d+):(?P<columnNumber>\d+): (?P<errorCode>\w+\d+) (?P<errorDesc>.*)' \
            --commit="$HEAD_SHA" \
            > flake8-output/annotations.json
      - name: Fail if there were any warnings
        run: |
          set -eu
          # Re-output flake8 status so GitHub logs show it on the step that actually failed
          cat "${GITHUB_WORKSPACE}"/flake8-output.txt
          if [ -s "${GITHUB_WORKSPACE}"/flake8-output.txt ]; then
            echo 'Please fix the above Flake8 warnings.'
            false
          fi
      - name: Add annotations
        # Don't run on forked pull requests
        if: ${{ failure() && github.event.pull_request.head.repo.full_name == github.repository }}
        uses: pytorch/add-annotations-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          check_name: 'flake8-py3'
          linter_output_path: flake8-output/annotations.json
          commit_sha: ${{ github.event.pull_request.head.sha }}
          mode: json

  lint-clang-tidy:
    runs-on: [self-hosted, linux.2xlarge]
    container:
      # ubuntu20.04-cuda11.2-py3.8-tidy11
      image: ghcr.io/pytorch/cilint-clang-tidy:d8f0c777964d0dd8a147360de80aed1a13eb613a
    steps:
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          fetch-depth: 0 # to allow tools/linter/clang_tidy.py to do its thing
      - name: Prepare output dir with HEAD commit SHA
        env:
          HEAD_SHA: ${{ github.event.pull_request.head.sha }}
        run: |
          cd "${GITHUB_WORKSPACE}"
          mkdir clang-tidy-output
          cd clang-tidy-output
          echo "$HEAD_SHA" > commit-sha.txt
      - name: Fetch PR diff
        if: ${{ github.event_name == 'pull_request' }}
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
        run: |
          cd "${GITHUB_WORKSPACE}"
          wget -O pr.diff "https://patch-diff.githubusercontent.com/raw/pytorch/pytorch/pull/$PR_NUMBER.diff"
      - name: Generate build files
        run: |
          cd "${GITHUB_WORKSPACE}"
          python3 -m tools.linter.clang_tidy.generate_build_files
      - name: Run PR clang-tidy
        if: ${{ github.event_name == 'pull_request' }}
        run: |
          cd "${GITHUB_WORKSPACE}"

          # The Docker image has our custom build, so we don't need to install it
          python3 -m tools.linter.clang_tidy \
            --clang-tidy-exe "$(which clang-tidy)" \
            --diff-file pr.diff \
            --disable-progress-bar 2>&1 | tee "${GITHUB_WORKSPACE}"/clang-tidy-output.txt

      # Run clang-tidy on a smaller subset of the codebase on master until we
      # make the repository clang-tidy clean
      - name: Run master clang-tidy
        run: |
          cd "${GITHUB_WORKSPACE}"

          python3 -m tools.linter.clang_tidy \
            --paths \
              torch/csrc/fx \
              torch/csrc/utils \
              torch/csrc/generic \
              torch/csrc/deploy \
              torch/csrc/tensor \
            --clang-tidy-exe "$(which clang-tidy)" \
            --disable-progress-bar 2>&1 | tee -a "${GITHUB_WORKSPACE}"/clang-tidy-output.txt

      - name: Annotate output
        if: ${{ github.event_name == 'pull_request' }}
        env:
          HEAD_SHA: ${{ github.event.pull_request.head.sha }}
        run: |
          cd "${GITHUB_WORKSPACE}"
          sed --in-place 's/^\.\.\///g' clang-tidy-output.txt
          tools/linter/translate_annotations.py \
            --file=clang-tidy-output.txt \
            --regex='^(?P<filename>.*?):(?P<lineNumber>\d+):(?P<columnNumber>\d+): (?P<errorDesc>.*?) \[(?P<errorCode>.*)\]' \
            --commit="$HEAD_SHA" \
            > clang-tidy-output/annotations.json
      - name: Check for warnings
        run: |
          cd "${GITHUB_WORKSPACE}"
          set -eu
          cat "${GITHUB_WORKSPACE}"/clang-tidy-output.txt
          if grep -Fq "Warnings detected!" "${GITHUB_WORKSPACE}"/clang-tidy-output.txt; then
            echo 'Please fix the above clang-tidy warnings.'
            false
          fi
      - name: Add annotations
        # Don't run on forked pull requests
        if: ${{ failure() && github.event.pull_request.head.repo.full_name == github.repository }}
        uses: pytorch/add-annotations-github-action@master
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        with:
          check_name: 'clang-tidy'
          linter_output_path: clang-tidy/annotations.json
          commit_sha: ${{ github.event.pull_request.head.sha }}
          mode: json

  lint-cmakelint:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.x
          architecture: x64
      - name: Fetch PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Install dependencies
        run: |
          set -eux
          pip3 install cmakelint==1.4.1 --user
          cmakelint --version
      - name: Run cmakelint
        run: |
          set -eux
          git ls-files -z -- bootstrap '*.cmake' '*.cmake.in' '*CMakeLists.txt' | \
          grep -E -z -v '^(cmake/Modules/|cmake/Modules_CUDA_fix/|cmake/Caffe2Config.cmake.in|aten/src/ATen/ATenConfig.cmake.in|cmake/Caffe2ConfigVersion.cmake.in|cmake/TorchConfig.cmake.in|cmake/TorchConfigVersion.cmake.in|cmake/cmake_uninstall.cmake.in)' | \
          xargs -0 cmakelint --config=.cmakelintrc --spaces=2 --quiet

  lint-mypy:
    runs-on: ubuntu-18.04
    steps:
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.8
          architecture: x64
      - name: Fetch PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
      - name: Install dependencies
        run: |
          set -eux
          python3 -mpip install -r requirements.txt --user
          python3 -mpip install numpy==1.20 --user # https://github.com/pytorch/pytorch/pull/60472
          python3 -mpip install expecttest==0.1.3 mypy==0.812 --user
          # Needed to check tools/render_junit.py
          python3 -mpip install junitparser==2.1.1 rich==10.9.0 --user
      - name: Run autogen
        run: |
          set -eux
          time python3 -mtools.generate_torch_version --is_debug=false
          time python3 -mtools.codegen.gen -s aten/src/ATen -d build/aten/src/ATen
          time python3 -mtools.pyi.gen_pyi --native-functions-path aten/src/ATen/native/native_functions.yaml --deprecated-functions-path "tools/autograd/deprecated.yaml"
      - name: Run mypy
        env:
          MYPY_FORCE_COLOR: 1
          TERM: xterm-color
        run: |
          set -eux
          STATUS=
          for CONFIG in mypy*.ini; do
            if ! python3 -mmypy --config="$CONFIG"; then
              STATUS=fail
            fi
          done
          if [ -n "$STATUS" ]; then
            echo 'Please fix the above mypy warnings.'
            false
          fi

  win-vs2019-cpu-py3-build:
    runs-on: [self-hosted, "windows.4xlarge"]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cpu-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "cpu"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      USE_CUDA: 0
      JOB_BASE_NAME: win-vs2019-cpu-py3-build
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
    steps:
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          .jenkins/pytorch/win-build.sh
      # Upload to github so that people can click and download artifacts
      - name: Upload artifacts to s3
        uses: seemethere/upload-artifact-s3@v3
        with:
          retention-days: 14
          if-no-files-found: error
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Cleanup build-results and workspaces
        if: always()
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf "${PYTORCH_FINAL_PACKAGE_DIR}"
          rm -rf ./*
  win-vs2019-cpu-py3-test_distributed_1_1:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cpu-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "cpu"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      USE_CUDA: 0
      JOB_BASE_NAME: win-vs2019-cpu-py3-test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      TEST_CONFIG: distributed
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cpu-py3-build]
    runs-on: [self-hosted, windows.4xlarge]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cpu-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  win-vs2019-cpu-py3-test_default_1_2:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cpu-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "cpu"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      USE_CUDA: 0
      JOB_BASE_NAME: win-vs2019-cpu-py3-test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      TEST_CONFIG: default
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cpu-py3-build]
    runs-on: [self-hosted, windows.4xlarge]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cpu-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  win-vs2019-cpu-py3-test_default_2_2:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cpu-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "cpu"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      USE_CUDA: 0
      JOB_BASE_NAME: win-vs2019-cpu-py3-test
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      TEST_CONFIG: default
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cpu-py3-build]
    runs-on: [self-hosted, windows.4xlarge]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cpu-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  win-vs2019-cuda11_3-py3-build:
    runs-on: [self-hosted, "windows.4xlarge"]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cuda11.3-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "11.3"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      TORCH_CUDA_ARCH_LIST: "7.0"
      USE_CUDA: 1
      JOB_BASE_NAME: win-vs2019-cuda11.3-py3-build
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
    steps:
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - name: Install Cuda
        shell: bash
        run: |
          .circleci/scripts/windows_cuda_install.sh
      - name: Install Cudnn
        shell: bash
        run: |
          .circleci/scripts/windows_cudnn_install.sh
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          .jenkins/pytorch/win-build.sh
      # Upload to github so that people can click and download artifacts
      - name: Upload artifacts to s3
        uses: seemethere/upload-artifact-s3@v3
        with:
          retention-days: 14
          if-no-files-found: error
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Cleanup build-results and workspaces
        if: always()
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf "${PYTORCH_FINAL_PACKAGE_DIR}"
          rm -rf ./*
  win-vs2019-cuda11_3-py3-test_force_on_cpu_1_1:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cuda11.3-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "11.3"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      TORCH_CUDA_ARCH_LIST: "7.0"
      USE_CUDA: 1
      JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      TEST_CONFIG: force_on_cpu
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cuda11_3-py3-build]
    runs-on: [self-hosted, windows.4xlarge]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-force_on_cpu-1-1-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-force_on_cpu-1-1-windows.4xlarge'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  win-vs2019-cuda11_3-py3-test_distributed_1_1:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cuda11.3-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "11.3"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      TORCH_CUDA_ARCH_LIST: "7.0"
      USE_CUDA: 1
      JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      TEST_CONFIG: distributed
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cuda11_3-py3-build]
    runs-on: [self-hosted, windows.8xlarge.nvidia.gpu]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - name: Install Cuda
        shell: bash
        run: |
          .circleci/scripts/windows_cuda_install.sh
      - name: Install Cudnn
        shell: bash
        run: |
          .circleci/scripts/windows_cudnn_install.sh
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-windows.8xlarge.nvidia.gpu'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-windows.8xlarge.nvidia.gpu'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  win-vs2019-cuda11_3-py3-test_smoke_tests_1_1:
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: win-vs2019-cuda11.3-py3
      BUILD_WHEEL: 1
      MAX_JOBS: 8
      CUDA_VERSION: "11.3"
      IN_CI: 1
      IS_GHA: 1
      INSTALL_WINDOWS_SDK: 1
      PYTHON_VERSION: "3.8"
      PYTORCH_RETRY_TEST_CASES: 1
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      SCCACHE_BUCKET: "ossci-compiler-cache"
      VC_PRODUCT: "BuildTools"
      VC_VERSION: ""
      VS_VERSION: "16.8.6"
      VC_YEAR: "2019"
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      no_proxy: localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      TORCH_CUDA_ARCH_LIST: "7.0"
      USE_CUDA: 1
      JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      TEST_CONFIG: smoke_tests
      http_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      https_proxy: "http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128"
      # TODO
    needs: [win-vs2019-cuda11_3-py3-build]
    runs-on: [self-hosted, windows.8xlarge.nvidia.gpu]
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Install Visual Studio 2019 toolchain
        shell: powershell
        run: |
          .\.circleci\scripts\vs_install.ps1
      - name: Install Cuda
        shell: bash
        run: |
          .circleci/scripts/windows_cuda_install.sh
      - name: Install Cudnn
        shell: bash
        run: |
          .circleci/scripts/windows_cudnn_install.sh
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          path: C:\${{ github.run_id }}\build-results
      - name: Check build-results folder
        shell: powershell
        run: |
          tree /F C:\$Env:GITHUB_RUN_ID\build-results
      # Needed for coverage in win-test.sh
      - uses: actions/setup-python@v2
        name: Setup Python3
        with:
          python-version: '3.x'
      - name: Test
        shell: bash
        env:
          PYTORCH_FINAL_PACKAGE_DIR: /c/${{ github.run_id }}/build-results/
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
            .jenkins/pytorch/win-test.sh
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-smoke_tests-1-1-windows.8xlarge.nvidia.gpu'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-jsons-$Env:FILE_SUFFIX.zip" -ir'!test\*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-smoke_tests-1-1-windows.8xlarge.nvidia.gpu'
        shell: powershell
        run: |
          # -ir => recursive include all files in pattern
          7z a "test-reports-$Env:FILE_SUFFIX.zip" -ir'!test\*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Wait until all sessions have drained
        shell: powershell
        if: always()
        timeout-minutes: 120
        run: |
          .github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        if: always()
        run: |
          .github\scripts\kill_active_ssh_sessions.ps1
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: win-vs2019-cuda11.3-py3-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Cleanup workspace
        if: always()
        shell: bash
        # Should remove the entirety of pytorch-${{ github.run_id }}
        run: |
          rm -rf ./*
  linux-xenial-py3_6-gcc5_4-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_jit_legacy_1_1:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: jit_legacy
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-jit_legacy-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-jit_legacy-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_distributed_1_1:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: distributed
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_docs_test_1_1:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: docs_test
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-docs_test-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-docs_test-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_backwards_compat_1_1:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: backwards_compat
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-backwards_compat-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-backwards_compat-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_default_1_2:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc5_4-test_default_2_2:
    needs: [linux-xenial-py3_6-gcc5_4-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc5.4
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc5_4-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc5.4-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-docs-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-docs
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-docs-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-docs-docs:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    strategy:
      matrix:
        docs_type: [cpp, python]
    needs: [linux-docs-build]
    env:
      AWS_DEFAULT_REGION: us-east-1
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      BUILD_ENVIRONMENT: linux-docs
      DOCKER_IMAGE: ${{ needs.linux-docs-build.outputs.docker_image }}
      DOCS_TYPE: ${{ matrix.docs_type }}
      WITH_PUSH: ${{ github.event_name == 'schedule' }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Build ${{ matrix.docs_type }} docs
        run: |
          set -ex
          time docker pull "${DOCKER_IMAGE}" > /dev/null
          echo "${GITHUB_REF}"
          # TODO: Set it correctly when workflows are scheduled on tags
          target="master"
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e IN_CI \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SHA1="$GITHUB_SHA" \
            -e DOCS_VERSION="${target}" \
            -e DOCS_TYPE \
            -e PR_LABELS \
            -e WITH_PUSH \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" bash -c "sudo chown -R jenkins . && pip install dist/*.whl && ./.circleci/scripts/${DOCS_TYPE}_doc_push_script.sh"
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - uses: seemethere/upload-artifact-s3@v3
        name: Upload Python Docs Preview
        if: ${{ github.event_name == 'pull_request' && matrix.docs_type == 'python' }}
        with:
          retention-days: 14
          s3-bucket: doc-previews
          if-no-files-found: error
          path: pytorch.github.io/docs/master/
          s3-prefix: pytorch/${{ github.event.pull_request.number }}
      - uses: seemethere/upload-artifact-s3@v3
        name: Upload C++ Docs Preview
        if: ${{ github.event_name == 'pull_request' && matrix.docs_type == 'cpp' }}
        with:
          retention-days: 14
          if-no-files-found: error
          s3-bucket: doc-previews
          path: cppdocs/
          s3-prefix: pytorch/${{ github.event.pull_request.number }}/cppdocs
  linux-xenial-py3_6-gcc7-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3.6-gcc7-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc7-test_distributed_1_1:
    needs: [linux-xenial-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
      TEST_CONFIG: distributed
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc7-test_default_1_2:
    needs: [linux-xenial-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-gcc7-test_default_2_2:
    needs: [linux-xenial-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3-clang5-mobile-build-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3-clang5-mobile-build
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-asan
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3-clang5-mobile-build-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3-clang5-mobile-custom-build-static-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3-clang5-mobile-custom-build-static
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3-clang5-mobile-custom-build-static-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-asan-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-asan
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-asan
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-asan-test_default_1_3:
    needs: [linux-xenial-py3_6-clang7-asan-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-asan
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-asan
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-clang7-asan-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 3
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-3-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-3-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-asan-test_default_2_3:
    needs: [linux-xenial-py3_6-clang7-asan-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-asan
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-asan
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-clang7-asan-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 3
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-3-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-3-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-asan-test_default_3_3:
    needs: [linux-xenial-py3_6-clang7-asan-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-asan
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-asan
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-clang7-asan-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
      TEST_CONFIG: default
      SHARD_NUMBER: 3
      NUM_TEST_SHARDS: 3
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-3-3-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-3-3-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-clang7-asan-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-onnx-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-onnx
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-onnx
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-onnx-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-onnx-test_default_1_2:
    needs: [linux-xenial-py3_6-clang7-onnx-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-onnx
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-onnx
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-clang7-onnx-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-onnx-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-clang7-onnx-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-py3_6-clang7-onnx-test_default_2_2:
    needs: [linux-xenial-py3_6-clang7-onnx-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-py3.6-clang7-onnx
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang7-onnx
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-py3_6-clang7-onnx-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-py3.6-clang7-onnx-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-py3.6-clang7-onnx-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-cuda11_3-py3_6-gcc7-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-cuda11.3-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda11.3-cudnn8-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-cuda11_3-py3_6-gcc7-test_distributed_1_1:
    needs: [linux-xenial-cuda11_3-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.8xlarge.nvidia.gpu]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-cuda11.3-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda11.3-cudnn8-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-cuda11_3-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
      TEST_CONFIG: distributed
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        run: |
          bash .github/scripts/install_nvidia_utils_linux.sh
          echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.8xlarge.nvidia.gpu'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-distributed-1-1-linux.8xlarge.nvidia.gpu'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-cuda11_3-py3_6-gcc7-test_default_1_2:
    needs: [linux-xenial-cuda11_3-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.4xlarge.nvidia.gpu]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-cuda11.3-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda11.3-cudnn8-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-cuda11_3-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        run: |
          bash .github/scripts/install_nvidia_utils_linux.sh
          echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-xenial-cuda11_3-py3_6-gcc7-test_default_2_2:
    needs: [linux-xenial-cuda11_3-py3_6-gcc7-build]
    runs-on: [self-hosted, linux.4xlarge.nvidia.gpu]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-xenial-cuda11.3-py3.6-gcc7
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda11.3-cudnn8-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-xenial-cuda11_3-py3_6-gcc7-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        run: |
          bash .github/scripts/install_nvidia_utils_linux.sh
          echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-bionic-py3_6-clang9-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-bionic-py3.6-clang9-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-bionic-py3_6-clang9-test_xla_1_1:
    needs: [linux-bionic-py3_6-clang9-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-bionic-py3_6-clang9-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
      TEST_CONFIG: xla
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-xla-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-xla-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-bionic-py3_6-clang9-test_noarch_1_1:
    needs: [linux-bionic-py3_6-clang9-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-bionic-py3_6-clang9-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
      TEST_CONFIG: noarch
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-noarch-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-noarch-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-bionic-py3_6-clang9-test_default_1_2:
    needs: [linux-bionic-py3_6-clang9-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-bionic-py3_6-clang9-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-bionic-py3_6-clang9-test_default_2_2:
    needs: [linux-bionic-py3_6-clang9-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-bionic-py3_6-clang9-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-bionic-py3.6-clang9-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  linux-vulkan-bionic-py3_6-clang9-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-vulkan-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-vulkan-bionic-py3.6-clang9-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  linux-vulkan-bionic-py3_6-clang9-test_default_1_1:
    needs: [linux-vulkan-bionic-py3_6-clang9-build]
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: linux-vulkan-bionic-py3.6-clang9
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-bionic-py3.6-clang9
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.linux-vulkan-bionic-py3_6-clang9-build.outputs.docker_image }}
      JOB_BASE_NAME: linux-vulkan-bionic-py3.6-clang9-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 1
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 240
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-1-linux.2xlarge'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-1-linux.2xlarge'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-vulkan-bionic-py3.6-clang9-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-build:
    runs-on: [self-hosted, linux.2xlarge]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck-build
    outputs:
      docker_image: ${{ steps.calculate-tag.outputs.docker_image }}
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Build
        env:
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e AWS_DEFAULT_REGION \
            -e IS_GHA \
            -e PR_NUMBER \
            -e SHA1 \
            -e BRANCH \
            -e GITHUB_RUN_ID \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && .jenkins/pytorch/build.sh'
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba || exit 0
      - name: Chown workspace
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Archive artifacts into zip
        run: |
          zip -1 -r artifacts.zip dist/ build/custom_test_artifacts build/lib build/bin .pytorch-test-times.json
      - uses: seemethere/upload-artifact-s3@v3
        name: Store PyTorch Build Artifacts on S3
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
          retention-days: 14
          if-no-files-found: error
          path:
            artifacts.zip
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Clean up docker images
        if: always()
        run: |
          # Prune all of the docker images
          docker system prune -af
  periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-test_default_1_2:
    needs: [periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-build]
    runs-on: [self-hosted, linux.4xlarge.nvidia.gpu]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-build.outputs.docker_image }}
      JOB_BASE_NAME: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck-test
      TEST_CONFIG: default
      SHARD_NUMBER: 1
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        run: |
          bash .github/scripts/install_nvidia_utils_linux.sh
          echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 360
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-1-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-test_default_2_2:
    needs: [periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-build]
    runs-on: [self-hosted, linux.4xlarge.nvidia.gpu]
    timeout-minutes: 240
    env:
      BUILD_ENVIRONMENT: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda10.2-cudnn7-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      DOCKER_IMAGE: ${{ needs.periodic-linux-xenial-cuda10_2-py3-gcc7-slow-gradcheck-build.outputs.docker_image }}
      JOB_BASE_NAME: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck-test
      TEST_CONFIG: default
      SHARD_NUMBER: 2
      NUM_TEST_SHARDS: 2
      # TODO
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Install nvidia driver, nvidia-docker runtime, set GPU_FLAG
        run: |
          bash .github/scripts/install_nvidia_utils_linux.sh
          echo "GPU_FLAG=--gpus all" >> "${GITHUB_ENV}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - uses: seemethere/download-artifact-s3@0504774707cbc8603d7dca922e8026eb8bf3b47b
        name: Download PyTorch Build Artifacts
        with:
          name: ${{ env.BUILD_ENVIRONMENT }}
      - name: Unzip artifacts
        run: |
          unzip -o artifacts.zip
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Test
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
        timeout-minutes: 360
        run: |
          set -x

          if [[ $TEST_CONFIG == 'multigpu' ]]; then
            TEST_COMMAND=.jenkins/pytorch/multigpu-test.sh
          elif [[ $BUILD_ENVIRONMENT == *onnx* ]]; then
            TEST_COMMAND=.jenkins/caffe2/test.sh
          else
            TEST_COMMAND=.jenkins/pytorch/test.sh
          fi
          PROXY_ENV=
          # NOTE: XLA multiprocessing tests appear to have issues with squid proxy, going to disable for now
          #       We should investigate whether or not there's a list of hostnames we can add to no_proxy to
          #       make it so that we shouldn't have to fully disable squid for XLA tests
          if [[ $TEST_CONFIG != 'xla' ]]; then
            # shellcheck disable=SC2089
            PROXY_ENV="-e http_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e https_proxy=http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128 -e no_proxy=localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock"
          fi
          # detached container should get cleaned up by teardown_ec2_linux
          # TODO: Stop building test binaries as part of the build phase
          # Used for GPU_FLAG since that doesn't play nice
          # shellcheck disable=SC2086,SC2090
          container_name=$(docker run \
            ${GPU_FLAG:-} \
            -e BUILD_ENVIRONMENT \
            -e PR_NUMBER \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e IS_GHA \
            -e BRANCH \
            -e SHA1 \
            -e AWS_DEFAULT_REGION \
            -e IN_WHEEL_TEST \
            -e SHARD_NUMBER \
            -e JOB_BASE_NAME \
            -e TEST_CONFIG \
            -e NUM_TEST_SHARDS \
            -e PYTORCH_IGNORE_DISABLED_ISSUES \
            -e PYTORCH_RETRY_TEST_CASES \
            -e PR_LABELS \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e XLA_CLANG_CACHE_S3_BUCKET_NAME \
            ${PROXY_ENV} \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --ulimit stack=10485760:83886080 \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --ipc=host \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --name="${container_name}" \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c "sudo chown -R jenkins . && pip install dist/*.whl && ${TEST_COMMAND}"
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Install render_test_results dependencies
        if: always()
        shell: bash
        run: |
          python3 -m pip install junitparser==2.1.1 rich==10.9.0
      - name: "[[ Click me for rendered test results (useful for finding failing tests) ]]"
        if: always()
        shell: bash
        # Encoding is weird on windows, just try to default to utf-8 if possible
        env:
          PYTHONIOENCODING: "utf-8"
        run: |
          python3 tools/render_junit.py test/
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: '${{ github.job }}-default-2-2-linux.4xlarge.nvidia.gpu'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: periodic-linux-xenial-cuda10.2-py3-gcc7-slow-gradcheck-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  # building and testing in a single job since bazel runs only small subset of tests
  linux-xenial-cuda11_3-py3_6-gcc7-bazel-test-build-and-test:
    runs-on: [self-hosted, linux.2xlarge]
    env:
      BUILD_ENVIRONMENT: linux-xenial-cuda11.3-py3.6-gcc7-bazel-test
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda11.3-cudnn8-py3-gcc7
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-bazel-test-build-and-test
      NUM_TEST_SHARDS: 1
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Build
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e PR_LABELS \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && sudo chown -R jenkins /dev && .jenkins/pytorch/build.sh'
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          # The artifact file is created inside docker container, which contains the result binaries.
          # Now unpackage it into the project folder. The subsequent script will scan project folder
          # to locate result binaries and report their sizes.
          # If artifact file is not provided it assumes that the project folder has been mounted in
          # the docker during build and already contains the result binaries, so this step can be skipped.
          export ARTIFACTS=
          if [ -n "${ARTIFACTS}" ]; then
            tar xf "${ARTIFACTS}" -C "${GITHUB_WORKSPACE}"
            cd "${GITHUB_WORKSPACE}"
          fi
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          ANDROID_BUILD_TYPE=
          export ANDROID_BUILD_TYPE
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba "android" || exit 0
      - name: Test
        # Time out the test phase after 3.5 hours
        timeout-minutes: 210
        run: |
          # detached container should get cleaned up by teardown_ec2_linux
          export SHARD_NUMBER=0
          # TODO: Stop building test binaries as part of the build phase
          # Make sure we copy test results from bazel-testlogs symlink to
          # a regular directory ./test/test-reports
          container_name=$(docker run \
            -e BUILD_ENVIRONMENT \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e GITHUB_ACTIONS \
            -e IN_CI \
            -e SHARD_NUMBER \
            -e NUM_TEST_SHARDS \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e PR_LABELS \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --shm-size="${SHM_SIZE}" \
            --tty \
            --detach \
            --user jenkins \
            -v "${GITHUB_WORKSPACE}:/var/lib/jenkins/workspace" \
            -w /var/lib/jenkins/workspace \
            "${DOCKER_IMAGE}"
          )
          docker exec -t "${container_name}" sh -c 'sudo chown -R jenkins . && sudo chown -R jenkins /dev && .jenkins/pytorch/test.sh && cp -Lr ./bazel-testlogs ./test/test-reports'
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Zip test reports for upload
        if: always()
        env:
          FILE_SUFFIX: 'bazel-${{ github.job }}'
        run: |
          # Remove any previous test reports if they exist
          rm -f test-reports-*.zip
          zip -r "test-reports-${FILE_SUFFIX}.zip" test -i '*.xml'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Reports on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: error
          path:
            test-reports-*.zip
      - name: Zip JSONs for upload
        if: always()
        env:
          FILE_SUFFIX: 'bazel-${{ github.job }}'
        run: |
          # Remove any previous test jsons if they exist
          rm -f test-jsons-*.zip
          zip -r "test-jsons-${FILE_SUFFIX}.zip" test -i '*.json'
      - uses: seemethere/upload-artifact-s3@v3
        name: Store Test Downloaded JSONs on S3
        if: always()
        with:
          retention-days: 14
          if-no-files-found: warn
          path:
            test-jsons-*.zip
      - name: Display and upload test statistics (Click Me)
        if: always()
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          JOB_BASE_NAME: linux-xenial-cuda11.3-py3.6-gcc7-bazel-test-test
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        shell: bash
        run: |
          python3 -m pip install -r requirements.txt
          python3 -m pip install boto3==1.19.12
          python3 -m tools.stats.print_test_stats --upload-to-s3 --compare-with-s3 test
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  # building and testing in a single job since bazel runs only small subset of tests
  pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-build-and-test:
    runs-on: [self-hosted, linux.2xlarge]
    env:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-build-and-test
      NUM_TEST_SHARDS: 1
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Build
        run: |
          set -e
          # Unlike other gradle jobs, it's not worth building libtorch in a separate CI job and share via docker, because:
          # 1) Not shareable: it's custom selective build, which is different from default libtorch mobile build;
          # 2) Not parallelizable by architecture: it only builds libtorch for one architecture;

          echo "DOCKER_IMAGE: ${DOCKER_IMAGE}"
          time docker pull "${DOCKER_IMAGE}" >/dev/null

          export BUILD_LITE_INTERPRETER
          BUILD_LITE_INTERPRETER="1"
          if [[ "${BUILD_ENVIRONMENT}" == *"full-jit" ]]; then
            BUILD_LITE_INTERPRETER="0"
          fi

          git submodule sync && git submodule update -q --init --recursive --depth 1 --jobs 0
          # shellcheck disable=SC2016
          export id
          id=$(docker run -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e PR_LABELS \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e BUILD_LITE_INTERPRETER \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "$(pwd):/var/lib/jenkins/workspace" \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            -t -d -w /var/lib/jenkins "${DOCKER_IMAGE}")

          # shellcheck disable=SC2016
          export COMMAND
          # shellcheck disable=SC2016
          COMMAND='((echo "export GRADLE_OFFLINE=1" && echo "export BUILD_LITE_INTERPRETER=${BUILD_LITE_INTERPRETER}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo "${COMMAND}" > ./command.sh && bash ./command.sh
          # Skip docker push as this job is purely for size analysis purpose.
          # Result binaries are already in `/home/circleci/project/` as it's mounted instead of copied.
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          # The artifact file is created inside docker container, which contains the result binaries.
          # Now unpackage it into the project folder. The subsequent script will scan project folder
          # to locate result binaries and report their sizes.
          # If artifact file is not provided it assumes that the project folder has been mounted in
          # the docker during build and already contains the result binaries, so this step can be skipped.
          export ARTIFACTS=
          if [ -n "${ARTIFACTS}" ]; then
            tar xf "${ARTIFACTS}" -C "${GITHUB_WORKSPACE}"
            cd "${GITHUB_WORKSPACE}"
          fi
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          ANDROID_BUILD_TYPE=custom-build-single
          export ANDROID_BUILD_TYPE
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba "android" || exit 0
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
  # building and testing in a single job since bazel runs only small subset of tests
  pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit-build-and-test:
    runs-on: [self-hosted, linux.2xlarge]
    env:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit
      DOCKER_IMAGE_BASE: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
      JOB_BASE_NAME: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single-full-jit-build-and-test
      NUM_TEST_SHARDS: 1
    steps:
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
      - name: Log in to ECR
        env:
          AWS_RETRY_MODE: standard
          AWS_MAX_ATTEMPTS: 5
        run: |
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity|grep Account|cut -f4 -d\")
          aws ecr get-login-password --region "$AWS_DEFAULT_REGION" | docker login --username AWS \
              --password-stdin "$AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com"
      - name: Chown workspace
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${ALPINE_IMAGE}"
          # Ensure the working directory gets chowned back to the current user
          docker run --pull=never --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Clean workspace
        run: |
          rm -rf "${GITHUB_WORKSPACE:?}/*"
          rm -f ~/.ssh/authorized_keys
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: seemethere/add-github-ssh-key@v1
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Checkout PyTorch
        uses: zhouzhuojie/checkout@05b13c9a0d21f08f6d5e64a1d5042246d13619d9
        with:
          # deep clone, to allow use of git merge-base
          fetch-depth: 0
          submodules: recursive
      - name: Calculate docker image tag
        id: calculate-tag
        run: |
          DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
          echo "DOCKER_TAG=${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "DOCKER_IMAGE=${DOCKER_IMAGE_BASE}:${DOCKER_TAG}" >> "${GITHUB_ENV}"
          echo "::set-output name=docker_tag::${DOCKER_TAG}"
          echo "::set-output name=docker_image::${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"
      - name: Check if image should be built
        id: check
        env:
          BASE_REVISION: ${{ github.event.pull_request.base.sha || github.sha }}
        run: |
          set -x
          # Check if image already exists, if it does then skip building it
          if docker manifest inspect "${DOCKER_IMAGE_BASE}:${DOCKER_TAG}"; then
            exit 0
          fi
          if [[ "$BASE_REVISION" = "$(git rev-parse HEAD)" ]]; then
            # if we're on the base branch then use the parent commit
            MERGE_BASE=$(git rev-parse HEAD~)
          else
            # otherwise we're on a PR, so use the most recent base commit
            MERGE_BASE=$(git merge-base HEAD "$BASE_REVISION")
          fi
          # Covers the case where a previous tag doesn't exist for the tree
          # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
          if ! git rev-parse "$MERGE_BASE:.circleci/docker"; then
            echo "Directory '.circleci/docker' not found in commit $MERGE_BASE, you should probably rebase onto a more recent commit"
            exit 1
          fi
          PREVIOUS_DOCKER_TAG=$(git rev-parse "$MERGE_BASE:.circleci/docker")
          # If no image exists but the hash is the same as the previous hash then we should error out here
          if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
            echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
            echo "       contact the PyTorch team to restore the original images"
            exit 1
          fi
          echo ::set-output name=rebuild::yes
      - name: Build and push docker image
        if: ${{ steps.check.outputs.rebuild }}
        env:
          DOCKER_SKIP_S3_UPLOAD: 1
        working-directory: .circleci/docker
        run: |
          export IMAGE_NAME=${DOCKER_IMAGE_BASE#308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/}
          ./build_docker.sh
      - name: Pull Docker image
        run: |
          retry () {
              "$@"  || (sleep 1 && "$@") || (sleep 2 && "$@")
          }
          retry docker pull "${DOCKER_IMAGE}"
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Build
        run: |
          set -e
          # Unlike other gradle jobs, it's not worth building libtorch in a separate CI job and share via docker, because:
          # 1) Not shareable: it's custom selective build, which is different from default libtorch mobile build;
          # 2) Not parallelizable by architecture: it only builds libtorch for one architecture;

          echo "DOCKER_IMAGE: ${DOCKER_IMAGE}"
          time docker pull "${DOCKER_IMAGE}" >/dev/null

          export BUILD_LITE_INTERPRETER
          BUILD_LITE_INTERPRETER="1"
          if [[ "${BUILD_ENVIRONMENT}" == *"full-jit" ]]; then
            BUILD_LITE_INTERPRETER="0"
          fi

          git submodule sync && git submodule update -q --init --recursive --depth 1 --jobs 0
          # shellcheck disable=SC2016
          export id
          id=$(docker run -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e PR_LABELS \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e BUILD_LITE_INTERPRETER \
            -e http_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e https_proxy="http://internal-tf-lb-20210727220640487900000002-835786077.us-east-1.elb.amazonaws.com:3128" -e no_proxy="localhost,127.0.0.1,github.com,amazonaws.com,s3.amazonaws.com,169.254.169.254,169.254.170.2,/var/run/docker.sock" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "$(pwd):/var/lib/jenkins/workspace" \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            -t -d -w /var/lib/jenkins "${DOCKER_IMAGE}")

          # shellcheck disable=SC2016
          export COMMAND
          # shellcheck disable=SC2016
          COMMAND='((echo "export GRADLE_OFFLINE=1" && echo "export BUILD_LITE_INTERPRETER=${BUILD_LITE_INTERPRETER}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo "${COMMAND}" > ./command.sh && bash ./command.sh
          # Skip docker push as this job is purely for size analysis purpose.
          # Result binaries are already in `/home/circleci/project/` as it's mounted instead of copied.
      - name: Parse ref
        id: parse-ref
        run: .github/scripts/parse_ref.py
      - name: Display and upload binary build size statistics (Click Me)
        # temporary hack: set CIRCLE_* vars, until we update
        # tools/stats/print_test_stats.py to natively support GitHub Actions
        env:
          AWS_DEFAULT_REGION: us-east-1
          SCRIBE_GRAPHQL_ACCESS_TOKEN: ${{ secrets.SCRIBE_GRAPHQL_ACCESS_TOKEN }}
          BRANCH: ${{ steps.parse-ref.outputs.branch }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
          TAG: ${{ steps.parse-ref.outputs.tag }}
          WORKFLOW_ID: '${{ github.run_id }}_${{ github.run_number }}'
        run: |
          # The artifact file is created inside docker container, which contains the result binaries.
          # Now unpackage it into the project folder. The subsequent script will scan project folder
          # to locate result binaries and report their sizes.
          # If artifact file is not provided it assumes that the project folder has been mounted in
          # the docker during build and already contains the result binaries, so this step can be skipped.
          export ARTIFACTS=
          if [ -n "${ARTIFACTS}" ]; then
            tar xf "${ARTIFACTS}" -C "${GITHUB_WORKSPACE}"
            cd "${GITHUB_WORKSPACE}"
          fi
          COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
          export COMMIT_TIME
          ANDROID_BUILD_TYPE=custom-build-single
          export ANDROID_BUILD_TYPE
          pip3 install requests==2.26 boto3==1.16.34
          python3 -m tools.stats.upload_binary_size_to_scuba "android" || exit 0
      - name: Hold runner for 2 hours or until ssh sessions have drained
        # Always hold for active ssh sessions
        if: always()
        run: .github/scripts/wait_for_ssh_to_drain.sh
      - name: Chown workspace
        if: always()
        run: |
          # Ensure the working directory gets chowned back to the current user
          docker run --rm -v "$(pwd)":/v -w /v "${ALPINE_IMAGE}" chown -R "$(id -u):$(id -g)" .
      - name: Kill containers, clean up images
        if: always()
        run: |
          # ignore expansion of "docker ps -q" since it could be empty
          # shellcheck disable=SC2046
          docker stop $(docker ps -q) || true
          # Prune all of the docker images
          docker system prune -af
