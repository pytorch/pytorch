name: Build Flash Attention 3 wheels

on:
  workflow_dispatch:
  # TODO: Remove this trigger before merging - only for testing from PR branch
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.sha }}-${{ github.event_name == 'workflow_dispatch' }}
  cancel-in-progress: true

jobs:
  get-label-type:
    if: github.repository_owner == 'pytorch'
    name: get-label-type
    uses: pytorch/pytorch/.github/workflows/_runner-determinator.yml@main
    with:
      triggering_actor: ${{ github.triggering_actor }}
      issue_owner: ${{ github.event.pull_request.user.login || github.event.issue.user.login }}
      curr_branch: ${{ github.head_ref || github.ref_name }}
      curr_ref_type: ${{ github.ref_type }}

  build-wheel:
    name: "Build FA3 ${{ matrix.cuda_version }} ${{ matrix.arch }}"
    needs: get-label-type
    runs-on: "${{ matrix.runner }}"
    strategy:
      fail-fast: false
      matrix:
        include:
          - cuda_version: "12.6"
            arch: "x86_64"
            cuda_short: "126"
            torch_cuda_arch_list: "8.0;8.6;9.0"
            runner: "windows.12xlarge"
          # - cuda_version: "13.0"
          #   arch: "x86_64"
          #   cuda_short: "130"
          #   torch_cuda_arch_list: "8.0;8.6;9.0;10.0"
          #   runner: "windows.12xlarge"
          # - cuda_version: "12.6"
          #   arch: "aarch64"
          #   cuda_short: "126"
          #   torch_cuda_arch_list: "8.0;8.6;9.0"
          #   runner: "windows-11-arm64-preview"
          # - cuda_version: "13.0"
          #   arch: "aarch64"
          #   cuda_short: "130"
          #   torch_cuda_arch_list: "8.0;8.6;9.0;10.0"
          #   runner: "windows-11-arm64-preview"
    env:
      CUDA_VERSION: ${{ matrix.cuda_version }}
      CUDA_SHORT: ${{ matrix.cuda_short }}
      MAX_JOBS: 6 # (FA3 is memory hungry!)
      DISTUTILS_USE_SDK: 1 # otherwise distutils will complain on windows about multiple versions of msvc
      SCCACHE_IDLE_TIMEOUT: "0"
      PY: python3
      TORCH_CUDA_ARCH_LIST: "8.0+PTX 8.6 9.0a"

    steps:
      - name: Populate binary env
        shell: bash
        run: |
          echo "BINARY_ENV_FILE=${RUNNER_TEMP}/env" >> "${GITHUB_ENV}"
          echo "PYTORCH_FINAL_PACKAGE_DIR=${RUNNER_TEMP}/artifacts" >> "${GITHUB_ENV}"
          echo "WIN_PACKAGE_WORK_DIR=${RUNNER_TEMP}"
      - name: Display EC2 information
        shell: bash
        run: |
          set -euo pipefail
          function get_ec2_metadata() {
            # Pulled from instance metadata endpoint for EC2
            # see https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/instancedata-data-retrieval.html
            category=$1
            curl -H "X-aws-ec2-metadata-token: $(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 30")" -fsSL "http://169.254.169.254/latest/meta-data/${category}"
          }
          echo "ami-id: $(get_ec2_metadata ami-id)"
          echo "instance-id: $(get_ec2_metadata instance-id)"
          echo "instance-type: $(get_ec2_metadata instance-type)"
          echo "system info $(uname -a)"
      - name: "[FB EMPLOYEES] Enable SSH (Click me for login details)"
        uses: pytorch/test-infra/.github/actions/setup-ssh@main
        continue-on-error: true
        with:
          github-secret: ${{ secrets.GITHUB_TOKEN }}
      - name: Enable git long paths and symlinks on Windows and disable fsmonitor daemon
        shell: bash
        run: |
          git config --global core.longpaths true
          git config --global core.symlinks true

          # https://git-scm.com/docs/git-fsmonitor--daemon.  The daemon could lock
          # the directory on Windows and prevent GHA from checking out as reported
          # in https://github.com/actions/checkout/issues/1018
          git config --global core.fsmonitor false
      # Needed for binary builds, see: https://github.com/pytorch/pytorch/issues/73339#issuecomment-1058981560
      - name: Enable long paths on Windows
        shell: powershell
        run: |
          Set-ItemProperty -Path "HKLM:\\SYSTEM\CurrentControlSet\Control\FileSystem" -Name "LongPathsEnabled" -Value 1
      # Since it's just a defensive command, the workflow should continue even the command fails. This step can be
      # removed once Windows Defender is removed from the AMI
      - name: Disables Windows Defender scheduled and real-time scanning for files in directories used by PyTorch
        continue-on-error: true
        shell: powershell
        run: |
          Add-MpPreference -ExclusionPath $(Get-Location).tostring(),$Env:TEMP -ErrorAction Ignore
          # Let's both exclude the path and disable Windows Defender completely just to be sure
          # that it doesn't interfere
          Set-MpPreference -DisableRealtimeMonitoring $True -ErrorAction Ignore
      - name: Checkout PyTorch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event_name == 'pull_request' && github.event.pull_request.head.sha || github.sha }}
          # submodules: recursive
          path: pytorch
          show-progress: false
      - name: Checkout Flash-Attention
        uses: actions/checkout@v4
        with:
          repository: Dao-AILab/flash-attention
          ref: bc0e4ac01484ffb61ddc694724826bec4d9cf1c2
          submodules: recursive
          path: flash-attention
          show-progress: false
      - name: Clean Flash-Attention checkout
        run: |
          # Remove any artifacts from the previous checkouts
          git clean -fxd
        working-directory: flash-attention
      # - name: Populate binary env
      #   shell: bash
      #   run: |
      #     "${PYTORCH_ROOT}/.circleci/scripts/binary_populate_env.sh"

      - name: volume info
        shell: powershell
        run: Get-Volume

      - name: Print pagefile size
        shell: powershell
        run: |
          systeminfo | find "Virtual Memory"

      - name: Setup Runner
        uses: ./pytorch/.github/actions/setup-build-cuda
        with:
          toolkit_type: cuda
          toolkit_short_version: ${{ matrix.cuda_short }}
          python: "3.10"

      # - name: Setup sccache
      #   uses: mozilla-actions/sccache-action@v0.0.9

      - name: Install corresponding PyTorch
        shell: bash -l {0}
        run: |
          pip install torch==2.9.0 --index-url https://download.pytorch.org/whl/cu${CUDA_SHORT}
          pip install packaging ninja wheel setuptools twine numpy

      - name: Build wheel
        shell: bash -l {0}
        run: |
          CUDA_HOME_NORMALIZED="${CUDA_HOME//\\//}"
          # export PYTORCH_NVCC="sccache \"${CUDA_HOME_NORMALIZED}/bin/nvcc\""
          cd flash-attention/hopper
          df -h
          $PY setup.py bdist_wheel -d dist/ -k $PLAT_ARG
          df -h
        env:
          PLAT_ARG: ''
          # sccache configuration for PyTorch extensions
          USE_SCCACHE: 1
          SCCACHE_BUCKET: ossci-compiler-cache
          SCCACHE_IGNORE_SERVER_IO_ERROR: 1
          SCCACHE_GHA_ENABLED: "true"
          TORCH_EXTENSION_SKIP_NVCC_GEN_DEPENDENCIES: "1"
          FLASH_ATTENTION_FORCE_BUILD: "TRUE"
          # FLASH_ATTENTION_DISABLE_SPLIT: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_PAGEDKV: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_APPENDKV: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_LOCAL: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_SOFTCAP: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_PACKGQA: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_FP16: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_FP8: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_VARLEN: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_CLUSTER: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIM64: "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIM96: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIM128: "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIM192: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIM256: "FALSE"
          # FLASH_ATTENTION_DISABLE_SM80: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_ENABLE_VCOLMAJOR: "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIMDIFF64: "TRUE"  # "FALSE"
          # FLASH_ATTENTION_DISABLE_HDIMDIFF192: "TRUE"  # "FALSE"

      # - name: Show sccache stats
      #   run: ${SCCACHE_PATH} --show-stats

      - run: du -h flash-attention/hopper/dist/*.whl
      - uses: actions/upload-artifact@v4.4.0
        if: always()
        with:
          name: wheel-py3_10-cuda${{ matrix.cuda_short }}
          retention-days: 14
          if-no-files-found: error
          path: flash-attention/hopper/dist/*.whl
      - name: Wait until all sessions have drained
        shell: powershell
        working-directory: flash-attention
        if: always()
        timeout-minutes: 120
        run: |
          pytorch\.github\scripts\wait_for_ssh_to_drain.ps1
      - name: Kill active ssh sessions if still around (Useful if workflow was cancelled)
        shell: powershell
        working-directory: flash-attention
        if: always()
        run: |
          pytorch\.github\scripts\kill_active_ssh_sessions.ps1
