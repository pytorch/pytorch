  # building and testing in a single job since bazel runs only small subset of tests
  !{{ job.normalized_build_environment("-build-and-test") }}:
    runs-on: [self-hosted, !{{ job.test_runner_type }}]
    env:
      BUILD_ENVIRONMENT: !{{ job.build_environment }}
      DOCKER_IMAGE_BASE: !{{ job.docker_image_base }}
      SCCACHE_BUCKET: ossci-compiler-cache-circleci-v2
      XLA_CLANG_CACHE_S3_BUCKET_NAME: ossci-compiler-clang-cache-circleci-xla
      TORCH_CUDA_ARCH_LIST: 5.2
      IN_CI: 1
      IS_GHA: 1
      # This is used for the phase of adding wheel tests only, will be removed once completed
      IN_WHEEL_TEST: 1
      # Used for custom_opertor, jit_hooks, custom_backend, see .jenkins/pytorch/build.sh
      CUSTOM_TEST_ARTIFACT_BUILD_DIR: build/custom_test_artifacts
      ALPINE_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/tool/alpine"
      PR_LABELS: ${{ toJson(github.event.pull_request.labels.*.name) }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
      AWS_DEFAULT_REGION: us-east-1
      PR_NUMBER: ${{ github.event.pull_request.number }}
      SHA1: ${{ github.event.pull_request.head.sha || github.sha }}
      PYTORCH_RETRY_TEST_CASES: 1
    {%- if job.build_with_debug %}
      DEBUG: 1
    {%- endif %}
      JOB_BASE_NAME: !{{ job.build_environment }}-build-and-test
      NUM_TEST_SHARDS: !{{ job.num_test_shards }}
    steps:
      !{{ common.setup_ec2_linux() }}
      !{{ common.checkout_pytorch("recursive") }}
      !{{ common.calculate_docker_image(false) }}
      - name: Pull Docker image
        run: |
          !{{ common.pull_docker("${DOCKER_IMAGE}") }}
      - name: Determine shm-size
        run: |
          shm_size="1g"
          case "${BUILD_ENVIRONMENT}" in
            *cuda*)
              shm_size="2g"
              ;;
            *rocm*)
              shm_size="8g"
              ;;
          esac
          echo "SHM_SIZE=${shm_size}" >> "${GITHUB_ENV}"
      - name: Output disk space left
        run: |
          sudo df -H
      - name: Preserve github env variables for use in docker
        run: |
          env | grep '^GITHUB' > "/tmp/github_env_${GITHUB_RUN_ID}"
      - name: Build
        run: |
          set -e
          # Unlike other gradle jobs, it's not worth building libtorch in a separate CI job and share via docker, because:
          # 1) Not shareable: it's custom selective build, which is different from default libtorch mobile build;
          # 2) Not parallelizable by architecture: it only builds libtorch for one architecture;

          echo "DOCKER_IMAGE: ${DOCKER_IMAGE}"
          time docker pull "${DOCKER_IMAGE}" >/dev/null

          export BUILD_LITE_INTERPRETER
          BUILD_LITE_INTERPRETER="1"
          if [[ "${BUILD_ENVIRONMENT}" == *"full-jit" ]]; then
            BUILD_LITE_INTERPRETER="0"
          fi

          git submodule sync && git submodule update -q --init --recursive --depth 1 --jobs 0
          # shellcheck disable=SC2016
          export id
          id=$(docker run -e BUILD_ENVIRONMENT \
            -e JOB_BASE_NAME \
            -e MAX_JOBS="$(nproc --ignore=2)" \
            -e SCCACHE_BUCKET \
            -e CUSTOM_TEST_ARTIFACT_BUILD_DIR \
            -e PR_LABELS \
            -e SKIP_SCCACHE_INITIALIZATION=1 \
            -e TORCH_CUDA_ARCH_LIST \
            -e BUILD_LITE_INTERPRETER \
            -e http_proxy="!{{ common.squid_proxy }}" -e https_proxy="!{{ common.squid_proxy }}" -e no_proxy="!{{ common.squid_no_proxy }}" \
            --env-file="/tmp/github_env_${GITHUB_RUN_ID}" \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --tty \
            --detach \
            --user jenkins \
            -v "$(pwd):/var/lib/jenkins/workspace" \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            --cap-add=SYS_PTRACE \
            --security-opt seccomp=unconfined \
            -t -d -w /var/lib/jenkins "${DOCKER_IMAGE}")

          # shellcheck disable=SC2016
          export COMMAND
          # shellcheck disable=SC2016
          COMMAND='((echo "export GRADLE_OFFLINE=1" && echo "export BUILD_LITE_INTERPRETER=${BUILD_LITE_INTERPRETER}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo "${COMMAND}" > ./command.sh && bash ./command.sh
          # Skip docker push as this job is purely for size analysis purpose.
          # Result binaries are already in `/home/circleci/project/` as it's mounted instead of copied.
      !{{ common.parse_ref() }}
      !{{ common.upload_android_binary_size("custom-build-single", "") }}
      !{{ common.teardown_ec2_linux() }}
