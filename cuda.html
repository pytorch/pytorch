


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/cuda.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Automatic Mixed Precision package - torch.cuda.amp" href="amp.html" />
    <link rel="prev" title="Automatic differentiation package - torch.autograd" href="autograd.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/cuda.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="autograd.html">torch.autograd</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>torch.cuda</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="_sources/cuda.rst.txt" rel="nofollow"><img src="_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="module-torch.cuda">
<span id="torch-cuda"></span><h1>torch.cuda<a class="headerlink" href="#module-torch.cuda" title="Permalink to this headline">¶</a></h1>
<p>This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.</p>
<p>It is lazily initialized, so you can always import it, and use
<a class="reference internal" href="#torch.cuda.is_available" title="torch.cuda.is_available"><code class="xref py py-func docutils literal notranslate"><span class="pre">is_available()</span></code></a> to determine if your system supports CUDA.</p>
<p><a class="reference internal" href="notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p>
<dl class="function">
<dt id="torch.cuda.current_blas_handle">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">current_blas_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#current_blas_handle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_blas_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns cublasHandle_t pointer to current cuBLAS handle</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_device">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">current_device</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda.html#current_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the index of a currently selected device.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.current_stream">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">current_stream</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">int</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span> &#x2192; torch.cuda.streams.Stream<a class="reference internal" href="_modules/torch/cuda.html#current_stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.current_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for the current device, given
by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.default_stream">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">default_stream</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">int</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span> &#x2192; torch.cuda.streams.Stream<a class="reference internal" href="_modules/torch/cuda.html#default_stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.default_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code class="xref py py-class docutils literal notranslate"><span class="pre">Stream</span></code></a> for the current device, given by
<a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
</dl>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device">
<em class="property">class </em><code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">device</code><span class="sig-paren">(</span><em class="sig-param">device</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the selected device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – device index to select. It’s a no-op if
this argument is a negative integer or <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.device_count">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">device_count</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda.html#device_count"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device_count" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the number of GPUs available.</p>
</dd></dl>

<dl class="class">
<dt id="torch.cuda.device_of">
<em class="property">class </em><code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">device_of</code><span class="sig-paren">(</span><em class="sig-param">obj</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#device_of"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.device_of" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that changes the current device to that of given object.</p>
<p>You can use both tensors and storages as arguments. If a given object is
not allocated on a GPU, this is a no-op.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>obj</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em> or </em><em>Storage</em>) – object allocated on the selected device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_arch_list">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_arch_list</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List[str]<a class="reference internal" href="_modules/torch/cuda.html#get_arch_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_arch_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns list CUDA architectures this library was compiled for.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_capability">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_device_capability</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">int</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span> &#x2192; Tuple[int, int]<a class="reference internal" href="_modules/torch/cuda.html#get_device_capability"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_device_capability" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the cuda capability of a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
device capability. This function is a no-op if this argument is
a negative integer. It uses the current device, given by
<a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code>
(default).</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the major and minor cuda capability of the device</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.8)">tuple</a>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)">int</a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)">int</a>)</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_device_name">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_device_name</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">int</em>, <em class="sig-param">None] = None</em><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="_modules/torch/cuda.html#get_device_name"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_device_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the name of a device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to return the
name. This function is a no-op if this argument is a negative
integer. It uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_gencode_flags">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_gencode_flags</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="_modules/torch/cuda.html#get_gencode_flags"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_gencode_flags" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns NVCC gencode flags this library were compiled with.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.init">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">init</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#init"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.init" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize PyTorch’s CUDA state.  You may need to call
this explicitly if you are interacting with PyTorch via
its C API, as Python bindings for CUDA functionality will not
be available until this initialization takes place.  Ordinary users
should not need this, as all of PyTorch’s CUDA methods
automatically initialize CUDA state on-demand.</p>
<p>Does nothing if the CUDA state is already initialized.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.ipc_collect">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">ipc_collect</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#ipc_collect"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.ipc_collect" title="Permalink to this definition">¶</a></dt>
<dd><p>Force collects GPU memory after it has been released by CUDA IPC.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Checks if any sent CUDA tensors could be cleaned from the memory. Force
closes shared memory file used for reference counting if there is no
active counters. Useful when the producer process stopped actively sending
tensors and want to release unused memory.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.is_available">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">is_available</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; bool<a class="reference internal" href="_modules/torch/cuda.html#is_available"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.is_available" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a bool indicating if CUDA is currently available.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.is_initialized">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">is_initialized</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#is_initialized"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.is_initialized" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns whether PyTorch’s CUDA state has been initialized.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_device">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">set_device</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device, str, int]</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda.html#set_device"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.set_device" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the current device.</p>
<p>Usage of this function is discouraged in favor of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref any py py-class docutils literal notranslate"><span class="pre">device</span></code></a>. In most
cases it’s better to use <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> environmental variable.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – selected device. This function is a no-op
if this argument is negative.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.stream">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">stream</code><span class="sig-paren">(</span><em class="sig-param">stream</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda.html#stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Context-manager that selects a given stream.</p>
<p>All CUDA kernels queued within its context will be enqueued on a selected
stream.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – selected stream. This manager is a no-op if it’s
<code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Streams are per-device. If the selected stream is not on the
current device, this function will also change the current device to
match the stream.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.synchronize">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">synchronize</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda.html#synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for all kernels in all streams on a CUDA device to complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – device for which to synchronize.
It uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
</dd></dl>

<div class="section" id="random-number-generator">
<h2>Random Number Generator<a class="headerlink" href="#random-number-generator" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.get_rng_state">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_rng_state</code><span class="sig-paren">(</span><em class="sig-param">device: Union[int</em>, <em class="sig-param">str</em>, <em class="sig-param">torch.device] = 'cuda'</em><span class="sig-paren">)</span> &#x2192; torch.Tensor<a class="reference internal" href="_modules/torch/cuda/random.html#get_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the random number generator state of the specified GPU as a ByteTensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – The device to return the RNG state of.
Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">torch.device('cuda')</span></code>, the current CUDA device).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.get_rng_state_all">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">get_rng_state_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List[torch.Tensor]<a class="reference internal" href="_modules/torch/cuda/random.html#get_rng_state_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.get_rng_state_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of ByteTensor representing the random number states of all devices.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_rng_state">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">set_rng_state</code><span class="sig-paren">(</span><em class="sig-param">new_state: torch.Tensor</em>, <em class="sig-param">device: Union[int</em>, <em class="sig-param">str</em>, <em class="sig-param">torch.device] = 'cuda'</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#set_rng_state"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.set_rng_state" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state of the specified GPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>new_state</strong> (<em>torch.ByteTensor</em>) – The desired state</p></li>
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – The device to set the RNG state.
Default: <code class="docutils literal notranslate"><span class="pre">'cuda'</span></code> (i.e., <code class="docutils literal notranslate"><span class="pre">torch.device('cuda')</span></code>, the current CUDA device).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.set_rng_state_all">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">set_rng_state_all</code><span class="sig-paren">(</span><em class="sig-param">new_states: Iterable[torch.Tensor]</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#set_rng_state_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.set_rng_state_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the random number generator state of all devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>new_states</strong> (<em>Iterable of torch.ByteTensor</em>) – The desired state for each device</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">manual_seed</code><span class="sig-paren">(</span><em class="sig-param">seed: int</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#manual_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.manual_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The desired seed.</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are working with a multi-GPU model, this function is insufficient
to get determinism.  To seed all GPUs, use <a class="reference internal" href="#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">manual_seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.manual_seed_all">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">manual_seed_all</code><span class="sig-paren">(</span><em class="sig-param">seed: int</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#manual_seed_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.manual_seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – The desired seed.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number for the current GPU.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you are working with a multi-GPU model, this function will only initialize
the seed on one GPU.  To initialize all GPUs, use <a class="reference internal" href="#torch.cuda.seed_all" title="torch.cuda.seed_all"><code class="xref py py-func docutils literal notranslate"><span class="pre">seed_all()</span></code></a>.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.seed_all">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">seed_all</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/random.html#seed_all"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.seed_all" title="Permalink to this definition">¶</a></dt>
<dd><p>Sets the seed for generating random numbers to a random number on all GPUs.
It’s safe to call this function if CUDA is not available; in that
case, it is silently ignored.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.initial_seed">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">initial_seed</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/random.html#initial_seed"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.initial_seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current random seed of the current GPU.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function eagerly initializes CUDA.</p>
</div>
</dd></dl>

</div>
<div class="section" id="communication-collectives">
<h2>Communication collectives<a class="headerlink" href="#communication-collectives" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.comm.broadcast">
<code class="sig-prename descclassname">torch.cuda.comm.</code><code class="sig-name descname">broadcast</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">devices=None</em>, <em class="sig-param">*</em>, <em class="sig-param">out=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/comm.html#broadcast"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a tensor to specified GPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to broadcast. Can be on CPU or GPU.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – an iterable of
GPU devices, among which to broadcast.</p></li>
<li><p><strong>out</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to
store output results.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Exactly one of <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> must be specified.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code> is specified,</dt><dd><p>a tuple containing copies of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>, placed on
<code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is specified,</dt><dd><p>a tuple containing <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensors, each containing a copy of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.broadcast_coalesced">
<code class="sig-prename descclassname">torch.cuda.comm.</code><code class="sig-name descname">broadcast_coalesced</code><span class="sig-paren">(</span><em class="sig-param">tensors</em>, <em class="sig-param">devices</em>, <em class="sig-param">buffer_size=10485760</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/comm.html#broadcast_coalesced"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.broadcast_coalesced" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcasts a sequence tensors to the specified GPUs.
Small tensors are first coalesced into a buffer to reduce the number
of synchronizations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>sequence</em>) – tensors to broadcast. Must be on the same device,
either CPU or GPU.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em>) – an iterable of GPU
devices, among which to broadcast.</p></li>
<li><p><strong>buffer_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a>) – maximum size of the buffer used for coalescing</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple containing copies of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>, placed on <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.reduce_add">
<code class="sig-prename descclassname">torch.cuda.comm.</code><code class="sig-name descname">reduce_add</code><span class="sig-paren">(</span><em class="sig-param">inputs</em>, <em class="sig-param">destination=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/comm.html#reduce_add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.reduce_add" title="Permalink to this definition">¶</a></dt>
<dd><p>Sums tensors from multiple GPUs.</p>
<p>All inputs should have matching shapes, dtype, and layout. The output tensor
will be of the same shape, dtype, and layout.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterable of tensors to add.</p></li>
<li><p><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which the output will be
placed (default: current device).</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor containing an elementwise sum of all inputs, placed on the
<code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code> device.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.scatter">
<code class="sig-prename descclassname">torch.cuda.comm.</code><code class="sig-name descname">scatter</code><span class="sig-paren">(</span><em class="sig-param">tensor</em>, <em class="sig-param">devices=None</em>, <em class="sig-param">chunk_sizes=None</em>, <em class="sig-param">dim=0</em>, <em class="sig-param">streams=None</em>, <em class="sig-param">*</em>, <em class="sig-param">out=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/comm.html#scatter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.scatter" title="Permalink to this definition">¶</a></dt>
<dd><p>Scatters tensor across multiple GPUs.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensor</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a>) – tensor to scatter. Can be on CPU or GPU.</p></li>
<li><p><strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – an iterable of
GPU devices, among which to scatter.</p></li>
<li><p><strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>]</em><em>, </em><em>optional</em>) – sizes of chunks to be placed on
each device. It should match <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code> in length and sums to
<code class="docutils literal notranslate"><span class="pre">tensor.size(dim)</span></code>. If not specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code> will be divided
into equal chunks.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – A dimension along which to chunk <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>.
Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>streams</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a><em>]</em><em>, </em><em>optional</em>) – an iterable of Streams, among
which to execute the scatter. If not specified, the default stream will
be utilized.</p></li>
<li><p><strong>out</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to
store output results. Sizes of these tensors must match that of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>, except for <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>, where the total size must
sum to <code class="docutils literal notranslate"><span class="pre">tensor.size(dim)</span></code>.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Exactly one of <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> must be specified. When
<code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is specified, <code class="xref py py-attr docutils literal notranslate"><span class="pre">chunk_sizes</span></code> must not be specified and
will be inferred from sizes of <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code> is specified,</dt><dd><p>a tuple containing chunks of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>, placed on
<code class="xref py py-attr docutils literal notranslate"><span class="pre">devices</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is specified,</dt><dd><p>a tuple containing <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensors, each containing a chunk of
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensor</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.comm.gather">
<code class="sig-prename descclassname">torch.cuda.comm.</code><code class="sig-name descname">gather</code><span class="sig-paren">(</span><em class="sig-param">tensors</em>, <em class="sig-param">dim=0</em>, <em class="sig-param">destination=None</em>, <em class="sig-param">*</em>, <em class="sig-param">out=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/nn/parallel/comm.html#gather"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.comm.gather" title="Permalink to this definition">¶</a></dt>
<dd><p>Gathers tensors from multiple GPU devices.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tensors</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>]</em>) – an iterable of tensors to gather.
Tensor sizes in all dimensions other than <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code> have to match.</p></li>
<li><p><strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – a dimension along which the tensors will be
concatenated. Default: <code class="docutils literal notranslate"><span class="pre">0</span></code>.</p></li>
<li><p><strong>destination</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.8)"><em>str</em></a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – the output device.
Can be CPU or CUDA. Default: the current CUDA device.</p></li>
<li><p><strong>out</strong> (<a class="reference internal" href="tensors.html#torch.Tensor" title="torch.Tensor"><em>Tensor</em></a><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the tensor to store gather result.
Its sizes must match those of <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code>, except for <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>,
where the size must equal <code class="docutils literal notranslate"><span class="pre">sum(tensor.size(dim)</span> <span class="pre">for</span> <span class="pre">tensor</span> <span class="pre">in</span> <span class="pre">tensors)</span></code>.
Can be on CPU or CUDA.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code> must not be specified when <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is specified.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p><ul class="simple">
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code> is specified,</dt><dd><p>a tensor located on <code class="xref py py-attr docutils literal notranslate"><span class="pre">destination</span></code> device, that is a result of
concatenating <code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> is specified,</dt><dd><p>the <code class="xref py py-attr docutils literal notranslate"><span class="pre">out</span></code> tensor, now containing results of concatenating
<code class="xref py py-attr docutils literal notranslate"><span class="pre">tensors</span></code> along <code class="xref py py-attr docutils literal notranslate"><span class="pre">dim</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="streams-and-events">
<h2>Streams and events<a class="headerlink" href="#streams-and-events" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="torch.cuda.Stream">
<em class="property">class </em><code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">Stream</code><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA stream.</p>
<p>A CUDA stream is a linear sequence of execution that belongs to a specific
device, independent from other streams.  See <a class="reference internal" href="notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> for
details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – a device on which to allocate
the stream. If <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default) or a negative
integer, this will use the current device.</p></li>
<li><p><strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – priority of the stream. Can be either
-1 (high priority) or 0 (low priority). By default, streams have
priority 0.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although CUDA versions &gt;= 11 support more than two levels of
priorities, in PyTorch, we only support two levels of priorities.</p>
</div>
<dl class="method">
<dt id="torch.cuda.Stream.query">
<code class="sig-name descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.query"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all the work submitted has been completed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A boolean indicating if all kernels in this stream are completed.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.record_event">
<code class="sig-name descname">record_event</code><span class="sig-paren">(</span><em class="sig-param">event=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.record_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.record_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Records an event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a><em>, </em><em>optional</em>) – event to record. If not given, a new one
will be allocated.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Recorded event.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.synchronize">
<code class="sig-name descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Wait for all the kernels in this stream to complete.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamSynchronize()</span></code>: see
<a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA Stream documentation</a> for more info.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_event">
<code class="sig-name descname">wait_event</code><span class="sig-paren">(</span><em class="sig-param">event</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.wait_event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.wait_event" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the stream wait for an event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event"><em>Event</em></a>) – an event to wait for.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaStreamWaitEvent()</span></code>: see
<a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA Stream documentation</a> for more info.</p>
<p>This function returns without waiting for <code class="xref py py-attr docutils literal notranslate"><span class="pre">event</span></code>: only future
operations are affected.</p>
</div>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Stream.wait_stream">
<code class="sig-name descname">wait_stream</code><span class="sig-paren">(</span><em class="sig-param">stream</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Stream.wait_stream"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Stream.wait_stream" title="Permalink to this definition">¶</a></dt>
<dd><p>Synchronizes with another stream.</p>
<p>All future work submitted to this stream will wait until all kernels
submitted to a given stream at the time of call complete.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><em>Stream</em></a>) – a stream to synchronize.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This function returns without waiting for currently enqueued
kernels in <a class="reference internal" href="#torch.cuda.stream" title="torch.cuda.stream"><code class="xref py py-attr docutils literal notranslate"><span class="pre">stream</span></code></a>: only future operations are affected.</p>
</div>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="torch.cuda.Event">
<em class="property">class </em><code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">Event</code><a class="reference internal" href="_modules/torch/cuda/streams.html#Event"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper around a CUDA event.</p>
<p>CUDA events are synchronization markers that can be used to monitor the
device’s progress, to accurately measure timing, and to synchronize CUDA
streams.</p>
<p>The underlying CUDA events are lazily initialized when the event is first
recorded or exported to another process. After creation, only streams on the
same device may record the event. However, streams on any device can wait on
the event.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>enable_timing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – indicates if the event should measure time
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, <a class="reference internal" href="#torch.cuda.Event.wait" title="torch.cuda.Event.wait"><code class="xref py py-meth docutils literal notranslate"><span class="pre">wait()</span></code></a> will be blocking (default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>interprocess</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a>) – if <code class="docutils literal notranslate"><span class="pre">True</span></code>, the event can be shared between processes
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="torch.cuda.Event.elapsed_time">
<code class="sig-name descname">elapsed_time</code><span class="sig-paren">(</span><em class="sig-param">end_event</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.elapsed_time"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.elapsed_time" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the time elapsed in milliseconds after the event was
recorded and before the end_event was recorded.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.from_ipc_handle">
<em class="property">classmethod </em><code class="sig-name descname">from_ipc_handle</code><span class="sig-paren">(</span><em class="sig-param">device</em>, <em class="sig-param">handle</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.from_ipc_handle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.from_ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Reconstruct an event from an IPC handle on the given device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.ipc_handle">
<code class="sig-name descname">ipc_handle</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.ipc_handle"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.ipc_handle" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns an IPC handle of this event. If not recorded yet, the event
will use the current device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.query">
<code class="sig-name descname">query</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.query"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.query" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks if all work currently captured by event has completed.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A boolean indicating if all work currently captured by event has
completed.</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.record">
<code class="sig-name descname">record</code><span class="sig-paren">(</span><em class="sig-param">stream=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.record"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.record" title="Permalink to this definition">¶</a></dt>
<dd><p>Records the event in a given stream.</p>
<p>Uses <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_stream()</span></code> if no stream is specified. The
stream’s device must match the event’s device.</p>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.synchronize">
<code class="sig-name descname">synchronize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.synchronize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.synchronize" title="Permalink to this definition">¶</a></dt>
<dd><p>Waits for the event to complete.</p>
<p>Waits until the completion of all work currently captured in this event.
This prevents the CPU thread from proceeding until the event completes.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is a wrapper around <code class="docutils literal notranslate"><span class="pre">cudaEventSynchronize()</span></code>: see
<a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html">CUDA Event documentation</a> for more info.</p>
</div>
</div></blockquote>
</dd></dl>

<dl class="method">
<dt id="torch.cuda.Event.wait">
<code class="sig-name descname">wait</code><span class="sig-paren">(</span><em class="sig-param">stream=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/streams.html#Event.wait"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.Event.wait" title="Permalink to this definition">¶</a></dt>
<dd><p>Makes all future work submitted to the given stream wait for this
event.</p>
<p>Use <code class="docutils literal notranslate"><span class="pre">torch.cuda.current_stream()</span></code> if no stream is specified.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="memory-management">
<h2>Memory management<a class="headerlink" href="#memory-management" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.empty_cache">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">empty_cache</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/memory.html#empty_cache"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.empty_cache" title="Permalink to this definition">¶</a></dt>
<dd><p>Releases all unoccupied cached memory currently held by the caching
allocator so that those can be used in other GPU application and visible in
<cite>nvidia-smi</cite>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code class="xref py py-func docutils literal notranslate"><span class="pre">empty_cache()</span></code></a> doesn’t increase the amount of GPU
memory available for PyTorch. However, it may help reduce fragmentation
of GPU memory in certain cases. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for
more details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_stats">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_stats</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; Dict[str, Any]<a class="reference internal" href="_modules/torch/cuda/memory.html#memory_stats"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_stats" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary of CUDA memory allocator statistics for a
given device.</p>
<p>The return value of this function is a dictionary of statistics, each of
which is a non-negative integer.</p>
<p>Core statistics:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
number of allocation requests received by the memory allocator.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
amount of allocated memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
number of reserved segments from <code class="docutils literal notranslate"><span class="pre">cudaMalloc()</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
amount of reserved memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;active.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
number of active memory blocks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
amount of active memory.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
number of inactive, non-releasable memory blocks.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;</span></code>:
amount of inactive, non-releasable memory.</p></li>
</ul>
<p>For these core statistics, values are broken down as follows.</p>
<p>Pool type:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">all</span></code>: combined statistics across all memory pools.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">large_pool</span></code>: statistics for the large allocation pool
(as of October 2019, for size &gt;= 1MB allocations).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">small_pool</span></code>: statistics for the small allocation pool
(as of October 2019, for size &lt; 1MB allocations).</p></li>
</ul>
<p>Metric type:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">current</span></code>: current value of this metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">peak</span></code>: maximum value of this metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">allocated</span></code>: historical total increase in this metric.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">freed</span></code>: historical total decrease in this metric.</p></li>
</ul>
<p>In addition to the core statistics, we also provide some simple event
counters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;num_alloc_retries&quot;</span></code>: number of failed <code class="docutils literal notranslate"><span class="pre">cudaMalloc</span></code> calls that
result in a cache flush and retry.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;num_ooms&quot;</span></code>: number of out-of-memory errors thrown.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistics for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_summary">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_summary</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em>, <em class="sig-param">abbreviated: bool = False</em><span class="sig-paren">)</span> &#x2192; str<a class="reference internal" href="_modules/torch/cuda/memory.html#memory_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a human-readable printout of the current memory allocator
statistics for a given device.</p>
<p>This can be useful to display periodically during training, or when
handling out-of-memory exceptions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
printout for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p></li>
<li><p><strong>abbreviated</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.8)"><em>bool</em></a><em>, </em><em>optional</em>) – whether to return an abbreviated summary
(default: False).</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_snapshot">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_snapshot</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/memory.html#memory_snapshot"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_snapshot" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a snapshot of the CUDA memory allocator state across all devices.</p>
<p>Interpreting the output of this function requires familiarity with the
memory allocator internals.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_allocated">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_allocated</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#memory_allocated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory occupied by tensors in bytes for a given
device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is likely less than the amount shown in <cite>nvidia-smi</cite> since some
unused memory can be held by the caching allocator and some context
needs to be created on GPU. See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more
details about GPU memory management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_allocated">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">max_memory_allocated</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#max_memory_allocated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.max_memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory occupied by tensors in bytes for a given
device.</p>
<p>By default, this returns the peak allocated memory since the beginning of
this program. <code class="xref py py-func docutils literal notranslate"><span class="pre">reset_peak_stats()</span></code> can be used to
reset the starting point in tracking this metric. For example, these two
functions can measure the peak allocated memory usage of each iteration in a
training loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.reset_max_memory_allocated">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">reset_max_memory_allocated</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/memory.html#reset_max_memory_allocated"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.reset_max_memory_allocated" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory occupied by
tensors for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_allocated()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function now calls <code class="xref py py-func docutils literal notranslate"><span class="pre">reset_peak_memory_stats()</span></code>, which resets
/all/ peak memory stats.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_reserved">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_reserved</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#memory_reserved"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_reserved" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the current GPU memory managed by the caching allocator in bytes
for a given device.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_reserved">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">max_memory_reserved</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#max_memory_reserved"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.max_memory_reserved" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the maximum GPU memory managed by the caching allocator in bytes
for a given device.</p>
<p>By default, this returns the peak cached memory since the beginning of this
program. <code class="xref py py-func docutils literal notranslate"><span class="pre">reset_peak_stats()</span></code> can be used to reset
the starting point in tracking this metric. For example, these two functions
can measure the peak cached memory amount of each iteration in a training
loop.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.memory_cached">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">memory_cached</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#memory_cached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated; see <a class="reference internal" href="#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code class="xref py py-func docutils literal notranslate"><span class="pre">memory_reserved()</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.max_memory_cached">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">max_memory_cached</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; int<a class="reference internal" href="_modules/torch/cuda/memory.html#max_memory_cached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.max_memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Deprecated; see <a class="reference internal" href="#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_reserved()</span></code></a>.</p>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.reset_max_memory_cached">
<code class="sig-prename descclassname">torch.cuda.</code><code class="sig-name descname">reset_max_memory_cached</code><span class="sig-paren">(</span><em class="sig-param">device: Union[torch.device</em>, <em class="sig-param">str</em>, <em class="sig-param">None</em>, <em class="sig-param">int] = None</em><span class="sig-paren">)</span> &#x2192; None<a class="reference internal" href="_modules/torch/cuda/memory.html#reset_max_memory_cached"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.reset_max_memory_cached" title="Permalink to this definition">¶</a></dt>
<dd><p>Resets the starting point in tracking maximum GPU memory managed by the
caching allocator for a given device.</p>
<p>See <a class="reference internal" href="#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code class="xref py py-func docutils literal notranslate"><span class="pre">max_memory_cached()</span></code></a> for details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference internal" href="tensor_attributes.html#torch.torch.device" title="torch.torch.device"><em>torch.device</em></a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.8)"><em>int</em></a><em>, </em><em>optional</em>) – selected device. Returns
statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">current_device()</span></code></a>,
if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code class="xref py py-attr docutils literal notranslate"><span class="pre">device</span></code></a> is <code class="docutils literal notranslate"><span class="pre">None</span></code> (default).</p>
</dd>
</dl>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This function now calls <code class="xref py py-func docutils literal notranslate"><span class="pre">reset_peak_memory_stats()</span></code>, which resets
/all/ peak memory stats.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory
management.</p>
</div>
</dd></dl>

</div>
<div class="section" id="nvidia-tools-extension-nvtx">
<h2>NVIDIA Tools Extension (NVTX)<a class="headerlink" href="#nvidia-tools-extension-nvtx" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="torch.cuda.nvtx.mark">
<code class="sig-prename descclassname">torch.cuda.nvtx.</code><code class="sig-name descname">mark</code><span class="sig-paren">(</span><em class="sig-param">msg</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#mark"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.mark" title="Permalink to this definition">¶</a></dt>
<dd><p>Describe an instantaneous event that occurred at some point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with the event.</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_push">
<code class="sig-prename descclassname">torch.cuda.nvtx.</code><code class="sig-name descname">range_push</code><span class="sig-paren">(</span><em class="sig-param">msg</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#range_push"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.range_push" title="Permalink to this definition">¶</a></dt>
<dd><p>Pushes a range onto a stack of nested range span.  Returns zero-based
depth of the range that is started.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with range</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="torch.cuda.nvtx.range_pop">
<code class="sig-prename descclassname">torch.cuda.nvtx.</code><code class="sig-name descname">range_pop</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/torch/cuda/nvtx.html#range_pop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#torch.cuda.nvtx.range_pop" title="Permalink to this definition">¶</a></dt>
<dd><p>Pops a range off of a stack of nested range spans.  Returns the
zero-based depth of the range that is ended.</p>
</dd></dl>

</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="amp.html" class="btn btn-neutral float-right" title="Automatic Mixed Precision package - torch.cuda.amp" accesskey="n" rel="next">Next <img src="_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="autograd.html" class="btn btn-neutral" title="Automatic differentiation package - torch.autograd" accesskey="p" rel="prev"><img src="_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">torch.cuda</a><ul>
<li><a class="reference internal" href="#random-number-generator">Random Number Generator</a></li>
<li><a class="reference internal" href="#communication-collectives">Communication collectives</a></li>
<li><a class="reference internal" href="#streams-and-events">Streams and events</a></li>
<li><a class="reference internal" href="#memory-management">Memory management</a></li>
<li><a class="reference internal" href="#nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
         <script src="_static/jquery.js"></script>
         <script src="_static/underscore.js"></script>
         <script src="_static/doctools.js"></script>
         <script src="_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>