diff --git a/aten/src/ATen/templates/aten_xla_type.h b/aten/src/ATen/templates/aten_xla_type.h
index 4dc34bc1a7..bf745196c8 100644
--- a/aten/src/ATen/templates/aten_xla_type.h
+++ b/aten/src/ATen/templates/aten_xla_type.h
@@ -19,4 +19,4 @@ class AtenXlaType {
 ${dispatch_xla_declarations}
 };
 
-}  // namespace torch_xla
+}  // namespace ${cpp_namespace}
diff --git a/aten/src/ATen/templates/aten_xla_type_default.cpp b/aten/src/ATen/templates/aten_xla_type_default.cpp
index 56c2916697..6d704aabea 100644
--- a/aten/src/ATen/templates/aten_xla_type_default.cpp
+++ b/aten/src/ATen/templates/aten_xla_type_default.cpp
@@ -1,16 +1,16 @@
 // ${generated_comment}
-#include <torch_xla/csrc/aten_xla_type_default.h>
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type_default.h>
 
 #include <ATen/Context.h>
 #include <torch/library.h>
 #include <ATen/CPUGeneratorImpl.h>
 
-#include <tensorflow/compiler/xla/xla_client/debug_macros.h>
-#include <tensorflow/compiler/xla/xla_client/metrics.h>
-#include <tensorflow/compiler/xla/xla_client/tf_logging.h>
-#include <torch_xla/csrc/aten_xla_bridge.h>
-#include <torch_xla/csrc/aten_xla_type.h>
-#include <torch_xla/csrc/function_call_tracker.h>
+#include <lazy_tensors/computation_client/debug_macros.h>
+#include <lazy_tensors/computation_client/metrics.h>
+#include <lazy_tensors/computation_client/ltc_logging.h>
+#include <lazy_tensor_core/csrc/aten_ltc_bridge.h>
+#include <lazy_tensor_core/csrc/function_call_tracker.h>
+#include <lazy_tensor_core/csrc/ts_backend/aten_xla_type.h>
 
 namespace ${cpp_namespace} {
 
diff --git a/tools/codegen/dest/gen_external_aten_fallbacks.py b/tools/codegen/dest/gen_external_aten_fallbacks.py
index ee4a2c30d6..b516ebac8f 100644
--- a/tools/codegen/dest/gen_external_aten_fallbacks.py
+++ b/tools/codegen/dest/gen_external_aten_fallbacks.py
@@ -30,40 +30,7 @@ _FN_DENYLIST_REGEX = [
 # Instead, the codegen will figure out which ops to generate _out wrappers for
 # entirely from the yaml. Maintaining the same behavior as current XLA codegen for now.
 _FN_OUT = [
-    'abs',
     'add',
-    'acos',
-    'acosh',
-    'asin',
-    'asinh',
-    'atan',
-    'atan2',
-    'atanh',
-    'baddbmm',
-    'bernoulli',
-    'binary_cross_entropy',
-    'binary_cross_entropy_backward',
-    'clamp',
-    'div',
-    'gather',
-    'ger',
-    'hardsigmoid',
-    'kthvalue',
-    'index_select',
-    'inverse',
-    'log',
-    'masked_select',
-    'maximum',
-    'minimum',
-    'pow',
-    'prod',
-    'nonzero',
-    'round',
-    'normal',
-    'std',
-    'take',
-    'topk',
-    'var',
 ]
 
 # See Note [Auto generated composite kernels]
@@ -90,14 +57,14 @@ def xla_tensor_creation_api(
         # Only raw Tensor (non-reference) returns need to go through the XLA tensor creation API.
         # Tensor references can be returned directly, since they've already been converted to XLA tensors.
         # See Note [Tensor Copy Returns]
-        bridge_api = 'CreateXlaTensor'
+        bridge_api = 'CreateLtcTensor'
     elif isinstance(ret.type, ListType) and ret.type.elem == BaseType(BaseTy.Tensor):
-        bridge_api = 'CreateXlaTensors'
+        bridge_api = 'CreateLtcTensors'
     else:
         # for non tensor-types, there's no need to wrap the output in an xla bridge api.
         return ret_name
 
-    return f"bridge::{bridge_api}({cpu_result_name}, bridge::GetXlaDevice({device_param_name}))"
+    return f"bridge::{bridge_api}({cpu_result_name}, bridge::GetLtcDevice({device_param_name}))"
 
 
 
@@ -137,20 +104,20 @@ class GenExternalAtenFallback:
             return_names = cpp.return_names(g.out.native_function)
             if len(return_names) > 1:
                 updates = '\n  '.join(
-                    f'bridge::XlaUpdateTensors({{{ret_name}}}, {{std::get<{i}>({functional_result_name})}}, {{0}});'
+                    f'bridge::LtcUpdateTensors({{{ret_name}}}, {{std::get<{i}>({functional_result_name})}}, {{0}});'
                     for i, ret_name in enumerate(return_names))
                 returns = f'{dispatcher_sig.returns_type().cpp_type()}({", ".join(return_names)})'
             else:
                 ret_name = return_names[0]
-                updates = f'bridge::XlaUpdateTensors({{{ret_name}}}, {{{functional_result_name}}}, {{0}});'
+                updates = f'bridge::LtcUpdateTensors({{{ret_name}}}, {{{functional_result_name}}}, {{0}});'
                 returns = ret_name
 
             functional_sig = DispatcherSignature.from_schema(g.functional.native_function.func)
 
             return f"""\
 {dispatcher_sig.defn(name=func_name)} {{
-  XLA_FN_TRACK(3);
-  TF_VLOG(3) << "XLA {name} :"{print_args_str};
+  LTC_FN_TRACK(3);
+  LTC_VLOG(3) << "XLA {name} :"{print_args_str};
   auto {functional_result_name} = AtenXlaType::{functional_sig.name()}({", ".join(a.name for a in functional_sig.arguments())});
   {updates}
   return {returns};
@@ -232,20 +199,20 @@ class GenExternalAtenFallback:
 
             tensorlist_intermediates_str = ''
             if len(tensorlist_args) > 0:
-                tensorlist_intermediates_str = '\n'.join([f'  auto {updated_name} = bridge::XlaCreateTensorList({arg.name});'
+                tensorlist_intermediates_str = '\n'.join([f'  auto {updated_name} = bridge::LtcCreateTensorList({arg.name});'
                                                           for arg, updated_name in tensorlist_args.items()])
 
             opt_tensor_intermediates_str = ''
             if len(opt_tensor_args) > 0:
                 arg_str = ", ".join([a.name for a in opt_tensor_args.keys()])
                 opt_tensor_intermediates_str = f'\n  std::vector<c10::optional<at::Tensor>> xlatens_opt_tensors = {{{arg_str}}};'
-                opt_tensor_intermediates_str += '\n  auto xlatens_opt = bridge::XlaCreateOptTensorList(xlatens_opt_tensors);'
+                opt_tensor_intermediates_str += '\n  auto xlatens_opt = bridge::LtcCreateOptTensorList(xlatens_opt_tensors);'
 
             intermediates = ''
             if tensorlist_intermediates_str != '':
                 intermediates += tensorlist_intermediates_str + '\n'
             intermediates += f"  std::vector<at::Tensor> xlatens_tensors = {{{', '.join([a.name for a in tensor_args.keys()])}}};"
-            intermediates += "\n  auto xlatens = bridge::XlaCreateTensorList(xlatens_tensors);"
+            intermediates += "\n  auto xlatens = bridge::LtcCreateTensorList(xlatens_tensors);"
             if opt_tensor_intermediates_str != '':
                 intermediates += opt_tensor_intermediates_str
 
@@ -278,7 +245,7 @@ class GenExternalAtenFallback:
             if len(annotated_tensor_indices) > 0:
                 indices_str = ", ".join([str(i) for i in annotated_tensor_indices])
                 collect_mutated_tensors = f'\n  std::vector<size_t> xlatens_update_indices = {{{indices_str}}};'
-                update_tensors = '\n  bridge::XlaUpdateTensors(xlatens_tensors, xlatens, xlatens_update_indices);'
+                update_tensors = '\n  bridge::LtcUpdateTensors(xlatens_tensors, xlatens, xlatens_update_indices);'
 
             returns = ''
             if f.native_function.func.returns:
@@ -300,9 +267,9 @@ class GenExternalAtenFallback:
 
             return f"""\
 {dispatcher_sig.defn(name=func_name)} {{
-  XLA_FN_TRACK(3);
-  XLA_COUNTER("aten::{name}", 1);
-  TF_VLOG(3) << "XLA {name} :"{print_args_str};
+  LTC_FN_TRACK(3);
+  LTC_COUNTER("aten::{name}", 1);
+  LTC_VLOG(3) << "XLA {name} :"{print_args_str};
 {intermediates}
   {at_call}{collect_mutated_tensors}{update_tensors}{avoid_warning}{return_str}
 }}
