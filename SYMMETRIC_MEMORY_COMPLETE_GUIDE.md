# Symmetric Memory in torch.compile - Complete Guide

## ç›®å½•
1. [å½“å‰æœºåˆ¶ç¡®è®¤](#å½“å‰æœºåˆ¶ç¡®è®¤)
2. [ç”¨æˆ·å¦‚ä½•è§¦å‘](#ç”¨æˆ·å¦‚ä½•è§¦å‘)
3. [å·¥ä½œæµç¨‹è¯¦è§£](#å·¥ä½œæµç¨‹è¯¦è§£)
4. [Phase 1 å®ç°è®¡åˆ’](#phase-1-å®ç°è®¡åˆ’)
5. [æ€§èƒ½è€ƒè™‘](#æ€§èƒ½è€ƒè™‘)

---

## å½“å‰æœºåˆ¶ç¡®è®¤

### âœ… ç°çŠ¶ï¼š`one_shot_all_reduce` å·²åœ¨ä½¿ç”¨ Symmetric Memory

PyTorch Inductor **å·²ç»å®ç°äº†** `one_shot_all_reduce` çš„ symmetric memory æ”¯æŒï¼

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`

```python
# Line 196-199: all_reduce çš„ lowering æ³¨å†Œ
@register_comm_lowering(c10d.all_reduce)
def _all_reduce(inp: ir.TensorBox, reduce_op: str, group_name: str):
    if _should_lower_as_one_shot_all_reduce(inp, reduce_op, group_name):
        return _one_shot_all_reduce(inp, reduce_op, group_name)  # ä½¿ç”¨ symmetric memory!

    # å¦åˆ™ä½¿ç”¨æ™®é€šçš„ all_reduce_
    inp = clone(inp)
    ...

# Line 159-169: one_shot_all_reduce å®ç°
def _one_shot_all_reduce(inp: ir.TensorBox, reduce_op, group_name):
    realize_as_comm_buffer(inp, ir.CommBufferType.SYMM_MEM, group_name)  # <-- æ ‡è®°ä¸º SYMM_MEM!
    return pytree.tree_map(
        ir.TensorBox.create,
        ir.FallbackKernel.create(
            torch.ops.symm_mem.one_shot_all_reduce.default,
            inp,
            reduce_op,
            group_name,
        ),
    )
```

### è§¦å‘æ¡ä»¶

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py` (Line 144-156)

```python
def _should_lower_as_one_shot_all_reduce(inp: ir.TensorBox, reduce_op: str, group_name: str):
    from torch.distributed._symmetric_memory import is_symm_mem_enabled_for_group

    inp_size = inp.get_numel() * inp.get_dtype().itemsize
    return (
        config._collective.auto_select            # æ¡ä»¶ 1: éœ€è¦å¼€å¯ (é»˜è®¤ False)
        and is_symm_mem_enabled_for_group(group_name)  # æ¡ä»¶ 2: symmetric memory å·²å¯ç”¨
        and can_realize_as_comm_buffer(inp, ir.CommBufferType.SYMM_MEM)  # æ¡ä»¶ 3: buffer å¯ä»¥ realize
        and reduce_op == "sum"                    # æ¡ä»¶ 4: reduce_op å¿…é¡»æ˜¯ "sum"
        and inp_size <= config._collective.one_shot_all_reduce_threshold_bytes  # æ¡ä»¶ 5: å¤§å° <= 128KB
    )
```

---

## ç”¨æˆ·å¦‚ä½•è§¦å‘

### âŒ é»˜è®¤æƒ…å†µï¼šä¸ä¼šè‡ªåŠ¨è§¦å‘

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/config.py` (Line 891-893)

```python
class _collective:
    auto_select: bool = False  # <-- é»˜è®¤æ˜¯ Falseï¼
    one_shot_all_reduce_threshold_bytes: int = 128 * 1024
```

### âœ… éœ€è¦ç”¨æˆ·æ‰‹åŠ¨è®¾ç½®ä¸¤ä»¶äº‹

#### ç¬¬ 1 æ­¥ï¼šå¯ç”¨ `auto_select` config

```python
import torch._inductor.config as config
config._collective.auto_select = True
```

#### ç¬¬ 2 æ­¥ï¼šå¯ç”¨ Symmetric Memory for Process Group

```python
from torch.distributed._symmetric_memory import enable_symm_mem_for_group
import torch.distributed as dist

dist.init_process_group(backend="nccl", world_size=2, rank=rank)
enable_symm_mem_for_group("default")
```

### å®Œæ•´ç”¨æˆ·ä»£ç ç¤ºä¾‹

```python
import torch
import torch.distributed as dist
import torch._inductor.config as config

# åˆå§‹åŒ–åˆ†å¸ƒå¼
dist.init_process_group(backend="nccl", world_size=2, rank=rank)

# ç¬¬ 1 æ­¥ï¼šå¯ç”¨ auto_select
config._collective.auto_select = True

# ç¬¬ 2 æ­¥ï¼šå¯ç”¨ symmetric memory
from torch.distributed._symmetric_memory import enable_symm_mem_for_group
enable_symm_mem_for_group("default")

# ç¬¬ 3 æ­¥ï¼šä½¿ç”¨ torch.compile
@torch.compile(backend="inductor")
def my_model(x):
    y = x * 2.0
    # è¿™ä¸ª all_reduce ä¼šè‡ªåŠ¨è¢«ä¼˜åŒ–ä¸º one_shot_all_reduce (ä½¿ç”¨ symmetric memory)
    dist.all_reduce(y, op=dist.ReduceOp.SUM)
    return y

x = torch.randn(100, 100, device=f"cuda:{rank}")
output = my_model(x)
```

---

## å·¥ä½œæµç¨‹è¯¦è§£

### å®Œæ•´æ‰§è¡Œæµç¨‹ï¼ˆä»ç”¨æˆ·ä»£ç åˆ°ç”Ÿæˆä»£ç ï¼‰

```
ç”¨æˆ·ä»£ç : dist.all_reduce(tensor)
    â†“
[Dynamo] æ•è· FX Graph
    â†“
    graph():
        %x : [num_users=1] = placeholder[target=x]
        %mul : [num_users=1] = call_function[target=operator.mul](args = (%x, 2.0))
        # all_reduce ä½œä¸º call_function node
        %all_reduce : [num_users=1] = call_function[
            target=torch.ops._c10d_functional.all_reduce
        ](args = (%mul, 'sum', 'default'))
        return %all_reduce
    â†“
[Inductor] æŸ¥æ‰¾ lowering å‡½æ•°
    â†“
    lowering_func = lowerings.get(c10d.all_reduce)
    # æ‰¾åˆ° _all_reduce å‡½æ•°
    â†“
[comm_lowering._all_reduce] æ£€æŸ¥æ¡ä»¶
    â†“
    if _should_lower_as_one_shot_all_reduce(...):
        return _one_shot_all_reduce(...)  # <-- é€‰æ‹© symmetric memory è·¯å¾„!
    â†“
[comm_lowering._one_shot_all_reduce]
    â†“
    realize_as_comm_buffer(inp, ir.CommBufferType.SYMM_MEM, group_name)
    # **å…³é”®ï¼šä¿®æ”¹ buffer.layout**
    â†“
    buffer.layout = ir.CommBufferLayout(
        layout=original_layout,
        comm_buffer_type=CommBufferType.SYMM_MEM,  # <-- æ ‡è®°åœ¨è¿™é‡Œï¼
        group_name=group_name
    )
    â†“
[Scheduler] éå† buffers
    â†“
    for buf in self.buffers:
        layout = buf.get_layout()
        if isinstance(layout, ir.CommBufferLayout):
            # ç”Ÿæˆç‰¹æ®Šçš„åˆ†é…ä»£ç 
            self.wrapper_code.generate_comm_buffer_allocation(buf)
    â†“
[WrapperCodegen] ç”Ÿæˆä»£ç 
    â†“
    if comm_buffer_type == ir.CommBufferType.SYMM_MEM:
        # **ç”Ÿæˆ symmetric memory åˆ†é…ä»£ç ï¼**
        return f"{name} = empty_strided_p2p(..., group_name='default', alloc_id=12345)"
    â†“
ç”Ÿæˆçš„ Python ä»£ç :
    buf0 = empty_strided_cuda((128, 128), ...)  # æ™®é€šåˆ†é…
    buf1 = empty_strided_p2p((128, 128), ..., group_name="default")  # <-- Symmetric!
    buf2 = torch.ops.symm_mem.one_shot_all_reduce(buf1, "sum", "default")
```

### æ ¸å¿ƒæœºåˆ¶ï¼šLayout ç±»å‹ä¼ é€’

**å…³é”®æ€æƒ³**ï¼šé€šè¿‡åœ¨ `realize_as_comm_buffer()` ä¸­æ”¹å˜ buffer çš„ **layout ç±»å‹**ï¼Œæˆ‘ä»¬åœ¨ IR ä¸­"æ ‡è®°"äº†å“ªäº› buffers éœ€è¦ symmetric memoryã€‚è¿™ä¸ªæ ‡è®°åœ¨æ•´ä¸ªç¼–è¯‘æµç¨‹ä¸­ä¼ é€’ï¼Œæœ€ç»ˆåœ¨ä»£ç ç”Ÿæˆé˜¶æ®µè¢«è¯†åˆ«ï¼Œç”Ÿæˆ `empty_strided_p2p()` è°ƒç”¨ã€‚

```python
# æ•°æ®æµ
æ™®é€š Buffer (FlexibleLayout)
    â†“ realize_as_comm_buffer()
Symmetric Buffer (CommBufferLayout with SYMM_MEM)
    â†“ Scheduler
ä¼ é€’åˆ° Codegen
    â†“ WrapperCodegen
æ£€æŸ¥ layout ç±»å‹
    â†“
ç”Ÿæˆ empty_strided_p2p() è°ƒç”¨
```

### å…³é”®å‡½æ•°

| å‡½æ•° | ä½ç½® | ä½œç”¨ |
|------|------|------|
| `realize_as_comm_buffer()` | `comm_lowering.py:77-110` | æ ‡è®° buffer ä¸º symmetric memory |
| `empty_strided_p2p()` | C++ binding | å®é™…ä» symmetric memory åˆ†é… |
| `CommBufferLayout` | `ir.py` | ç‰¹æ®Š layout ç±»å‹ï¼Œæºå¸¦ SYMM_MEM æ ‡å¿— |

---

## Phase 1 å®ç°è®¡åˆ’

### ç›®æ ‡

**å½“å‰**: åªæœ‰ `all_reduce` ä¼šè¢«è‡ªåŠ¨ä¼˜åŒ–ä¸º `one_shot_all_reduce` + symmetric memory

**Phase 1**: ä¸ºæ‰€æœ‰ `torch.ops.symm_mem.*` æ“ä½œæ·»åŠ  loweringï¼Œè®©å®ƒä»¬éƒ½èƒ½åƒ `one_shot_all_reduce` ä¸€æ ·è‡ªåŠ¨ä½¿ç”¨ symmetric memory

### è®¾è®¡åŸåˆ™

1. **å¿½ç•¥ç”¨æˆ·æ·»åŠ çš„ `torch.cuda.use_mem_pool()` context managers** - ç¼–è¯‘å™¨åŸºäºæ“ä½œç±»å‹è‡ªåŠ¨å†³å®š
2. **å¤ç”¨ç°æœ‰åŸºç¡€è®¾æ–½** - ä½¿ç”¨å·²æœ‰çš„ `realize_as_comm_buffer()` å’Œ `empty_strided_p2p()`
3. **ä¸éœ€è¦æ”¹ Dynamo å±‚** - æ‰€æœ‰å†³ç­–åœ¨ Inductor lowering é˜¶æ®µå®Œæˆ

### å®ç°æ­¥éª¤

#### Step 1: é€šç”¨æ“ä½œæ£€æµ‹å‡½æ•°

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`

**ä½ç½®**: åœ¨ `_should_lower_as_one_shot_all_reduce()` å‡½æ•°åæ·»åŠ  (çº¦ line 157)

```python
def requires_symmetric_memory_allocation(target) -> bool:
    """
    åˆ¤æ–­ä¸€ä¸ªæ“ä½œæ˜¯å¦éœ€è¦ symmetric memoryã€‚

    Args:
        target: æ“ä½œçš„ target (torch.ops.* OpOverload)

    Returns:
        True å¦‚æœæ“ä½œéœ€è¦ symmetric memory
    """
    # æ£€æŸ¥æ˜¯å¦æ˜¯ symm_mem å‘½åç©ºé—´çš„æ“ä½œ
    if hasattr(target, "__module__"):
        module_parts = target.__module__.split(".")
        if "symm_mem" in module_parts:
            return True

    # é€šè¿‡å­—ç¬¦ä¸²è¡¨ç¤ºæ£€æŸ¥ (é€‚ç”¨äº OpOverload å¯¹è±¡)
    target_str = str(target)
    if "symm_mem" in target_str:
        return True

    return False
```

#### Step 2: é€šç”¨ Lowering æ³¨å†ŒåŠ©æ‰‹

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`

**ä½ç½®**: åœ¨æ–‡ä»¶ä¸­éƒ¨æ·»åŠ 

```python
def create_symm_mem_lowering(op_overload, extract_group_name_fn=None):
    """
    ä¸º symm_mem æ“ä½œåˆ›å»ºç»Ÿä¸€çš„ lowering å‡½æ•°ã€‚

    è¿™ä¸ªåŠ©æ‰‹æ ‡å‡†åŒ–äº†æ¨¡å¼ï¼š
    1. å°†æ‰€æœ‰ tensor è¾“å…¥ realize ä¸º CommBufferType.SYMM_MEM
    2. é€šè¿‡ FallbackKernel æ‰§è¡Œæ“ä½œ

    Args:
        op_overload: è¦æ³¨å†Œçš„ torch.ops.symm_mem.* æ“ä½œ
        extract_group_name_fn: å¯é€‰çš„å‡½æ•°ï¼Œä» args/kwargs æå– group_name
    """
    from .lowering import register_lowering, add_layout_constraint, constrain_to_fx_strides

    add_layout_constraint(op_overload, constrain_to_fx_strides)

    @register_lowering(op_overload)
    def _symm_mem_generic(*args, **kwargs):
        """Generic lowering for symm_mem operations"""
        # æå– group_name
        if extract_group_name_fn:
            group_name = extract_group_name_fn(args, kwargs)
        else:
            # é»˜è®¤æå–é€»è¾‘
            group_name = kwargs.get('group_name', 'default')
            if not group_name and args:
                # å°è¯•åœ¨ä½ç½®å‚æ•°ä¸­æ‰¾ group_name (é€šå¸¸æ˜¯æœ€åä¸€ä¸ª string å‚æ•°)
                for arg in reversed(args):
                    if isinstance(arg, str) and arg:
                        group_name = arg
                        break

        # å°†æ‰€æœ‰ tensor è¾“å…¥ realize ä¸º symmetric memory buffers
        realized_args = []
        for arg in args:
            if isinstance(arg, ir.TensorBox):
                if can_realize_as_comm_buffer(arg, ir.CommBufferType.SYMM_MEM):
                    realize_as_comm_buffer(arg, ir.CommBufferType.SYMM_MEM, group_name)
                realized_args.append(arg)
            else:
                realized_args.append(arg)

        # æ‰§è¡Œæ“ä½œ
        return pytree.tree_map(
            ir.TensorBox.create,
            ir.FallbackKernel.create(op_overload, *realized_args, **kwargs),
        )

    return _symm_mem_generic
```

#### Step 3: æ‰©å±• `register_comm_lowerings()`

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`

**ä½ç½®**: åœ¨ `register_comm_lowerings()` å‡½æ•°æœ«å°¾æ·»åŠ 

```python
def register_comm_lowerings():
    # ... ç°æœ‰ä»£ç  ...

    # æ³¨å†Œæ‰€æœ‰ symm_mem.* æ“ä½œ
    try:
        import torch.ops.symm_mem

        # one_shot_all_reduce å·²ç»æœ‰å®ç°äº†
        @register_comm_lowering(torch.ops.symm_mem.one_shot_all_reduce)
        def _symm_mem_one_shot_all_reduce(
            inp: ir.TensorBox, reduce_op: str, group_name: str
        ) -> ir.TensorBox:
            return _one_shot_all_reduce(inp, reduce_op, group_name)

        # ä¸ºå…¶ä»– symm_mem æ“ä½œæ³¨å†Œ lowering
        # ç¤ºä¾‹ï¼šå¦‚æœæœ‰ torch.ops.symm_mem.barrier
        if hasattr(torch.ops.symm_mem, 'barrier'):
            create_symm_mem_lowering(torch.ops.symm_mem.barrier)

        # ç¤ºä¾‹ï¼šå¦‚æœæœ‰ torch.ops.symm_mem.all_gather
        if hasattr(torch.ops.symm_mem, 'all_gather'):
            create_symm_mem_lowering(torch.ops.symm_mem.all_gather)

        # æ ¹æ®å®é™…å­˜åœ¨çš„æ“ä½œç»§ç»­æ·»åŠ ...

    except (AttributeError, ImportError):
        log.info("symm_mem operations not available")
```

#### Step 4: æ·»åŠ éªŒè¯å‡½æ•°ï¼ˆå¯é€‰ï¼‰

**æ–‡ä»¶**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`

```python
def validate_symmetric_memory_usage(buffer: ir.Buffer, group_name: str) -> bool:
    """
    éªŒè¯ buffer å¯ä»¥å®‰å…¨åœ°ç”¨ä½œ symmetric memoryã€‚

    æ£€æŸ¥ï¼š
    - Process group æ˜¯å¦å¯ç”¨äº† symmetric memory
    - Buffer å¤§å°æ˜¯å¦åœ¨é™åˆ¶å†…
    - Buffer dtype æ˜¯å¦æ”¯æŒ
    """
    try:
        from torch.distributed._symmetric_memory import is_symm_mem_enabled_for_group

        if not is_symm_mem_enabled_for_group(group_name):
            log.warning(
                f"Symmetric memory not enabled for group '{group_name}'. "
                f"Buffer {buffer.get_name()} may fail to allocate."
            )
            return False

        # æ£€æŸ¥ buffer å¤§å°
        buffer_size = buffer.get_numel() * buffer.get_dtype().itemsize
        max_size = config._collective.one_shot_all_reduce_threshold_bytes

        if buffer_size > max_size:
            log.info(
                f"Buffer {buffer.get_name()} size ({buffer_size} bytes) exceeds "
                f"threshold ({max_size} bytes). This may impact performance."
            )

        return True

    except (AttributeError, ImportError) as e:
        log.warning(f"Could not validate symmetric memory: {e}")
        return False
```

### å®ç°æ£€æŸ¥æ¸…å•

- [ ] **Step 1**: æ·»åŠ  `requires_symmetric_memory_allocation()` å‡½æ•°
- [ ] **Step 2**: å®ç° `create_symm_mem_lowering()` åŠ©æ‰‹
- [ ] **Step 3**: æ‰©å±• `register_comm_lowerings()` æ³¨å†Œæ–°æ“ä½œ
- [ ] **Step 4** (å¯é€‰): æ·»åŠ  `validate_symmetric_memory_usage()` éªŒè¯å‡½æ•°
- [ ] **Step 5**: æ·»åŠ æµ‹è¯•ç”¨ä¾‹

### æµ‹è¯•ç¤ºä¾‹

**æ–‡ä»¶**: `/data/users/tianren/pytorch/test/inductor/test_symmetric_memory.py` (æ–°æ–‡ä»¶)

```python
import unittest
import torch
import torch.distributed as dist
from torch._dynamo.test_case import run_tests, TestCase
from torch.testing._internal.common_distributed import MultiProcessTestCase, skip_if_lt_x_gpu

class TestSymmetricMemoryCompile(MultiProcessTestCase):
    @property
    def world_size(self):
        return 2

    def setUp(self):
        super().setUp()
        self._spawn_processes()

    @skip_if_lt_x_gpu(2)
    def test_symm_mem_one_shot_all_reduce(self):
        """æµ‹è¯• one_shot_all_reduce è‡ªåŠ¨è·å¾— symmetric memory"""
        dist.init_process_group(backend="nccl", world_size=self.world_size, rank=self.rank)

        # å¯ç”¨ symmetric memory
        from torch.distributed._symmetric_memory import enable_symm_mem_for_group
        import torch._inductor.config as config

        enable_symm_mem_for_group("default")
        config._collective.auto_select = True

        @torch.compile(backend="inductor", fullgraph=True)
        def forward(x):
            y = x * 2.0
            z = y + 1.0
            result = torch.ops.symm_mem.one_shot_all_reduce(z, "sum", "default")
            return result

        x = torch.randn(128, 128, device=f"cuda:{self.rank}")
        output = forward(x)

        expected = (x * 2.0 + 1.0) * self.world_size
        torch.testing.assert_close(output, expected)

        dist.destroy_process_group()

if __name__ == "__main__":
    run_tests()
```

---

## æ€§èƒ½è€ƒè™‘

### é¢„æœŸæ”¹è¿›

1. **é›¶æ‹·è´ Collectives**: æ— éœ€ memcpy è¿›å‡ºé€šä¿¡ç¼“å†²åŒº
2. **æ›´å¥½çš„å†…å­˜å±€éƒ¨æ€§**: Symmetric buffers æ”¾ç½®ä½ç½®æœ€ä¼˜
3. **é™ä½å»¶è¿Ÿ**: P2P è®¿é—®æ¯”é€šè¿‡ CPU å¿«

### æ½œåœ¨å¼€é”€

1. **åˆ†é…æˆæœ¬**: `empty_strided_p2p()` å¯èƒ½æ¯”æ™®é€šåˆ†é…æ…¢ï¼ˆéœ€è¦åè°ƒï¼‰
2. **å†…å­˜ç¢ç‰‡**: Symmetric memory æœ‰ä¸åŒçš„åˆ†é…æ¨¡å¼

### ç¼“è§£ç­–ç•¥

1. **Pool é‡ç”¨**: ä¸€æ—¦åˆ†é…ï¼Œsymmetric buffers å¯ä»¥è·¨è¿­ä»£é‡ç”¨
2. **å†…å­˜è§„åˆ’**: Inductor çš„ buffer planning ä¼˜åŒ–æ•´ä½“å†…å­˜ä½¿ç”¨
3. **åŸºäºé˜ˆå€¼**: åªåœ¨æœ‰ç›Šæ—¶ä½¿ç”¨ symmetric memoryï¼ˆå¤§å°é™åˆ¶ç­‰ï¼‰

---

## å·²çŸ¥é™åˆ¶å’Œ Phase 2 TODO

1. **ç»†ç²’åº¦æ§åˆ¶**: Phase 1 å¯¹ symm_mem ops çš„æ‰€æœ‰ tensor è¾“å…¥éƒ½åˆ†é… symmetric memoryã€‚Phase 2 å¯ä»¥æ·»åŠ é€å‚æ•°æ³¨è§£ã€‚

2. **Buffer é‡ç”¨**: å¦‚æœä¸€ä¸ª buffer è¢« symmetric å’Œé symmetric ops ä½¿ç”¨ï¼Œå¯èƒ½éœ€è¦æ’å…¥æ‹·è´ã€‚Phase 2 å¯ä»¥ä¼˜åŒ–è¿™ä¸ªã€‚

3. **å¤š Process Group**: å½“å‰å‡è®¾æ¯ä¸ªæ“ä½œä¸€ä¸ª group nameã€‚Phase 2 å¯ä»¥è·Ÿè¸ªå¤šä¸ª groupsã€‚

4. **C++ Wrapper**: `empty_strided_p2p()` åªåœ¨ Python wrapper æ¨¡å¼å¯ç”¨ã€‚Phase 2 éœ€è¦ C++ API æ”¯æŒã€‚

5. **åŠ¨æ€ Shapes**: æœ‰ symbolic shapes çš„ buffers ä¸èƒ½æ˜¯ symmetricï¼ˆç”± `realize_as_comm_buffer` æ£€æŸ¥ï¼‰ã€‚Phase 2 å¯ä»¥é€šè¿‡è¿è¡Œæ—¶åˆ†é…æ”¯æŒè¿™ä¸ªã€‚

6. **Eager æ¨¡å¼å›é€€**: å½“ symm_mem ops åœ¨ eager æ¨¡å¼è¿è¡Œæ—¶ï¼Œè¾“å…¥å¯èƒ½ä¸æ˜¯ symmetricã€‚Phase 2 åº”åœ¨ dispatcher å±‚æ·»åŠ è‡ªåŠ¨å…‹éš†ã€‚

---

## éªŒè¯æ–¹æ³•

### æ–¹æ³• 1: æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç 

```python
import torch._dynamo
import torch._inductor.config

# å¼€å¯è°ƒè¯•æ—¥å¿—
torch._dynamo.config.verbose = True
torch._inductor.config.debug = True

# ç¼–è¯‘åæ£€æŸ¥ç”Ÿæˆçš„ä»£ç 
# ä¼šè¾“å‡ºåˆ° /tmp/torchinductor_<user>/xxx.py
```

åœ¨ç”Ÿæˆçš„ä»£ç ä¸­æŸ¥æ‰¾ï¼š
- âœ… `empty_strided_p2p(...)` - è¡¨ç¤ºä½¿ç”¨äº† symmetric memory
- âŒ `empty_strided_cuda(...)` - è¡¨ç¤ºä½¿ç”¨äº†æ™®é€šå†…å­˜

### æ–¹æ³• 2: æ·»åŠ æ—¥å¿—

åœ¨ `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py` ä¸­ï¼š

```python
def _one_shot_all_reduce(inp: ir.TensorBox, reduce_op, group_name):
    print(f"ğŸ”¥ Using one_shot_all_reduce with symmetric memory!")
    realize_as_comm_buffer(inp, ir.CommBufferType.SYMM_MEM, group_name)
    ...
```

---

## ç›¸å…³ä»£ç ä½ç½®

| ç»„ä»¶ | æ–‡ä»¶ | è¡Œå· | è¯´æ˜ |
|------|------|------|------|
| Lowering æ³¨å†Œ | `comm_lowering.py` | 196-199 | all_reduce çš„ lowering |
| one_shot_all_reduce | `comm_lowering.py` | 159-169 | æ ‡è®°ä¸º SYMM_MEM |
| æ¡ä»¶æ£€æŸ¥ | `comm_lowering.py` | 144-156 | 5 ä¸ªè§¦å‘æ¡ä»¶ |
| realize_as_comm_buffer | `comm_lowering.py` | 77-110 | æ ‡è®° buffer ä¸º symmetric |
| Config é»˜è®¤å€¼ | `config.py` | 891-893 | auto_select = False |
| å¯ç”¨å‡½æ•° | `_symmetric_memory/__init__.py` | 24-48 | enable_symm_mem_for_group |
| æ£€æŸ¥å‡½æ•° | `_symmetric_memory/__init__.py` | 76-85 | is_symm_mem_enabled_for_group |
| CommBufferLayout | `ir.py` | - | ç‰¹æ®Š layout ç±»å‹ |
| åˆ†é…ä»£ç ç”Ÿæˆ | `wrapper.py` | ~870 | empty_strided_p2p() ç”Ÿæˆ |

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **ç°æœ‰æœºåˆ¶å·²å®Œå–„**: `one_shot_all_reduce` å·²ç»åœ¨ä½¿ç”¨ symmetric memoryï¼Œæœºåˆ¶å®Œæ•´ä¸”éªŒè¯è¿‡
2. **é»˜è®¤ä¸è§¦å‘**: éœ€è¦ç”¨æˆ·æ‰‹åŠ¨è®¾ç½® `config._collective.auto_select = True` å’Œ `enable_symm_mem_for_group()`
3. **Phase 1 ç›®æ ‡**: æ‰©å±•åˆ°æ‰€æœ‰ `symm_mem` æ“ä½œï¼Œå¤ç”¨ç°æœ‰åŸºç¡€è®¾æ–½
4. **å®ç°ç®€å•**: ä¸»è¦å·¥ä½œæ˜¯ä¸ºæ–°æ“ä½œæ³¨å†Œ loweringï¼Œè°ƒç”¨ `realize_as_comm_buffer()`
5. **ä¸éœ€è¦æ”¹ Dynamo**: æ‰€æœ‰é€»è¾‘åœ¨ Inductor lowering é˜¶æ®µ

### å…³é”®å‡½æ•°è°ƒç”¨é“¾

```
ç”¨æˆ·è°ƒç”¨ torch.ops.symm_mem.*
    â†“
Inductor æŸ¥æ‰¾ lowering å‡½æ•°
    â†“
è°ƒç”¨ realize_as_comm_buffer(inp, SYMM_MEM, group_name)
    â†“
ä¿®æ”¹ buffer.layout = CommBufferLayout(type=SYMM_MEM)
    â†“
Codegen æ£€æŸ¥ layout ç±»å‹
    â†“
ç”Ÿæˆ empty_strided_p2p() è°ƒç”¨
    â†“
ä» symmetric memory åˆ†é…ï¼
```

---

## å‚è€ƒèµ„æ–™

- **Existing Implementation**: `/data/users/tianren/pytorch/torch/_inductor/comm_lowering.py`
- **Allocation Code**: `/data/users/tianren/pytorch/torch/_inductor/codegen/wrapper.py`
- **Buffer Layout**: `/data/users/tianren/pytorch/torch/_inductor/ir.py`
- **Symmetric Memory API**: `/data/users/tianren/pytorch/torch/distributed/_symmetric_memory/__init__.py`
