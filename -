class inner_f(torch.nn.Module):
    def forward(
        self,
        primals,
        tangents,
    ):
        primals_1: "bf16[514, 256][256, 1]cuda:0"  # PlainAOTInput(idx=0)
        primals_2: "bf16[514, 256][256, 1]cuda:0"  # PlainAOTInput(idx=1)
        primals_3: "f32[10, 64][64, 1]cuda:0"  # PlainAOTInput(idx=2)
        primals_4: "f32[256, 64][64, 1]cuda:0"  # PlainAOTInput(idx=3)
        primals_5: "f32[256][1]cuda:0"  # PlainAOTInput(idx=4)
        primals_6: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=5)
        primals_7: "f32[10, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=6)
        primals_8: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=7)
        primals_9: "f32[256, 128][128, 1]cuda:0"  # PlainAOTInput(idx=8)
        primals_10: "f32[10, 128][128, 1]cuda:0"  # PlainAOTInput(idx=9)
        primals_11: "f32[256][1]cuda:0"  # PlainAOTInput(idx=10)
        primals_12: "f32[256, 64][64, 1]cuda:0"  # PlainAOTInput(idx=11)
        primals_13: "f32[10, 64][64, 1]cuda:0"  # PlainAOTInput(idx=12)
        primals_14: "f32[256][1]cuda:0"  # PlainAOTInput(idx=13)
        primals_15: "f32[256, 100][100, 1]cuda:0"  # PlainAOTInput(idx=14)
        primals_16: "f32[10, 100][100, 1]cuda:0"  # PlainAOTInput(idx=15)
        primals_17: "f32[256][1]cuda:0"  # PlainAOTInput(idx=16)
        primals_18: "f32[256, 128][128, 1]cuda:0"  # PlainAOTInput(idx=17)
        primals_19: "f32[10, 128][128, 1]cuda:0"  # PlainAOTInput(idx=18)
        primals_20: "f32[256][1]cuda:0"  # PlainAOTInput(idx=19)
        primals_21: "f32[10, 1297][1297, 1]cuda:0"  # PlainAOTInput(idx=20)
        primals_22: "f32[88, 1304][1304, 1]cuda:0"  # PlainAOTInput(idx=21)
        primals_23: "f32[88][1]cuda:0"  # PlainAOTInput(idx=22)
        primals_24: "f32[1304, 88][88, 1]cuda:0"  # PlainAOTInput(idx=23)
        primals_25: "f32[1304][1]cuda:0"  # PlainAOTInput(idx=24)
        primals_26: "f32[2048, 1304][1304, 1]cuda:0"  # PlainAOTInput(idx=25)
        primals_27: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=26)
        primals_28: "f32[4096, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=27)
        primals_29: "f32[4096][1]cuda:0"  # PlainAOTInput(idx=28)
        primals_30: "f32[2048, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=29)
        primals_31: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=30)
        primals_32: "f32[4096, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=31)
        primals_33: "f32[4096][1]cuda:0"  # PlainAOTInput(idx=32)
        primals_34: "f32[1024, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=33)
        primals_35: "f32[10, 230, 256][58880, 256, 1]cuda:0"  # PlainAOTInput(idx=34)
        primals_36: "bf16[10, 4, 256][1024, 256, 1]cuda:0"  # PlainAOTInput(idx=35)
        primals_37: "f32[256][1]cuda:0"  # PlainAOTInput(idx=36)
        primals_38: "f32[256][1]cuda:0"  # PlainAOTInput(idx=37)
        primals_39: "f32[256][1]cuda:0"  # PlainAOTInput(idx=38)
        primals_40: "f32[2328][1]cuda:0"  # PlainAOTInput(idx=39)
        primals_41: "f32[144, 251][251, 1]cuda:0"  # PlainAOTInput(idx=40)
        primals_42: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=41)
        primals_43: "f32[512][1]cuda:0"  # PlainAOTInput(idx=42)
        primals_44: "f32[8032, 512][512, 1]cuda:0"  # PlainAOTInput(idx=43)
        primals_45: "f32[14456][1]cuda:0"  # PlainAOTInput(idx=44)
        primals_46: "f32[2048, 14456][14456, 1]cuda:0"  # PlainAOTInput(idx=45)
        primals_47: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=46)
        primals_48: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=47)
        primals_49: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=48)
        primals_50: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=49)
        primals_51: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=50)
        primals_52: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=51)
        primals_53: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=52)
        primals_54: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=53)
        primals_55: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=54)
        primals_56: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=55)
        primals_57: "f32[256][1]cuda:0"  # PlainAOTInput(idx=56)
        primals_58: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=57)
        primals_59: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=58)
        primals_60: "f32[512][1]cuda:0"  # PlainAOTInput(idx=59)
        primals_61: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=60)
        primals_62: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=61)
        primals_63: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=62)
        primals_64: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=63)
        primals_65: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=64)
        primals_66: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=65)
        primals_67: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=66)
        primals_68: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=67)
        primals_69: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=68)
        primals_70: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=69)
        primals_71: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=70)
        primals_72: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=71)
        primals_73: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=72)
        primals_74: "f32[256][1]cuda:0"  # PlainAOTInput(idx=73)
        primals_75: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=74)
        primals_76: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=75)
        primals_77: "f32[512][1]cuda:0"  # PlainAOTInput(idx=76)
        primals_78: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=77)
        primals_79: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=78)
        primals_80: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=79)
        primals_81: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=80)
        primals_82: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=81)
        primals_83: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=82)
        primals_84: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=83)
        primals_85: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=84)
        primals_86: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=85)
        primals_87: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=86)
        primals_88: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=87)
        primals_89: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=88)
        primals_90: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=89)
        primals_91: "f32[256][1]cuda:0"  # PlainAOTInput(idx=90)
        primals_92: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=91)
        primals_93: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=92)
        primals_94: "f32[512][1]cuda:0"  # PlainAOTInput(idx=93)
        primals_95: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=94)
        primals_96: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=95)
        primals_97: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=96)
        primals_98: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=97)
        primals_99: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=98)
        primals_100: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=99)
        primals_101: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=100)
        primals_102: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=101)
        primals_103: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=102)
        primals_104: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=103)
        primals_105: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=104)
        primals_106: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=105)
        primals_107: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=106)
        primals_108: "f32[256][1]cuda:0"  # PlainAOTInput(idx=107)
        primals_109: "f32[256][1]cuda:0"  # PlainAOTInput(idx=108)
        primals_110: "f32[256][1]cuda:0"  # PlainAOTInput(idx=109)
        primals_111: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=110)
        primals_112: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=111)
        primals_113: "i64[3][1]cuda:0"  # PlainAOTInput(idx=112)
        primals_114: "f32[256][1]cuda:0"  # PlainAOTInput(idx=113)
        primals_115: "f32[256][1]cuda:0"  # PlainAOTInput(idx=114)
        primals_116: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=115)
        primals_117: "i64[][]cpu"  # PlainAOTInput(idx=116)
        primals_118: "f32[256][1]cuda:0"  # PlainAOTInput(idx=117)
        primals_119: "f32[256][1]cuda:0"  # PlainAOTInput(idx=118)
        primals_120: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=119)
        primals_121: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=120)
        primals_122: "i64[3][1]cuda:0"  # PlainAOTInput(idx=121)
        primals_123: "f32[256][1]cuda:0"  # PlainAOTInput(idx=122)
        primals_124: "f32[256][1]cuda:0"  # PlainAOTInput(idx=123)
        primals_125: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=124)
        primals_126: "i64[][]cpu"  # PlainAOTInput(idx=125)
        primals_127: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=126)
        primals_128: "i64[3][1]cuda:0"  # PlainAOTInput(idx=127)
        primals_129: "f32[256][1]cuda:0"  # PlainAOTInput(idx=128)
        primals_130: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=129)
        primals_131: "f32[256][1]cuda:0"  # PlainAOTInput(idx=130)
        primals_132: "f32[256][1]cuda:0"  # PlainAOTInput(idx=131)
        primals_133: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=132)
        primals_134: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=133)
        primals_135: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=134)
        primals_136: "f32[256][1]cuda:0"  # PlainAOTInput(idx=135)
        primals_137: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=136)
        primals_138: "f32[256][1]cuda:0"  # PlainAOTInput(idx=137)
        primals_139: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=138)
        primals_140: "f32[256][1]cuda:0"  # PlainAOTInput(idx=139)
        primals_141: "f32[256][1]cuda:0"  # PlainAOTInput(idx=140)
        primals_142: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=141)
        primals_143: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=142)
        primals_144: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=143)
        primals_145: "f32[256][1]cuda:0"  # PlainAOTInput(idx=144)
        primals_146: "f32[64, 112][112, 1]cuda:0"  # PlainAOTInput(idx=145)
        primals_147: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=146)
        primals_148: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=147)
        primals_149: "f32[512][1]cuda:0"  # PlainAOTInput(idx=148)
        primals_150: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=149)
        primals_151: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=150)
        primals_152: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=151)
        primals_153: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=152)
        primals_154: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=153)
        primals_155: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=154)
        primals_156: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=155)
        primals_157: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=156)
        primals_158: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=157)
        primals_159: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=158)
        primals_160: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=159)
        primals_161: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=160)
        primals_162: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=161)
        primals_163: "f32[256][1]cuda:0"  # PlainAOTInput(idx=162)
        primals_164: "f32[256][1]cuda:0"  # PlainAOTInput(idx=163)
        primals_165: "f32[256][1]cuda:0"  # PlainAOTInput(idx=164)
        primals_166: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=165)
        primals_167: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=166)
        primals_168: "f32[256][1]cuda:0"  # PlainAOTInput(idx=167)
        primals_169: "f32[256][1]cuda:0"  # PlainAOTInput(idx=168)
        primals_170: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=169)
        primals_171: "i64[][]cpu"  # PlainAOTInput(idx=170)
        primals_172: "f32[256][1]cuda:0"  # PlainAOTInput(idx=171)
        primals_173: "f32[256][1]cuda:0"  # PlainAOTInput(idx=172)
        primals_174: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=173)
        primals_175: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=174)
        primals_176: "f32[256][1]cuda:0"  # PlainAOTInput(idx=175)
        primals_177: "f32[256][1]cuda:0"  # PlainAOTInput(idx=176)
        primals_178: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=177)
        primals_179: "i64[][]cpu"  # PlainAOTInput(idx=178)
        primals_180: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=179)
        primals_181: "f32[256][1]cuda:0"  # PlainAOTInput(idx=180)
        primals_182: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=181)
        primals_183: "f32[256][1]cuda:0"  # PlainAOTInput(idx=182)
        primals_184: "f32[256][1]cuda:0"  # PlainAOTInput(idx=183)
        primals_185: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=184)
        primals_186: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=185)
        primals_187: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=186)
        primals_188: "f32[256][1]cuda:0"  # PlainAOTInput(idx=187)
        primals_189: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=188)
        primals_190: "f32[256][1]cuda:0"  # PlainAOTInput(idx=189)
        primals_191: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=190)
        primals_192: "f32[256][1]cuda:0"  # PlainAOTInput(idx=191)
        primals_193: "f32[256][1]cuda:0"  # PlainAOTInput(idx=192)
        primals_194: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=193)
        primals_195: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=194)
        primals_196: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=195)
        primals_197: "f32[256][1]cuda:0"  # PlainAOTInput(idx=196)
        primals_198: "f32[64, 112][112, 1]cuda:0"  # PlainAOTInput(idx=197)
        primals_199: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=198)
        primals_200: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=199)
        primals_201: "f32[512][1]cuda:0"  # PlainAOTInput(idx=200)
        primals_202: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=201)
        primals_203: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=202)
        primals_204: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=203)
        primals_205: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=204)
        primals_206: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=205)
        primals_207: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=206)
        primals_208: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=207)
        primals_209: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=208)
        primals_210: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=209)
        primals_211: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=210)
        primals_212: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=211)
        primals_213: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=212)
        primals_214: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=213)
        primals_215: "f32[256][1]cuda:0"  # PlainAOTInput(idx=214)
        primals_216: "f32[256][1]cuda:0"  # PlainAOTInput(idx=215)
        primals_217: "f32[256][1]cuda:0"  # PlainAOTInput(idx=216)
        primals_218: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=217)
        primals_219: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=218)
        primals_220: "f32[256][1]cuda:0"  # PlainAOTInput(idx=219)
        primals_221: "f32[256][1]cuda:0"  # PlainAOTInput(idx=220)
        primals_222: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=221)
        primals_223: "i64[][]cpu"  # PlainAOTInput(idx=222)
        primals_224: "f32[256][1]cuda:0"  # PlainAOTInput(idx=223)
        primals_225: "f32[256][1]cuda:0"  # PlainAOTInput(idx=224)
        primals_226: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=225)
        primals_227: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=226)
        primals_228: "f32[256][1]cuda:0"  # PlainAOTInput(idx=227)
        primals_229: "f32[256][1]cuda:0"  # PlainAOTInput(idx=228)
        primals_230: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=229)
        primals_231: "i64[][]cpu"  # PlainAOTInput(idx=230)
        primals_232: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=231)
        primals_233: "f32[256][1]cuda:0"  # PlainAOTInput(idx=232)
        primals_234: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=233)
        primals_235: "f32[256][1]cuda:0"  # PlainAOTInput(idx=234)
        primals_236: "f32[256][1]cuda:0"  # PlainAOTInput(idx=235)
        primals_237: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=236)
        primals_238: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=237)
        primals_239: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=238)
        primals_240: "f32[256][1]cuda:0"  # PlainAOTInput(idx=239)
        primals_241: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=240)
        primals_242: "f32[256][1]cuda:0"  # PlainAOTInput(idx=241)
        primals_243: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=242)
        primals_244: "f32[256][1]cuda:0"  # PlainAOTInput(idx=243)
        primals_245: "f32[256][1]cuda:0"  # PlainAOTInput(idx=244)
        primals_246: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=245)
        primals_247: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=246)
        primals_248: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=247)
        primals_249: "f32[256][1]cuda:0"  # PlainAOTInput(idx=248)
        primals_250: "f32[64, 112][112, 1]cuda:0"  # PlainAOTInput(idx=249)
        primals_251: "f32[80, 64][64, 1]cuda:0"  # PlainAOTInput(idx=250)
        primals_252: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=251)
        primals_253: "f32[512][1]cuda:0"  # PlainAOTInput(idx=252)
        primals_254: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=253)
        primals_255: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=254)
        primals_256: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=255)
        primals_257: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=256)
        primals_258: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=257)
        primals_259: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=258)
        primals_260: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=259)
        primals_261: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=260)
        primals_262: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=261)
        primals_263: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=262)
        primals_264: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=263)
        primals_265: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=264)
        primals_266: "f32[8192, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=265)
        primals_267: "f32[256][1]cuda:0"  # PlainAOTInput(idx=266)
        primals_268: "f32[256][1]cuda:0"  # PlainAOTInput(idx=267)
        primals_269: "f32[256][1]cuda:0"  # PlainAOTInput(idx=268)
        primals_270: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=269)
        primals_271: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=270)
        primals_272: "f32[256][1]cuda:0"  # PlainAOTInput(idx=271)
        primals_273: "f32[256][1]cuda:0"  # PlainAOTInput(idx=272)
        primals_274: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=273)
        primals_275: "i64[][]cpu"  # PlainAOTInput(idx=274)
        primals_276: "f32[256][1]cuda:0"  # PlainAOTInput(idx=275)
        primals_277: "f32[256][1]cuda:0"  # PlainAOTInput(idx=276)
        primals_278: "f32[256, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=277)
        primals_279: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=278)
        primals_280: "f32[256][1]cuda:0"  # PlainAOTInput(idx=279)
        primals_281: "f32[256][1]cuda:0"  # PlainAOTInput(idx=280)
        primals_282: "f32[768, 256][256, 1]cuda:0"  # PlainAOTInput(idx=281)
        primals_283: "i64[][]cpu"  # PlainAOTInput(idx=282)
        primals_284: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=283)
        primals_285: "f32[256][1]cuda:0"  # PlainAOTInput(idx=284)
        primals_286: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=285)
        primals_287: "f32[256][1]cuda:0"  # PlainAOTInput(idx=286)
        primals_288: "f32[256][1]cuda:0"  # PlainAOTInput(idx=287)
        primals_289: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=288)
        primals_290: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=289)
        primals_291: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=290)
        primals_292: "f32[256][1]cuda:0"  # PlainAOTInput(idx=291)
        primals_293: "f32[16, 64][64, 1]cuda:0"  # PlainAOTInput(idx=292)
        primals_294: "f32[256][1]cuda:0"  # PlainAOTInput(idx=293)
        primals_295: "f32[256, 256][256, 1]cuda:0"  # PlainAOTInput(idx=294)
        primals_296: "f32[256][1]cuda:0"  # PlainAOTInput(idx=295)
        primals_297: "f32[256][1]cuda:0"  # PlainAOTInput(idx=296)
        primals_298: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=297)
        primals_299: "f32[8192, 4096][4096, 1]cuda:0"  # PlainAOTInput(idx=298)
        primals_300: "f32[4096, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=299)
        primals_301: "f32[256][1]cuda:0"  # PlainAOTInput(idx=300)
        primals_302: "f32[64, 112][112, 1]cuda:0"  # PlainAOTInput(idx=301)
        primals_303: "f32[48, 64][64, 1]cuda:0"  # PlainAOTInput(idx=302)
        primals_304: "f32[512, 8192][8192, 1]cuda:0"  # PlainAOTInput(idx=303)
        primals_305: "f32[512][1]cuda:0"  # PlainAOTInput(idx=304)
        primals_306: "f32[2048, 512][512, 1]cuda:0"  # PlainAOTInput(idx=305)
        primals_307: "f32[8472][1]cuda:0"  # PlainAOTInput(idx=306)
        primals_308: "f32[2048, 8472][8472, 1]cuda:0"  # PlainAOTInput(idx=307)
        primals_309: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=308)
        primals_310: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=309)
        primals_311: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=310)
        primals_312: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=311)
        primals_313: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=312)
        primals_314: "f32[1024, 2048][2048, 1]cuda:0"  # PlainAOTInput(idx=313)
        primals_315: "f32[1024][1]cuda:0"  # PlainAOTInput(idx=314)
        primals_316: "f32[2048, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=315)
        primals_317: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=316)
        primals_318: "f32[2048][1]cuda:0"  # PlainAOTInput(idx=317)
        primals_319: "f32[18, 2048, 1024][2097152, 1024, 1]cuda:0"  # PlainAOTInput(idx=318)
        primals_320: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=319)
        primals_321: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=320)
        primals_322: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0"  # PlainAOTInput(idx=321)
        primals_323: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=322)
        primals_324: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=323)
        primals_325: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0"  # PlainAOTInput(idx=324)
        primals_326: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=325)
        primals_327: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=326)
        primals_328: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0"  # PlainAOTInput(idx=327)
        primals_329: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=328)
        primals_330: "f32[18, 1024][1024, 1]cuda:0"  # PlainAOTInput(idx=329)
        primals_331: "f32[18, 1024, 1][1024, 1, 1]cuda:0"  # PlainAOTInput(idx=330)
        tangents_1: "bf16[u31, 256][256, 1]cuda:0"  # TangentAOTInput(output=PlainAOTOutput(idx=18))
        tangents_2: "bf16[u23, 256][256, 1]cuda:0"  # TangentAOTInput(output=PlainAOTOutput(idx=19))
        tangents_3: "f32[18, 10, 1][10, 1, 1]cuda:0"  # TangentAOTInput(output=IntermediateBaseAOTOutput(base_of=PlainAOTOutput(idx=0)))
        primals_1, primals_2, primals_3, primals_4, primals_5, primals_6, primals_7, primals_8, primals_9, primals_10, primals_11, primals_12, primals_13, primals_14, primals_15, primals_16, primals_17, primals_18, primals_19, primals_20, primals_21, primals_22, primals_23, primals_24, primals_25, primals_26, primals_27, primals_28, primals_29, primals_30, primals_31, primals_32, primals_33, primals_34, primals_35, primals_36, primals_37, primals_38, primals_39, primals_40, primals_41, primals_42, primals_43, primals_44, primals_45, primals_46, primals_47, primals_48, primals_49, primals_50, primals_51, primals_52, primals_53, primals_54, primals_55, primals_56, primals_57, primals_58, primals_59, primals_60, primals_61, primals_62, primals_63, primals_64, primals_65, primals_66, primals_67, primals_68, primals_69, primals_70, primals_71, primals_72, primals_73, primals_74, primals_75, primals_76, primals_77, primals_78, primals_79, primals_80, primals_81, primals_82, primals_83, primals_84, primals_85, primals_86, primals_87, primals_88, primals_89, primals_90, primals_91, primals_92, primals_93, primals_94, primals_95, primals_96, primals_97, primals_98, primals_99, primals_100, primals_101, primals_102, primals_103, primals_104, primals_105, primals_106, primals_107, primals_108, primals_109, primals_110, primals_111, primals_112, primals_113, primals_114, primals_115, primals_116, primals_117, primals_118, primals_119, primals_120, primals_121, primals_122, primals_123, primals_124, primals_125, primals_126, primals_127, primals_128, primals_129, primals_130, primals_131, primals_132, primals_133, primals_134, primals_135, primals_136, primals_137, primals_138, primals_139, primals_140, primals_141, primals_142, primals_143, primals_144, primals_145, primals_146, primals_147, primals_148, primals_149, primals_150, primals_151, primals_152, primals_153, primals_154, primals_155, primals_156, primals_157, primals_158, primals_159, primals_160, primals_161, primals_162, primals_163, primals_164, primals_165, primals_166, primals_167, primals_168, primals_169, primals_170, primals_171, primals_172, primals_173, primals_174, primals_175, primals_176, primals_177, primals_178, primals_179, primals_180, primals_181, primals_182, primals_183, primals_184, primals_185, primals_186, primals_187, primals_188, primals_189, primals_190, primals_191, primals_192, primals_193, primals_194, primals_195, primals_196, primals_197, primals_198, primals_199, primals_200, primals_201, primals_202, primals_203, primals_204, primals_205, primals_206, primals_207, primals_208, primals_209, primals_210, primals_211, primals_212, primals_213, primals_214, primals_215, primals_216, primals_217, primals_218, primals_219, primals_220, primals_221, primals_222, primals_223, primals_224, primals_225, primals_226, primals_227, primals_228, primals_229, primals_230, primals_231, primals_232, primals_233, primals_234, primals_235, primals_236, primals_237, primals_238, primals_239, primals_240, primals_241, primals_242, primals_243, primals_244, primals_245, primals_246, primals_247, primals_248, primals_249, primals_250, primals_251, primals_252, primals_253, primals_254, primals_255, primals_256, primals_257, primals_258, primals_259, primals_260, primals_261, primals_262, primals_263, primals_264, primals_265, primals_266, primals_267, primals_268, primals_269, primals_270, primals_271, primals_272, primals_273, primals_274, primals_275, primals_276, primals_277, primals_278, primals_279, primals_280, primals_281, primals_282, primals_283, primals_284, primals_285, primals_286, primals_287, primals_288, primals_289, primals_290, primals_291, primals_292, primals_293, primals_294, primals_295, primals_296, primals_297, primals_298, primals_299, primals_300, primals_301, primals_302, primals_303, primals_304, primals_305, primals_306, primals_307, primals_308, primals_309, primals_310, primals_311, primals_312, primals_313, primals_314, primals_315, primals_316, primals_317, primals_318, primals_319, primals_320, primals_321, primals_322, primals_323, primals_324, primals_325, primals_326, primals_327, primals_328, primals_329, primals_330, primals_331, tangents_1, tangents_2, tangents_3, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type: "bf16[10, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_3, torch.bfloat16);  primals_3 = None
        convert_element_type_1: "bf16[256, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_4, torch.bfloat16);  primals_4 = None
        permute: "bf16[64, 256][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1, [1, 0]);  convert_element_type_1 = None
        mm: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type, permute);  permute = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm, [-1, 256]);  mm = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_4: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view, torch.float32);  view = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_1: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_4, 2)
        mean: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_1, [1], True);  pow_1 = None
        add: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean, 1e-05);  mean = None
        rsqrt: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add);  add = None
        mul: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, rsqrt)
        mul_1: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul, primals_5);  mul = None
        alias: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt);  rsqrt = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_5: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1, torch.bfloat16);  mul_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_5, [10, 256]);  convert_element_type_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type_6: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_7, torch.bfloat16);  primals_7 = None
        convert_element_type_7: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_6, torch.bfloat16);  primals_6 = None
        permute_1: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_7, [1, 0]);  convert_element_type_7 = None
        mm_1: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_6, permute_1);  permute_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_2: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_1, [-1, 2048]);  mm_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_10: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_2, torch.float32);  view_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_2: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_10, 2)
        mean_1: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_2, [1], True);  pow_2 = None
        add_1: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_1, 1e-05);  mean_1 = None
        rsqrt_1: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_1);  add_1 = None
        mul_2: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, rsqrt_1)
        mul_3: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_2, primals_8);  mul_2 = None
        alias_1: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_1);  rsqrt_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_11: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_3, torch.bfloat16);  mul_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_3: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_11, [10, 2048]);  convert_element_type_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type_12: "bf16[10, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_10, torch.bfloat16);  primals_10 = None
        convert_element_type_13: "bf16[256, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_9, torch.bfloat16);  primals_9 = None
        permute_2: "bf16[128, 256][1, 128]cuda:0" = torch.ops.aten.permute.default(convert_element_type_13, [1, 0]);  convert_element_type_13 = None
        mm_2: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_12, permute_2);  permute_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_4: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_2, [-1, 256]);  mm_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_16: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_4, torch.float32);  view_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_3: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_16, 2)
        mean_2: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_3, [1], True);  pow_3 = None
        add_2: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_2, 1e-05);  mean_2 = None
        rsqrt_2: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_2);  add_2 = None
        mul_4: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, rsqrt_2)
        mul_5: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_4, primals_11);  mul_4 = None
        alias_2: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_2);  rsqrt_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_17: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_5, torch.bfloat16);  mul_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_5: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_17, [10, 256]);  convert_element_type_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type_18: "bf16[10, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_13, torch.bfloat16);  primals_13 = None
        convert_element_type_19: "bf16[256, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_12, torch.bfloat16);  primals_12 = None
        permute_3: "bf16[64, 256][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_19, [1, 0]);  convert_element_type_19 = None
        mm_3: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_18, permute_3);  permute_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_6: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_3, [-1, 256]);  mm_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_22: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_6, torch.float32);  view_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_4: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_22, 2)
        mean_3: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_4, [1], True);  pow_4 = None
        add_3: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_3, 1e-05);  mean_3 = None
        rsqrt_3: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_3);  add_3 = None
        mul_6: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_22, rsqrt_3)
        mul_7: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_6, primals_14);  mul_6 = None
        alias_3: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_3);  rsqrt_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_23: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_7, torch.bfloat16);  mul_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_7: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_23, [10, 256]);  convert_element_type_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type_24: "bf16[10, 100][100, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_16, torch.bfloat16);  primals_16 = None
        convert_element_type_25: "bf16[256, 100][100, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_15, torch.bfloat16);  primals_15 = None
        permute_4: "bf16[100, 256][1, 100]cuda:0" = torch.ops.aten.permute.default(convert_element_type_25, [1, 0]);  convert_element_type_25 = None
        mm_4: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_24, permute_4);  permute_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_8: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_4, [-1, 256]);  mm_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_28: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_8, torch.float32);  view_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_5: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_28, 2)
        mean_4: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_5, [1], True);  pow_5 = None
        add_4: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_4, 1e-05);  mean_4 = None
        rsqrt_4: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_4);  add_4 = None
        mul_8: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_28, rsqrt_4)
        mul_9: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_8, primals_17);  mul_8 = None
        alias_4: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_4);  rsqrt_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_29: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_9, torch.bfloat16);  mul_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_9: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_29, [10, 256]);  convert_element_type_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        convert_element_type_30: "bf16[10, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_19, torch.bfloat16);  primals_19 = None
        convert_element_type_31: "bf16[256, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_18, torch.bfloat16);  primals_18 = None
        permute_5: "bf16[128, 256][1, 128]cuda:0" = torch.ops.aten.permute.default(convert_element_type_31, [1, 0]);  convert_element_type_31 = None
        mm_5: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_30, permute_5);  permute_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_10: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_5, [-1, 256]);  mm_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_34: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_10, torch.float32);  view_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_6: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_34, 2)
        mean_5: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_6, [1], True);  pow_6 = None
        add_5: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_5, 1e-05);  mean_5 = None
        rsqrt_5: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_5);  add_5 = None
        mul_10: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_34, rsqrt_5)
        mul_11: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_10, primals_20);  mul_10 = None
        alias_5: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_5);  rsqrt_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_35: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_11, torch.bfloat16);  mul_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_11: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_35, [10, 256]);  convert_element_type_35 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:807 in forward, code: output = torch.cat(
        cat: "bf16[10, 3328][3328, 1]cuda:0" = torch.ops.aten.cat.default([view_1, view_3, view_5, view_7, view_9, view_11], 1);  view_1 = view_3 = view_5 = view_7 = view_9 = view_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:813 in forward, code: ).view(
        view_12: "bf16[10, 13, 256][3328, 256, 1]cuda:0" = torch.ops.aten.view.default(cat, [-1, 13, 256]);  cat = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:5429 in pad, code: return torch._C._nn.pad(input, pad, mode, value)
        constant_pad_nd: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.constant_pad_nd.default(primals_21, [0, 7], 0.0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:369 in forward, code: return input_tensor * self.excitation(input_tensor)
        convert_element_type_36: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(constant_pad_nd, torch.bfloat16)
        convert_element_type_37: "bf16[88, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_22, torch.bfloat16);  primals_22 = None
        convert_element_type_38: "bf16[88][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_23, torch.bfloat16);  primals_23 = None
        permute_6: "bf16[1304, 88][1, 1304]cuda:0" = torch.ops.aten.permute.default(convert_element_type_37, [1, 0]);  convert_element_type_37 = None
        addmm: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_38, convert_element_type_36, permute_6);  convert_element_type_38 = permute_6 = None
        relu: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.relu.default(addmm);  addmm = None
        alias_6: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.alias.default(relu)
        convert_element_type_42: "bf16[1304, 88][88, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_24, torch.bfloat16);  primals_24 = None
        convert_element_type_43: "bf16[1304][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_25, torch.bfloat16);  primals_25 = None
        permute_7: "bf16[88, 1304][1, 88]cuda:0" = torch.ops.aten.permute.default(convert_element_type_42, [1, 0]);  convert_element_type_42 = None
        addmm_1: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_43, relu, permute_7);  convert_element_type_43 = None
        sigmoid: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.aten.sigmoid.default(addmm_1);  addmm_1 = None
        alias_7: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.aten.alias.default(sigmoid)
        mul_12: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.mul.Tensor(constant_pad_nd, sigmoid);  sigmoid = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_47: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_12, torch.bfloat16);  mul_12 = None
        convert_element_type_48: "bf16[2048, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_26, torch.bfloat16);  primals_26 = None
        permute_8: "bf16[1304, 2048][1, 1304]cuda:0" = torch.ops.aten.permute.default(convert_element_type_48, [1, 0]);  convert_element_type_48 = None
        mm_6: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_47, permute_8)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_13: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_6, [-1, 2048]);  mm_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_51: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_13, torch.float32);  view_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_7: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_51, 2)
        mean_6: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_7, [1], True);  pow_7 = None
        add_6: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_6, 1e-05);  mean_6 = None
        rsqrt_6: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_6);  add_6 = None
        mul_13: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_51, rsqrt_6)
        mul_14: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_13, primals_27);  mul_13 = None
        alias_8: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_6);  rsqrt_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_52: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_14, torch.bfloat16);  mul_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_14: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_52, [10, 2048]);  convert_element_type_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_53: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_14, torch.float32)
        neg: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_53)
        exp: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg);  neg = None
        add_7: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp, 1);  exp = None
        div: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_53, add_7);  convert_element_type_53 = add_7 = None
        convert_element_type_54: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div, torch.bfloat16);  div = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_55: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_28, torch.bfloat16);  primals_28 = None
        permute_9: "bf16[2048, 4096][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_55, [1, 0]);  convert_element_type_55 = None
        mm_7: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_54, permute_9)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_15: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(mm_7, [-1, 4096]);  mm_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_58: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_15, torch.float32);  view_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_8: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_58, 2)
        mean_7: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_8, [1], True);  pow_8 = None
        add_8: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_7, 1e-05);  mean_7 = None
        rsqrt_7: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_8);  add_8 = None
        mul_15: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_58, rsqrt_7)
        mul_16: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_15, primals_29);  mul_15 = None
        alias_9: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_7);  rsqrt_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_59: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_16, torch.bfloat16);  mul_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_16: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_59, [10, 4096]);  convert_element_type_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_60: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_16, torch.float32)
        neg_1: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_60)
        exp_1: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.exp.default(neg_1);  neg_1 = None
        add_9: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_1, 1);  exp_1 = None
        div_1: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_60, add_9);  convert_element_type_60 = add_9 = None
        convert_element_type_61: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_1, torch.bfloat16);  div_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_62: "bf16[2048, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_30, torch.bfloat16);  primals_30 = None
        permute_10: "bf16[4096, 2048][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_62, [1, 0]);  convert_element_type_62 = None
        mm_8: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_61, permute_10)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_10: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_54, mm_8);  mm_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_17: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_10, [-1, 2048]);  add_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_65: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_17, torch.float32);  view_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_9: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_65, 2)
        mean_8: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_9, [1], True);  pow_9 = None
        add_11: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_8, 1e-05);  mean_8 = None
        rsqrt_8: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_11);  add_11 = None
        mul_17: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_65, rsqrt_8)
        mul_18: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_17, primals_31);  mul_17 = None
        alias_10: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_8);  rsqrt_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_66: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_18, torch.bfloat16);  mul_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_18: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_66, [10, 2048]);  convert_element_type_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_67: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_18, torch.float32)
        neg_2: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_67)
        exp_2: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_2);  neg_2 = None
        add_12: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_2, 1);  exp_2 = None
        div_2: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_67, add_12);  convert_element_type_67 = add_12 = None
        convert_element_type_68: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_2, torch.bfloat16);  div_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_69: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_32, torch.bfloat16);  primals_32 = None
        permute_11: "bf16[2048, 4096][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_69, [1, 0]);  convert_element_type_69 = None
        mm_9: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_68, permute_11)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_19: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(mm_9, [-1, 4096]);  mm_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_72: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_19, torch.float32);  view_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_10: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_72, 2)
        mean_9: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_10, [1], True);  pow_10 = None
        add_13: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_9, 1e-05);  mean_9 = None
        rsqrt_9: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_13);  add_13 = None
        mul_19: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_72, rsqrt_9)
        mul_20: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_19, primals_33);  mul_19 = None
        alias_11: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_9);  rsqrt_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_73: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_20, torch.bfloat16);  mul_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_20: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_73, [10, 4096]);  convert_element_type_73 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_74: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_20, torch.float32)
        neg_3: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_74)
        exp_3: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.exp.default(neg_3);  neg_3 = None
        add_14: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_3, 1);  exp_3 = None
        div_3: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_74, add_14);  convert_element_type_74 = add_14 = None
        convert_element_type_75: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_3, torch.bfloat16);  div_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_76: "bf16[1024, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_34, torch.bfloat16);  primals_34 = None
        permute_12: "bf16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_76, [1, 0]);  convert_element_type_76 = None
        mm_10: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_75, permute_12)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:544 in forward, code: output = self._mlp(gated_output).reshape(
        view_21: "bf16[10, 4, 256][1024, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_10, [-1, 4, 256]);  mm_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:166 in forward, code: dense_proj = dense_embedding.reshape(
        view_22: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(view_21, [-1, 1024])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:5429 in pad, code: return torch._C._nn.pad(input, pad, mode, value)
        constant_pad_nd_1: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.constant_pad_nd.default(primals_21, [0, 7], 0.0);  primals_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:458 in forward, code: dense_proj = torch.cat([dense_projection, dense_features], dim=1)
        cat_1: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.cat.default([view_22, constant_pad_nd_1], 1);  view_22 = constant_pad_nd_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:460 in forward, code: x = torch.cat(
        cat_2: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.cat.default([primals_35, view_12, primals_36, view_21], 1);  primals_35 = view_12 = primals_36 = view_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_23: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(primals_2, [-1, 256]);  primals_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_1: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 20, grid = [(514, 1, 1), (514, 1, 1), (129, 1, 1), (129, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_23, 'Y': empty, 'W': primals_37, 'Rstd': empty_1}, tensors_to_clone = ['Y', 'Rstd']);  empty = empty_1 = None
        getitem: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy['Y']
        getitem_1: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy['Rstd'];  triton_kernel_wrapper_functional_proxy = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_25: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(primals_1, [-1, 256]);  primals_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_2: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_3: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_1 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 22, grid = [(514, 1, 1), (514, 1, 1), (129, 1, 1), (129, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_25, 'Y': empty_2, 'W': primals_38, 'Rstd': empty_3}, tensors_to_clone = ['Y', 'Rstd']);  empty_2 = empty_3 = None
        getitem_2: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_1['Y']
        getitem_3: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_1['Rstd'];  triton_kernel_wrapper_functional_proxy_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_27: "f32[2510, 256][256, 1]cuda:0" = torch.ops.aten.view.default(cat_2, [-1, 256]);  cat_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_4: "f32[2510, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([2510, 256], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_5: "f32[2510][1]cuda:0" = torch.ops.aten.empty.memory_format([2510], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_2 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 24, grid = [(2510, 1, 1), (2510, 1, 1), (628, 1, 1), (628, 1, 1), (157, 1, 1), (157, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_27, 'Y': empty_4, 'W': primals_39, 'Rstd': empty_5}, tensors_to_clone = ['Y', 'Rstd']);  empty_4 = empty_5 = None
        getitem_4: "f32[2510, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_2['Y']
        getitem_5: "f32[2510][1]cuda:0" = triton_kernel_wrapper_functional_proxy_2['Rstd'];  triton_kernel_wrapper_functional_proxy_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_29: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.view.default(cat_1, [-1, 2328]);  cat_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_6: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 2328], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_7: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_3 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 26, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_29, 'Y': empty_6, 'W': primals_40, 'Rstd': empty_7}, tensors_to_clone = ['Y', 'Rstd']);  empty_6 = empty_7 = None
        getitem_6: "f32[10, 2328][2328, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_3['Y']
        getitem_7: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_3['Rstd'];  triton_kernel_wrapper_functional_proxy_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_31: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_4, [10, 251, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_32: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_4, [10, 251, 256])
        permute_14: "f32[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_32, [0, 2, 1]);  view_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_79: "bf16[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.prims.convert_element_type.default(permute_14, torch.bfloat16);  permute_14 = None
        convert_element_type_80: "bf16[144, 251][251, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_41, torch.bfloat16);  primals_41 = None
        permute_15: "bf16[251, 144][1, 251]cuda:0" = torch.ops.aten.permute.default(convert_element_type_80, [1, 0]);  convert_element_type_80 = None
        clone: "bf16[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.aten.clone.default(convert_element_type_79, memory_format = torch.contiguous_format);  convert_element_type_79 = None
        view_33: "bf16[2560, 251][251, 1]cuda:0" = torch.ops.aten.view.default(clone, [2560, 251]);  clone = None
        mm_11: "bf16[2560, 144][144, 1]cuda:0" = torch.ops.aten.mm.default(view_33, permute_15)
        view_34: "bf16[10, 256, 144][36864, 144, 1]cuda:0" = torch.ops.aten.view.default(mm_11, [10, 256, 144]);  mm_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_16: "bf16[10, 144, 256][36864, 1, 144]cuda:0" = torch.ops.aten.permute.default(view_34, [0, 2, 1]);  view_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes = torch.ops.aten.split_with_sizes.default(permute_16, [64, 32, 32, 16], 1);  permute_16 = None
        getitem_8: "bf16[10, 64, 256][36864, 1, 144]cuda:0" = split_with_sizes[0]
        getitem_9: "bf16[10, 32, 256][36864, 1, 144]cuda:0" = split_with_sizes[1]
        getitem_10: "bf16[10, 32, 256][36864, 1, 144]cuda:0" = split_with_sizes[2]
        getitem_11: "bf16[10, 16, 256][36864, 1, 144]cuda:0" = split_with_sizes[3];  split_with_sizes = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_1: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_10, memory_format = torch.contiguous_format);  getitem_10 = None
        view_35: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_1, [10, 8192]);  clone_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_83: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_42, torch.bfloat16);  primals_42 = None
        permute_17: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_83, [1, 0]);  convert_element_type_83 = None
        mm_12: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_35, permute_17)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_36: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_12, [-1, 512]);  mm_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_86: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_36, torch.float32);  view_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_11: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_86, 2)
        mean_10: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_11, [1], True);  pow_11 = None
        add_15: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_10, 1e-05);  mean_10 = None
        rsqrt_10: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_15);  add_15 = None
        mul_21: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_86, rsqrt_10)
        mul_22: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_21, primals_43);  mul_21 = None
        alias_12: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_10);  rsqrt_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_87: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_22, torch.bfloat16);  mul_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_37: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_87, [10, 512]);  convert_element_type_87 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_88: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_37, torch.float32)
        neg_4: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_88)
        exp_4: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_4);  neg_4 = None
        add_16: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_4, 1);  exp_4 = None
        div_4: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_88, add_16);  convert_element_type_88 = add_16 = None
        convert_element_type_89: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_4, torch.bfloat16);  div_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_90: "bf16[8032, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_44, torch.bfloat16);  primals_44 = None
        permute_18: "bf16[512, 8032][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_90, [1, 0]);  convert_element_type_90 = None
        mm_13: "bf16[10, 8032][8032, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_89, permute_18)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_38: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.view.default(mm_13, [10, -1, 251]);  mm_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        convert_element_type_93: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_31, torch.bfloat16);  view_31 = None
        expand: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.expand.default(view_38, [10, 32, 251]);  view_38 = None
        view_39: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.view.default(expand, [10, 32, 251]);  expand = None
        expand_1: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.expand.default(convert_element_type_93, [10, 251, 256]);  convert_element_type_93 = None
        view_40: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_1, [10, 251, 256]);  expand_1 = None
        bmm: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_39, view_40)
        view_41: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm, [10, 32, 256]);  bmm = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_42: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_4, [10, 251, 256]);  getitem_4 = None
        permute_20: "f32[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_42, [0, 2, 1]);  view_42 = None
        convert_element_type_96: "bf16[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.prims.convert_element_type.default(permute_20, torch.bfloat16);  permute_20 = None
        expand_2: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_41, [10, 32, 256]);  view_41 = None
        view_43: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_2, [10, 32, 256]);  expand_2 = None
        expand_3: "bf16[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.aten.expand.default(convert_element_type_96, [10, 256, 251]);  convert_element_type_96 = None
        view_44: "bf16[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_3, [10, 256, 251]);  expand_3 = None
        bmm_1: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.bmm.default(view_43, view_44)
        view_45: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.view.default(bmm_1, [10, 32, 251]);  bmm_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_46: "bf16[10, 8032][8032, 1]cuda:0" = torch.ops.aten.view.default(view_45, [10, -1]);  view_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_2: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_11, memory_format = torch.contiguous_format);  getitem_11 = None
        view_47: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_2, [10, 4096]);  clone_2 = None
        cat_3: "bf16[10, 12128][12128, 1]cuda:0" = torch.ops.aten.cat.default([view_46, view_47], 1);  view_46 = view_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_48: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.view.default(getitem_6, [10, 2328]);  getitem_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_4: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.cat.default([cat_3, view_48], 1);  cat_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_49: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.view.default(cat_4, [-1, 14456]);  cat_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_8: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 14456], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_9: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_4 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 28, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_49, 'Y': empty_8, 'W': primals_45, 'Rstd': empty_9}, tensors_to_clone = ['Y', 'Rstd']);  empty_8 = empty_9 = None
        getitem_12: "f32[10, 14456][14456, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_4['Y']
        getitem_13: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_4['Rstd'];  triton_kernel_wrapper_functional_proxy_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_51: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.view.default(getitem_12, [10, 14456]);  getitem_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_99: "bf16[10, 14456][14456, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_51, torch.bfloat16);  view_51 = None
        convert_element_type_100: "bf16[2048, 14456][14456, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_46, torch.bfloat16);  primals_46 = None
        permute_21: "bf16[14456, 2048][1, 14456]cuda:0" = torch.ops.aten.permute.default(convert_element_type_100, [1, 0]);  convert_element_type_100 = None
        mm_14: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_99, permute_21)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_52: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_14, [-1, 2048]);  mm_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_103: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_52, torch.float32);  view_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_12: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_103, 2)
        mean_11: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_12, [1], True);  pow_12 = None
        add_17: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_11, 1e-05);  mean_11 = None
        rsqrt_11: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_17);  add_17 = None
        mul_23: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_103, rsqrt_11)
        mul_24: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_23, primals_47);  mul_23 = None
        alias_13: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_11);  rsqrt_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_104: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_24, torch.bfloat16);  mul_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_53: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_104, [10, 2048]);  convert_element_type_104 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_105: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_53, torch.float32)
        neg_5: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_105)
        exp_5: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_5);  neg_5 = None
        add_18: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_5, 1);  exp_5 = None
        div_5: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_105, add_18);  convert_element_type_105 = add_18 = None
        convert_element_type_106: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_5, torch.bfloat16);  div_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_107: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_48, torch.bfloat16);  primals_48 = None
        permute_22: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_107, [1, 0]);  convert_element_type_107 = None
        mm_15: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_106, permute_22)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_54: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_15, [-1, 1024]);  mm_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_110: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_54, torch.float32);  view_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_13: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_110, 2)
        mean_12: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_13, [1], True);  pow_13 = None
        add_19: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_12, 1e-05);  mean_12 = None
        rsqrt_12: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_19);  add_19 = None
        mul_25: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_110, rsqrt_12)
        mul_26: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_25, primals_49);  mul_25 = None
        alias_14: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_12);  rsqrt_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_111: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_26, torch.bfloat16);  mul_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_55: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_111, [10, 1024]);  convert_element_type_111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_112: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_55, torch.float32)
        neg_6: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_112)
        exp_6: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_6);  neg_6 = None
        add_20: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_6, 1);  exp_6 = None
        div_6: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_112, add_20);  convert_element_type_112 = add_20 = None
        convert_element_type_113: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_6, torch.bfloat16);  div_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_114: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_50, torch.bfloat16);  primals_50 = None
        permute_23: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_114, [1, 0]);  convert_element_type_114 = None
        mm_16: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_113, permute_23)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_21: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_106, mm_16);  mm_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_56: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_21, [-1, 2048]);  add_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_117: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_56, torch.float32);  view_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_14: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_117, 2)
        mean_13: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_14, [1], True);  pow_14 = None
        add_22: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_13, 1e-05);  mean_13 = None
        rsqrt_13: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_22);  add_22 = None
        mul_27: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_117, rsqrt_13)
        mul_28: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_27, primals_51);  mul_27 = None
        alias_15: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_13);  rsqrt_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_118: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_28, torch.bfloat16);  mul_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_57: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_118, [10, 2048]);  convert_element_type_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_119: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_57, torch.float32)
        neg_7: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_119)
        exp_7: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_7);  neg_7 = None
        add_23: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_7, 1);  exp_7 = None
        div_7: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_119, add_23);  convert_element_type_119 = add_23 = None
        convert_element_type_120: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_7, torch.bfloat16);  div_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_121: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_52, torch.bfloat16);  primals_52 = None
        permute_24: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_121, [1, 0]);  convert_element_type_121 = None
        mm_17: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_120, permute_24)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_58: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_17, [-1, 1024]);  mm_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_124: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_58, torch.float32);  view_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_15: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_124, 2)
        mean_14: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_15, [1], True);  pow_15 = None
        add_24: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_14, 1e-05);  mean_14 = None
        rsqrt_14: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_24);  add_24 = None
        mul_29: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_124, rsqrt_14)
        mul_30: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_29, primals_53);  mul_29 = None
        alias_16: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_14);  rsqrt_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_125: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_30, torch.bfloat16);  mul_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_59: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_125, [10, 1024]);  convert_element_type_125 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_126: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_59, torch.float32)
        neg_8: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_126)
        exp_8: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_8);  neg_8 = None
        add_25: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_8, 1);  exp_8 = None
        div_8: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_126, add_25);  convert_element_type_126 = add_25 = None
        convert_element_type_127: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_8, torch.bfloat16);  div_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_128: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_54, torch.bfloat16);  primals_54 = None
        permute_25: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_128, [1, 0]);  convert_element_type_128 = None
        mm_18: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_127, permute_25)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_26: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_120, mm_18);  mm_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_60: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_26, [-1, 2048]);  add_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_131: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_60, torch.float32);  view_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_16: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_131, 2)
        mean_15: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_16, [1], True);  pow_16 = None
        add_27: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_15, 1e-05);  mean_15 = None
        rsqrt_15: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_27);  add_27 = None
        mul_31: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_131, rsqrt_15)
        mul_32: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_31, primals_55);  mul_31 = None
        alias_17: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_15);  rsqrt_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_132: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_32, torch.bfloat16);  mul_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_61: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_132, [10, 2048]);  convert_element_type_132 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_133: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_61, torch.float32)
        neg_9: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_133)
        exp_9: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_9);  neg_9 = None
        add_28: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_9, 1);  exp_9 = None
        div_9: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_133, add_28);  convert_element_type_133 = add_28 = None
        convert_element_type_134: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_9, torch.bfloat16);  div_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_135: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_56, torch.bfloat16);  primals_56 = None
        permute_26: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_135, [1, 0]);  convert_element_type_135 = None
        mm_19: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_134, permute_26)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_62: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_19, [10, -1, 256]);  mm_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_5: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_62, getitem_9], 1);  view_62 = getitem_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_29: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_5, getitem_8);  cat_5 = getitem_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_63: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_29, [-1, 256]);  add_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_10: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_11: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_5 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 30, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_63, 'Y': empty_10, 'W': primals_57, 'Rstd': empty_11}, tensors_to_clone = ['Y', 'Rstd']);  empty_10 = empty_11 = None
        getitem_14: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_5['Y']
        getitem_15: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_5['Rstd'];  triton_kernel_wrapper_functional_proxy_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_65: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_138: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_58, torch.bfloat16);  primals_58 = None
        permute_28: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_138, [1, 0]);  convert_element_type_138 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_66: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256])
        permute_29: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_66, [0, 2, 1]);  view_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_3: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_29, memory_format = torch.contiguous_format);  permute_29 = None
        view_67: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_3, [2560, 64]);  clone_3 = None
        mm_20: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_67, permute_28)
        view_68: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_20, [10, 256, 80]);  mm_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_30: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_68, [0, 2, 1]);  view_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_1 = torch.ops.aten.split_with_sizes.default(permute_30, [32, 32, 16], 1);  permute_30 = None
        getitem_16: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_1[0]
        getitem_17: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_1[1]
        getitem_18: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_1[2];  split_with_sizes_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_4: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_17, memory_format = torch.contiguous_format);  getitem_17 = None
        view_69: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_4, [10, 8192]);  clone_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_141: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_59, torch.bfloat16);  primals_59 = None
        permute_31: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_141, [1, 0]);  convert_element_type_141 = None
        mm_21: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_69, permute_31)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_70: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_21, [-1, 512]);  mm_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_144: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_70, torch.float32);  view_70 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_17: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_144, 2)
        mean_16: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_17, [1], True);  pow_17 = None
        add_30: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_16, 1e-05);  mean_16 = None
        rsqrt_16: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_30);  add_30 = None
        mul_33: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_144, rsqrt_16)
        mul_34: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_33, primals_60);  mul_33 = None
        alias_18: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_16);  rsqrt_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_145: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_34, torch.bfloat16);  mul_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_71: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_145, [10, 512]);  convert_element_type_145 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_146: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_71, torch.float32)
        neg_10: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_146)
        exp_10: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_10);  neg_10 = None
        add_31: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_10, 1);  exp_10 = None
        div_10: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_146, add_31);  convert_element_type_146 = add_31 = None
        convert_element_type_147: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_10, torch.bfloat16);  div_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_148: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_61, torch.bfloat16);  primals_61 = None
        permute_32: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_148, [1, 0]);  convert_element_type_148 = None
        mm_22: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_147, permute_32)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_72: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_22, [10, -1, 64]);  mm_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_4: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_72, [10, 32, 64]);  view_72 = None
        view_73: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_4, [10, 32, 64]);  expand_4 = None
        view_76: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256])
        expand_7: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_76, [10, 64, 256]);  view_76 = None
        view_77: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_7, [10, 64, 256]);  expand_7 = None
        bmm_2: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_73, view_77);  view_77 = None
        view_78: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_2, [10, 32, 256]);  bmm_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        expand_8: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_78, [10, 32, 256]);  view_78 = None
        view_79: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_8, [10, 32, 256]);  expand_8 = None
        view_83: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256])
        permute_36: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_83, [0, 2, 1]);  view_83 = None
        expand_11: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_36, [10, 256, 64]);  permute_36 = None
        view_84: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_11, [10, 256, 64]);  expand_11 = None
        bmm_3: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_79, view_84);  view_84 = None
        view_85: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_3, [10, 32, 64]);  bmm_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_86: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_85, [10, -1]);  view_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_5: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_18, memory_format = torch.contiguous_format);  getitem_18 = None
        view_87: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_5, [10, 4096]);  clone_5 = None
        cat_6: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_86, view_87], 1);  view_86 = view_87 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_7: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_6, view_48], 1);  cat_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_88: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_7, [-1, 8472]);  cat_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_12: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_13: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_6 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 32, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_88, 'Y': empty_12, 'W': primals_62, 'Rstd': empty_13}, tensors_to_clone = ['Y', 'Rstd']);  empty_12 = empty_13 = None
        getitem_19: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_6['Y']
        getitem_20: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_6['Rstd'];  triton_kernel_wrapper_functional_proxy_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_90: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_19, [10, 8472]);  getitem_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_155: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_90, torch.bfloat16);  view_90 = None
        convert_element_type_156: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_63, torch.bfloat16);  primals_63 = None
        permute_37: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_156, [1, 0]);  convert_element_type_156 = None
        mm_23: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_155, permute_37)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_91: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_23, [-1, 2048]);  mm_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_159: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_91, torch.float32);  view_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_18: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_159, 2)
        mean_17: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_18, [1], True);  pow_18 = None
        add_32: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_17, 1e-05);  mean_17 = None
        rsqrt_17: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_32);  add_32 = None
        mul_35: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_159, rsqrt_17)
        mul_36: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_35, primals_64);  mul_35 = None
        alias_19: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_17);  rsqrt_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_160: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_36, torch.bfloat16);  mul_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_92: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_160, [10, 2048]);  convert_element_type_160 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_161: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_92, torch.float32)
        neg_11: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_161)
        exp_11: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_11);  neg_11 = None
        add_33: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_11, 1);  exp_11 = None
        div_11: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_161, add_33);  convert_element_type_161 = add_33 = None
        convert_element_type_162: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_11, torch.bfloat16);  div_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_163: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_65, torch.bfloat16);  primals_65 = None
        permute_38: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_163, [1, 0]);  convert_element_type_163 = None
        mm_24: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_162, permute_38)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_93: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_24, [-1, 1024]);  mm_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_166: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_93, torch.float32);  view_93 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_19: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_166, 2)
        mean_18: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_19, [1], True);  pow_19 = None
        add_34: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_18, 1e-05);  mean_18 = None
        rsqrt_18: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_34);  add_34 = None
        mul_37: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_166, rsqrt_18)
        mul_38: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_37, primals_66);  mul_37 = None
        alias_20: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_18);  rsqrt_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_167: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_38, torch.bfloat16);  mul_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_94: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_167, [10, 1024]);  convert_element_type_167 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_168: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_94, torch.float32)
        neg_12: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_168)
        exp_12: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_12);  neg_12 = None
        add_35: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_12, 1);  exp_12 = None
        div_12: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_168, add_35);  convert_element_type_168 = add_35 = None
        convert_element_type_169: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_12, torch.bfloat16);  div_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_170: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_67, torch.bfloat16);  primals_67 = None
        permute_39: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_170, [1, 0]);  convert_element_type_170 = None
        mm_25: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_169, permute_39)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_36: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_162, mm_25);  mm_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_95: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_36, [-1, 2048]);  add_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_173: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_95, torch.float32);  view_95 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_20: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_173, 2)
        mean_19: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_20, [1], True);  pow_20 = None
        add_37: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_19, 1e-05);  mean_19 = None
        rsqrt_19: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_37);  add_37 = None
        mul_39: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_173, rsqrt_19)
        mul_40: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_39, primals_68);  mul_39 = None
        alias_21: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_19);  rsqrt_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_174: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_40, torch.bfloat16);  mul_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_96: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_174, [10, 2048]);  convert_element_type_174 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_175: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_96, torch.float32)
        neg_13: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_175)
        exp_13: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_13);  neg_13 = None
        add_38: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_13, 1);  exp_13 = None
        div_13: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_175, add_38);  convert_element_type_175 = add_38 = None
        convert_element_type_176: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_13, torch.bfloat16);  div_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_177: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_69, torch.bfloat16);  primals_69 = None
        permute_40: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_177, [1, 0]);  convert_element_type_177 = None
        mm_26: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_176, permute_40)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_97: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_26, [-1, 1024]);  mm_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_180: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_97, torch.float32);  view_97 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_21: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_180, 2)
        mean_20: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_21, [1], True);  pow_21 = None
        add_39: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_20, 1e-05);  mean_20 = None
        rsqrt_20: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_39);  add_39 = None
        mul_41: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_180, rsqrt_20)
        mul_42: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_41, primals_70);  mul_41 = None
        alias_22: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_20);  rsqrt_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_181: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_42, torch.bfloat16);  mul_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_98: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_181, [10, 1024]);  convert_element_type_181 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_182: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_98, torch.float32)
        neg_14: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_182)
        exp_14: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_14);  neg_14 = None
        add_40: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_14, 1);  exp_14 = None
        div_14: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_182, add_40);  convert_element_type_182 = add_40 = None
        convert_element_type_183: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_14, torch.bfloat16);  div_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_184: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_71, torch.bfloat16);  primals_71 = None
        permute_41: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_184, [1, 0]);  convert_element_type_184 = None
        mm_27: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_183, permute_41)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_41: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_176, mm_27);  mm_27 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_99: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_41, [-1, 2048]);  add_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_187: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_99, torch.float32);  view_99 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_22: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_187, 2)
        mean_21: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_22, [1], True);  pow_22 = None
        add_42: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_21, 1e-05);  mean_21 = None
        rsqrt_21: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_42);  add_42 = None
        mul_43: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_187, rsqrt_21)
        mul_44: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_43, primals_72);  mul_43 = None
        alias_23: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_21);  rsqrt_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_188: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_44, torch.bfloat16);  mul_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_100: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_188, [10, 2048]);  convert_element_type_188 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_189: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_100, torch.float32)
        neg_15: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_189)
        exp_15: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_15);  neg_15 = None
        add_43: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_15, 1);  exp_15 = None
        div_15: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_189, add_43);  convert_element_type_189 = add_43 = None
        convert_element_type_190: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_15, torch.bfloat16);  div_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_191: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_73, torch.bfloat16);  primals_73 = None
        permute_42: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_191, [1, 0]);  convert_element_type_191 = None
        mm_28: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_190, permute_42)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_101: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_28, [10, -1, 256]);  mm_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_8: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_101, getitem_16], 1);  view_101 = getitem_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_44: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_8, view_65);  cat_8 = view_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_102: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_44, [-1, 256]);  add_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_14: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_15: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_7 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 34, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_102, 'Y': empty_14, 'W': primals_74, 'Rstd': empty_15}, tensors_to_clone = ['Y', 'Rstd']);  empty_14 = empty_15 = None
        getitem_21: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_7['Y']
        getitem_22: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_7['Rstd'];  triton_kernel_wrapper_functional_proxy_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_104: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_194: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_75, torch.bfloat16);  primals_75 = None
        permute_44: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_194, [1, 0]);  convert_element_type_194 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_105: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256])
        permute_45: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_105, [0, 2, 1]);  view_105 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_6: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_45, memory_format = torch.contiguous_format);  permute_45 = None
        view_106: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_6, [2560, 64]);  clone_6 = None
        mm_29: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_106, permute_44)
        view_107: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_29, [10, 256, 80]);  mm_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_46: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_107, [0, 2, 1]);  view_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_2 = torch.ops.aten.split_with_sizes.default(permute_46, [32, 32, 16], 1);  permute_46 = None
        getitem_23: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_2[0]
        getitem_24: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_2[1]
        getitem_25: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_2[2];  split_with_sizes_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_7: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_24, memory_format = torch.contiguous_format);  getitem_24 = None
        view_108: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_7, [10, 8192]);  clone_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_197: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_76, torch.bfloat16);  primals_76 = None
        permute_47: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_197, [1, 0]);  convert_element_type_197 = None
        mm_30: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_108, permute_47)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_109: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_30, [-1, 512]);  mm_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_200: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_109, torch.float32);  view_109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_23: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_200, 2)
        mean_22: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_23, [1], True);  pow_23 = None
        add_45: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_22, 1e-05);  mean_22 = None
        rsqrt_22: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_45);  add_45 = None
        mul_45: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_200, rsqrt_22)
        mul_46: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_45, primals_77);  mul_45 = None
        alias_24: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_22);  rsqrt_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_201: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_46, torch.bfloat16);  mul_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_110: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_201, [10, 512]);  convert_element_type_201 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_202: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_110, torch.float32)
        neg_16: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_202)
        exp_16: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_16);  neg_16 = None
        add_46: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_16, 1);  exp_16 = None
        div_16: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_202, add_46);  convert_element_type_202 = add_46 = None
        convert_element_type_203: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_16, torch.bfloat16);  div_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_204: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_78, torch.bfloat16);  primals_78 = None
        permute_48: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_204, [1, 0]);  convert_element_type_204 = None
        mm_31: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_203, permute_48)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_111: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_31, [10, -1, 64]);  mm_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_12: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_111, [10, 32, 64]);  view_111 = None
        view_112: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_12, [10, 32, 64]);  expand_12 = None
        view_115: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256])
        expand_15: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_115, [10, 64, 256]);  view_115 = None
        view_116: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_15, [10, 64, 256]);  expand_15 = None
        bmm_4: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_112, view_116);  view_116 = None
        view_117: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_4, [10, 32, 256]);  bmm_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        expand_16: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_117, [10, 32, 256]);  view_117 = None
        view_118: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_16, [10, 32, 256]);  expand_16 = None
        view_122: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256])
        permute_52: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_122, [0, 2, 1]);  view_122 = None
        expand_19: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_52, [10, 256, 64]);  permute_52 = None
        view_123: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_19, [10, 256, 64]);  expand_19 = None
        bmm_5: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_118, view_123);  view_123 = None
        view_124: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_5, [10, 32, 64]);  bmm_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_125: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_124, [10, -1]);  view_124 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_8: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_25, memory_format = torch.contiguous_format);  getitem_25 = None
        view_126: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_8, [10, 4096]);  clone_8 = None
        cat_9: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_125, view_126], 1);  view_125 = view_126 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_10: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_9, view_48], 1);  cat_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_127: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_10, [-1, 8472]);  cat_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_16: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_17: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_8 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 36, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_127, 'Y': empty_16, 'W': primals_79, 'Rstd': empty_17}, tensors_to_clone = ['Y', 'Rstd']);  empty_16 = empty_17 = None
        getitem_26: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_8['Y']
        getitem_27: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_8['Rstd'];  triton_kernel_wrapper_functional_proxy_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_129: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_26, [10, 8472]);  getitem_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_211: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_129, torch.bfloat16);  view_129 = None
        convert_element_type_212: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_80, torch.bfloat16);  primals_80 = None
        permute_53: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_212, [1, 0]);  convert_element_type_212 = None
        mm_32: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_211, permute_53)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_130: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_32, [-1, 2048]);  mm_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_215: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_130, torch.float32);  view_130 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_24: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_215, 2)
        mean_23: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_24, [1], True);  pow_24 = None
        add_47: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_23, 1e-05);  mean_23 = None
        rsqrt_23: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_47);  add_47 = None
        mul_47: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_215, rsqrt_23)
        mul_48: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_47, primals_81);  mul_47 = None
        alias_25: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_23);  rsqrt_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_216: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_48, torch.bfloat16);  mul_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_131: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_216, [10, 2048]);  convert_element_type_216 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_217: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_131, torch.float32)
        neg_17: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_217)
        exp_17: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_17);  neg_17 = None
        add_48: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_17, 1);  exp_17 = None
        div_17: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_217, add_48);  convert_element_type_217 = add_48 = None
        convert_element_type_218: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_17, torch.bfloat16);  div_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_219: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_82, torch.bfloat16);  primals_82 = None
        permute_54: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_219, [1, 0]);  convert_element_type_219 = None
        mm_33: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_218, permute_54)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_132: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_33, [-1, 1024]);  mm_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_222: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_132, torch.float32);  view_132 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_25: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_222, 2)
        mean_24: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_25, [1], True);  pow_25 = None
        add_49: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_24, 1e-05);  mean_24 = None
        rsqrt_24: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_49);  add_49 = None
        mul_49: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_222, rsqrt_24)
        mul_50: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_49, primals_83);  mul_49 = None
        alias_26: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_24);  rsqrt_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_223: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_50, torch.bfloat16);  mul_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_133: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_223, [10, 1024]);  convert_element_type_223 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_224: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_133, torch.float32)
        neg_18: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_224)
        exp_18: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_18);  neg_18 = None
        add_50: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_18, 1);  exp_18 = None
        div_18: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_224, add_50);  convert_element_type_224 = add_50 = None
        convert_element_type_225: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_18, torch.bfloat16);  div_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_226: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_84, torch.bfloat16);  primals_84 = None
        permute_55: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_226, [1, 0]);  convert_element_type_226 = None
        mm_34: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_225, permute_55)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_51: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_218, mm_34);  mm_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_134: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_51, [-1, 2048]);  add_51 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_229: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_134, torch.float32);  view_134 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_26: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_229, 2)
        mean_25: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_26, [1], True);  pow_26 = None
        add_52: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_25, 1e-05);  mean_25 = None
        rsqrt_25: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_52);  add_52 = None
        mul_51: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_229, rsqrt_25)
        mul_52: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_51, primals_85);  mul_51 = None
        alias_27: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_25);  rsqrt_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_230: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_52, torch.bfloat16);  mul_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_135: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_230, [10, 2048]);  convert_element_type_230 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_231: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_135, torch.float32)
        neg_19: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_231)
        exp_19: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_19);  neg_19 = None
        add_53: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_19, 1);  exp_19 = None
        div_19: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_231, add_53);  convert_element_type_231 = add_53 = None
        convert_element_type_232: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_19, torch.bfloat16);  div_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_233: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_86, torch.bfloat16);  primals_86 = None
        permute_56: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_233, [1, 0]);  convert_element_type_233 = None
        mm_35: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_232, permute_56)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_136: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_35, [-1, 1024]);  mm_35 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_236: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_136, torch.float32);  view_136 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_27: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_236, 2)
        mean_26: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_27, [1], True);  pow_27 = None
        add_54: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_26, 1e-05);  mean_26 = None
        rsqrt_26: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_54);  add_54 = None
        mul_53: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_236, rsqrt_26)
        mul_54: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_53, primals_87);  mul_53 = None
        alias_28: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_26);  rsqrt_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_237: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_54, torch.bfloat16);  mul_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_137: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_237, [10, 1024]);  convert_element_type_237 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_238: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_137, torch.float32)
        neg_20: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_238)
        exp_20: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_20);  neg_20 = None
        add_55: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_20, 1);  exp_20 = None
        div_20: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_238, add_55);  convert_element_type_238 = add_55 = None
        convert_element_type_239: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_20, torch.bfloat16);  div_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_240: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_88, torch.bfloat16);  primals_88 = None
        permute_57: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_240, [1, 0]);  convert_element_type_240 = None
        mm_36: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_239, permute_57)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_56: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_232, mm_36);  mm_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_138: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_56, [-1, 2048]);  add_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_243: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_138, torch.float32);  view_138 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_28: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_243, 2)
        mean_27: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_28, [1], True);  pow_28 = None
        add_57: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_27, 1e-05);  mean_27 = None
        rsqrt_27: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_57);  add_57 = None
        mul_55: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_243, rsqrt_27)
        mul_56: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_55, primals_89);  mul_55 = None
        alias_29: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_27);  rsqrt_27 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_244: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_56, torch.bfloat16);  mul_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_139: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_244, [10, 2048]);  convert_element_type_244 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_245: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_139, torch.float32)
        neg_21: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_245)
        exp_21: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_21);  neg_21 = None
        add_58: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_21, 1);  exp_21 = None
        div_21: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_245, add_58);  convert_element_type_245 = add_58 = None
        convert_element_type_246: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_21, torch.bfloat16);  div_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_247: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_90, torch.bfloat16);  primals_90 = None
        permute_58: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_247, [1, 0]);  convert_element_type_247 = None
        mm_37: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_246, permute_58)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_140: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_37, [10, -1, 256]);  mm_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_11: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_140, getitem_23], 1);  view_140 = getitem_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_59: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_11, view_104);  cat_11 = view_104 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_141: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_59, [-1, 256]);  add_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_18: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_19: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_9 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 38, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_141, 'Y': empty_18, 'W': primals_91, 'Rstd': empty_19}, tensors_to_clone = ['Y', 'Rstd']);  empty_18 = empty_19 = None
        getitem_28: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_9['Y']
        getitem_29: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_9['Rstd'];  triton_kernel_wrapper_functional_proxy_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_143: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_250: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_92, torch.bfloat16);  primals_92 = None
        permute_60: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_250, [1, 0]);  convert_element_type_250 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_144: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256])
        permute_61: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_144, [0, 2, 1]);  view_144 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_9: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_61, memory_format = torch.contiguous_format);  permute_61 = None
        view_145: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_9, [2560, 64]);  clone_9 = None
        mm_38: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_145, permute_60)
        view_146: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_38, [10, 256, 80]);  mm_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_62: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_146, [0, 2, 1]);  view_146 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_3 = torch.ops.aten.split_with_sizes.default(permute_62, [32, 32, 16], 1);  permute_62 = None
        getitem_30: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_3[0]
        getitem_31: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_3[1]
        getitem_32: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_3[2];  split_with_sizes_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_10: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_31, memory_format = torch.contiguous_format);  getitem_31 = None
        view_147: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_10, [10, 8192]);  clone_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_253: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_93, torch.bfloat16);  primals_93 = None
        permute_63: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_253, [1, 0]);  convert_element_type_253 = None
        mm_39: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_147, permute_63)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_148: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_39, [-1, 512]);  mm_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_256: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_148, torch.float32);  view_148 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_29: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_256, 2)
        mean_28: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_29, [1], True);  pow_29 = None
        add_60: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_28, 1e-05);  mean_28 = None
        rsqrt_28: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_60);  add_60 = None
        mul_57: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_256, rsqrt_28)
        mul_58: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_57, primals_94);  mul_57 = None
        alias_30: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_28);  rsqrt_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_257: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_58, torch.bfloat16);  mul_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_149: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_257, [10, 512]);  convert_element_type_257 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_258: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_149, torch.float32)
        neg_22: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_258)
        exp_22: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_22);  neg_22 = None
        add_61: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_22, 1);  exp_22 = None
        div_22: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_258, add_61);  convert_element_type_258 = add_61 = None
        convert_element_type_259: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_22, torch.bfloat16);  div_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_260: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_95, torch.bfloat16);  primals_95 = None
        permute_64: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_260, [1, 0]);  convert_element_type_260 = None
        mm_40: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_259, permute_64)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_150: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_40, [10, -1, 64]);  mm_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_20: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_150, [10, 32, 64]);  view_150 = None
        view_151: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_20, [10, 32, 64]);  expand_20 = None
        view_154: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256])
        expand_23: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_154, [10, 64, 256]);  view_154 = None
        view_155: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_23, [10, 64, 256]);  expand_23 = None
        bmm_6: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_151, view_155);  view_155 = None
        view_156: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_6, [10, 32, 256]);  bmm_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        expand_24: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_156, [10, 32, 256]);  view_156 = None
        view_157: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_24, [10, 32, 256]);  expand_24 = None
        view_161: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256])
        permute_68: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_161, [0, 2, 1]);  view_161 = None
        expand_27: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_68, [10, 256, 64]);  permute_68 = None
        view_162: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_27, [10, 256, 64]);  expand_27 = None
        bmm_7: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_157, view_162);  view_162 = None
        view_163: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_7, [10, 32, 64]);  bmm_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_164: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_163, [10, -1]);  view_163 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_11: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_32, memory_format = torch.contiguous_format);  getitem_32 = None
        view_165: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_11, [10, 4096]);  clone_11 = None
        cat_12: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_164, view_165], 1);  view_164 = view_165 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_13: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_12, view_48], 1);  cat_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_166: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_13, [-1, 8472]);  cat_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_20: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_21: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_10 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 40, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_166, 'Y': empty_20, 'W': primals_96, 'Rstd': empty_21}, tensors_to_clone = ['Y', 'Rstd']);  empty_20 = empty_21 = None
        getitem_33: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_10['Y']
        getitem_34: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_10['Rstd'];  triton_kernel_wrapper_functional_proxy_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_168: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_33, [10, 8472]);  getitem_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_267: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_168, torch.bfloat16);  view_168 = None
        convert_element_type_268: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_97, torch.bfloat16);  primals_97 = None
        permute_69: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_268, [1, 0]);  convert_element_type_268 = None
        mm_41: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_267, permute_69)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_169: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_41, [-1, 2048]);  mm_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_271: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_169, torch.float32);  view_169 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_30: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_271, 2)
        mean_29: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_30, [1], True);  pow_30 = None
        add_62: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_29, 1e-05);  mean_29 = None
        rsqrt_29: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_62);  add_62 = None
        mul_59: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_271, rsqrt_29)
        mul_60: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_59, primals_98);  mul_59 = None
        alias_31: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_29);  rsqrt_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_272: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_60, torch.bfloat16);  mul_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_170: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_272, [10, 2048]);  convert_element_type_272 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_273: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_170, torch.float32)
        neg_23: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_273)
        exp_23: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_23);  neg_23 = None
        add_63: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_23, 1);  exp_23 = None
        div_23: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_273, add_63);  convert_element_type_273 = add_63 = None
        convert_element_type_274: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_23, torch.bfloat16);  div_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_275: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_99, torch.bfloat16);  primals_99 = None
        permute_70: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_275, [1, 0]);  convert_element_type_275 = None
        mm_42: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_274, permute_70)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_171: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_42, [-1, 1024]);  mm_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_278: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_171, torch.float32);  view_171 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_31: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_278, 2)
        mean_30: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_31, [1], True);  pow_31 = None
        add_64: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_30, 1e-05);  mean_30 = None
        rsqrt_30: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_64);  add_64 = None
        mul_61: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_278, rsqrt_30)
        mul_62: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_61, primals_100);  mul_61 = None
        alias_32: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_30);  rsqrt_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_279: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_62, torch.bfloat16);  mul_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_172: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_279, [10, 1024]);  convert_element_type_279 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_280: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_172, torch.float32)
        neg_24: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_280)
        exp_24: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_24);  neg_24 = None
        add_65: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_24, 1);  exp_24 = None
        div_24: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_280, add_65);  convert_element_type_280 = add_65 = None
        convert_element_type_281: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_24, torch.bfloat16);  div_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_282: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_101, torch.bfloat16);  primals_101 = None
        permute_71: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_282, [1, 0]);  convert_element_type_282 = None
        mm_43: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_281, permute_71)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_66: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_274, mm_43);  mm_43 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_173: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_66, [-1, 2048]);  add_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_285: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_173, torch.float32);  view_173 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_32: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_285, 2)
        mean_31: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_32, [1], True);  pow_32 = None
        add_67: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_31, 1e-05);  mean_31 = None
        rsqrt_31: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_67);  add_67 = None
        mul_63: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_285, rsqrt_31)
        mul_64: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_63, primals_102);  mul_63 = None
        alias_33: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_31);  rsqrt_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_286: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_64, torch.bfloat16);  mul_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_174: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_286, [10, 2048]);  convert_element_type_286 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_287: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_174, torch.float32)
        neg_25: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_287)
        exp_25: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_25);  neg_25 = None
        add_68: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_25, 1);  exp_25 = None
        div_25: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_287, add_68);  convert_element_type_287 = add_68 = None
        convert_element_type_288: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_25, torch.bfloat16);  div_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_289: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_103, torch.bfloat16);  primals_103 = None
        permute_72: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_289, [1, 0]);  convert_element_type_289 = None
        mm_44: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_288, permute_72)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_175: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_44, [-1, 1024]);  mm_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_292: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_175, torch.float32);  view_175 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_33: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_292, 2)
        mean_32: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_33, [1], True);  pow_33 = None
        add_69: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_32, 1e-05);  mean_32 = None
        rsqrt_32: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_69);  add_69 = None
        mul_65: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_292, rsqrt_32)
        mul_66: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_65, primals_104);  mul_65 = None
        alias_34: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_32);  rsqrt_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_293: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_66, torch.bfloat16);  mul_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_176: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_293, [10, 1024]);  convert_element_type_293 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_294: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_176, torch.float32)
        neg_26: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_294)
        exp_26: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_26);  neg_26 = None
        add_70: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_26, 1);  exp_26 = None
        div_26: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_294, add_70);  convert_element_type_294 = add_70 = None
        convert_element_type_295: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_26, torch.bfloat16);  div_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_296: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_105, torch.bfloat16);  primals_105 = None
        permute_73: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_296, [1, 0]);  convert_element_type_296 = None
        mm_45: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_295, permute_73)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_71: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_288, mm_45);  mm_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_177: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_71, [-1, 2048]);  add_71 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_299: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_177, torch.float32);  view_177 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_34: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_299, 2)
        mean_33: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_34, [1], True);  pow_34 = None
        add_72: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_33, 1e-05);  mean_33 = None
        rsqrt_33: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_72);  add_72 = None
        mul_67: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_299, rsqrt_33)
        mul_68: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_67, primals_106);  mul_67 = None
        alias_35: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_33);  rsqrt_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_300: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_68, torch.bfloat16);  mul_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_178: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_300, [10, 2048]);  convert_element_type_300 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_301: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_178, torch.float32)
        neg_27: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_301)
        exp_27: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_27);  neg_27 = None
        add_73: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_27, 1);  exp_27 = None
        div_27: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_301, add_73);  convert_element_type_301 = add_73 = None
        convert_element_type_302: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_27, torch.bfloat16);  div_27 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_303: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_107, torch.bfloat16);  primals_107 = None
        permute_74: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_303, [1, 0]);  convert_element_type_303 = None
        mm_46: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_302, permute_74)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_179: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_46, [10, -1, 256]);  mm_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_14: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_179, getitem_30], 1);  view_179 = getitem_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_74: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_14, view_143);  cat_14 = view_143 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_180: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_74, [-1, 256]);  add_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_22: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_23: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_11 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 42, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_180, 'Y': empty_22, 'W': primals_108, 'Rstd': empty_23}, tensors_to_clone = ['Y', 'Rstd']);  empty_22 = empty_23 = None
        getitem_35: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_11['Y']
        getitem_36: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_11['Rstd'];  triton_kernel_wrapper_functional_proxy_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant0: "bf16[][]cuda:0" = self._tensor_constant0
        lift_fresh_copy: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant0);  _tensor_constant0 = None
        convert_element_type_306: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy, torch.float32);  lift_fresh_copy = None
        clamp_min: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_306, 1);  convert_element_type_306 = None
        convert_element_type_307: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min, torch.bfloat16);  clamp_min = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_308: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_307, torch.float32);  convert_element_type_307 = None
        reciprocal: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_308);  convert_element_type_308 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_309: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_109, torch.bfloat16);  primals_109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_310: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_110, torch.bfloat16);  primals_110 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_311: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_111, torch.bfloat16);  primals_111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_312: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_112, torch.bfloat16);  primals_112 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_182: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem, [514, 256]);  getitem = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_24: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_25: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_26: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_12 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 44, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_182, 'Y': empty_24, 'W': convert_element_type_309, 'B': convert_element_type_310, 'Mean': empty_25, 'Rstd': empty_26}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_24 = empty_25 = empty_26 = None
        getitem_37: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_12['Y']
        getitem_38: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_12['Mean']
        getitem_39: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_12['Rstd'];  triton_kernel_wrapper_functional_proxy_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_2: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_312, getitem_37, convert_element_type_311);  getitem_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_4 = torch.ops.aten.split_with_sizes.default(addmm_2, [256, 256, 256, 256], 1);  addmm_2 = None
        getitem_40: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_4[0]
        getitem_41: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_4[1]
        getitem_42: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_4[2]
        getitem_43: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_4[3];  split_with_sizes_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_183: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_42, [-1, 2, 128]);  getitem_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_184: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_43, [-1, 2, 128]);  getitem_43 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_185: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_41, [-1, 2, 128]);  getitem_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_316: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_40, torch.float32);  getitem_40 = None
        neg_28: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_316)
        exp_28: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_28);  neg_28 = None
        add_75: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_28, 1);  exp_28 = None
        div_28: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_316, add_75);  convert_element_type_316 = add_75 = None
        convert_element_type_317: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_28, torch.bfloat16);  div_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_1: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(primals_113, 0, 1, 9223372036854775807)
        slice_2: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(primals_113, 0, 0, -1)
        sub: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_1, slice_2);  slice_1 = slice_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort = torch.ops.aten.sort.stable(sub, stable = False, descending = True);  sub = None
        getitem_45: "i64[2][1]cuda:0" = sort[1];  sort = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_27: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_186: "bf16[131584][1]cuda:0" = torch.ops.aten.view.default(empty_27, [-1]);  empty_27 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_13 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 45, grid = [(17, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_183, 'K': view_184, 'V': view_185, 'seq_offsets': primals_113, 'attn_scale': reciprocal, 'Out': view_186}, tensors_to_clone = ['Out']);  view_183 = view_184 = view_185 = view_186 = None
        getitem_46: "bf16[131584][1]cuda:0" = triton_kernel_wrapper_functional_proxy_13['Out'];  triton_kernel_wrapper_functional_proxy_13 = None
        view_187: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_46, [514, 2, 128]);  getitem_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_318: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_114, torch.bfloat16);  primals_114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_319: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_115, torch.bfloat16);  primals_115 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_320: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_116, torch.bfloat16);  primals_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_29: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_30: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_31: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_190: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_187, [-1, 256]);  view_187 = None
        triton_kernel_wrapper_functional_proxy_14 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 12, constant_args_idx = 50, grid = [(514, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_190, 'U': convert_element_type_317, 'Y': empty_29, 'W': convert_element_type_318, 'B': convert_element_type_319, 'Mean': empty_30, 'Rstd': empty_31, 'seed': primals_117}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_29 = empty_30 = empty_31 = None
        getitem_47: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_14['Y']
        getitem_48: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_14['Mean']
        getitem_49: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_14['Rstd'];  triton_kernel_wrapper_functional_proxy_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_3: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(view_182, getitem_47, convert_element_type_320);  getitem_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant1: "bf16[][]cuda:0" = self._tensor_constant1
        lift_fresh_copy_1: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant1);  _tensor_constant1 = None
        convert_element_type_324: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_1, torch.float32);  lift_fresh_copy_1 = None
        clamp_min_1: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_324, 1);  convert_element_type_324 = None
        convert_element_type_325: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_1, torch.bfloat16);  clamp_min_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_326: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_325, torch.float32);  convert_element_type_325 = None
        reciprocal_1: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_326);  convert_element_type_326 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_327: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_118, torch.bfloat16);  primals_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_328: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_119, torch.bfloat16);  primals_119 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_329: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_120, torch.bfloat16);  primals_120 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_330: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_121, torch.bfloat16);  primals_121 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_191: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_2, [514, 256]);  getitem_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_32: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_33: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_34: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_15 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 53, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_191, 'Y': empty_32, 'W': convert_element_type_327, 'B': convert_element_type_328, 'Mean': empty_33, 'Rstd': empty_34}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_32 = empty_33 = empty_34 = None
        getitem_50: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_15['Y']
        getitem_51: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_15['Mean']
        getitem_52: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_15['Rstd'];  triton_kernel_wrapper_functional_proxy_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_4: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_330, getitem_50, convert_element_type_329);  getitem_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_5 = torch.ops.aten.split_with_sizes.default(addmm_4, [256, 256, 256, 256], 1);  addmm_4 = None
        getitem_53: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_5[0]
        getitem_54: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_5[1]
        getitem_55: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_5[2]
        getitem_56: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_5[3];  split_with_sizes_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_192: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_55, [-1, 2, 128]);  getitem_55 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_193: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_56, [-1, 2, 128]);  getitem_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_194: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_54, [-1, 2, 128]);  getitem_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_334: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_53, torch.float32);  getitem_53 = None
        neg_29: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_334)
        exp_29: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_29);  neg_29 = None
        add_76: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_29, 1);  exp_29 = None
        div_29: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_334, add_76);  convert_element_type_334 = add_76 = None
        convert_element_type_335: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_29, torch.bfloat16);  div_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_3: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(primals_122, 0, 1, 9223372036854775807)
        slice_4: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(primals_122, 0, 0, -1)
        sub_1: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_3, slice_4);  slice_3 = slice_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_1 = torch.ops.aten.sort.stable(sub_1, stable = False, descending = True);  sub_1 = None
        getitem_58: "i64[2][1]cuda:0" = sort_1[1];  sort_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_35: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_195: "bf16[131584][1]cuda:0" = torch.ops.aten.view.default(empty_35, [-1]);  empty_35 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_16 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 54, grid = [(17, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_192, 'K': view_193, 'V': view_194, 'seq_offsets': primals_122, 'attn_scale': reciprocal_1, 'Out': view_195}, tensors_to_clone = ['Out']);  view_192 = view_193 = view_194 = view_195 = None
        getitem_59: "bf16[131584][1]cuda:0" = triton_kernel_wrapper_functional_proxy_16['Out'];  triton_kernel_wrapper_functional_proxy_16 = None
        view_196: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_59, [514, 2, 128]);  getitem_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_336: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_123, torch.bfloat16);  primals_123 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_337: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_124, torch.bfloat16);  primals_124 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_338: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_125, torch.bfloat16);  primals_125 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_37: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_38: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_39: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_199: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_196, [-1, 256]);  view_196 = None
        triton_kernel_wrapper_functional_proxy_17 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 16, constant_args_idx = 59, grid = [(514, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_199, 'U': convert_element_type_335, 'Y': empty_37, 'W': convert_element_type_336, 'B': convert_element_type_337, 'Mean': empty_38, 'Rstd': empty_39, 'seed': primals_126}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_37 = empty_38 = empty_39 = None
        getitem_60: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_17['Y']
        getitem_61: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_17['Mean']
        getitem_62: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_17['Rstd'];  triton_kernel_wrapper_functional_proxy_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_5: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(view_191, getitem_60, convert_element_type_338);  getitem_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_200: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_35, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_342: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_127, torch.bfloat16);  primals_127 = None
        permute_76: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_342, [1, 0]);  convert_element_type_342 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_201: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_35, [10, 64, 256])
        permute_77: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_201, [0, 2, 1]);  view_201 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_12: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_77, memory_format = torch.contiguous_format);  permute_77 = None
        view_202: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_12, [2560, 64]);  clone_12 = None
        mm_47: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_202, permute_76)
        view_203: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_47, [10, 256, 16]);  mm_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_78: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_203, [0, 2, 1]);  view_203 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_69: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_13: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_78, memory_format = torch.contiguous_format);  permute_78 = None
        view_204: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_13, [160, 256]);  clone_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_205: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_204, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_40: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_41: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_18 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 62, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_205, 'Y': empty_40, 'W': primals_129, 'Rstd': empty_41}, tensors_to_clone = ['Y', 'Rstd']);  empty_40 = empty_41 = None
        getitem_63: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_18['Y']
        getitem_64: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_18['Rstd'];  triton_kernel_wrapper_functional_proxy_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_345: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_130, torch.bfloat16);  primals_130 = None
        permute_79: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_345, [1, 0]);  convert_element_type_345 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_207: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_63, [160, 256]);  getitem_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_48: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_207, permute_79)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant2: "bf16[][]cuda:0" = self._tensor_constant2
        lift_fresh_copy_2: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant2);  _tensor_constant2 = None
        convert_element_type_348: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_2, torch.float32);  lift_fresh_copy_2 = None
        clamp_min_2: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_348, 1);  convert_element_type_348 = None
        convert_element_type_349: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_2, torch.bfloat16);  clamp_min_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_350: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_349, torch.float32);  convert_element_type_349 = None
        reciprocal_2: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_350);  convert_element_type_350 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_208: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_48, [-1, 2, 128]);  mm_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_209: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_3, [-1, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_42: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_19 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 64, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_42, 'Q': view_208, 'K': view_209, 'V': view_209, 'Out': full, 'seq_offsets_q': mul_69, 'seq_offsets': primals_113, 'attn_scale': reciprocal_2}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_42 = full = mul_69 = None
        getitem_65: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_19['Out']
        getitem_66: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_19['seq_offsets_q']
        getitem_67: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_19['seq_offsets'];  triton_kernel_wrapper_functional_proxy_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_211: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_65, [-1, 256]);  getitem_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_77: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_211, view_204);  view_211 = view_204 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_351: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_77, torch.float32)
        var_mean = torch.ops.aten.var_mean.correction(convert_element_type_351, [1], correction = 0, keepdim = True)
        getitem_68: "f32[160, 1][1, 1]cuda:0" = var_mean[0]
        getitem_69: "f32[160, 1][1, 1]cuda:0" = var_mean[1];  var_mean = None
        add_78: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_68, 1e-05);  getitem_68 = None
        rsqrt_34: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_78);  add_78 = None
        sub_2: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_351, getitem_69)
        mul_70: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_2, rsqrt_34);  sub_2 = None
        mul_71: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_70, primals_131);  mul_70 = None
        add_79: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_71, primals_132);  mul_71 = primals_132 = None
        view_212: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_79, [10, 4096]);  add_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_352: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_212, torch.bfloat16)
        convert_element_type_353: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_133, torch.bfloat16);  primals_133 = None
        permute_80: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_353, [1, 0]);  convert_element_type_353 = None
        mm_49: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_352, permute_80)
        convert_element_type_356: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_49, torch.float32)
        neg_30: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_356)
        exp_30: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_30);  neg_30 = None
        add_80: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_30, 1);  exp_30 = None
        div_30: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_356, add_80);  convert_element_type_356 = add_80 = None
        convert_element_type_357: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_30, torch.bfloat16);  div_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_358: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_212, torch.bfloat16);  view_212 = None
        convert_element_type_359: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_134, torch.bfloat16);  primals_134 = None
        permute_81: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_359, [1, 0]);  convert_element_type_359 = None
        mm_50: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_358, permute_81)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_72: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_357, mm_50)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_362: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_135, torch.bfloat16);  primals_135 = None
        permute_82: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_362, [1, 0]);  convert_element_type_362 = None
        mm_51: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_72, permute_82)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_213: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_51, [160, 256]);  mm_51 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_81: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_77, view_213);  add_77 = view_213 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_214: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_81, [10, 16, 256]);  add_81 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_215: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_214, [-1, 256]);  view_214 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_43: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_44: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_20 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 66, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_215, 'Y': empty_43, 'W': primals_136, 'Rstd': empty_44}, tensors_to_clone = ['Y', 'Rstd']);  empty_43 = empty_44 = None
        getitem_70: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_20['Y']
        getitem_71: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_20['Rstd'];  triton_kernel_wrapper_functional_proxy_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_365: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_137, torch.bfloat16);  primals_137 = None
        permute_84: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_365, [1, 0]);  convert_element_type_365 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_217: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_35, [10, 64, 256]);  getitem_35 = None
        permute_85: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_217, [0, 2, 1]);  view_217 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_14: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_85, memory_format = torch.contiguous_format);  permute_85 = None
        view_218: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_14, [2560, 64]);  clone_14 = None
        mm_52: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_218, permute_84)
        view_219: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_52, [10, 256, 16]);  mm_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_86: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_219, [0, 2, 1]);  view_219 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_73: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_15: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_86, memory_format = torch.contiguous_format);  permute_86 = None
        view_220: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_15, [160, 256]);  clone_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_221: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_220, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_45: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_46: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_21 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 68, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_221, 'Y': empty_45, 'W': primals_138, 'Rstd': empty_46}, tensors_to_clone = ['Y', 'Rstd']);  empty_45 = empty_46 = None
        getitem_72: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_21['Y']
        getitem_73: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_21['Rstd'];  triton_kernel_wrapper_functional_proxy_21 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_368: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_139, torch.bfloat16);  primals_139 = None
        permute_87: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_368, [1, 0]);  convert_element_type_368 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_223: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_72, [160, 256]);  getitem_72 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_53: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_223, permute_87)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant3: "bf16[][]cuda:0" = self._tensor_constant3
        lift_fresh_copy_3: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant3);  _tensor_constant3 = None
        convert_element_type_371: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_3, torch.float32);  lift_fresh_copy_3 = None
        clamp_min_3: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_371, 1);  convert_element_type_371 = None
        convert_element_type_372: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_3, torch.bfloat16);  clamp_min_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_373: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_372, torch.float32);  convert_element_type_372 = None
        reciprocal_3: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_373);  convert_element_type_373 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_224: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_53, [-1, 2, 128]);  mm_53 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_225: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_5, [-1, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_1: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_47: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_22 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 70, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_47, 'Q': view_224, 'K': view_225, 'V': view_225, 'Out': full_1, 'seq_offsets_q': mul_73, 'seq_offsets': primals_122, 'attn_scale': reciprocal_3}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_47 = full_1 = mul_73 = None
        getitem_74: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_22['Out']
        getitem_75: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_22['seq_offsets_q']
        getitem_76: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_22['seq_offsets'];  triton_kernel_wrapper_functional_proxy_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_227: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_74, [-1, 256]);  getitem_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_82: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_227, view_220);  view_227 = view_220 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_374: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_82, torch.float32)
        var_mean_1 = torch.ops.aten.var_mean.correction(convert_element_type_374, [1], correction = 0, keepdim = True)
        getitem_77: "f32[160, 1][1, 1]cuda:0" = var_mean_1[0]
        getitem_78: "f32[160, 1][1, 1]cuda:0" = var_mean_1[1];  var_mean_1 = None
        add_83: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_77, 1e-05);  getitem_77 = None
        rsqrt_35: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_83);  add_83 = None
        sub_3: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_374, getitem_78)
        mul_74: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_3, rsqrt_35);  sub_3 = None
        mul_75: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_74, primals_140);  mul_74 = None
        add_84: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_75, primals_141);  mul_75 = primals_141 = None
        view_228: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_84, [10, 4096]);  add_84 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_375: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_228, torch.bfloat16)
        convert_element_type_376: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_142, torch.bfloat16);  primals_142 = None
        permute_88: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_376, [1, 0]);  convert_element_type_376 = None
        mm_54: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_375, permute_88)
        convert_element_type_379: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_54, torch.float32)
        neg_31: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_379)
        exp_31: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_31);  neg_31 = None
        add_85: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_31, 1);  exp_31 = None
        div_31: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_379, add_85);  convert_element_type_379 = add_85 = None
        convert_element_type_380: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_31, torch.bfloat16);  div_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_381: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_228, torch.bfloat16);  view_228 = None
        convert_element_type_382: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_143, torch.bfloat16);  primals_143 = None
        permute_89: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_382, [1, 0]);  convert_element_type_382 = None
        mm_55: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_381, permute_89)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_76: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_380, mm_55)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_385: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_144, torch.bfloat16);  primals_144 = None
        permute_90: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_385, [1, 0]);  convert_element_type_385 = None
        mm_56: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_76, permute_90)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_229: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_56, [160, 256]);  mm_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_86: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_82, view_229);  add_82 = view_229 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_230: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_86, [10, 16, 256]);  add_86 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_231: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_230, [-1, 256]);  view_230 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_48: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_49: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_23 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 72, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_231, 'Y': empty_48, 'W': primals_145, 'Rstd': empty_49}, tensors_to_clone = ['Y', 'Rstd']);  empty_48 = empty_49 = None
        getitem_79: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_23['Y']
        getitem_80: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_23['Rstd'];  triton_kernel_wrapper_functional_proxy_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_233: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_70, [10, 16, 256]);  getitem_70 = None
        view_234: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_79, [10, 16, 256]);  getitem_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        cat_15: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_233, view_234], 1)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        cat_16: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_233, view_234], 1);  view_233 = view_234 = None
        view_235: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.view.default(cat_16, [10, 2, 16, 256]);  cat_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:293 in forward, code: triu_indices = torch.triu_indices(
        iota_1: "i64[1][1]cuda:0" = torch.ops.prims.iota.default(1, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
        mul_77: "i64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(iota_1, 1);  iota_1 = None
        add_87: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(mul_77, 0);  mul_77 = None
        convert_element_type_388: "f64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(add_87, torch.float64);  add_87 = None
        mul_78: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_388, 2)
        sub_4: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(2.25, mul_78);  mul_78 = None
        sqrt: "f64[1][1]cuda:0" = torch.ops.aten.sqrt.default(sub_4);  sub_4 = None
        sub_5: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1.5, sqrt);  sqrt = None
        floor: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_5);  sub_5 = None
        sub_6: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1, floor)
        mul_79: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(sub_6, floor);  sub_6 = None
        mul_80: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(mul_79, 0.5);  mul_79 = None
        sub_7: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_388, mul_80);  convert_element_type_388 = mul_80 = None
        floor_1: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_7);  sub_7 = None
        convert_element_type_389: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor, torch.int64);  floor = None
        convert_element_type_390: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_1, torch.int64);  floor_1 = None
        add_88: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_389, 0);  convert_element_type_389 = None
        add_89: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_390, 1);  convert_element_type_390 = None
        clone_16: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_88);  add_88 = None
        clone_17: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_89);  add_89 = None
        cat_17: "i64[2][1]cuda:0" = torch.ops.aten.cat.default([clone_16, clone_17]);  clone_16 = clone_17 = None
        view_236: "i64[2, 1][1, 1]cuda:0" = torch.ops.aten.view.default(cat_17, [2, 1]);  cat_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:299 in forward, code: i_indices, j_indices = triu_indices.unbind()
        unbind = torch.ops.aten.unbind.int(view_236);  view_236 = None
        getitem_81: "i64[1][1]cuda:0" = unbind[0]
        getitem_82: "i64[1][1]cuda:0" = unbind[1];  unbind = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        index: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_235, [None, getitem_81])
        index_1: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_235, [None, getitem_82]);  view_235 = None
        mul_81: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.mul.Tensor(index, index_1)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_237: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(mul_81, [10, -1, 256]);  mul_81 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        cat_18: "bf16[10, 112, 256][28672, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_200, cat_15, view_237], 1);  view_200 = cat_15 = view_237 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_91: "bf16[10, 256, 112][28672, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_18, [0, 2, 1]);  cat_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_391: "bf16[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_146, torch.bfloat16);  primals_146 = None
        permute_92: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(convert_element_type_391, [1, 0]);  convert_element_type_391 = None
        clone_18: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.clone.default(permute_91, memory_format = torch.contiguous_format);  permute_91 = None
        view_238: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.view.default(clone_18, [2560, 112]);  clone_18 = None
        mm_57: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_238, permute_92)
        view_239: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_57, [10, 256, 64]);  mm_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_93: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_239, [0, 2, 1]);  view_239 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_94: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_93, [0, 2, 1])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_394: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_147, torch.bfloat16);  primals_147 = None
        permute_95: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_394, [1, 0]);  convert_element_type_394 = None
        view_240: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(permute_94, [2560, 64]);  permute_94 = None
        mm_58: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_240, permute_95)
        view_241: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_58, [10, 256, 80]);  mm_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_96: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_241, [0, 2, 1]);  view_241 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_6 = torch.ops.aten.split_with_sizes.default(permute_96, [32, 32, 16], 1);  permute_96 = None
        getitem_83: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_6[0]
        getitem_84: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_6[1]
        getitem_85: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_6[2];  split_with_sizes_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_19: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_84, memory_format = torch.contiguous_format);  getitem_84 = None
        view_242: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_19, [10, 8192]);  clone_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_397: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_148, torch.bfloat16);  primals_148 = None
        permute_97: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_397, [1, 0]);  convert_element_type_397 = None
        mm_59: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_242, permute_97)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_243: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_59, [-1, 512]);  mm_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_400: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_243, torch.float32);  view_243 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_35: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_400, 2)
        mean_34: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_35, [1], True);  pow_35 = None
        add_90: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_34, 1e-05);  mean_34 = None
        rsqrt_36: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_90);  add_90 = None
        mul_82: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_400, rsqrt_36)
        mul_83: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_82, primals_149);  mul_82 = None
        alias_36: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_36);  rsqrt_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_401: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_83, torch.bfloat16);  mul_83 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_244: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_401, [10, 512]);  convert_element_type_401 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_402: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_244, torch.float32)
        neg_32: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_402)
        exp_32: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_32);  neg_32 = None
        add_91: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_32, 1);  exp_32 = None
        div_33: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_402, add_91);  convert_element_type_402 = add_91 = None
        convert_element_type_403: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_33, torch.bfloat16);  div_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_404: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_150, torch.bfloat16);  primals_150 = None
        permute_98: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_404, [1, 0]);  convert_element_type_404 = None
        mm_60: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_403, permute_98)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_245: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_60, [10, -1, 64]);  mm_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_28: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_245, [10, 32, 64]);  view_245 = None
        view_246: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_28, [10, 32, 64]);  expand_28 = None
        expand_29: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.expand.default(permute_93, [10, 64, 256])
        view_247: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.view.default(expand_29, [10, 64, 256]);  expand_29 = None
        bmm_8: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_246, view_247)
        view_248: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_8, [10, 32, 256]);  bmm_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        permute_99: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_93, [0, 2, 1])
        expand_30: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_248, [10, 32, 256]);  view_248 = None
        view_249: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_30, [10, 32, 256]);  expand_30 = None
        expand_31: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.expand.default(permute_99, [10, 256, 64]);  permute_99 = None
        view_250: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_31, [10, 256, 64]);  expand_31 = None
        bmm_9: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_249, view_250)
        view_251: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_9, [10, 32, 64]);  bmm_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_252: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_251, [10, -1]);  view_251 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_20: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_85, memory_format = torch.contiguous_format);  getitem_85 = None
        view_253: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_20, [10, 4096]);  clone_20 = None
        cat_19: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_252, view_253], 1);  view_252 = view_253 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_20: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_19, view_48], 1);  cat_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_254: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_20, [-1, 8472]);  cat_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_50: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_51: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_24 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 74, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_254, 'Y': empty_50, 'W': primals_151, 'Rstd': empty_51}, tensors_to_clone = ['Y', 'Rstd']);  empty_50 = empty_51 = None
        getitem_86: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_24['Y']
        getitem_87: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_24['Rstd'];  triton_kernel_wrapper_functional_proxy_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_256: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_86, [10, 8472]);  getitem_86 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_411: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_256, torch.bfloat16);  view_256 = None
        convert_element_type_412: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_152, torch.bfloat16);  primals_152 = None
        permute_100: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_412, [1, 0]);  convert_element_type_412 = None
        mm_61: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_411, permute_100)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_257: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_61, [-1, 2048]);  mm_61 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_415: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_257, torch.float32);  view_257 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_36: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_415, 2)
        mean_35: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_36, [1], True);  pow_36 = None
        add_92: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_35, 1e-05);  mean_35 = None
        rsqrt_37: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_92);  add_92 = None
        mul_84: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_415, rsqrt_37)
        mul_85: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_84, primals_153);  mul_84 = None
        alias_37: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_37);  rsqrt_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_416: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_85, torch.bfloat16);  mul_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_258: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_416, [10, 2048]);  convert_element_type_416 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_417: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_258, torch.float32)
        neg_33: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_417)
        exp_33: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_33);  neg_33 = None
        add_93: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_33, 1);  exp_33 = None
        div_34: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_417, add_93);  convert_element_type_417 = add_93 = None
        convert_element_type_418: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_34, torch.bfloat16);  div_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_419: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_154, torch.bfloat16);  primals_154 = None
        permute_101: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_419, [1, 0]);  convert_element_type_419 = None
        mm_62: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_418, permute_101)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_259: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_62, [-1, 1024]);  mm_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_422: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_259, torch.float32);  view_259 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_37: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_422, 2)
        mean_36: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_37, [1], True);  pow_37 = None
        add_94: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_36, 1e-05);  mean_36 = None
        rsqrt_38: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_94);  add_94 = None
        mul_86: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_422, rsqrt_38)
        mul_87: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_86, primals_155);  mul_86 = None
        alias_38: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_38);  rsqrt_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_423: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_87, torch.bfloat16);  mul_87 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_260: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_423, [10, 1024]);  convert_element_type_423 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_424: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_260, torch.float32)
        neg_34: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_424)
        exp_34: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_34);  neg_34 = None
        add_95: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_34, 1);  exp_34 = None
        div_35: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_424, add_95);  convert_element_type_424 = add_95 = None
        convert_element_type_425: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_35, torch.bfloat16);  div_35 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_426: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_156, torch.bfloat16);  primals_156 = None
        permute_102: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_426, [1, 0]);  convert_element_type_426 = None
        mm_63: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_425, permute_102)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_96: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_418, mm_63);  mm_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_261: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_96, [-1, 2048]);  add_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_429: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_261, torch.float32);  view_261 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_38: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_429, 2)
        mean_37: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_38, [1], True);  pow_38 = None
        add_97: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_37, 1e-05);  mean_37 = None
        rsqrt_39: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_97);  add_97 = None
        mul_88: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_429, rsqrt_39)
        mul_89: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_88, primals_157);  mul_88 = None
        alias_39: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_39);  rsqrt_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_430: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_89, torch.bfloat16);  mul_89 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_262: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_430, [10, 2048]);  convert_element_type_430 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_431: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_262, torch.float32)
        neg_35: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_431)
        exp_35: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_35);  neg_35 = None
        add_98: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_35, 1);  exp_35 = None
        div_36: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_431, add_98);  convert_element_type_431 = add_98 = None
        convert_element_type_432: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_36, torch.bfloat16);  div_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_433: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_158, torch.bfloat16);  primals_158 = None
        permute_103: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_433, [1, 0]);  convert_element_type_433 = None
        mm_64: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_432, permute_103)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_263: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_64, [-1, 1024]);  mm_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_436: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_263, torch.float32);  view_263 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_39: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_436, 2)
        mean_38: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_39, [1], True);  pow_39 = None
        add_99: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_38, 1e-05);  mean_38 = None
        rsqrt_40: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_99);  add_99 = None
        mul_90: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_436, rsqrt_40)
        mul_91: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_90, primals_159);  mul_90 = None
        alias_40: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_40);  rsqrt_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_437: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_91, torch.bfloat16);  mul_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_264: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_437, [10, 1024]);  convert_element_type_437 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_438: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_264, torch.float32)
        neg_36: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_438)
        exp_36: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_36);  neg_36 = None
        add_100: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_36, 1);  exp_36 = None
        div_37: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_438, add_100);  convert_element_type_438 = add_100 = None
        convert_element_type_439: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_37, torch.bfloat16);  div_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_440: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_160, torch.bfloat16);  primals_160 = None
        permute_104: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_440, [1, 0]);  convert_element_type_440 = None
        mm_65: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_439, permute_104)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_101: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_432, mm_65);  mm_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_265: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_101, [-1, 2048]);  add_101 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_443: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_265, torch.float32);  view_265 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_40: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_443, 2)
        mean_39: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_40, [1], True);  pow_40 = None
        add_102: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_39, 1e-05);  mean_39 = None
        rsqrt_41: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_102);  add_102 = None
        mul_92: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_443, rsqrt_41)
        mul_93: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_92, primals_161);  mul_92 = None
        alias_41: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_41);  rsqrt_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_444: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_93, torch.bfloat16);  mul_93 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_266: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_444, [10, 2048]);  convert_element_type_444 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_445: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_266, torch.float32)
        neg_37: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_445)
        exp_37: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_37);  neg_37 = None
        add_103: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_37, 1);  exp_37 = None
        div_38: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_445, add_103);  convert_element_type_445 = add_103 = None
        convert_element_type_446: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_38, torch.bfloat16);  div_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_447: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_162, torch.bfloat16);  primals_162 = None
        permute_105: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_447, [1, 0]);  convert_element_type_447 = None
        mm_66: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_446, permute_105)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_267: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_66, [10, -1, 256]);  mm_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_21: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_267, getitem_83], 1);  view_267 = getitem_83 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_104: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_21, permute_93);  cat_21 = permute_93 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_268: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_104, [-1, 256]);  add_104 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_52: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_53: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_25 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 76, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_268, 'Y': empty_52, 'W': primals_163, 'Rstd': empty_53}, tensors_to_clone = ['Y', 'Rstd']);  empty_52 = empty_53 = None
        getitem_88: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_25['Y']
        getitem_89: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_25['Rstd'];  triton_kernel_wrapper_functional_proxy_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant4: "bf16[][]cuda:0" = self._tensor_constant4
        lift_fresh_copy_4: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant4);  _tensor_constant4 = None
        convert_element_type_450: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_4, torch.float32);  lift_fresh_copy_4 = None
        clamp_min_4: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_450, 1);  convert_element_type_450 = None
        convert_element_type_451: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_4, torch.bfloat16);  clamp_min_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_452: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_451, torch.float32);  convert_element_type_451 = None
        reciprocal_4: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_452);  convert_element_type_452 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_453: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_164, torch.bfloat16);  primals_164 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_454: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_165, torch.bfloat16);  primals_165 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_455: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_166, torch.bfloat16);  primals_166 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_456: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_167, torch.bfloat16);  primals_167 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_54: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_55: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_56: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_26 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 78, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': addmm_3, 'Y': empty_54, 'W': convert_element_type_453, 'B': convert_element_type_454, 'Mean': empty_55, 'Rstd': empty_56}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_54 = empty_55 = empty_56 = None
        getitem_90: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_26['Y']
        getitem_91: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_26['Mean']
        getitem_92: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_26['Rstd'];  triton_kernel_wrapper_functional_proxy_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_6: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_456, getitem_90, convert_element_type_455);  getitem_90 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_7 = torch.ops.aten.split_with_sizes.default(addmm_6, [256, 256, 256, 256], 1);  addmm_6 = None
        getitem_93: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_7[0]
        getitem_94: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_7[1]
        getitem_95: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_7[2]
        getitem_96: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_7[3];  split_with_sizes_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_270: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_95, [-1, 2, 128]);  getitem_95 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_271: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_96, [-1, 2, 128]);  getitem_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_272: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_94, [-1, 2, 128]);  getitem_94 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_460: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_93, torch.float32);  getitem_93 = None
        neg_38: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_460)
        exp_38: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_38);  neg_38 = None
        add_105: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_38, 1);  exp_38 = None
        div_39: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_460, add_105);  convert_element_type_460 = add_105 = None
        convert_element_type_461: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_39, torch.bfloat16);  div_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_7: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_67, 0, 1, 9223372036854775807)
        slice_8: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_67, 0, 0, -1)
        sub_8: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_7, slice_8);  slice_7 = slice_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_2 = torch.ops.aten.sort.stable(sub_8, stable = False, descending = True);  sub_8 = None
        getitem_98: "i64[2][1]cuda:0" = sort_2[1];  sort_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_57: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_273: "bf16[131584][1]cuda:0" = torch.ops.aten.view.default(empty_57, [-1]);  empty_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_27 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 79, grid = [(17, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_270, 'K': view_271, 'V': view_272, 'seq_offsets': getitem_67, 'attn_scale': reciprocal_4, 'Out': view_273}, tensors_to_clone = ['Out']);  view_270 = view_271 = view_272 = view_273 = None
        getitem_99: "bf16[131584][1]cuda:0" = triton_kernel_wrapper_functional_proxy_27['Out'];  triton_kernel_wrapper_functional_proxy_27 = None
        view_274: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_99, [514, 2, 128]);  getitem_99 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_462: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_168, torch.bfloat16);  primals_168 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_463: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_169, torch.bfloat16);  primals_169 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_464: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_170, torch.bfloat16);  primals_170 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_59: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_60: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_61: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_277: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_274, [-1, 256]);  view_274 = None
        triton_kernel_wrapper_functional_proxy_28 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 20, constant_args_idx = 84, grid = [(514, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_277, 'U': convert_element_type_461, 'Y': empty_59, 'W': convert_element_type_462, 'B': convert_element_type_463, 'Mean': empty_60, 'Rstd': empty_61, 'seed': primals_171}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_59 = empty_60 = empty_61 = None
        getitem_100: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_28['Y']
        getitem_101: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_28['Mean']
        getitem_102: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_28['Rstd'];  triton_kernel_wrapper_functional_proxy_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_7: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_3, getitem_100, convert_element_type_464);  getitem_100 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant5: "bf16[][]cuda:0" = self._tensor_constant5
        lift_fresh_copy_5: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant5);  _tensor_constant5 = None
        convert_element_type_468: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_5, torch.float32);  lift_fresh_copy_5 = None
        clamp_min_5: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_468, 1);  convert_element_type_468 = None
        convert_element_type_469: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_5, torch.bfloat16);  clamp_min_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_470: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_469, torch.float32);  convert_element_type_469 = None
        reciprocal_5: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_470);  convert_element_type_470 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_471: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_172, torch.bfloat16);  primals_172 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_472: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_173, torch.bfloat16);  primals_173 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_473: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_174, torch.bfloat16);  primals_174 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_474: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_175, torch.bfloat16);  primals_175 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_62: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_63: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_64: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_29 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 87, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': addmm_5, 'Y': empty_62, 'W': convert_element_type_471, 'B': convert_element_type_472, 'Mean': empty_63, 'Rstd': empty_64}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_62 = empty_63 = empty_64 = None
        getitem_103: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_29['Y']
        getitem_104: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_29['Mean']
        getitem_105: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_29['Rstd'];  triton_kernel_wrapper_functional_proxy_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_8: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_474, getitem_103, convert_element_type_473);  getitem_103 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_8 = torch.ops.aten.split_with_sizes.default(addmm_8, [256, 256, 256, 256], 1);  addmm_8 = None
        getitem_106: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_8[0]
        getitem_107: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_8[1]
        getitem_108: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_8[2]
        getitem_109: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_8[3];  split_with_sizes_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_278: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_108, [-1, 2, 128]);  getitem_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_279: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_109, [-1, 2, 128]);  getitem_109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_280: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_107, [-1, 2, 128]);  getitem_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_478: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_106, torch.float32);  getitem_106 = None
        neg_39: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_478)
        exp_39: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_39);  neg_39 = None
        add_106: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_39, 1);  exp_39 = None
        div_40: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_478, add_106);  convert_element_type_478 = add_106 = None
        convert_element_type_479: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_40, torch.bfloat16);  div_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_11: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_76, 0, 1, 9223372036854775807)
        slice_12: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_76, 0, 0, -1)
        sub_9: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_11, slice_12);  slice_11 = slice_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_3 = torch.ops.aten.sort.stable(sub_9, stable = False, descending = True);  sub_9 = None
        getitem_111: "i64[2][1]cuda:0" = sort_3[1];  sort_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_65: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_281: "bf16[131584][1]cuda:0" = torch.ops.aten.view.default(empty_65, [-1]);  empty_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_30 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 88, grid = [(17, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (9, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (5, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1), (3, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_278, 'K': view_279, 'V': view_280, 'seq_offsets': getitem_76, 'attn_scale': reciprocal_5, 'Out': view_281}, tensors_to_clone = ['Out']);  view_278 = view_279 = view_280 = view_281 = None
        getitem_112: "bf16[131584][1]cuda:0" = triton_kernel_wrapper_functional_proxy_30['Out'];  triton_kernel_wrapper_functional_proxy_30 = None
        view_282: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_112, [514, 2, 128]);  getitem_112 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_480: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_176, torch.bfloat16);  primals_176 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_481: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_177, torch.bfloat16);  primals_177 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_482: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_178, torch.bfloat16);  primals_178 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_67: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_68: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_69: "f32[514][1]cuda:0" = torch.ops.aten.empty.memory_format([514], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_285: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_282, [-1, 256]);  view_282 = None
        triton_kernel_wrapper_functional_proxy_31 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 22, constant_args_idx = 93, grid = [(514, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_285, 'U': convert_element_type_479, 'Y': empty_67, 'W': convert_element_type_480, 'B': convert_element_type_481, 'Mean': empty_68, 'Rstd': empty_69, 'seed': primals_179}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_67 = empty_68 = empty_69 = None
        getitem_113: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_31['Y']
        getitem_114: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_31['Mean']
        getitem_115: "f32[514][1]cuda:0" = triton_kernel_wrapper_functional_proxy_31['Rstd'];  triton_kernel_wrapper_functional_proxy_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_9: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(addmm_5, getitem_113, convert_element_type_482);  getitem_113 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_286: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_88, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_486: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_180, torch.bfloat16);  primals_180 = None
        permute_107: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_486, [1, 0]);  convert_element_type_486 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_287: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_88, [10, 64, 256])
        permute_108: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_287, [0, 2, 1]);  view_287 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_21: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_108, memory_format = torch.contiguous_format);  permute_108 = None
        view_288: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_21, [2560, 64]);  clone_21 = None
        mm_67: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_288, permute_107)
        view_289: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_67, [10, 256, 16]);  mm_67 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_109: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_289, [0, 2, 1]);  view_289 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_94: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_22: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_109, memory_format = torch.contiguous_format);  permute_109 = None
        view_290: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_22, [160, 256]);  clone_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_291: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_290, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_70: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_71: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_32 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 96, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_291, 'Y': empty_70, 'W': primals_181, 'Rstd': empty_71}, tensors_to_clone = ['Y', 'Rstd']);  empty_70 = empty_71 = None
        getitem_116: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_32['Y']
        getitem_117: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_32['Rstd'];  triton_kernel_wrapper_functional_proxy_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_489: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_182, torch.bfloat16);  primals_182 = None
        permute_110: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_489, [1, 0]);  convert_element_type_489 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_293: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_116, [160, 256]);  getitem_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_68: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_293, permute_110)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant6: "bf16[][]cuda:0" = self._tensor_constant6
        lift_fresh_copy_6: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant6);  _tensor_constant6 = None
        convert_element_type_492: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_6, torch.float32);  lift_fresh_copy_6 = None
        clamp_min_6: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_492, 1);  convert_element_type_492 = None
        convert_element_type_493: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_6, torch.bfloat16);  clamp_min_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_494: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_493, torch.float32);  convert_element_type_493 = None
        reciprocal_6: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_494);  convert_element_type_494 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_294: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_68, [-1, 2, 128]);  mm_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_295: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_7, [-1, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_2: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_72: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_33 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 98, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_72, 'Q': view_294, 'K': view_295, 'V': view_295, 'Out': full_2, 'seq_offsets_q': mul_94, 'seq_offsets': getitem_67, 'attn_scale': reciprocal_6}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_72 = full_2 = mul_94 = getitem_67 = None
        getitem_118: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_33['Out']
        getitem_119: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_33['seq_offsets_q']
        getitem_120: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_33['seq_offsets'];  triton_kernel_wrapper_functional_proxy_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_297: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_118, [-1, 256]);  getitem_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_107: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_297, view_290);  view_297 = view_290 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_495: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_107, torch.float32)
        var_mean_2 = torch.ops.aten.var_mean.correction(convert_element_type_495, [1], correction = 0, keepdim = True)
        getitem_121: "f32[160, 1][1, 1]cuda:0" = var_mean_2[0]
        getitem_122: "f32[160, 1][1, 1]cuda:0" = var_mean_2[1];  var_mean_2 = None
        add_108: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_121, 1e-05);  getitem_121 = None
        rsqrt_42: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_108);  add_108 = None
        sub_10: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_495, getitem_122)
        mul_95: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_10, rsqrt_42);  sub_10 = None
        mul_96: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_95, primals_183);  mul_95 = None
        add_109: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_96, primals_184);  mul_96 = primals_184 = None
        view_298: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_109, [10, 4096]);  add_109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_496: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_298, torch.bfloat16)
        convert_element_type_497: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_185, torch.bfloat16);  primals_185 = None
        permute_111: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_497, [1, 0]);  convert_element_type_497 = None
        mm_69: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_496, permute_111)
        convert_element_type_500: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_69, torch.float32)
        neg_40: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_500)
        exp_40: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_40);  neg_40 = None
        add_110: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_40, 1);  exp_40 = None
        div_41: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_500, add_110);  convert_element_type_500 = add_110 = None
        convert_element_type_501: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_41, torch.bfloat16);  div_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_502: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_298, torch.bfloat16);  view_298 = None
        convert_element_type_503: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_186, torch.bfloat16);  primals_186 = None
        permute_112: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_503, [1, 0]);  convert_element_type_503 = None
        mm_70: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_502, permute_112)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_97: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_501, mm_70)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_506: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_187, torch.bfloat16);  primals_187 = None
        permute_113: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_506, [1, 0]);  convert_element_type_506 = None
        mm_71: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_97, permute_113)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_299: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_71, [160, 256]);  mm_71 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_111: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_107, view_299);  add_107 = view_299 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_300: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_111, [10, 16, 256]);  add_111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_301: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_300, [-1, 256]);  view_300 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_73: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_74: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_34 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 100, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_301, 'Y': empty_73, 'W': primals_188, 'Rstd': empty_74}, tensors_to_clone = ['Y', 'Rstd']);  empty_73 = empty_74 = None
        getitem_123: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_34['Y']
        getitem_124: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_34['Rstd'];  triton_kernel_wrapper_functional_proxy_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_509: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_189, torch.bfloat16);  primals_189 = None
        permute_115: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_509, [1, 0]);  convert_element_type_509 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_303: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_88, [10, 64, 256]);  getitem_88 = None
        permute_116: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_303, [0, 2, 1]);  view_303 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_23: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_116, memory_format = torch.contiguous_format);  permute_116 = None
        view_304: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_23, [2560, 64]);  clone_23 = None
        mm_72: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_304, permute_115)
        view_305: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_72, [10, 256, 16]);  mm_72 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_117: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_305, [0, 2, 1]);  view_305 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_98: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_24: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_117, memory_format = torch.contiguous_format);  permute_117 = None
        view_306: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_24, [160, 256]);  clone_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_307: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_306, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_75: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_76: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_35 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 102, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_307, 'Y': empty_75, 'W': primals_190, 'Rstd': empty_76}, tensors_to_clone = ['Y', 'Rstd']);  empty_75 = empty_76 = None
        getitem_125: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_35['Y']
        getitem_126: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_35['Rstd'];  triton_kernel_wrapper_functional_proxy_35 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_512: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_191, torch.bfloat16);  primals_191 = None
        permute_118: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_512, [1, 0]);  convert_element_type_512 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_309: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_125, [160, 256]);  getitem_125 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_73: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_309, permute_118)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        _tensor_constant7: "bf16[][]cuda:0" = self._tensor_constant7
        lift_fresh_copy_7: "bf16[][]cuda:0" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant7);  _tensor_constant7 = None
        convert_element_type_515: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(lift_fresh_copy_7, torch.float32);  lift_fresh_copy_7 = None
        clamp_min_7: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_515, 1);  convert_element_type_515 = None
        convert_element_type_516: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_7, torch.bfloat16);  clamp_min_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_517: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_516, torch.float32);  convert_element_type_516 = None
        reciprocal_7: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_517);  convert_element_type_517 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_310: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_73, [-1, 2, 128]);  mm_73 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_311: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_9, [-1, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_3: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_77: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_36 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 104, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_77, 'Q': view_310, 'K': view_311, 'V': view_311, 'Out': full_3, 'seq_offsets_q': mul_98, 'seq_offsets': getitem_76, 'attn_scale': reciprocal_7}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_77 = full_3 = mul_98 = getitem_76 = None
        getitem_127: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_36['Out']
        getitem_128: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_36['seq_offsets_q']
        getitem_129: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_36['seq_offsets'];  triton_kernel_wrapper_functional_proxy_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_313: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_127, [-1, 256]);  getitem_127 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_112: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_313, view_306);  view_313 = view_306 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_518: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_112, torch.float32)
        var_mean_3 = torch.ops.aten.var_mean.correction(convert_element_type_518, [1], correction = 0, keepdim = True)
        getitem_130: "f32[160, 1][1, 1]cuda:0" = var_mean_3[0]
        getitem_131: "f32[160, 1][1, 1]cuda:0" = var_mean_3[1];  var_mean_3 = None
        add_113: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_130, 1e-05);  getitem_130 = None
        rsqrt_43: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_113);  add_113 = None
        sub_11: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_518, getitem_131)
        mul_99: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_11, rsqrt_43);  sub_11 = None
        mul_100: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_99, primals_192);  mul_99 = None
        add_114: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_100, primals_193);  mul_100 = primals_193 = None
        view_314: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_114, [10, 4096]);  add_114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_519: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_314, torch.bfloat16)
        convert_element_type_520: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_194, torch.bfloat16);  primals_194 = None
        permute_119: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_520, [1, 0]);  convert_element_type_520 = None
        mm_74: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_519, permute_119)
        convert_element_type_523: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_74, torch.float32)
        neg_41: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_523)
        exp_41: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_41);  neg_41 = None
        add_115: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_41, 1);  exp_41 = None
        div_42: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_523, add_115);  convert_element_type_523 = add_115 = None
        convert_element_type_524: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_42, torch.bfloat16);  div_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_525: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_314, torch.bfloat16);  view_314 = None
        convert_element_type_526: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_195, torch.bfloat16);  primals_195 = None
        permute_120: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_526, [1, 0]);  convert_element_type_526 = None
        mm_75: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_525, permute_120)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_101: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_524, mm_75)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_529: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_196, torch.bfloat16);  primals_196 = None
        permute_121: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_529, [1, 0]);  convert_element_type_529 = None
        mm_76: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_101, permute_121)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_315: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_76, [160, 256]);  mm_76 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_116: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_112, view_315);  add_112 = view_315 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_316: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_116, [10, 16, 256]);  add_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_317: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_316, [-1, 256]);  view_316 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_78: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_79: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_37 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 106, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_317, 'Y': empty_78, 'W': primals_197, 'Rstd': empty_79}, tensors_to_clone = ['Y', 'Rstd']);  empty_78 = empty_79 = None
        getitem_132: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_37['Y']
        getitem_133: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_37['Rstd'];  triton_kernel_wrapper_functional_proxy_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_319: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_123, [10, 16, 256]);  getitem_123 = None
        view_320: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_132, [10, 16, 256]);  getitem_132 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        cat_22: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_319, view_320], 1)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        cat_23: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_319, view_320], 1);  view_319 = view_320 = None
        view_321: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.view.default(cat_23, [10, 2, 16, 256]);  cat_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:293 in forward, code: triu_indices = torch.triu_indices(
        iota_3: "i64[1][1]cuda:0" = torch.ops.prims.iota.default(1, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
        mul_102: "i64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(iota_3, 1);  iota_3 = None
        add_117: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(mul_102, 0);  mul_102 = None
        convert_element_type_532: "f64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(add_117, torch.float64);  add_117 = None
        mul_103: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_532, 2)
        sub_12: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(2.25, mul_103);  mul_103 = None
        sqrt_1: "f64[1][1]cuda:0" = torch.ops.aten.sqrt.default(sub_12);  sub_12 = None
        sub_13: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1.5, sqrt_1);  sqrt_1 = None
        floor_2: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_13);  sub_13 = None
        sub_14: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1, floor_2)
        mul_104: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(sub_14, floor_2);  sub_14 = None
        mul_105: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(mul_104, 0.5);  mul_104 = None
        sub_15: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_532, mul_105);  convert_element_type_532 = mul_105 = None
        floor_3: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_15);  sub_15 = None
        convert_element_type_533: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_2, torch.int64);  floor_2 = None
        convert_element_type_534: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_3, torch.int64);  floor_3 = None
        add_118: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_533, 0);  convert_element_type_533 = None
        add_119: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_534, 1);  convert_element_type_534 = None
        clone_25: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_118);  add_118 = None
        clone_26: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_119);  add_119 = None
        cat_24: "i64[2][1]cuda:0" = torch.ops.aten.cat.default([clone_25, clone_26]);  clone_25 = clone_26 = None
        view_322: "i64[2, 1][1, 1]cuda:0" = torch.ops.aten.view.default(cat_24, [2, 1]);  cat_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:299 in forward, code: i_indices, j_indices = triu_indices.unbind()
        unbind_1 = torch.ops.aten.unbind.int(view_322);  view_322 = None
        getitem_134: "i64[1][1]cuda:0" = unbind_1[0]
        getitem_135: "i64[1][1]cuda:0" = unbind_1[1];  unbind_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        index_2: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_321, [None, getitem_134])
        index_3: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_321, [None, getitem_135]);  view_321 = None
        mul_106: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.mul.Tensor(index_2, index_3)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_323: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(mul_106, [10, -1, 256]);  mul_106 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        cat_25: "bf16[10, 112, 256][28672, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_286, cat_22, view_323], 1);  view_286 = cat_22 = view_323 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_122: "bf16[10, 256, 112][28672, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_25, [0, 2, 1]);  cat_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_535: "bf16[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_198, torch.bfloat16);  primals_198 = None
        permute_123: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(convert_element_type_535, [1, 0]);  convert_element_type_535 = None
        clone_27: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.clone.default(permute_122, memory_format = torch.contiguous_format);  permute_122 = None
        view_324: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.view.default(clone_27, [2560, 112]);  clone_27 = None
        mm_77: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_324, permute_123)
        view_325: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_77, [10, 256, 64]);  mm_77 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_124: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_325, [0, 2, 1]);  view_325 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_125: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_124, [0, 2, 1])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_538: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_199, torch.bfloat16);  primals_199 = None
        permute_126: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_538, [1, 0]);  convert_element_type_538 = None
        view_326: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(permute_125, [2560, 64]);  permute_125 = None
        mm_78: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_326, permute_126)
        view_327: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_78, [10, 256, 80]);  mm_78 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_127: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_327, [0, 2, 1]);  view_327 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_9 = torch.ops.aten.split_with_sizes.default(permute_127, [32, 32, 16], 1);  permute_127 = None
        getitem_136: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_9[0]
        getitem_137: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_9[1]
        getitem_138: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_9[2];  split_with_sizes_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_28: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_137, memory_format = torch.contiguous_format);  getitem_137 = None
        view_328: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_28, [10, 8192]);  clone_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_541: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_200, torch.bfloat16);  primals_200 = None
        permute_128: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_541, [1, 0]);  convert_element_type_541 = None
        mm_79: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_328, permute_128)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_329: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_79, [-1, 512]);  mm_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_544: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_329, torch.float32);  view_329 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_41: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_544, 2)
        mean_40: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_41, [1], True);  pow_41 = None
        add_120: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_40, 1e-05);  mean_40 = None
        rsqrt_44: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_120);  add_120 = None
        mul_107: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_544, rsqrt_44)
        mul_108: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_107, primals_201);  mul_107 = None
        alias_42: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_44);  rsqrt_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_545: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_108, torch.bfloat16);  mul_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_330: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_545, [10, 512]);  convert_element_type_545 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_546: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_330, torch.float32)
        neg_42: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_546)
        exp_42: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_42);  neg_42 = None
        add_121: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_42, 1);  exp_42 = None
        div_44: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_546, add_121);  convert_element_type_546 = add_121 = None
        convert_element_type_547: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_44, torch.bfloat16);  div_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_548: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_202, torch.bfloat16);  primals_202 = None
        permute_129: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_548, [1, 0]);  convert_element_type_548 = None
        mm_80: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_547, permute_129)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_331: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_80, [10, -1, 64]);  mm_80 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_32: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_331, [10, 32, 64]);  view_331 = None
        view_332: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_32, [10, 32, 64]);  expand_32 = None
        expand_33: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.expand.default(permute_124, [10, 64, 256])
        view_333: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.view.default(expand_33, [10, 64, 256]);  expand_33 = None
        bmm_10: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_332, view_333)
        view_334: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_10, [10, 32, 256]);  bmm_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        permute_130: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_124, [0, 2, 1])
        expand_34: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_334, [10, 32, 256]);  view_334 = None
        view_335: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_34, [10, 32, 256]);  expand_34 = None
        expand_35: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.expand.default(permute_130, [10, 256, 64]);  permute_130 = None
        view_336: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_35, [10, 256, 64]);  expand_35 = None
        bmm_11: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_335, view_336)
        view_337: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_11, [10, 32, 64]);  bmm_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_338: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_337, [10, -1]);  view_337 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_29: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_138, memory_format = torch.contiguous_format);  getitem_138 = None
        view_339: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_29, [10, 4096]);  clone_29 = None
        cat_26: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_338, view_339], 1);  view_338 = view_339 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_27: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_26, view_48], 1);  cat_26 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_340: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_27, [-1, 8472]);  cat_27 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_80: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_81: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_38 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 108, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_340, 'Y': empty_80, 'W': primals_203, 'Rstd': empty_81}, tensors_to_clone = ['Y', 'Rstd']);  empty_80 = empty_81 = None
        getitem_139: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_38['Y']
        getitem_140: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_38['Rstd'];  triton_kernel_wrapper_functional_proxy_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_342: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_139, [10, 8472]);  getitem_139 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_555: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_342, torch.bfloat16);  view_342 = None
        convert_element_type_556: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_204, torch.bfloat16);  primals_204 = None
        permute_131: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_556, [1, 0]);  convert_element_type_556 = None
        mm_81: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_555, permute_131)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_343: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_81, [-1, 2048]);  mm_81 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_559: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_343, torch.float32);  view_343 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_42: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_559, 2)
        mean_41: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_42, [1], True);  pow_42 = None
        add_122: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_41, 1e-05);  mean_41 = None
        rsqrt_45: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_122);  add_122 = None
        mul_109: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_559, rsqrt_45)
        mul_110: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_109, primals_205);  mul_109 = None
        alias_43: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_45);  rsqrt_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_560: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_110, torch.bfloat16);  mul_110 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_344: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_560, [10, 2048]);  convert_element_type_560 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_561: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_344, torch.float32)
        neg_43: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_561)
        exp_43: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_43);  neg_43 = None
        add_123: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_43, 1);  exp_43 = None
        div_45: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_561, add_123);  convert_element_type_561 = add_123 = None
        convert_element_type_562: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_45, torch.bfloat16);  div_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_563: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_206, torch.bfloat16);  primals_206 = None
        permute_132: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_563, [1, 0]);  convert_element_type_563 = None
        mm_82: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_562, permute_132)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_345: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_82, [-1, 1024]);  mm_82 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_566: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_345, torch.float32);  view_345 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_43: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_566, 2)
        mean_42: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_43, [1], True);  pow_43 = None
        add_124: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_42, 1e-05);  mean_42 = None
        rsqrt_46: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_124);  add_124 = None
        mul_111: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_566, rsqrt_46)
        mul_112: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_111, primals_207);  mul_111 = None
        alias_44: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_46);  rsqrt_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_567: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_112, torch.bfloat16);  mul_112 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_346: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_567, [10, 1024]);  convert_element_type_567 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_568: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_346, torch.float32)
        neg_44: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_568)
        exp_44: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_44);  neg_44 = None
        add_125: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_44, 1);  exp_44 = None
        div_46: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_568, add_125);  convert_element_type_568 = add_125 = None
        convert_element_type_569: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_46, torch.bfloat16);  div_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_570: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_208, torch.bfloat16);  primals_208 = None
        permute_133: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_570, [1, 0]);  convert_element_type_570 = None
        mm_83: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_569, permute_133)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_126: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_562, mm_83);  mm_83 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_347: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_126, [-1, 2048]);  add_126 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_573: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_347, torch.float32);  view_347 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_44: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_573, 2)
        mean_43: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_44, [1], True);  pow_44 = None
        add_127: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_43, 1e-05);  mean_43 = None
        rsqrt_47: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_127);  add_127 = None
        mul_113: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_573, rsqrt_47)
        mul_114: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_113, primals_209);  mul_113 = None
        alias_45: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_47);  rsqrt_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_574: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_114, torch.bfloat16);  mul_114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_348: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_574, [10, 2048]);  convert_element_type_574 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_575: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_348, torch.float32)
        neg_45: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_575)
        exp_45: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_45);  neg_45 = None
        add_128: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_45, 1);  exp_45 = None
        div_47: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_575, add_128);  convert_element_type_575 = add_128 = None
        convert_element_type_576: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_47, torch.bfloat16);  div_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_577: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_210, torch.bfloat16);  primals_210 = None
        permute_134: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_577, [1, 0]);  convert_element_type_577 = None
        mm_84: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_576, permute_134)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_349: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_84, [-1, 1024]);  mm_84 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_580: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_349, torch.float32);  view_349 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_45: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_580, 2)
        mean_44: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_45, [1], True);  pow_45 = None
        add_129: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_44, 1e-05);  mean_44 = None
        rsqrt_48: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_129);  add_129 = None
        mul_115: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_580, rsqrt_48)
        mul_116: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_115, primals_211);  mul_115 = None
        alias_46: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_48);  rsqrt_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_581: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_116, torch.bfloat16);  mul_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_350: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_581, [10, 1024]);  convert_element_type_581 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_582: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_350, torch.float32)
        neg_46: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_582)
        exp_46: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_46);  neg_46 = None
        add_130: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_46, 1);  exp_46 = None
        div_48: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_582, add_130);  convert_element_type_582 = add_130 = None
        convert_element_type_583: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_48, torch.bfloat16);  div_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_584: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_212, torch.bfloat16);  primals_212 = None
        permute_135: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_584, [1, 0]);  convert_element_type_584 = None
        mm_85: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_583, permute_135)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_131: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_576, mm_85);  mm_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_351: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_131, [-1, 2048]);  add_131 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_587: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_351, torch.float32);  view_351 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_46: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_587, 2)
        mean_45: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_46, [1], True);  pow_46 = None
        add_132: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_45, 1e-05);  mean_45 = None
        rsqrt_49: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_132);  add_132 = None
        mul_117: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_587, rsqrt_49)
        mul_118: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_117, primals_213);  mul_117 = None
        alias_47: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_49);  rsqrt_49 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_588: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_118, torch.bfloat16);  mul_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_352: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_588, [10, 2048]);  convert_element_type_588 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_589: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_352, torch.float32)
        neg_47: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_589)
        exp_47: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_47);  neg_47 = None
        add_133: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_47, 1);  exp_47 = None
        div_49: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_589, add_133);  convert_element_type_589 = add_133 = None
        convert_element_type_590: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_49, torch.bfloat16);  div_49 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_591: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_214, torch.bfloat16);  primals_214 = None
        permute_136: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_591, [1, 0]);  convert_element_type_591 = None
        mm_86: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_590, permute_136)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_353: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_86, [10, -1, 256]);  mm_86 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_28: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_353, getitem_136], 1);  view_353 = getitem_136 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_134: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_28, permute_124);  cat_28 = permute_124 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_354: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_134, [-1, 256]);  add_134 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_82: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_83: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_39 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 110, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_354, 'Y': empty_82, 'W': primals_215, 'Rstd': empty_83}, tensors_to_clone = ['Y', 'Rstd']);  empty_82 = empty_83 = None
        getitem_141: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_39['Y']
        getitem_142: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_39['Rstd'];  triton_kernel_wrapper_functional_proxy_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:605 in forward, code: seq_lengths=offsets[1:] - offsets[:-1],
        slice_15: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_120, 0, 1, 9223372036854775807)
        slice_16: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_120, 0, 0, -1)
        sub_16: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_15, slice_16);  slice_15 = slice_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:125 in _truncate_last_k_embeddings, code: minus_k_lengths = torch.clamp(seq_lengths - K, min=0)
        sub_17: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_16, 256)
        clamp_min_8: "i64[2][1]cuda:0" = torch.ops.aten.clamp_min.default(sub_17, 0);  sub_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_1: "i64[][]cuda:0" = torch.ops.aten.max.default(clamp_min_8)
        _local_scalar_dense: "Sym(u0)" = torch.ops.aten._local_scalar_dense.default(max_1);  max_1 = None
        ge: "Sym(u0 >= 1)" = _local_scalar_dense >= 1
        _assert_scalar = torch.ops.aten._assert_scalar.default(ge, "Runtime assertion failed for expression u0 >= 1 on node 'ge_8'");  ge = _assert_scalar = None
        le: "Sym(u0 <= 999999999)" = _local_scalar_dense <= 999999999
        _assert_scalar_1 = torch.ops.aten._assert_scalar.default(le, "Runtime assertion failed for expression u0 <= 999999999 on node 'le'");  le = _assert_scalar_1 = None
        
        # No stacktrace found for following nodes
        lt: "Sym(u0 < 1000000000)" = _local_scalar_dense < 1000000000
        _assert_scalar_2 = torch.ops.aten._assert_scalar.default(lt, "Runtime assertion failed for expression u0 < 1000000000 on node 'lt_8'");  lt = _assert_scalar_2 = None
        gt: "Sym(u0 > 0)" = _local_scalar_dense > 0;  _local_scalar_dense = None
        _assert_scalar_3 = torch.ops.aten._assert_scalar.default(gt, "Runtime assertion failed for expression 0 < u0 on node 'gt_8'");  gt = _assert_scalar_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:41 in _truncate_last_embeddings, code: minus_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(left_lengths)
        asynchronous_complete_cumsum: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(clamp_min_8)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:42 in _truncate_last_embeddings, code: last_k_lengths = seq_lengths - left_lengths
        sub_18: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_16, clamp_min_8);  sub_16 = clamp_min_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:43 in _truncate_last_embeddings, code: last_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(last_k_lengths)
        asynchronous_complete_cumsum_1: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(sub_18)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_2: "i64[][]cuda:0" = torch.ops.aten.max.default(sub_18);  sub_18 = None
        _local_scalar_dense_1: "Sym(u1)" = torch.ops.aten._local_scalar_dense.default(max_2);  max_2 = None
        ge_2: "Sym(u1 >= 1)" = _local_scalar_dense_1 >= 1
        _assert_scalar_4 = torch.ops.aten._assert_scalar.default(ge_2, "Runtime assertion failed for expression u1 >= 1 on node 'ge_9'");  ge_2 = _assert_scalar_4 = None
        le_1: "Sym(u1 <= 999999999)" = _local_scalar_dense_1 <= 999999999
        _assert_scalar_5 = torch.ops.aten._assert_scalar.default(le_1, "Runtime assertion failed for expression u1 <= 999999999 on node 'le_1'");  le_1 = _assert_scalar_5 = None
        
        # No stacktrace found for following nodes
        lt_1: "Sym(u1 < 1000000000)" = _local_scalar_dense_1 < 1000000000
        _assert_scalar_6 = torch.ops.aten._assert_scalar.default(lt_1, "Runtime assertion failed for expression u1 < 1000000000 on node 'lt_9'");  lt_1 = _assert_scalar_6 = None
        gt_2: "Sym(u1 > 0)" = _local_scalar_dense_1 > 0
        _assert_scalar_7 = torch.ops.aten._assert_scalar.default(gt_2, "Runtime assertion failed for expression 0 < u1 on node 'gt_9'");  gt_2 = _assert_scalar_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1446 in forward, code: offsets_a_last_idx = torch.tensor(offsets_a.size(0) - 1).to(
        _tensor_constant8: "i64[][]cpu" = self._tensor_constant8
        lift_fresh_copy_8: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant8);  _tensor_constant8 = None
        device_put: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_8, device(type='cuda', index=0), True);  lift_fresh_copy_8 = None
        convert_element_type_594: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put, torch.int64);  device_put = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1449 in forward, code: offsets_b_last_idx = torch.tensor(offsets_b.size(0) - 1).to(
        _tensor_constant9: "i64[][]cpu" = self._tensor_constant9
        lift_fresh_copy_9: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant9);  _tensor_constant9 = None
        device_put_1: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_9, device(type='cuda', index=0), True);  lift_fresh_copy_9 = None
        convert_element_type_595: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_1, torch.int64);  device_put_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1453 in forward, code: seq_len_a = offsets_a.index_select(dim=0, index=offsets_a_last_idx)
        unsqueeze: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_594, 0);  convert_element_type_594 = None
        index_4: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum, [unsqueeze]);  unsqueeze = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1455 in forward, code: seq_len_b = offsets_b.index_select(dim=0, index=offsets_b_last_idx)
        unsqueeze_1: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_595, 0);  convert_element_type_595 = None
        index_5: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_1, [unsqueeze_1]);  unsqueeze_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1464 in forward, code: values_a = torch.empty((seq_len_a, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_2: "Sym(u6)" = torch.ops.aten._local_scalar_dense.default(index_4)
        empty_84: "bf16[u6, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_2, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_4: "Sym(u6 >= 0)" = _local_scalar_dense_2 >= 0
        _assert_scalar_8 = torch.ops.aten._assert_scalar.default(ge_4, "Runtime assertion failed for expression u2 >= 0 on node 'ge'");  ge_4 = _assert_scalar_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1466 in forward, code: values_b = torch.empty((seq_len_b, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_3: "Sym(u7)" = torch.ops.aten._local_scalar_dense.default(index_5)
        empty_85: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_5: "Sym(u7 >= 0)" = _local_scalar_dense_3 >= 0
        _assert_scalar_9 = torch.ops.aten._assert_scalar.default(ge_5, "Runtime assertion failed for expression u3 >= 0 on node 'ge_1'");  ge_5 = _assert_scalar_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1246 in _triton_split_2D_jagged_internal, code: split_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_40 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 3, constant_args_idx = 112, grid = [(257, 2, 1), (257, 2, 1), (257, 2, 1), (129, 2, 1), (129, 2, 1), (129, 2, 1), (65, 2, 1), (65, 2, 1), (65, 2, 1), (33, 2, 1), (33, 2, 1), (33, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'JaggedIn': addmm_7, 'OffsetsA': asynchronous_complete_cumsum, 'OffsetsB': asynchronous_complete_cumsum_1, 'OutA': empty_84, 'OutB': empty_85}, tensors_to_clone = ['OutA', 'OutB']);  addmm_7 = empty_84 = empty_85 = None
        getitem_144: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_40['OutB'];  triton_kernel_wrapper_functional_proxy_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        ge_6: "Sym(u6 >= 0)" = _local_scalar_dense_2 >= 0
        _assert_scalar_10 = torch.ops.aten._assert_scalar.default(ge_6, "Runtime assertion failed for expression u6 >= 0 on node 'ge_10'");  ge_6 = _assert_scalar_10 = None
        ge_7: "Sym(u7 >= 1)" = _local_scalar_dense_3 >= 1
        _assert_scalar_11 = torch.ops.aten._assert_scalar.default(ge_7, "Runtime assertion failed for expression u7 >= 1 on node 'ge_11'");  ge_7 = _assert_scalar_11 = None
        le_2: "Sym(u7 <= 999999999)" = _local_scalar_dense_3 <= 999999999
        _assert_scalar_12 = torch.ops.aten._assert_scalar.default(le_2, "Runtime assertion failed for expression u7 <= 999999999 on node 'le_2'");  le_2 = _assert_scalar_12 = None
        
        # No stacktrace found for following nodes
        gt_4: "Sym(u7 > 0)" = _local_scalar_dense_3 > 0
        _assert_scalar_13 = torch.ops.aten._assert_scalar.default(gt_4, "Runtime assertion failed for expression 0 < u7 on node 'gt_10'");  gt_4 = _assert_scalar_13 = None
        lt_2: "Sym(u7 < 1000000000)" = _local_scalar_dense_3 < 1000000000
        _assert_scalar_14 = torch.ops.aten._assert_scalar.default(lt_2, "Runtime assertion failed for expression u7 < 1000000000 on node 'lt_10'");  lt_2 = _assert_scalar_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:605 in forward, code: seq_lengths=offsets[1:] - offsets[:-1],
        slice_19: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_129, 0, 1, 9223372036854775807)
        slice_20: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_129, 0, 0, -1)
        sub_21: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_19, slice_20);  slice_19 = slice_20 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:125 in _truncate_last_k_embeddings, code: minus_k_lengths = torch.clamp(seq_lengths - K, min=0)
        sub_22: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_21, 256)
        clamp_min_9: "i64[2][1]cuda:0" = torch.ops.aten.clamp_min.default(sub_22, 0);  sub_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_3: "i64[][]cuda:0" = torch.ops.aten.max.default(clamp_min_9)
        _local_scalar_dense_4: "Sym(u8)" = torch.ops.aten._local_scalar_dense.default(max_3);  max_3 = None
        ge_8: "Sym(u8 >= 1)" = _local_scalar_dense_4 >= 1
        _assert_scalar_15 = torch.ops.aten._assert_scalar.default(ge_8, "Runtime assertion failed for expression u8 >= 1 on node 'ge_12'");  ge_8 = _assert_scalar_15 = None
        le_3: "Sym(u8 <= 999999999)" = _local_scalar_dense_4 <= 999999999
        _assert_scalar_16 = torch.ops.aten._assert_scalar.default(le_3, "Runtime assertion failed for expression u8 <= 999999999 on node 'le_3'");  le_3 = _assert_scalar_16 = None
        
        # No stacktrace found for following nodes
        lt_3: "Sym(u8 < 1000000000)" = _local_scalar_dense_4 < 1000000000
        _assert_scalar_17 = torch.ops.aten._assert_scalar.default(lt_3, "Runtime assertion failed for expression u8 < 1000000000 on node 'lt_11'");  lt_3 = _assert_scalar_17 = None
        gt_5: "Sym(u8 > 0)" = _local_scalar_dense_4 > 0;  _local_scalar_dense_4 = None
        _assert_scalar_18 = torch.ops.aten._assert_scalar.default(gt_5, "Runtime assertion failed for expression 0 < u8 on node 'gt_11'");  gt_5 = _assert_scalar_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:41 in _truncate_last_embeddings, code: minus_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(left_lengths)
        asynchronous_complete_cumsum_2: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(clamp_min_9)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:42 in _truncate_last_embeddings, code: last_k_lengths = seq_lengths - left_lengths
        sub_23: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_21, clamp_min_9);  sub_21 = clamp_min_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:43 in _truncate_last_embeddings, code: last_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(last_k_lengths)
        asynchronous_complete_cumsum_3: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(sub_23)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_4: "i64[][]cuda:0" = torch.ops.aten.max.default(sub_23);  sub_23 = None
        _local_scalar_dense_5: "Sym(u9)" = torch.ops.aten._local_scalar_dense.default(max_4);  max_4 = None
        ge_10: "Sym(u9 >= 1)" = _local_scalar_dense_5 >= 1
        _assert_scalar_19 = torch.ops.aten._assert_scalar.default(ge_10, "Runtime assertion failed for expression u9 >= 1 on node 'ge_13'");  ge_10 = _assert_scalar_19 = None
        le_4: "Sym(u9 <= 999999999)" = _local_scalar_dense_5 <= 999999999
        _assert_scalar_20 = torch.ops.aten._assert_scalar.default(le_4, "Runtime assertion failed for expression u9 <= 999999999 on node 'le_4'");  le_4 = _assert_scalar_20 = None
        
        # No stacktrace found for following nodes
        lt_4: "Sym(u9 < 1000000000)" = _local_scalar_dense_5 < 1000000000
        _assert_scalar_21 = torch.ops.aten._assert_scalar.default(lt_4, "Runtime assertion failed for expression u9 < 1000000000 on node 'lt_12'");  lt_4 = _assert_scalar_21 = None
        gt_7: "Sym(u9 > 0)" = _local_scalar_dense_5 > 0
        _assert_scalar_22 = torch.ops.aten._assert_scalar.default(gt_7, "Runtime assertion failed for expression 0 < u9 on node 'gt_12'");  gt_7 = _assert_scalar_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1446 in forward, code: offsets_a_last_idx = torch.tensor(offsets_a.size(0) - 1).to(
        _tensor_constant10: "i64[][]cpu" = self._tensor_constant10
        lift_fresh_copy_10: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant10);  _tensor_constant10 = None
        device_put_2: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_10, device(type='cuda', index=0), True);  lift_fresh_copy_10 = None
        convert_element_type_596: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_2, torch.int64);  device_put_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1449 in forward, code: offsets_b_last_idx = torch.tensor(offsets_b.size(0) - 1).to(
        _tensor_constant11: "i64[][]cpu" = self._tensor_constant11
        lift_fresh_copy_11: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant11);  _tensor_constant11 = None
        device_put_3: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_11, device(type='cuda', index=0), True);  lift_fresh_copy_11 = None
        convert_element_type_597: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_3, torch.int64);  device_put_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1453 in forward, code: seq_len_a = offsets_a.index_select(dim=0, index=offsets_a_last_idx)
        unsqueeze_2: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_596, 0);  convert_element_type_596 = None
        index_6: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_2, [unsqueeze_2]);  unsqueeze_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1455 in forward, code: seq_len_b = offsets_b.index_select(dim=0, index=offsets_b_last_idx)
        unsqueeze_3: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_597, 0);  convert_element_type_597 = None
        index_7: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_3, [unsqueeze_3]);  unsqueeze_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1464 in forward, code: values_a = torch.empty((seq_len_a, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_6: "Sym(u14)" = torch.ops.aten._local_scalar_dense.default(index_6)
        empty_86: "bf16[u14, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_6, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_12: "Sym(u14 >= 0)" = _local_scalar_dense_6 >= 0
        _assert_scalar_23 = torch.ops.aten._assert_scalar.default(ge_12, "Runtime assertion failed for expression u10 >= 0 on node 'ge'");  ge_12 = _assert_scalar_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1466 in forward, code: values_b = torch.empty((seq_len_b, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_7: "Sym(u15)" = torch.ops.aten._local_scalar_dense.default(index_7)
        empty_87: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_13: "Sym(u15 >= 0)" = _local_scalar_dense_7 >= 0
        _assert_scalar_24 = torch.ops.aten._assert_scalar.default(ge_13, "Runtime assertion failed for expression u11 >= 0 on node 'ge_1'");  ge_13 = _assert_scalar_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1246 in _triton_split_2D_jagged_internal, code: split_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_41 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 3, constant_args_idx = 114, grid = [(257, 2, 1), (257, 2, 1), (257, 2, 1), (129, 2, 1), (129, 2, 1), (129, 2, 1), (65, 2, 1), (65, 2, 1), (65, 2, 1), (33, 2, 1), (33, 2, 1), (33, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'JaggedIn': addmm_9, 'OffsetsA': asynchronous_complete_cumsum_2, 'OffsetsB': asynchronous_complete_cumsum_3, 'OutA': empty_86, 'OutB': empty_87}, tensors_to_clone = ['OutA', 'OutB']);  addmm_9 = empty_86 = empty_87 = None
        getitem_146: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_41['OutB'];  triton_kernel_wrapper_functional_proxy_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        ge_14: "Sym(u14 >= 0)" = _local_scalar_dense_6 >= 0
        _assert_scalar_25 = torch.ops.aten._assert_scalar.default(ge_14, "Runtime assertion failed for expression u14 >= 0 on node 'ge_14'");  ge_14 = _assert_scalar_25 = None
        ge_15: "Sym(u15 >= 1)" = _local_scalar_dense_7 >= 1
        _assert_scalar_26 = torch.ops.aten._assert_scalar.default(ge_15, "Runtime assertion failed for expression u15 >= 1 on node 'ge_15'");  ge_15 = _assert_scalar_26 = None
        le_5: "Sym(u15 <= 999999999)" = _local_scalar_dense_7 <= 999999999
        _assert_scalar_27 = torch.ops.aten._assert_scalar.default(le_5, "Runtime assertion failed for expression u15 <= 999999999 on node 'le_5'");  le_5 = _assert_scalar_27 = None
        
        # No stacktrace found for following nodes
        gt_9: "Sym(u15 > 0)" = _local_scalar_dense_7 > 0
        _assert_scalar_28 = torch.ops.aten._assert_scalar.default(gt_9, "Runtime assertion failed for expression 0 < u15 on node 'gt_13'");  gt_9 = _assert_scalar_28 = None
        lt_5: "Sym(u15 < 1000000000)" = _local_scalar_dense_7 < 1000000000
        _assert_scalar_29 = torch.ops.aten._assert_scalar.default(lt_5, "Runtime assertion failed for expression u15 < 1000000000 on node 'lt_13'");  lt_5 = _assert_scalar_29 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_1, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_4: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor, device(type='cuda', index=0));  scalar_tensor = None
        convert_element_type_598: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_4, torch.bfloat16);  device_put_4 = None
        convert_element_type_599: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_598, torch.float32);  convert_element_type_598 = None
        clamp_min_10: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_599, 1);  convert_element_type_599 = None
        convert_element_type_600: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_10, torch.bfloat16);  clamp_min_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_601: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_600, torch.float32);  convert_element_type_600 = None
        reciprocal_8: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_601);  convert_element_type_601 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_602: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_216, torch.bfloat16);  primals_216 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_603: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_217, torch.bfloat16);  primals_217 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_604: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_218, torch.bfloat16);  primals_218 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_605: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_219, torch.bfloat16);  primals_219 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_88: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_89: "f32[u7][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_90: "f32[u7][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_154: "Sym(u7 + 4)" = _local_scalar_dense_3 + 4
        sub_29: "Sym(u7 + 3)" = add_154 - 1;  add_154 = None
        floordiv: "Sym(((u7 + 3)//4))" = sub_29 // 4;  sub_29 = None
        add_155: "Sym(u7 + 8)" = _local_scalar_dense_3 + 8
        sub_30: "Sym(u7 + 7)" = add_155 - 1;  add_155 = None
        floordiv_1: "Sym(((u7 + 7)//8))" = sub_30 // 8;  sub_30 = None
        add_156: "Sym(u7 + 16)" = _local_scalar_dense_3 + 16
        sub_31: "Sym(u7 + 15)" = add_156 - 1;  add_156 = None
        floordiv_2: "Sym(((u7 + 15)//16))" = sub_31 // 16;  sub_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_42 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 116, grid = [(floordiv, 1, 1), (floordiv, 1, 1), (floordiv, 1, 1), (floordiv_1, 1, 1), (floordiv_1, 1, 1), (floordiv_1, 1, 1), (floordiv_2, 1, 1), (floordiv_2, 1, 1), (floordiv_2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_144, 'Y': empty_88, 'W': convert_element_type_602, 'B': convert_element_type_603, 'Mean': empty_89, 'Rstd': empty_90, 'N': _local_scalar_dense_3}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  floordiv = floordiv_1 = floordiv_2 = empty_88 = empty_89 = empty_90 = None
        getitem_147: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_42['Y']
        getitem_148: "f32[u7][1]cuda:0" = triton_kernel_wrapper_functional_proxy_42['Mean']
        getitem_149: "f32[u7][1]cuda:0" = triton_kernel_wrapper_functional_proxy_42['Rstd'];  triton_kernel_wrapper_functional_proxy_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_10: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_605, getitem_147, convert_element_type_604);  getitem_147 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_10 = torch.ops.aten.split_with_sizes.default(addmm_10, [256, 256, 256, 256], 1);  addmm_10 = None
        getitem_150: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_10[0]
        getitem_151: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_10[1]
        getitem_152: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_10[2]
        getitem_153: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_10[3];  split_with_sizes_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_356: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_152, [-1, 2, 128]);  getitem_152 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_357: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_153, [-1, 2, 128]);  getitem_153 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_358: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_151, [-1, 2, 128]);  getitem_151 = None
        sym_size_int_2: "Sym(u7)" = torch.ops.aten.sym_size.int(view_358, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_609: "f32[u7, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_150, torch.float32);  getitem_150 = None
        neg_48: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_609)
        exp_48: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_48);  neg_48 = None
        add_184: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_48, 1);  exp_48 = None
        div_50: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_609, add_184);  convert_element_type_609 = add_184 = None
        convert_element_type_610: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_50, torch.bfloat16);  div_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_21: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_1, 0, 1, 9223372036854775807)
        slice_22: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_1, 0, 0, -1)
        sub_41: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_21, slice_22);  slice_21 = slice_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_4 = torch.ops.aten.sort.stable(sub_41, stable = False, descending = True);  sub_41 = None
        getitem_155: "i64[2][1]cuda:0" = sort_4[1];  sort_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_91: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_2, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_359: "bf16[256*u7][1]cuda:0" = torch.ops.aten.view.default(empty_91, [-1]);  empty_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_194: "Sym(u1 + 16)" = _local_scalar_dense_1 + 16
        sub_44: "Sym(u1 + 15)" = add_194 - 1;  add_194 = None
        floordiv_3: "Sym(((u1 + 15)//16))" = sub_44 // 16;  sub_44 = None
        add_195: "Sym(u1 + 32)" = _local_scalar_dense_1 + 32
        sub_45: "Sym(u1 + 31)" = add_195 - 1;  add_195 = None
        floordiv_4: "Sym(((u1 + 31)//32))" = sub_45 // 32;  sub_45 = None
        add_196: "Sym(u1 + 64)" = _local_scalar_dense_1 + 64
        sub_46: "Sym(u1 + 63)" = add_196 - 1;  add_196 = None
        floordiv_5: "Sym(((u1 + 63)//64))" = sub_46 // 64;  sub_46 = None
        add_197: "Sym(u1 + 128)" = _local_scalar_dense_1 + 128
        sub_47: "Sym(u1 + 127)" = add_197 - 1;  add_197 = None
        floordiv_6: "Sym(((u1 + 127)//128))" = sub_47 // 128;  sub_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_43 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 117, grid = [(floordiv_3, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_4, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_5, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1), (floordiv_6, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_356, 'K': view_357, 'V': view_358, 'seq_offsets': asynchronous_complete_cumsum_1, 'attn_scale': reciprocal_8, 'Out': view_359, 'MAX_SEQ_LEN': _local_scalar_dense_1, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_1}, tensors_to_clone = ['Out']);  floordiv_3 = floordiv_4 = floordiv_5 = floordiv_6 = view_356 = view_357 = view_358 = view_359 = None
        getitem_156: "bf16[256*u7][1]cuda:0" = triton_kernel_wrapper_functional_proxy_43['Out'];  triton_kernel_wrapper_functional_proxy_43 = None
        view_360: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_156, [sym_size_int_2, 2, 128]);  getitem_156 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_611: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_220, torch.bfloat16);  primals_220 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_612: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_221, torch.bfloat16);  primals_221 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_613: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_222, torch.bfloat16);  primals_222 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_93: "bf16[u7, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_94: "f32[u7][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_95: "f32[u7][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_363: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_360, [-1, 256]);  view_360 = None
        triton_kernel_wrapper_functional_proxy_44 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 24, constant_args_idx = 122, grid = [(_local_scalar_dense_3, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_363, 'U': convert_element_type_610, 'Y': empty_93, 'W': convert_element_type_611, 'B': convert_element_type_612, 'Mean': empty_94, 'Rstd': empty_95, 'seed': primals_223}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_93 = empty_94 = empty_95 = None
        getitem_157: "bf16[u7, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_44['Y']
        getitem_158: "f32[u7][1]cuda:0" = triton_kernel_wrapper_functional_proxy_44['Mean']
        getitem_159: "f32[u7][1]cuda:0" = triton_kernel_wrapper_functional_proxy_44['Rstd'];  triton_kernel_wrapper_functional_proxy_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_11: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(getitem_144, getitem_157, convert_element_type_613);  getitem_157 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_1: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_5, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_5: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_1, device(type='cuda', index=0));  scalar_tensor_1 = None
        convert_element_type_617: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_5, torch.bfloat16);  device_put_5 = None
        convert_element_type_618: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_617, torch.float32);  convert_element_type_617 = None
        clamp_min_11: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_618, 1);  convert_element_type_618 = None
        convert_element_type_619: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_11, torch.bfloat16);  clamp_min_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_620: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_619, torch.float32);  convert_element_type_619 = None
        reciprocal_9: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_620);  convert_element_type_620 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_621: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_224, torch.bfloat16);  primals_224 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_622: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_225, torch.bfloat16);  primals_225 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_623: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_226, torch.bfloat16);  primals_226 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_624: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_227, torch.bfloat16);  primals_227 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_96: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_97: "f32[u15][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_98: "f32[u15][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_218: "Sym(u15 + 4)" = _local_scalar_dense_7 + 4
        sub_56: "Sym(u15 + 3)" = add_218 - 1;  add_218 = None
        floordiv_7: "Sym(((u15 + 3)//4))" = sub_56 // 4;  sub_56 = None
        add_219: "Sym(u15 + 8)" = _local_scalar_dense_7 + 8
        sub_57: "Sym(u15 + 7)" = add_219 - 1;  add_219 = None
        floordiv_8: "Sym(((u15 + 7)//8))" = sub_57 // 8;  sub_57 = None
        add_220: "Sym(u15 + 16)" = _local_scalar_dense_7 + 16
        sub_58: "Sym(u15 + 15)" = add_220 - 1;  add_220 = None
        floordiv_9: "Sym(((u15 + 15)//16))" = sub_58 // 16;  sub_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_45 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 125, grid = [(floordiv_7, 1, 1), (floordiv_7, 1, 1), (floordiv_7, 1, 1), (floordiv_8, 1, 1), (floordiv_8, 1, 1), (floordiv_8, 1, 1), (floordiv_9, 1, 1), (floordiv_9, 1, 1), (floordiv_9, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_146, 'Y': empty_96, 'W': convert_element_type_621, 'B': convert_element_type_622, 'Mean': empty_97, 'Rstd': empty_98, 'N': _local_scalar_dense_7}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  floordiv_7 = floordiv_8 = floordiv_9 = empty_96 = empty_97 = empty_98 = None
        getitem_160: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_45['Y']
        getitem_161: "f32[u15][1]cuda:0" = triton_kernel_wrapper_functional_proxy_45['Mean']
        getitem_162: "f32[u15][1]cuda:0" = triton_kernel_wrapper_functional_proxy_45['Rstd'];  triton_kernel_wrapper_functional_proxy_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_12: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_624, getitem_160, convert_element_type_623);  getitem_160 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_11 = torch.ops.aten.split_with_sizes.default(addmm_12, [256, 256, 256, 256], 1);  addmm_12 = None
        getitem_163: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_11[0]
        getitem_164: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_11[1]
        getitem_165: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_11[2]
        getitem_166: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_11[3];  split_with_sizes_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_364: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_165, [-1, 2, 128]);  getitem_165 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_365: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_166, [-1, 2, 128]);  getitem_166 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_366: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_164, [-1, 2, 128]);  getitem_164 = None
        sym_size_int_7: "Sym(u15)" = torch.ops.aten.sym_size.int(view_366, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_628: "f32[u15, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_163, torch.float32);  getitem_163 = None
        neg_49: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_628)
        exp_49: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_49);  neg_49 = None
        add_248: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_49, 1);  exp_49 = None
        div_51: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_628, add_248);  convert_element_type_628 = add_248 = None
        convert_element_type_629: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_51, torch.bfloat16);  div_51 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_23: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_3, 0, 1, 9223372036854775807)
        slice_24: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_3, 0, 0, -1)
        sub_68: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_23, slice_24);  slice_23 = slice_24 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_5 = torch.ops.aten.sort.stable(sub_68, stable = False, descending = True);  sub_68 = None
        getitem_168: "i64[2][1]cuda:0" = sort_5[1];  sort_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_99: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_7, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_367: "bf16[256*u15][1]cuda:0" = torch.ops.aten.view.default(empty_99, [-1]);  empty_99 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_258: "Sym(u9 + 16)" = _local_scalar_dense_5 + 16
        sub_71: "Sym(u9 + 15)" = add_258 - 1;  add_258 = None
        floordiv_10: "Sym(((u9 + 15)//16))" = sub_71 // 16;  sub_71 = None
        add_259: "Sym(u9 + 32)" = _local_scalar_dense_5 + 32
        sub_72: "Sym(u9 + 31)" = add_259 - 1;  add_259 = None
        floordiv_11: "Sym(((u9 + 31)//32))" = sub_72 // 32;  sub_72 = None
        add_260: "Sym(u9 + 64)" = _local_scalar_dense_5 + 64
        sub_73: "Sym(u9 + 63)" = add_260 - 1;  add_260 = None
        floordiv_12: "Sym(((u9 + 63)//64))" = sub_73 // 64;  sub_73 = None
        add_261: "Sym(u9 + 128)" = _local_scalar_dense_5 + 128
        sub_74: "Sym(u9 + 127)" = add_261 - 1;  add_261 = None
        floordiv_13: "Sym(((u9 + 127)//128))" = sub_74 // 128;  sub_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_46 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 126, grid = [(floordiv_10, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_11, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_12, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1), (floordiv_13, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_364, 'K': view_365, 'V': view_366, 'seq_offsets': asynchronous_complete_cumsum_3, 'attn_scale': reciprocal_9, 'Out': view_367, 'MAX_SEQ_LEN': _local_scalar_dense_5, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_5}, tensors_to_clone = ['Out']);  floordiv_10 = floordiv_11 = floordiv_12 = floordiv_13 = view_364 = view_365 = view_366 = view_367 = None
        getitem_169: "bf16[256*u15][1]cuda:0" = triton_kernel_wrapper_functional_proxy_46['Out'];  triton_kernel_wrapper_functional_proxy_46 = None
        view_368: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_169, [sym_size_int_7, 2, 128]);  getitem_169 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_630: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_228, torch.bfloat16);  primals_228 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_631: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_229, torch.bfloat16);  primals_229 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_632: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_230, torch.bfloat16);  primals_230 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_101: "bf16[u15, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_102: "f32[u15][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_103: "f32[u15][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_371: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_368, [-1, 256]);  view_368 = None
        triton_kernel_wrapper_functional_proxy_47 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 26, constant_args_idx = 131, grid = [(_local_scalar_dense_7, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_371, 'U': convert_element_type_629, 'Y': empty_101, 'W': convert_element_type_630, 'B': convert_element_type_631, 'Mean': empty_102, 'Rstd': empty_103, 'seed': primals_231}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_101 = empty_102 = empty_103 = None
        getitem_170: "bf16[u15, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_47['Y']
        getitem_171: "f32[u15][1]cuda:0" = triton_kernel_wrapper_functional_proxy_47['Mean']
        getitem_172: "f32[u15][1]cuda:0" = triton_kernel_wrapper_functional_proxy_47['Rstd'];  triton_kernel_wrapper_functional_proxy_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_13: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(getitem_146, getitem_170, convert_element_type_632);  getitem_170 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_372: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_141, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_636: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_232, torch.bfloat16);  primals_232 = None
        permute_138: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_636, [1, 0]);  convert_element_type_636 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_373: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_141, [10, 64, 256])
        permute_139: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_373, [0, 2, 1]);  view_373 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_30: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_139, memory_format = torch.contiguous_format);  permute_139 = None
        view_374: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_30, [2560, 64]);  clone_30 = None
        mm_87: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_374, permute_138)
        view_375: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_87, [10, 256, 16]);  mm_87 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_140: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_375, [0, 2, 1]);  view_375 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_203: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_31: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_140, memory_format = torch.contiguous_format);  permute_140 = None
        view_376: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_31, [160, 256]);  clone_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_377: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_376, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_104: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_105: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_48 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 134, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_377, 'Y': empty_104, 'W': primals_233, 'Rstd': empty_105}, tensors_to_clone = ['Y', 'Rstd']);  empty_104 = empty_105 = None
        getitem_173: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_48['Y']
        getitem_174: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_48['Rstd'];  triton_kernel_wrapper_functional_proxy_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_639: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_234, torch.bfloat16);  primals_234 = None
        permute_141: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_639, [1, 0]);  convert_element_type_639 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_379: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_173, [160, 256]);  getitem_173 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_88: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_379, permute_141)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_2: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_1, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_6: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_2, device(type='cuda', index=0));  scalar_tensor_2 = None
        convert_element_type_642: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_6, torch.bfloat16);  device_put_6 = None
        convert_element_type_643: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_642, torch.float32);  convert_element_type_642 = None
        clamp_min_12: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_643, 1);  convert_element_type_643 = None
        convert_element_type_644: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_12, torch.bfloat16);  clamp_min_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_645: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_644, torch.float32);  convert_element_type_644 = None
        reciprocal_10: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_645);  convert_element_type_645 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_380: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_88, [-1, 2, 128]);  mm_88 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_381: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_11, [-1, 2, 128])
        sym_size_int_10: "Sym(u7)" = torch.ops.aten.sym_size.int(view_381, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_4: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_106: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_49 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 136, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_106, 'Q': view_380, 'K': view_381, 'V': view_381, 'total_seq_len_kv': _local_scalar_dense_3, 'Out': full_4, 'seq_offsets_q': mul_203, 'seq_offsets': asynchronous_complete_cumsum_1, 'max_seq_len': _local_scalar_dense_1, 'attn_scale': reciprocal_10}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_106 = full_4 = mul_203 = asynchronous_complete_cumsum_1 = None
        getitem_175: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_49['Out']
        getitem_176: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_49['seq_offsets_q']
        getitem_177: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_49['seq_offsets'];  triton_kernel_wrapper_functional_proxy_49 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_383: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_175, [-1, 256]);  getitem_175 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_279: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_383, view_376);  view_383 = view_376 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_646: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_279, torch.float32)
        var_mean_4 = torch.ops.aten.var_mean.correction(convert_element_type_646, [1], correction = 0, keepdim = True)
        getitem_178: "f32[160, 1][1, 1]cuda:0" = var_mean_4[0]
        getitem_179: "f32[160, 1][1, 1]cuda:0" = var_mean_4[1];  var_mean_4 = None
        add_280: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_178, 1e-05);  getitem_178 = None
        rsqrt_50: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_280);  add_280 = None
        sub_81: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_646, getitem_179)
        mul_206: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_81, rsqrt_50);  sub_81 = None
        mul_207: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_206, primals_235);  mul_206 = None
        add_281: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_207, primals_236);  mul_207 = primals_236 = None
        view_384: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_281, [10, 4096]);  add_281 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_647: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_384, torch.bfloat16)
        convert_element_type_648: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_237, torch.bfloat16);  primals_237 = None
        permute_142: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_648, [1, 0]);  convert_element_type_648 = None
        mm_89: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_647, permute_142)
        convert_element_type_651: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_89, torch.float32)
        neg_50: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_651)
        exp_50: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_50);  neg_50 = None
        add_282: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_50, 1);  exp_50 = None
        div_52: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_651, add_282);  convert_element_type_651 = add_282 = None
        convert_element_type_652: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_52, torch.bfloat16);  div_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_653: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_384, torch.bfloat16);  view_384 = None
        convert_element_type_654: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_238, torch.bfloat16);  primals_238 = None
        permute_143: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_654, [1, 0]);  convert_element_type_654 = None
        mm_90: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_653, permute_143)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_208: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_652, mm_90)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_657: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_239, torch.bfloat16);  primals_239 = None
        permute_144: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_657, [1, 0]);  convert_element_type_657 = None
        mm_91: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_208, permute_144)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_385: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_91, [160, 256]);  mm_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_283: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_279, view_385);  add_279 = view_385 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_386: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_283, [10, 16, 256]);  add_283 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_387: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_386, [-1, 256]);  view_386 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_107: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_108: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_50 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 138, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_387, 'Y': empty_107, 'W': primals_240, 'Rstd': empty_108}, tensors_to_clone = ['Y', 'Rstd']);  empty_107 = empty_108 = None
        getitem_180: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_50['Y']
        getitem_181: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_50['Rstd'];  triton_kernel_wrapper_functional_proxy_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_660: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_241, torch.bfloat16);  primals_241 = None
        permute_146: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_660, [1, 0]);  convert_element_type_660 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_389: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_141, [10, 64, 256]);  getitem_141 = None
        permute_147: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_389, [0, 2, 1]);  view_389 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_32: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_147, memory_format = torch.contiguous_format);  permute_147 = None
        view_390: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_32, [2560, 64]);  clone_32 = None
        mm_92: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_390, permute_146)
        view_391: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_92, [10, 256, 16]);  mm_92 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_148: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_391, [0, 2, 1]);  view_391 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_209: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_33: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_148, memory_format = torch.contiguous_format);  permute_148 = None
        view_392: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_33, [160, 256]);  clone_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_393: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_392, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_109: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_110: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_51 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 140, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_393, 'Y': empty_109, 'W': primals_242, 'Rstd': empty_110}, tensors_to_clone = ['Y', 'Rstd']);  empty_109 = empty_110 = None
        getitem_182: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_51['Y']
        getitem_183: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_51['Rstd'];  triton_kernel_wrapper_functional_proxy_51 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_663: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_243, torch.bfloat16);  primals_243 = None
        permute_149: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_663, [1, 0]);  convert_element_type_663 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_395: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_182, [160, 256]);  getitem_182 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_93: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_395, permute_149)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_3: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_5, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_7: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_3, device(type='cuda', index=0));  scalar_tensor_3 = None
        convert_element_type_666: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_7, torch.bfloat16);  device_put_7 = None
        convert_element_type_667: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_666, torch.float32);  convert_element_type_666 = None
        clamp_min_13: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_667, 1);  convert_element_type_667 = None
        convert_element_type_668: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_13, torch.bfloat16);  clamp_min_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_669: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_668, torch.float32);  convert_element_type_668 = None
        reciprocal_11: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_669);  convert_element_type_669 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_396: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_93, [-1, 2, 128]);  mm_93 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_397: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_13, [-1, 2, 128])
        sym_size_int_11: "Sym(u15)" = torch.ops.aten.sym_size.int(view_397, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_5: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_111: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_52 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 142, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_111, 'Q': view_396, 'K': view_397, 'V': view_397, 'total_seq_len_kv': _local_scalar_dense_7, 'Out': full_5, 'seq_offsets_q': mul_209, 'seq_offsets': asynchronous_complete_cumsum_3, 'max_seq_len': _local_scalar_dense_5, 'attn_scale': reciprocal_11}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_111 = full_5 = mul_209 = asynchronous_complete_cumsum_3 = None
        getitem_184: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_52['Out']
        getitem_185: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_52['seq_offsets_q']
        getitem_186: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_52['seq_offsets'];  triton_kernel_wrapper_functional_proxy_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_399: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_184, [-1, 256]);  getitem_184 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_288: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_399, view_392);  view_399 = view_392 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_670: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_288, torch.float32)
        var_mean_5 = torch.ops.aten.var_mean.correction(convert_element_type_670, [1], correction = 0, keepdim = True)
        getitem_187: "f32[160, 1][1, 1]cuda:0" = var_mean_5[0]
        getitem_188: "f32[160, 1][1, 1]cuda:0" = var_mean_5[1];  var_mean_5 = None
        add_289: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_187, 1e-05);  getitem_187 = None
        rsqrt_51: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_289);  add_289 = None
        sub_83: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_670, getitem_188)
        mul_212: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_83, rsqrt_51);  sub_83 = None
        mul_213: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_212, primals_244);  mul_212 = None
        add_290: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_213, primals_245);  mul_213 = primals_245 = None
        view_400: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_290, [10, 4096]);  add_290 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_671: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_400, torch.bfloat16)
        convert_element_type_672: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_246, torch.bfloat16);  primals_246 = None
        permute_150: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_672, [1, 0]);  convert_element_type_672 = None
        mm_94: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_671, permute_150)
        convert_element_type_675: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_94, torch.float32)
        neg_51: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_675)
        exp_51: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_51);  neg_51 = None
        add_291: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_51, 1);  exp_51 = None
        div_53: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_675, add_291);  convert_element_type_675 = add_291 = None
        convert_element_type_676: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_53, torch.bfloat16);  div_53 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_677: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_400, torch.bfloat16);  view_400 = None
        convert_element_type_678: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_247, torch.bfloat16);  primals_247 = None
        permute_151: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_678, [1, 0]);  convert_element_type_678 = None
        mm_95: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_677, permute_151)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_214: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_676, mm_95)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_681: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_248, torch.bfloat16);  primals_248 = None
        permute_152: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_681, [1, 0]);  convert_element_type_681 = None
        mm_96: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_214, permute_152)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_401: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_96, [160, 256]);  mm_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_292: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_288, view_401);  add_288 = view_401 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_402: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_292, [10, 16, 256]);  add_292 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_403: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_402, [-1, 256]);  view_402 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_112: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_113: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_53 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 144, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_403, 'Y': empty_112, 'W': primals_249, 'Rstd': empty_113}, tensors_to_clone = ['Y', 'Rstd']);  empty_112 = empty_113 = None
        getitem_189: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_53['Y']
        getitem_190: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_53['Rstd'];  triton_kernel_wrapper_functional_proxy_53 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_405: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_180, [10, 16, 256]);  getitem_180 = None
        view_406: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_189, [10, 16, 256]);  getitem_189 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        cat_29: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_405, view_406], 1)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        cat_30: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_405, view_406], 1);  view_405 = view_406 = None
        view_407: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.view.default(cat_30, [10, 2, 16, 256]);  cat_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:293 in forward, code: triu_indices = torch.triu_indices(
        iota_5: "i64[1][1]cuda:0" = torch.ops.prims.iota.default(1, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
        mul_215: "i64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(iota_5, 1);  iota_5 = None
        add_293: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(mul_215, 0);  mul_215 = None
        convert_element_type_684: "f64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(add_293, torch.float64);  add_293 = None
        mul_216: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_684, 2)
        sub_84: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(2.25, mul_216);  mul_216 = None
        sqrt_2: "f64[1][1]cuda:0" = torch.ops.aten.sqrt.default(sub_84);  sub_84 = None
        sub_85: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1.5, sqrt_2);  sqrt_2 = None
        floor_4: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_85);  sub_85 = None
        sub_86: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1, floor_4)
        mul_217: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(sub_86, floor_4);  sub_86 = None
        mul_218: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(mul_217, 0.5);  mul_217 = None
        sub_87: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_684, mul_218);  convert_element_type_684 = mul_218 = None
        floor_5: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_87);  sub_87 = None
        convert_element_type_685: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_4, torch.int64);  floor_4 = None
        convert_element_type_686: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_5, torch.int64);  floor_5 = None
        add_294: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_685, 0);  convert_element_type_685 = None
        add_295: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_686, 1);  convert_element_type_686 = None
        clone_34: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_294);  add_294 = None
        clone_35: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_295);  add_295 = None
        cat_31: "i64[2][1]cuda:0" = torch.ops.aten.cat.default([clone_34, clone_35]);  clone_34 = clone_35 = None
        view_408: "i64[2, 1][1, 1]cuda:0" = torch.ops.aten.view.default(cat_31, [2, 1]);  cat_31 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:299 in forward, code: i_indices, j_indices = triu_indices.unbind()
        unbind_2 = torch.ops.aten.unbind.int(view_408);  view_408 = None
        getitem_191: "i64[1][1]cuda:0" = unbind_2[0]
        getitem_192: "i64[1][1]cuda:0" = unbind_2[1];  unbind_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        index_8: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_407, [None, getitem_191])
        index_9: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_407, [None, getitem_192]);  view_407 = None
        mul_219: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.mul.Tensor(index_8, index_9)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_409: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(mul_219, [10, -1, 256]);  mul_219 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        cat_32: "bf16[10, 112, 256][28672, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_372, cat_29, view_409], 1);  view_372 = cat_29 = view_409 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_153: "bf16[10, 256, 112][28672, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_32, [0, 2, 1]);  cat_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_687: "bf16[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_250, torch.bfloat16);  primals_250 = None
        permute_154: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(convert_element_type_687, [1, 0]);  convert_element_type_687 = None
        clone_36: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.clone.default(permute_153, memory_format = torch.contiguous_format);  permute_153 = None
        view_410: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.view.default(clone_36, [2560, 112]);  clone_36 = None
        mm_97: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_410, permute_154)
        view_411: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_97, [10, 256, 64]);  mm_97 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_155: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_411, [0, 2, 1]);  view_411 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_156: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_155, [0, 2, 1])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_690: "bf16[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_251, torch.bfloat16);  primals_251 = None
        permute_157: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_690, [1, 0]);  convert_element_type_690 = None
        view_412: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(permute_156, [2560, 64]);  permute_156 = None
        mm_98: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.mm.default(view_412, permute_157)
        view_413: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.view.default(mm_98, [10, 256, 80]);  mm_98 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_158: "bf16[10, 80, 256][20480, 1, 80]cuda:0" = torch.ops.aten.permute.default(view_413, [0, 2, 1]);  view_413 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_12 = torch.ops.aten.split_with_sizes.default(permute_158, [32, 32, 16], 1);  permute_158 = None
        getitem_193: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_12[0]
        getitem_194: "bf16[10, 32, 256][20480, 1, 80]cuda:0" = split_with_sizes_12[1]
        getitem_195: "bf16[10, 16, 256][20480, 1, 80]cuda:0" = split_with_sizes_12[2];  split_with_sizes_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_37: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_194, memory_format = torch.contiguous_format);  getitem_194 = None
        view_414: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_37, [10, 8192]);  clone_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_693: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_252, torch.bfloat16);  primals_252 = None
        permute_159: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_693, [1, 0]);  convert_element_type_693 = None
        mm_99: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_414, permute_159)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_415: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_99, [-1, 512]);  mm_99 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_696: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_415, torch.float32);  view_415 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_47: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_696, 2)
        mean_46: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_47, [1], True);  pow_47 = None
        add_296: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_46, 1e-05);  mean_46 = None
        rsqrt_52: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_296);  add_296 = None
        mul_220: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_696, rsqrt_52)
        mul_221: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_220, primals_253);  mul_220 = None
        alias_48: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_52);  rsqrt_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_697: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_221, torch.bfloat16);  mul_221 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_416: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_697, [10, 512]);  convert_element_type_697 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_698: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_416, torch.float32)
        neg_52: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_698)
        exp_52: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_52);  neg_52 = None
        add_297: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_52, 1);  exp_52 = None
        div_55: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_698, add_297);  convert_element_type_698 = add_297 = None
        convert_element_type_699: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_55, torch.bfloat16);  div_55 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_700: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_254, torch.bfloat16);  primals_254 = None
        permute_160: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_700, [1, 0]);  convert_element_type_700 = None
        mm_100: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_699, permute_160)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_417: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_100, [10, -1, 64]);  mm_100 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_36: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_417, [10, 32, 64]);  view_417 = None
        view_418: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_36, [10, 32, 64]);  expand_36 = None
        expand_37: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.expand.default(permute_155, [10, 64, 256])
        view_419: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.view.default(expand_37, [10, 64, 256]);  expand_37 = None
        bmm_12: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_418, view_419)
        view_420: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_12, [10, 32, 256]);  bmm_12 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        permute_161: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_155, [0, 2, 1])
        expand_38: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_420, [10, 32, 256]);  view_420 = None
        view_421: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_38, [10, 32, 256]);  expand_38 = None
        expand_39: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.expand.default(permute_161, [10, 256, 64]);  permute_161 = None
        view_422: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_39, [10, 256, 64]);  expand_39 = None
        bmm_13: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_421, view_422)
        view_423: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_13, [10, 32, 64]);  bmm_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_424: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_423, [10, -1]);  view_423 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_38: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_195, memory_format = torch.contiguous_format);  getitem_195 = None
        view_425: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_38, [10, 4096]);  clone_38 = None
        cat_33: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_424, view_425], 1);  view_424 = view_425 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_34: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_33, view_48], 1);  cat_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_426: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_34, [-1, 8472]);  cat_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_114: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_115: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_54 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 146, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_426, 'Y': empty_114, 'W': primals_255, 'Rstd': empty_115}, tensors_to_clone = ['Y', 'Rstd']);  empty_114 = empty_115 = None
        getitem_196: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_54['Y']
        getitem_197: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_54['Rstd'];  triton_kernel_wrapper_functional_proxy_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_428: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_196, [10, 8472]);  getitem_196 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_707: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_428, torch.bfloat16);  view_428 = None
        convert_element_type_708: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_256, torch.bfloat16);  primals_256 = None
        permute_162: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_708, [1, 0]);  convert_element_type_708 = None
        mm_101: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_707, permute_162)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_429: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_101, [-1, 2048]);  mm_101 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_711: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_429, torch.float32);  view_429 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_48: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_711, 2)
        mean_47: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_48, [1], True);  pow_48 = None
        add_298: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_47, 1e-05);  mean_47 = None
        rsqrt_53: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_298);  add_298 = None
        mul_222: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_711, rsqrt_53)
        mul_223: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_222, primals_257);  mul_222 = None
        alias_49: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_53);  rsqrt_53 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_712: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_223, torch.bfloat16);  mul_223 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_430: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_712, [10, 2048]);  convert_element_type_712 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_713: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_430, torch.float32)
        neg_53: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_713)
        exp_53: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_53);  neg_53 = None
        add_299: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_53, 1);  exp_53 = None
        div_56: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_713, add_299);  convert_element_type_713 = add_299 = None
        convert_element_type_714: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_56, torch.bfloat16);  div_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_715: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_258, torch.bfloat16);  primals_258 = None
        permute_163: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_715, [1, 0]);  convert_element_type_715 = None
        mm_102: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_714, permute_163)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_431: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_102, [-1, 1024]);  mm_102 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_718: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_431, torch.float32);  view_431 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_49: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_718, 2)
        mean_48: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_49, [1], True);  pow_49 = None
        add_300: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_48, 1e-05);  mean_48 = None
        rsqrt_54: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_300);  add_300 = None
        mul_224: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_718, rsqrt_54)
        mul_225: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_224, primals_259);  mul_224 = None
        alias_50: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_54);  rsqrt_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_719: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_225, torch.bfloat16);  mul_225 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_432: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_719, [10, 1024]);  convert_element_type_719 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_720: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_432, torch.float32)
        neg_54: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_720)
        exp_54: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_54);  neg_54 = None
        add_301: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_54, 1);  exp_54 = None
        div_57: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_720, add_301);  convert_element_type_720 = add_301 = None
        convert_element_type_721: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_57, torch.bfloat16);  div_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_722: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_260, torch.bfloat16);  primals_260 = None
        permute_164: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_722, [1, 0]);  convert_element_type_722 = None
        mm_103: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_721, permute_164)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_302: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_714, mm_103);  mm_103 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_433: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_302, [-1, 2048]);  add_302 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_725: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_433, torch.float32);  view_433 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_50: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_725, 2)
        mean_49: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_50, [1], True);  pow_50 = None
        add_303: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_49, 1e-05);  mean_49 = None
        rsqrt_55: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_303);  add_303 = None
        mul_226: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_725, rsqrt_55)
        mul_227: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_226, primals_261);  mul_226 = None
        alias_51: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_55);  rsqrt_55 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_726: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_227, torch.bfloat16);  mul_227 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_434: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_726, [10, 2048]);  convert_element_type_726 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_727: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_434, torch.float32)
        neg_55: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_727)
        exp_55: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_55);  neg_55 = None
        add_304: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_55, 1);  exp_55 = None
        div_58: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_727, add_304);  convert_element_type_727 = add_304 = None
        convert_element_type_728: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_58, torch.bfloat16);  div_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_729: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_262, torch.bfloat16);  primals_262 = None
        permute_165: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_729, [1, 0]);  convert_element_type_729 = None
        mm_104: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_728, permute_165)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_435: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_104, [-1, 1024]);  mm_104 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_732: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_435, torch.float32);  view_435 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_51: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_732, 2)
        mean_50: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_51, [1], True);  pow_51 = None
        add_305: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_50, 1e-05);  mean_50 = None
        rsqrt_56: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_305);  add_305 = None
        mul_228: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_732, rsqrt_56)
        mul_229: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_228, primals_263);  mul_228 = None
        alias_52: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_56);  rsqrt_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_733: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_229, torch.bfloat16);  mul_229 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_436: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_733, [10, 1024]);  convert_element_type_733 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_734: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_436, torch.float32)
        neg_56: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_734)
        exp_56: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_56);  neg_56 = None
        add_306: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_56, 1);  exp_56 = None
        div_59: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_734, add_306);  convert_element_type_734 = add_306 = None
        convert_element_type_735: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_59, torch.bfloat16);  div_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_736: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_264, torch.bfloat16);  primals_264 = None
        permute_166: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_736, [1, 0]);  convert_element_type_736 = None
        mm_105: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_735, permute_166)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_307: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_728, mm_105);  mm_105 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_437: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_307, [-1, 2048]);  add_307 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_739: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_437, torch.float32);  view_437 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_52: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_739, 2)
        mean_51: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_52, [1], True);  pow_52 = None
        add_308: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_51, 1e-05);  mean_51 = None
        rsqrt_57: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_308);  add_308 = None
        mul_230: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_739, rsqrt_57)
        mul_231: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_230, primals_265);  mul_230 = None
        alias_53: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_57);  rsqrt_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_740: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_231, torch.bfloat16);  mul_231 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_438: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_740, [10, 2048]);  convert_element_type_740 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_741: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_438, torch.float32)
        neg_57: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_741)
        exp_57: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_57);  neg_57 = None
        add_309: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_57, 1);  exp_57 = None
        div_60: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_741, add_309);  convert_element_type_741 = add_309 = None
        convert_element_type_742: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_60, torch.bfloat16);  div_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_743: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_266, torch.bfloat16);  primals_266 = None
        permute_167: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_743, [1, 0]);  convert_element_type_743 = None
        mm_106: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_742, permute_167)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_439: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_106, [10, -1, 256]);  mm_106 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        cat_35: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_439, getitem_193], 1);  view_439 = getitem_193 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:406 in forward, code: x = x + res_x
        add_310: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(cat_35, permute_155);  cat_35 = permute_155 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_440: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_310, [-1, 256]);  add_310 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_116: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_117: "f32[640][1]cuda:0" = torch.ops.aten.empty.memory_format([640], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_55 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 148, grid = [(640, 1, 1), (640, 1, 1), (160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_440, 'Y': empty_116, 'W': primals_267, 'Rstd': empty_117}, tensors_to_clone = ['Y', 'Rstd']);  empty_116 = empty_117 = None
        getitem_198: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_55['Y']
        getitem_199: "f32[640][1]cuda:0" = triton_kernel_wrapper_functional_proxy_55['Rstd'];  triton_kernel_wrapper_functional_proxy_55 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:605 in forward, code: seq_lengths=offsets[1:] - offsets[:-1],
        slice_27: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_177, 0, 1, 9223372036854775807)
        slice_28: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_177, 0, 0, -1)
        sub_88: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_27, slice_28);  slice_27 = slice_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:125 in _truncate_last_k_embeddings, code: minus_k_lengths = torch.clamp(seq_lengths - K, min=0)
        sub_89: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_88, 128)
        clamp_min_14: "i64[2][1]cuda:0" = torch.ops.aten.clamp_min.default(sub_89, 0);  sub_89 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_5: "i64[][]cuda:0" = torch.ops.aten.max.default(clamp_min_14)
        _local_scalar_dense_8: "Sym(u16)" = torch.ops.aten._local_scalar_dense.default(max_5);  max_5 = None
        ge_16: "Sym(u16 >= 1)" = _local_scalar_dense_8 >= 1
        _assert_scalar_30 = torch.ops.aten._assert_scalar.default(ge_16, "Runtime assertion failed for expression u16 >= 1 on node 'ge_16'");  ge_16 = _assert_scalar_30 = None
        le_6: "Sym(u16 <= 999999999)" = _local_scalar_dense_8 <= 999999999
        _assert_scalar_31 = torch.ops.aten._assert_scalar.default(le_6, "Runtime assertion failed for expression u16 <= 999999999 on node 'le_6'");  le_6 = _assert_scalar_31 = None
        
        # No stacktrace found for following nodes
        lt_16: "Sym(u16 < 1000000000)" = _local_scalar_dense_8 < 1000000000
        _assert_scalar_32 = torch.ops.aten._assert_scalar.default(lt_16, "Runtime assertion failed for expression u16 < 1000000000 on node 'lt_14'");  lt_16 = _assert_scalar_32 = None
        gt_16: "Sym(u16 > 0)" = _local_scalar_dense_8 > 0;  _local_scalar_dense_8 = None
        _assert_scalar_33 = torch.ops.aten._assert_scalar.default(gt_16, "Runtime assertion failed for expression 0 < u16 on node 'gt_14'");  gt_16 = _assert_scalar_33 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:41 in _truncate_last_embeddings, code: minus_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(left_lengths)
        asynchronous_complete_cumsum_4: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(clamp_min_14)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:42 in _truncate_last_embeddings, code: last_k_lengths = seq_lengths - left_lengths
        sub_90: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_88, clamp_min_14);  sub_88 = clamp_min_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:43 in _truncate_last_embeddings, code: last_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(last_k_lengths)
        asynchronous_complete_cumsum_5: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(sub_90)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_6: "i64[][]cuda:0" = torch.ops.aten.max.default(sub_90);  sub_90 = None
        _local_scalar_dense_9: "Sym(u17)" = torch.ops.aten._local_scalar_dense.default(max_6);  max_6 = None
        ge_18: "Sym(u17 >= 1)" = _local_scalar_dense_9 >= 1
        _assert_scalar_34 = torch.ops.aten._assert_scalar.default(ge_18, "Runtime assertion failed for expression u17 >= 1 on node 'ge_17'");  ge_18 = _assert_scalar_34 = None
        le_7: "Sym(u17 <= 999999999)" = _local_scalar_dense_9 <= 999999999
        _assert_scalar_35 = torch.ops.aten._assert_scalar.default(le_7, "Runtime assertion failed for expression u17 <= 999999999 on node 'le_7'");  le_7 = _assert_scalar_35 = None
        
        # No stacktrace found for following nodes
        lt_17: "Sym(u17 < 1000000000)" = _local_scalar_dense_9 < 1000000000
        _assert_scalar_36 = torch.ops.aten._assert_scalar.default(lt_17, "Runtime assertion failed for expression u17 < 1000000000 on node 'lt_15'");  lt_17 = _assert_scalar_36 = None
        gt_18: "Sym(u17 > 0)" = _local_scalar_dense_9 > 0
        _assert_scalar_37 = torch.ops.aten._assert_scalar.default(gt_18, "Runtime assertion failed for expression 0 < u17 on node 'gt_15'");  gt_18 = _assert_scalar_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1446 in forward, code: offsets_a_last_idx = torch.tensor(offsets_a.size(0) - 1).to(
        _tensor_constant12: "i64[][]cpu" = self._tensor_constant12
        lift_fresh_copy_12: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant12);  _tensor_constant12 = None
        device_put_8: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_12, device(type='cuda', index=0), True);  lift_fresh_copy_12 = None
        convert_element_type_746: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_8, torch.int64);  device_put_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1449 in forward, code: offsets_b_last_idx = torch.tensor(offsets_b.size(0) - 1).to(
        _tensor_constant13: "i64[][]cpu" = self._tensor_constant13
        lift_fresh_copy_13: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant13);  _tensor_constant13 = None
        device_put_9: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_13, device(type='cuda', index=0), True);  lift_fresh_copy_13 = None
        convert_element_type_747: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_9, torch.int64);  device_put_9 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1453 in forward, code: seq_len_a = offsets_a.index_select(dim=0, index=offsets_a_last_idx)
        unsqueeze_4: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_746, 0);  convert_element_type_746 = None
        index_10: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_4, [unsqueeze_4]);  unsqueeze_4 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1455 in forward, code: seq_len_b = offsets_b.index_select(dim=0, index=offsets_b_last_idx)
        unsqueeze_5: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_747, 0);  convert_element_type_747 = None
        index_11: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_5, [unsqueeze_5]);  unsqueeze_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1464 in forward, code: values_a = torch.empty((seq_len_a, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_10: "Sym(u22)" = torch.ops.aten._local_scalar_dense.default(index_10)
        empty_118: "bf16[u22, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_10, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_20: "Sym(u22 >= 0)" = _local_scalar_dense_10 >= 0
        _assert_scalar_38 = torch.ops.aten._assert_scalar.default(ge_20, "Runtime assertion failed for expression u18 >= 0 on node 'ge'");  ge_20 = _assert_scalar_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1466 in forward, code: values_b = torch.empty((seq_len_b, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_11: "Sym(u23)" = torch.ops.aten._local_scalar_dense.default(index_11)
        empty_119: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_21: "Sym(u23 >= 0)" = _local_scalar_dense_11 >= 0
        _assert_scalar_39 = torch.ops.aten._assert_scalar.default(ge_21, "Runtime assertion failed for expression u19 >= 0 on node 'ge_1'");  ge_21 = _assert_scalar_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_317: "Sym(u1 + 1)" = _local_scalar_dense_1 + 1
        floordiv_14: "Sym(((u1 + 1)//2))" = add_317 // 2;  add_317 = None
        add_319: "Sym(u1 + 4)" = _local_scalar_dense_1 + 4
        sub_93: "Sym(u1 + 3)" = add_319 - 1;  add_319 = None
        floordiv_15: "Sym(((u1 + 3)//4))" = sub_93 // 4;  sub_93 = None
        add_320: "Sym(u1 + 8)" = _local_scalar_dense_1 + 8
        sub_94: "Sym(u1 + 7)" = add_320 - 1;  add_320 = None
        floordiv_16: "Sym(((u1 + 7)//8))" = sub_94 // 8;  sub_94 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1246 in _triton_split_2D_jagged_internal, code: split_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_56 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 3, constant_args_idx = 150, grid = [(_local_scalar_dense_1, 2, 1), (_local_scalar_dense_1, 2, 1), (_local_scalar_dense_1, 2, 1), (floordiv_14, 2, 1), (floordiv_14, 2, 1), (floordiv_14, 2, 1), (floordiv_15, 2, 1), (floordiv_15, 2, 1), (floordiv_15, 2, 1), (floordiv_16, 2, 1), (floordiv_16, 2, 1), (floordiv_16, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'JaggedIn': addmm_11, 'OffsetsA': asynchronous_complete_cumsum_4, 'OffsetsB': asynchronous_complete_cumsum_5, 'OutA': empty_118, 'OutB': empty_119}, tensors_to_clone = ['OutA', 'OutB']);  floordiv_14 = floordiv_15 = floordiv_16 = addmm_11 = empty_118 = empty_119 = None
        getitem_201: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_56['OutB'];  triton_kernel_wrapper_functional_proxy_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        ge_22: "Sym(u22 >= 0)" = _local_scalar_dense_10 >= 0
        _assert_scalar_40 = torch.ops.aten._assert_scalar.default(ge_22, "Runtime assertion failed for expression u22 >= 0 on node 'ge_18'");  ge_22 = _assert_scalar_40 = None
        ge_23: "Sym(u23 >= 1)" = _local_scalar_dense_11 >= 1
        _assert_scalar_41 = torch.ops.aten._assert_scalar.default(ge_23, "Runtime assertion failed for expression u23 >= 1 on node 'ge_19'");  ge_23 = _assert_scalar_41 = None
        le_8: "Sym(u23 <= 999999999)" = _local_scalar_dense_11 <= 999999999
        _assert_scalar_42 = torch.ops.aten._assert_scalar.default(le_8, "Runtime assertion failed for expression u23 <= 999999999 on node 'le_8'");  le_8 = _assert_scalar_42 = None
        
        # No stacktrace found for following nodes
        gt_21: "Sym(u23 > 0)" = _local_scalar_dense_11 > 0
        _assert_scalar_43 = torch.ops.aten._assert_scalar.default(gt_21, "Runtime assertion failed for expression 0 < u23 on node 'gt_16'");  gt_21 = _assert_scalar_43 = None
        lt_19: "Sym(u23 < 1000000000)" = _local_scalar_dense_11 < 1000000000
        _assert_scalar_44 = torch.ops.aten._assert_scalar.default(lt_19, "Runtime assertion failed for expression u23 < 1000000000 on node 'lt_16'");  lt_19 = _assert_scalar_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:605 in forward, code: seq_lengths=offsets[1:] - offsets[:-1],
        slice_31: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_186, 0, 1, 9223372036854775807)
        slice_32: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(getitem_186, 0, 0, -1)
        sub_95: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_31, slice_32);  slice_31 = slice_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:125 in _truncate_last_k_embeddings, code: minus_k_lengths = torch.clamp(seq_lengths - K, min=0)
        sub_96: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_95, 128)
        clamp_min_15: "i64[2][1]cuda:0" = torch.ops.aten.clamp_min.default(sub_96, 0);  sub_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_7: "i64[][]cuda:0" = torch.ops.aten.max.default(clamp_min_15)
        _local_scalar_dense_12: "Sym(u24)" = torch.ops.aten._local_scalar_dense.default(max_7);  max_7 = None
        ge_24: "Sym(u24 >= 1)" = _local_scalar_dense_12 >= 1
        _assert_scalar_45 = torch.ops.aten._assert_scalar.default(ge_24, "Runtime assertion failed for expression u24 >= 1 on node 'ge_20'");  ge_24 = _assert_scalar_45 = None
        le_9: "Sym(u24 <= 999999999)" = _local_scalar_dense_12 <= 999999999
        _assert_scalar_46 = torch.ops.aten._assert_scalar.default(le_9, "Runtime assertion failed for expression u24 <= 999999999 on node 'le_9'");  le_9 = _assert_scalar_46 = None
        
        # No stacktrace found for following nodes
        lt_20: "Sym(u24 < 1000000000)" = _local_scalar_dense_12 < 1000000000
        _assert_scalar_47 = torch.ops.aten._assert_scalar.default(lt_20, "Runtime assertion failed for expression u24 < 1000000000 on node 'lt_17'");  lt_20 = _assert_scalar_47 = None
        gt_22: "Sym(u24 > 0)" = _local_scalar_dense_12 > 0;  _local_scalar_dense_12 = None
        _assert_scalar_48 = torch.ops.aten._assert_scalar.default(gt_22, "Runtime assertion failed for expression 0 < u24 on node 'gt_17'");  gt_22 = _assert_scalar_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:41 in _truncate_last_embeddings, code: minus_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(left_lengths)
        asynchronous_complete_cumsum_6: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(clamp_min_15)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:42 in _truncate_last_embeddings, code: last_k_lengths = seq_lengths - left_lengths
        sub_97: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(sub_95, clamp_min_15);  sub_95 = clamp_min_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/hstu_truncate.py:43 in _truncate_last_embeddings, code: last_k_offsets = torch.ops.fbgemm.asynchronous_complete_cumsum(last_k_lengths)
        asynchronous_complete_cumsum_7: "i64[3][1]cuda:0" = torch.ops.fbgemm.asynchronous_complete_cumsum.default(sub_97)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/fx_utils.py:105 in fx_infer_max_len, code: max_len = int(lengths.max())
        max_8: "i64[][]cuda:0" = torch.ops.aten.max.default(sub_97);  sub_97 = None
        _local_scalar_dense_13: "Sym(u25)" = torch.ops.aten._local_scalar_dense.default(max_8);  max_8 = None
        ge_26: "Sym(u25 >= 1)" = _local_scalar_dense_13 >= 1
        _assert_scalar_49 = torch.ops.aten._assert_scalar.default(ge_26, "Runtime assertion failed for expression u25 >= 1 on node 'ge_21'");  ge_26 = _assert_scalar_49 = None
        le_10: "Sym(u25 <= 999999999)" = _local_scalar_dense_13 <= 999999999
        _assert_scalar_50 = torch.ops.aten._assert_scalar.default(le_10, "Runtime assertion failed for expression u25 <= 999999999 on node 'le_10'");  le_10 = _assert_scalar_50 = None
        
        # No stacktrace found for following nodes
        lt_21: "Sym(u25 < 1000000000)" = _local_scalar_dense_13 < 1000000000
        _assert_scalar_51 = torch.ops.aten._assert_scalar.default(lt_21, "Runtime assertion failed for expression u25 < 1000000000 on node 'lt_18'");  lt_21 = _assert_scalar_51 = None
        gt_24: "Sym(u25 > 0)" = _local_scalar_dense_13 > 0
        _assert_scalar_52 = torch.ops.aten._assert_scalar.default(gt_24, "Runtime assertion failed for expression 0 < u25 on node 'gt_18'");  gt_24 = _assert_scalar_52 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1446 in forward, code: offsets_a_last_idx = torch.tensor(offsets_a.size(0) - 1).to(
        _tensor_constant14: "i64[][]cpu" = self._tensor_constant14
        lift_fresh_copy_14: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant14);  _tensor_constant14 = None
        device_put_10: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_14, device(type='cuda', index=0), True);  lift_fresh_copy_14 = None
        convert_element_type_748: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_10, torch.int64);  device_put_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1449 in forward, code: offsets_b_last_idx = torch.tensor(offsets_b.size(0) - 1).to(
        _tensor_constant15: "i64[][]cpu" = self._tensor_constant15
        lift_fresh_copy_15: "i64[][]cpu" = torch.ops.aten.lift_fresh_copy.default(_tensor_constant15);  _tensor_constant15 = None
        device_put_11: "i64[][]cuda:0" = torch.ops.prims.device_put.default(lift_fresh_copy_15, device(type='cuda', index=0), True);  lift_fresh_copy_15 = None
        convert_element_type_749: "i64[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_11, torch.int64);  device_put_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1453 in forward, code: seq_len_a = offsets_a.index_select(dim=0, index=offsets_a_last_idx)
        unsqueeze_6: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_748, 0);  convert_element_type_748 = None
        index_12: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_6, [unsqueeze_6]);  unsqueeze_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1455 in forward, code: seq_len_b = offsets_b.index_select(dim=0, index=offsets_b_last_idx)
        unsqueeze_7: "i64[1][1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_749, 0);  convert_element_type_749 = None
        index_13: "i64[1][1]cuda:0" = torch.ops.aten.index.Tensor(asynchronous_complete_cumsum_7, [unsqueeze_7]);  unsqueeze_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1464 in forward, code: values_a = torch.empty((seq_len_a, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_14: "Sym(u30)" = torch.ops.aten._local_scalar_dense.default(index_12)
        empty_120: "bf16[u30, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_14, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_28: "Sym(u30 >= 0)" = _local_scalar_dense_14 >= 0
        _assert_scalar_53 = torch.ops.aten._assert_scalar.default(ge_28, "Runtime assertion failed for expression u26 >= 0 on node 'ge'");  ge_28 = _assert_scalar_53 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1466 in forward, code: values_b = torch.empty((seq_len_b, D), device=values.device, dtype=values.dtype)
        _local_scalar_dense_15: "Sym(u31)" = torch.ops.aten._local_scalar_dense.default(index_13)
        empty_121: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_29: "Sym(u31 >= 0)" = _local_scalar_dense_15 >= 0
        _assert_scalar_54 = torch.ops.aten._assert_scalar.default(ge_29, "Runtime assertion failed for expression u27 >= 0 on node 'ge_1'");  ge_29 = _assert_scalar_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_327: "Sym(u9 + 1)" = _local_scalar_dense_5 + 1
        floordiv_17: "Sym(((u9 + 1)//2))" = add_327 // 2;  add_327 = None
        add_329: "Sym(u9 + 4)" = _local_scalar_dense_5 + 4
        sub_100: "Sym(u9 + 3)" = add_329 - 1;  add_329 = None
        floordiv_18: "Sym(((u9 + 3)//4))" = sub_100 // 4;  sub_100 = None
        add_330: "Sym(u9 + 8)" = _local_scalar_dense_5 + 8
        sub_101: "Sym(u9 + 7)" = add_330 - 1;  add_330 = None
        floordiv_19: "Sym(((u9 + 7)//8))" = sub_101 // 8;  sub_101 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1246 in _triton_split_2D_jagged_internal, code: split_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_57 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 3, constant_args_idx = 152, grid = [(_local_scalar_dense_5, 2, 1), (_local_scalar_dense_5, 2, 1), (_local_scalar_dense_5, 2, 1), (floordiv_17, 2, 1), (floordiv_17, 2, 1), (floordiv_17, 2, 1), (floordiv_18, 2, 1), (floordiv_18, 2, 1), (floordiv_18, 2, 1), (floordiv_19, 2, 1), (floordiv_19, 2, 1), (floordiv_19, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'JaggedIn': addmm_13, 'OffsetsA': asynchronous_complete_cumsum_6, 'OffsetsB': asynchronous_complete_cumsum_7, 'OutA': empty_120, 'OutB': empty_121}, tensors_to_clone = ['OutA', 'OutB']);  floordiv_17 = floordiv_18 = floordiv_19 = addmm_13 = empty_120 = empty_121 = None
        getitem_203: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_57['OutB'];  triton_kernel_wrapper_functional_proxy_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        ge_30: "Sym(u30 >= 0)" = _local_scalar_dense_14 >= 0
        _assert_scalar_55 = torch.ops.aten._assert_scalar.default(ge_30, "Runtime assertion failed for expression u30 >= 0 on node 'ge_22'");  ge_30 = _assert_scalar_55 = None
        ge_31: "Sym(u31 >= 1)" = _local_scalar_dense_15 >= 1
        _assert_scalar_56 = torch.ops.aten._assert_scalar.default(ge_31, "Runtime assertion failed for expression u31 >= 1 on node 'ge_23'");  ge_31 = _assert_scalar_56 = None
        le_11: "Sym(u31 <= 999999999)" = _local_scalar_dense_15 <= 999999999
        _assert_scalar_57 = torch.ops.aten._assert_scalar.default(le_11, "Runtime assertion failed for expression u31 <= 999999999 on node 'le_11'");  le_11 = _assert_scalar_57 = None
        
        # No stacktrace found for following nodes
        gt_27: "Sym(u31 > 0)" = _local_scalar_dense_15 > 0
        _assert_scalar_58 = torch.ops.aten._assert_scalar.default(gt_27, "Runtime assertion failed for expression 0 < u31 on node 'gt_19'");  gt_27 = _assert_scalar_58 = None
        lt_23: "Sym(u31 < 1000000000)" = _local_scalar_dense_15 < 1000000000
        _assert_scalar_59 = torch.ops.aten._assert_scalar.default(lt_23, "Runtime assertion failed for expression u31 < 1000000000 on node 'lt_19'");  lt_23 = _assert_scalar_59 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_4: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_9, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_12: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_4, device(type='cuda', index=0));  scalar_tensor_4 = None
        convert_element_type_750: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_12, torch.bfloat16);  device_put_12 = None
        convert_element_type_751: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_750, torch.float32);  convert_element_type_750 = None
        clamp_min_16: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_751, 1);  convert_element_type_751 = None
        convert_element_type_752: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_16, torch.bfloat16);  clamp_min_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_753: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_752, torch.float32);  convert_element_type_752 = None
        reciprocal_12: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_753);  convert_element_type_753 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_754: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_268, torch.bfloat16);  primals_268 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_755: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_269, torch.bfloat16);  primals_269 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_756: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_270, torch.bfloat16);  primals_270 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_757: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_271, torch.bfloat16);  primals_271 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_122: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_123: "f32[u23][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_124: "f32[u23][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_338: "Sym(u23 + 4)" = _local_scalar_dense_11 + 4
        sub_105: "Sym(u23 + 3)" = add_338 - 1;  add_338 = None
        floordiv_20: "Sym(((u23 + 3)//4))" = sub_105 // 4;  sub_105 = None
        add_339: "Sym(u23 + 8)" = _local_scalar_dense_11 + 8
        sub_106: "Sym(u23 + 7)" = add_339 - 1;  add_339 = None
        floordiv_21: "Sym(((u23 + 7)//8))" = sub_106 // 8;  sub_106 = None
        add_340: "Sym(u23 + 16)" = _local_scalar_dense_11 + 16
        sub_107: "Sym(u23 + 15)" = add_340 - 1;  add_340 = None
        floordiv_22: "Sym(((u23 + 15)//16))" = sub_107 // 16;  sub_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_58 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 154, grid = [(floordiv_20, 1, 1), (floordiv_20, 1, 1), (floordiv_20, 1, 1), (floordiv_21, 1, 1), (floordiv_21, 1, 1), (floordiv_21, 1, 1), (floordiv_22, 1, 1), (floordiv_22, 1, 1), (floordiv_22, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_201, 'Y': empty_122, 'W': convert_element_type_754, 'B': convert_element_type_755, 'Mean': empty_123, 'Rstd': empty_124, 'N': _local_scalar_dense_11}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  floordiv_20 = floordiv_21 = floordiv_22 = empty_122 = empty_123 = empty_124 = None
        getitem_204: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_58['Y']
        getitem_205: "f32[u23][1]cuda:0" = triton_kernel_wrapper_functional_proxy_58['Mean']
        getitem_206: "f32[u23][1]cuda:0" = triton_kernel_wrapper_functional_proxy_58['Rstd'];  triton_kernel_wrapper_functional_proxy_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_14: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_757, getitem_204, convert_element_type_756);  getitem_204 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_13 = torch.ops.aten.split_with_sizes.default(addmm_14, [256, 256, 256, 256], 1);  addmm_14 = None
        getitem_207: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_13[0]
        getitem_208: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_13[1]
        getitem_209: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_13[2]
        getitem_210: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_13[3];  split_with_sizes_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_442: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_209, [-1, 2, 128]);  getitem_209 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_443: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_210, [-1, 2, 128]);  getitem_210 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_444: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_208, [-1, 2, 128]);  getitem_208 = None
        sym_size_int_14: "Sym(u23)" = torch.ops.aten.sym_size.int(view_444, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_761: "f32[u23, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_207, torch.float32);  getitem_207 = None
        neg_58: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_761)
        exp_58: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_58);  neg_58 = None
        add_368: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_58, 1);  exp_58 = None
        div_61: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_761, add_368);  convert_element_type_761 = add_368 = None
        convert_element_type_762: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_61, torch.bfloat16);  div_61 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_33: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_5, 0, 1, 9223372036854775807)
        slice_34: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_5, 0, 0, -1)
        sub_117: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_33, slice_34);  slice_33 = slice_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_6 = torch.ops.aten.sort.stable(sub_117, stable = False, descending = True);  sub_117 = None
        getitem_212: "i64[2][1]cuda:0" = sort_6[1];  sort_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_125: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_14, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_445: "bf16[256*u23][1]cuda:0" = torch.ops.aten.view.default(empty_125, [-1]);  empty_125 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_378: "Sym(u17 + 16)" = _local_scalar_dense_9 + 16
        sub_120: "Sym(u17 + 15)" = add_378 - 1;  add_378 = None
        floordiv_23: "Sym(((u17 + 15)//16))" = sub_120 // 16;  sub_120 = None
        add_379: "Sym(u17 + 32)" = _local_scalar_dense_9 + 32
        sub_121: "Sym(u17 + 31)" = add_379 - 1;  add_379 = None
        floordiv_24: "Sym(((u17 + 31)//32))" = sub_121 // 32;  sub_121 = None
        add_380: "Sym(u17 + 64)" = _local_scalar_dense_9 + 64
        sub_122: "Sym(u17 + 63)" = add_380 - 1;  add_380 = None
        floordiv_25: "Sym(((u17 + 63)//64))" = sub_122 // 64;  sub_122 = None
        add_381: "Sym(u17 + 128)" = _local_scalar_dense_9 + 128
        sub_123: "Sym(u17 + 127)" = add_381 - 1;  add_381 = None
        floordiv_26: "Sym(((u17 + 127)//128))" = sub_123 // 128;  sub_123 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_59 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 155, grid = [(floordiv_23, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_24, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_25, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1), (floordiv_26, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_442, 'K': view_443, 'V': view_444, 'seq_offsets': asynchronous_complete_cumsum_5, 'attn_scale': reciprocal_12, 'Out': view_445, 'MAX_SEQ_LEN': _local_scalar_dense_9, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_9}, tensors_to_clone = ['Out']);  floordiv_23 = floordiv_24 = floordiv_25 = floordiv_26 = view_442 = view_443 = view_444 = view_445 = None
        getitem_213: "bf16[256*u23][1]cuda:0" = triton_kernel_wrapper_functional_proxy_59['Out'];  triton_kernel_wrapper_functional_proxy_59 = None
        view_446: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_213, [sym_size_int_14, 2, 128]);  getitem_213 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_763: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_272, torch.bfloat16);  primals_272 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_764: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_273, torch.bfloat16);  primals_273 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_765: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_274, torch.bfloat16);  primals_274 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_127: "bf16[u23, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_128: "f32[u23][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_129: "f32[u23][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_449: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_446, [-1, 256]);  view_446 = None
        triton_kernel_wrapper_functional_proxy_60 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 28, constant_args_idx = 160, grid = [(_local_scalar_dense_11, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_449, 'U': convert_element_type_762, 'Y': empty_127, 'W': convert_element_type_763, 'B': convert_element_type_764, 'Mean': empty_128, 'Rstd': empty_129, 'seed': primals_275}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_127 = empty_128 = empty_129 = None
        getitem_214: "bf16[u23, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_60['Y']
        getitem_215: "f32[u23][1]cuda:0" = triton_kernel_wrapper_functional_proxy_60['Mean']
        getitem_216: "f32[u23][1]cuda:0" = triton_kernel_wrapper_functional_proxy_60['Rstd'];  triton_kernel_wrapper_functional_proxy_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_15: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(getitem_201, getitem_214, convert_element_type_765);  getitem_214 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_5: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_13, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_13: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_5, device(type='cuda', index=0));  scalar_tensor_5 = None
        convert_element_type_769: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_13, torch.bfloat16);  device_put_13 = None
        convert_element_type_770: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_769, torch.float32);  convert_element_type_769 = None
        clamp_min_17: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_770, 1);  convert_element_type_770 = None
        convert_element_type_771: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_17, torch.bfloat16);  clamp_min_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_772: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_771, torch.float32);  convert_element_type_771 = None
        reciprocal_13: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_772);  convert_element_type_772 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_773: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_276, torch.bfloat16);  primals_276 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_774: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_277, torch.bfloat16);  primals_277 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_775: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_278, torch.bfloat16);  primals_278 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_776: "bf16[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_279, torch.bfloat16);  primals_279 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_130: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:494 in triton_weighted_layer_norm_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_131: "f32[u31][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:496 in triton_weighted_layer_norm_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_132: "f32[u31][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_402: "Sym(u31 + 4)" = _local_scalar_dense_15 + 4
        sub_132: "Sym(u31 + 3)" = add_402 - 1;  add_402 = None
        floordiv_27: "Sym(((u31 + 3)//4))" = sub_132 // 4;  sub_132 = None
        add_403: "Sym(u31 + 8)" = _local_scalar_dense_15 + 8
        sub_133: "Sym(u31 + 7)" = add_403 - 1;  add_403 = None
        floordiv_28: "Sym(((u31 + 7)//8))" = sub_133 // 8;  sub_133 = None
        add_404: "Sym(u31 + 16)" = _local_scalar_dense_15 + 16
        sub_134: "Sym(u31 + 15)" = add_404 - 1;  add_404 = None
        floordiv_29: "Sym(((u31 + 15)//16))" = sub_134 // 16;  sub_134 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_61 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 163, grid = [(floordiv_27, 1, 1), (floordiv_27, 1, 1), (floordiv_27, 1, 1), (floordiv_28, 1, 1), (floordiv_28, 1, 1), (floordiv_28, 1, 1), (floordiv_29, 1, 1), (floordiv_29, 1, 1), (floordiv_29, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_203, 'Y': empty_130, 'W': convert_element_type_773, 'B': convert_element_type_774, 'Mean': empty_131, 'Rstd': empty_132, 'N': _local_scalar_dense_15}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  floordiv_27 = floordiv_28 = floordiv_29 = empty_130 = empty_131 = empty_132 = None
        getitem_217: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_61['Y']
        getitem_218: "f32[u31][1]cuda:0" = triton_kernel_wrapper_functional_proxy_61['Mean']
        getitem_219: "f32[u31][1]cuda:0" = triton_kernel_wrapper_functional_proxy_61['Rstd'];  triton_kernel_wrapper_functional_proxy_61 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_16: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_776, getitem_217, convert_element_type_775);  getitem_217 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:128 in forward, code: u, v, q, k = uvqk.split(
        split_with_sizes_14 = torch.ops.aten.split_with_sizes.default(addmm_16, [256, 256, 256, 256], 1);  addmm_16 = None
        getitem_220: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_14[0]
        getitem_221: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_14[1]
        getitem_222: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_14[2]
        getitem_223: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_14[3];  split_with_sizes_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:160 in forward, code: q = q.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_450: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_222, [-1, 2, 128]);  getitem_222 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:161 in forward, code: k = k.view(-1, attn_config.num_heads, attn_config.attention_dim)
        view_451: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_223, [-1, 2, 128]);  getitem_223 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:162 in forward, code: v = v.view(-1, attn_config.num_heads, attn_config.linear_hidden_dim)
        view_452: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_221, [-1, 2, 128]);  getitem_221 = None
        sym_size_int_19: "Sym(u31)" = torch.ops.aten.sym_size.int(view_452, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:164 in forward, code: u = F.silu(u)
        convert_element_type_780: "f32[u31, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_220, torch.float32);  getitem_220 = None
        neg_59: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_780)
        exp_59: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_59);  neg_59 = None
        add_432: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_59, 1);  exp_59 = None
        div_62: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_780, add_432);  convert_element_type_780 = add_432 = None
        convert_element_type_781: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_62, torch.bfloat16);  div_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:169 in forward, code: seq_lengths = seq_offsets[1:] - seq_offsets[:-1]
        slice_35: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_7, 0, 1, 9223372036854775807)
        slice_36: "i64[2][1]cuda:0" = torch.ops.aten.slice.Tensor(asynchronous_complete_cumsum_7, 0, 0, -1)
        sub_144: "i64[2][1]cuda:0" = torch.ops.aten.sub.Tensor(slice_35, slice_36);  slice_35 = slice_36 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:170 in forward, code: _, sort_by_length_indices = torch.sort(
        sort_7 = torch.ops.aten.sort.stable(sub_144, stable = False, descending = True);  sub_144 = None
        getitem_225: "i64[2][1]cuda:0" = sort_7[1];  sort_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4099 in triton_ragged_attention_fwd, code: out = torch.empty_like(v)
        empty_133: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_19, 2, 128], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4100 in triton_ragged_attention_fwd, code: out_buffer = out.view(-1)
        view_453: "bf16[256*u31][1]cuda:0" = torch.ops.aten.view.default(empty_133, [-1]);  empty_133 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/triton/__init__.py:69 in cdiv, code: return (x + y - 1) // y
        add_442: "Sym(u25 + 16)" = _local_scalar_dense_13 + 16
        sub_147: "Sym(u25 + 15)" = add_442 - 1;  add_442 = None
        floordiv_30: "Sym(((u25 + 15)//16))" = sub_147 // 16;  sub_147 = None
        add_443: "Sym(u25 + 32)" = _local_scalar_dense_13 + 32
        sub_148: "Sym(u25 + 31)" = add_443 - 1;  add_443 = None
        floordiv_31: "Sym(((u25 + 31)//32))" = sub_148 // 32;  sub_148 = None
        add_444: "Sym(u25 + 64)" = _local_scalar_dense_13 + 64
        sub_149: "Sym(u25 + 63)" = add_444 - 1;  add_444 = None
        floordiv_32: "Sym(((u25 + 63)//64))" = sub_149 // 64;  sub_149 = None
        add_445: "Sym(u25 + 128)" = _local_scalar_dense_13 + 128
        sub_150: "Sym(u25 + 127)" = add_445 - 1;  add_445 = None
        floordiv_33: "Sym(((u25 + 127)//128))" = sub_150 // 128;  sub_150 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4128 in triton_ragged_attention_fwd, code: _ragged_hstu_attn_fwd[grid](
        triton_kernel_wrapper_functional_proxy_62 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 7, constant_args_idx = 164, grid = [(floordiv_30, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_31, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_32, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1), (floordiv_33, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_450, 'K': view_451, 'V': view_452, 'seq_offsets': asynchronous_complete_cumsum_7, 'attn_scale': reciprocal_13, 'Out': view_453, 'MAX_SEQ_LEN': _local_scalar_dense_13, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_13}, tensors_to_clone = ['Out']);  floordiv_30 = floordiv_31 = floordiv_32 = floordiv_33 = view_450 = view_451 = view_452 = view_453 = None
        getitem_226: "bf16[256*u31][1]cuda:0" = triton_kernel_wrapper_functional_proxy_62['Out'];  triton_kernel_wrapper_functional_proxy_62 = None
        view_454: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_226, [sym_size_int_19, 2, 128]);  getitem_226 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_782: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_280, torch.bfloat16);  primals_280 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_783: "bf16[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_281, torch.bfloat16);  primals_281 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_784: "bf16[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_282, torch.bfloat16);  primals_282 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:786 in triton_layer_norm_mul_dropout_fwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_135: "bf16[u31, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:789 in triton_layer_norm_mul_dropout_fwd, code: mean = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_136: "f32[u31][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:790 in triton_layer_norm_mul_dropout_fwd, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_137: "f32[u31][1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:854 in triton_layer_norm_mul_dropout_fwd, code: _ln_mul_dropout_fwd[(N,)](
        view_457: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_454, [-1, 256]);  view_454 = None
        triton_kernel_wrapper_functional_proxy_63 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 30, constant_args_idx = 169, grid = [(_local_scalar_dense_15, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_457, 'U': convert_element_type_781, 'Y': empty_135, 'W': convert_element_type_782, 'B': convert_element_type_783, 'Mean': empty_136, 'Rstd': empty_137, 'seed': primals_283}, tensors_to_clone = ['Y', 'Mean', 'Rstd']);  empty_135 = empty_136 = empty_137 = None
        getitem_227: "bf16[u31, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_63['Y']
        getitem_228: "f32[u31][1]cuda:0" = triton_kernel_wrapper_functional_proxy_63['Mean']
        getitem_229: "f32[u31][1]cuda:0" = triton_kernel_wrapper_functional_proxy_63['Rstd'];  triton_kernel_wrapper_functional_proxy_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_17: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.addmm.default(getitem_203, getitem_227, convert_element_type_784);  getitem_227 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_458: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_198, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_788: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_284, torch.bfloat16);  primals_284 = None
        permute_169: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_788, [1, 0]);  convert_element_type_788 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_459: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_198, [10, 64, 256])
        permute_170: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_459, [0, 2, 1]);  view_459 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_39: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_170, memory_format = torch.contiguous_format);  permute_170 = None
        view_460: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_39, [2560, 64]);  clone_39 = None
        mm_107: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_460, permute_169)
        view_461: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_107, [10, 256, 16]);  mm_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_171: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_461, [0, 2, 1]);  view_461 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_316: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_40: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_171, memory_format = torch.contiguous_format);  permute_171 = None
        view_462: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_40, [160, 256]);  clone_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_463: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_462, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_138: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_139: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_64 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 172, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_463, 'Y': empty_138, 'W': primals_285, 'Rstd': empty_139}, tensors_to_clone = ['Y', 'Rstd']);  empty_138 = empty_139 = None
        getitem_230: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_64['Y']
        getitem_231: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_64['Rstd'];  triton_kernel_wrapper_functional_proxy_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_791: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_286, torch.bfloat16);  primals_286 = None
        permute_172: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_791, [1, 0]);  convert_element_type_791 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_465: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_230, [160, 256]);  getitem_230 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_108: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_465, permute_172)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_6: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_9, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_14: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_6, device(type='cuda', index=0));  scalar_tensor_6 = None
        convert_element_type_794: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_14, torch.bfloat16);  device_put_14 = None
        convert_element_type_795: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_794, torch.float32);  convert_element_type_794 = None
        clamp_min_18: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_795, 1);  convert_element_type_795 = None
        convert_element_type_796: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_18, torch.bfloat16);  clamp_min_18 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_797: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_796, torch.float32);  convert_element_type_796 = None
        reciprocal_14: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_797);  convert_element_type_797 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_466: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_108, [-1, 2, 128]);  mm_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_467: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_15, [-1, 2, 128])
        sym_size_int_22: "Sym(u23)" = torch.ops.aten.sym_size.int(view_467, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_6: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_140: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_65 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 174, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_140, 'Q': view_466, 'K': view_467, 'V': view_467, 'total_seq_len_kv': _local_scalar_dense_11, 'Out': full_6, 'seq_offsets_q': mul_316, 'seq_offsets': asynchronous_complete_cumsum_5, 'max_seq_len': _local_scalar_dense_9, 'attn_scale': reciprocal_14}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_140 = full_6 = mul_316 = asynchronous_complete_cumsum_5 = None
        getitem_232: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_65['Out']
        getitem_233: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_65['seq_offsets_q']
        getitem_234: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_65['seq_offsets'];  triton_kernel_wrapper_functional_proxy_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_469: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_232, [-1, 256]);  getitem_232 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_463: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_469, view_462);  view_469 = view_462 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_798: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_463, torch.float32)
        var_mean_6 = torch.ops.aten.var_mean.correction(convert_element_type_798, [1], correction = 0, keepdim = True)
        getitem_235: "f32[160, 1][1, 1]cuda:0" = var_mean_6[0]
        getitem_236: "f32[160, 1][1, 1]cuda:0" = var_mean_6[1];  var_mean_6 = None
        add_464: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_235, 1e-05);  getitem_235 = None
        rsqrt_58: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_464);  add_464 = None
        sub_157: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_798, getitem_236)
        mul_319: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_157, rsqrt_58);  sub_157 = None
        mul_320: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_319, primals_287);  mul_319 = None
        add_465: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_320, primals_288);  mul_320 = primals_288 = None
        view_470: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_465, [10, 4096]);  add_465 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_799: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_470, torch.bfloat16)
        convert_element_type_800: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_289, torch.bfloat16);  primals_289 = None
        permute_173: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_800, [1, 0]);  convert_element_type_800 = None
        mm_109: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_799, permute_173)
        convert_element_type_803: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_109, torch.float32)
        neg_60: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_803)
        exp_60: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_60);  neg_60 = None
        add_466: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_60, 1);  exp_60 = None
        div_63: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_803, add_466);  convert_element_type_803 = add_466 = None
        convert_element_type_804: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_63, torch.bfloat16);  div_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_805: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_470, torch.bfloat16);  view_470 = None
        convert_element_type_806: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_290, torch.bfloat16);  primals_290 = None
        permute_174: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_806, [1, 0]);  convert_element_type_806 = None
        mm_110: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_805, permute_174)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_321: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_804, mm_110)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_809: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_291, torch.bfloat16);  primals_291 = None
        permute_175: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_809, [1, 0]);  convert_element_type_809 = None
        mm_111: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_321, permute_175)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_471: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_111, [160, 256]);  mm_111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_467: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_463, view_471);  add_463 = view_471 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_472: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_467, [10, 16, 256]);  add_467 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_473: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_472, [-1, 256]);  view_472 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_141: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_142: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_66 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 176, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_473, 'Y': empty_141, 'W': primals_292, 'Rstd': empty_142}, tensors_to_clone = ['Y', 'Rstd']);  empty_141 = empty_142 = None
        getitem_237: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_66['Y']
        getitem_238: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_66['Rstd'];  triton_kernel_wrapper_functional_proxy_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_812: "bf16[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_293, torch.bfloat16);  primals_293 = None
        permute_177: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_812, [1, 0]);  convert_element_type_812 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        view_475: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_198, [10, 64, 256]);  getitem_198 = None
        permute_178: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_475, [0, 2, 1]);  view_475 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_41: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_178, memory_format = torch.contiguous_format);  permute_178 = None
        view_476: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_41, [2560, 64]);  clone_41 = None
        mm_112: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.mm.default(view_476, permute_177)
        view_477: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.view.default(mm_112, [10, 256, 16]);  mm_112 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_179: "bf16[10, 16, 256][4096, 1, 16]cuda:0" = torch.ops.aten.permute.default(view_477, [0, 2, 1]);  view_477 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:270 in forward, code: q_offsets = q_offsets_base * n_query
        mul_322: "i64[3][1]cuda:0" = torch.ops.aten.mul.Tensor(primals_128, 16);  primals_128 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        clone_42: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(permute_179, memory_format = torch.contiguous_format);  permute_179 = None
        view_478: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_42, [160, 256]);  clone_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_479: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_478, [-1, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_143: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_144: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_67 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 178, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_479, 'Y': empty_143, 'W': primals_294, 'Rstd': empty_144}, tensors_to_clone = ['Y', 'Rstd']);  empty_143 = empty_144 = None
        getitem_239: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_67['Y']
        getitem_240: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_67['Rstd'];  triton_kernel_wrapper_functional_proxy_67 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        convert_element_type_815: "bf16[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_295, torch.bfloat16);  primals_295 = None
        permute_180: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_815, [1, 0]);  convert_element_type_815 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_481: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_239, [160, 256]);  getitem_239 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        mm_113: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_481, permute_180)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:52 in _prepare_attn_scale, code: torch.clamp(torch.tensor(max_len, device=device, dtype=dtype), min=1)
        scalar_tensor_7: "bf16[][]cpu" = torch.ops.aten.scalar_tensor.default(_local_scalar_dense_13, dtype = torch.bfloat16, device = device(type='cpu'), pin_memory = False)
        device_put_15: "bf16[][]cuda:0" = torch.ops.prims.device_put.default(scalar_tensor_7, device(type='cuda', index=0));  scalar_tensor_7 = None
        convert_element_type_818: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(device_put_15, torch.bfloat16);  device_put_15 = None
        convert_element_type_819: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_818, torch.float32);  convert_element_type_818 = None
        clamp_min_19: "f32[][]cuda:0" = torch.ops.aten.clamp_min.default(convert_element_type_819, 1);  convert_element_type_819 = None
        convert_element_type_820: "bf16[][]cuda:0" = torch.ops.prims.convert_element_type.default(clamp_min_19, torch.bfloat16);  clamp_min_19 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:51 in _prepare_attn_scale, code: attn_scale = torch.reciprocal(
        convert_element_type_821: "f32[][]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_820, torch.float32);  convert_element_type_820 = None
        reciprocal_15: "f32[][]cuda:0" = torch.ops.aten.reciprocal.default(convert_element_type_821);  convert_element_type_821 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_482: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(mm_113, [-1, 2, 128]);  mm_113 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_483: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(addmm_17, [-1, 2, 128])
        sym_size_int_23: "Sym(u31)" = torch.ops.aten.sym_size.int(view_483, 0)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1815 in triton_hstu_cross_attn_fwd, code: out = torch.zeros(total_seq_len_q, H, DimV, device=q.device, dtype=q.dtype)
        full_7: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:667 in get_fwd_tma_workspace, code: return torch.empty(
        empty_145: "u8[12288][1]cuda:0" = torch.ops.aten.empty.memory_format([12288], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1869 in triton_hstu_cross_attn_fwd, code: _attn_fwd_triton_spec[grid](
        triton_kernel_wrapper_functional_proxy_68 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 18, constant_args_idx = 180, grid = [(2, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_145, 'Q': view_482, 'K': view_483, 'V': view_483, 'total_seq_len_kv': _local_scalar_dense_15, 'Out': full_7, 'seq_offsets_q': mul_322, 'seq_offsets': asynchronous_complete_cumsum_7, 'max_seq_len': _local_scalar_dense_13, 'attn_scale': reciprocal_15}, tensors_to_clone = ['Out', 'seq_offsets_q', 'seq_offsets']);  empty_145 = full_7 = mul_322 = asynchronous_complete_cumsum_7 = None
        getitem_241: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_68['Out']
        getitem_242: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_68['seq_offsets_q']
        getitem_243: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_68['seq_offsets'];  triton_kernel_wrapper_functional_proxy_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_485: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_241, [-1, 256]);  getitem_241 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:212 in forward, code: attn_res = attn_res + x
        add_472: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_485, view_478);  view_485 = view_478 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        convert_element_type_822: "f32[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_472, torch.float32)
        var_mean_7 = torch.ops.aten.var_mean.correction(convert_element_type_822, [1], correction = 0, keepdim = True)
        getitem_244: "f32[160, 1][1, 1]cuda:0" = var_mean_7[0]
        getitem_245: "f32[160, 1][1, 1]cuda:0" = var_mean_7[1];  var_mean_7 = None
        add_473: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_244, 1e-05);  getitem_244 = None
        rsqrt_59: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_473);  add_473 = None
        sub_159: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_822, getitem_245)
        mul_325: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_159, rsqrt_59);  sub_159 = None
        mul_326: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_325, primals_296);  mul_325 = None
        add_474: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_326, primals_297);  mul_326 = primals_297 = None
        view_486: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(add_474, [10, 4096]);  add_474 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_823: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_486, torch.bfloat16)
        convert_element_type_824: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_298, torch.bfloat16);  primals_298 = None
        permute_181: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_824, [1, 0]);  convert_element_type_824 = None
        mm_114: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_823, permute_181)
        convert_element_type_827: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_114, torch.float32)
        neg_61: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_827)
        exp_61: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_61);  neg_61 = None
        add_475: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_61, 1);  exp_61 = None
        div_64: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_827, add_475);  convert_element_type_827 = add_475 = None
        convert_element_type_828: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_64, torch.bfloat16);  div_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        convert_element_type_829: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_486, torch.bfloat16);  view_486 = None
        convert_element_type_830: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_299, torch.bfloat16);  primals_299 = None
        permute_182: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(convert_element_type_830, [1, 0]);  convert_element_type_830 = None
        mm_115: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_829, permute_182)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_327: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_828, mm_115)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        convert_element_type_833: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_300, torch.bfloat16);  primals_300 = None
        permute_183: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_833, [1, 0]);  convert_element_type_833 = None
        mm_116: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_327, permute_183)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_487: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_116, [160, 256]);  mm_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:218 in forward, code: x = attn_res + ffn_output
        add_476: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_472, view_487);  add_472 = view_487 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_488: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_476, [10, 16, 256]);  add_476 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_489: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_488, [-1, 256]);  view_488 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_146: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_147: "f32[160][1]cuda:0" = torch.ops.aten.empty.memory_format([160], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_69 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 182, grid = [(160, 1, 1), (160, 1, 1), (40, 1, 1), (40, 1, 1), (10, 1, 1), (10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_489, 'Y': empty_146, 'W': primals_301, 'Rstd': empty_147}, tensors_to_clone = ['Y', 'Rstd']);  empty_146 = empty_147 = None
        getitem_246: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_69['Y']
        getitem_247: "f32[160][1]cuda:0" = triton_kernel_wrapper_functional_proxy_69['Rstd'];  triton_kernel_wrapper_functional_proxy_69 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_491: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_237, [10, 16, 256]);  getitem_237 = None
        view_492: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_246, [10, 16, 256]);  getitem_246 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        cat_36: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_491, view_492], 1)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        cat_37: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_491, view_492], 1);  view_491 = view_492 = None
        view_493: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.view.default(cat_37, [10, 2, 16, 256]);  cat_37 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:293 in forward, code: triu_indices = torch.triu_indices(
        iota_7: "i64[1][1]cuda:0" = torch.ops.prims.iota.default(1, start = 0, step = 1, dtype = torch.int64, device = device(type='cuda', index=0), requires_grad = False)
        mul_328: "i64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(iota_7, 1);  iota_7 = None
        add_477: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(mul_328, 0);  mul_328 = None
        convert_element_type_836: "f64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(add_477, torch.float64);  add_477 = None
        mul_329: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_836, 2)
        sub_160: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(2.25, mul_329);  mul_329 = None
        sqrt_3: "f64[1][1]cuda:0" = torch.ops.aten.sqrt.default(sub_160);  sub_160 = None
        sub_161: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1.5, sqrt_3);  sqrt_3 = None
        floor_6: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_161);  sub_161 = None
        sub_162: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(1, floor_6)
        mul_330: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(sub_162, floor_6);  sub_162 = None
        mul_331: "f64[1][1]cuda:0" = torch.ops.aten.mul.Tensor(mul_330, 0.5);  mul_330 = None
        sub_163: "f64[1][1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_836, mul_331);  convert_element_type_836 = mul_331 = None
        floor_7: "f64[1][1]cuda:0" = torch.ops.aten.floor.default(sub_163);  sub_163 = None
        convert_element_type_837: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_6, torch.int64);  floor_6 = None
        convert_element_type_838: "i64[1][1]cuda:0" = torch.ops.prims.convert_element_type.default(floor_7, torch.int64);  floor_7 = None
        add_478: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_837, 0);  convert_element_type_837 = None
        add_479: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_838, 1);  convert_element_type_838 = None
        clone_43: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_478);  add_478 = None
        clone_44: "i64[1][1]cuda:0" = torch.ops.aten.clone.default(add_479);  add_479 = None
        cat_38: "i64[2][1]cuda:0" = torch.ops.aten.cat.default([clone_43, clone_44]);  clone_43 = clone_44 = None
        view_494: "i64[2, 1][1, 1]cuda:0" = torch.ops.aten.view.default(cat_38, [2, 1]);  cat_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:299 in forward, code: i_indices, j_indices = triu_indices.unbind()
        unbind_3 = torch.ops.aten.unbind.int(view_494);  view_494 = None
        getitem_248: "i64[1][1]cuda:0" = unbind_3[0]
        getitem_249: "i64[1][1]cuda:0" = unbind_3[1];  unbind_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        index_14: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_493, [None, getitem_248])
        index_15: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.index.Tensor(view_493, [None, getitem_249]);  view_493 = None
        mul_332: "bf16[10, 1, 16, 256][4096, 4096, 256, 1]cuda:0" = torch.ops.aten.mul.Tensor(index_14, index_15)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_495: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(mul_332, [10, -1, 256]);  mul_332 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        cat_39: "bf16[10, 112, 256][28672, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_458, cat_36, view_495], 1);  view_458 = cat_36 = view_495 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_184: "bf16[10, 256, 112][28672, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_39, [0, 2, 1]);  cat_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_839: "bf16[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_302, torch.bfloat16);  primals_302 = None
        permute_185: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(convert_element_type_839, [1, 0]);  convert_element_type_839 = None
        clone_45: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.clone.default(permute_184, memory_format = torch.contiguous_format);  permute_184 = None
        view_496: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.view.default(clone_45, [2560, 112]);  clone_45 = None
        mm_117: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_496, permute_185)
        view_497: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_117, [10, 256, 64]);  mm_117 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_186: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_497, [0, 2, 1]);  view_497 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_187: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_186, [0, 2, 1])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        convert_element_type_842: "bf16[48, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_303, torch.bfloat16);  primals_303 = None
        permute_188: "bf16[64, 48][1, 64]cuda:0" = torch.ops.aten.permute.default(convert_element_type_842, [1, 0]);  convert_element_type_842 = None
        view_498: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(permute_187, [2560, 64]);  permute_187 = None
        mm_118: "bf16[2560, 48][48, 1]cuda:0" = torch.ops.aten.mm.default(view_498, permute_188)
        view_499: "bf16[10, 256, 48][12288, 48, 1]cuda:0" = torch.ops.aten.view.default(mm_118, [10, 256, 48]);  mm_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_189: "bf16[10, 48, 256][12288, 1, 48]cuda:0" = torch.ops.aten.permute.default(view_499, [0, 2, 1]);  view_499 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        split_with_sizes_15 = torch.ops.aten.split_with_sizes.default(permute_189, [32, 16], 1);  permute_189 = None
        getitem_250: "bf16[10, 32, 256][12288, 1, 48]cuda:0" = split_with_sizes_15[0]
        getitem_251: "bf16[10, 16, 256][12288, 1, 48]cuda:0" = split_with_sizes_15[1];  split_with_sizes_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        clone_46: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_250, memory_format = torch.contiguous_format);  getitem_250 = None
        view_500: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.view.default(clone_46, [10, 8192]);  clone_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_845: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_304, torch.bfloat16);  primals_304 = None
        permute_190: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_845, [1, 0]);  convert_element_type_845 = None
        mm_119: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_500, permute_190)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_501: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(mm_119, [-1, 512]);  mm_119 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_848: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_501, torch.float32);  view_501 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_53: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_848, 2)
        mean_52: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_53, [1], True);  pow_53 = None
        add_480: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_52, 1e-05);  mean_52 = None
        rsqrt_60: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_480);  add_480 = None
        mul_333: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_848, rsqrt_60)
        mul_334: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_333, primals_305);  mul_333 = None
        alias_54: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_60);  rsqrt_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_849: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_334, torch.bfloat16);  mul_334 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_502: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_849, [10, 512]);  convert_element_type_849 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_850: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_502, torch.float32)
        neg_62: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_850)
        exp_62: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_62);  neg_62 = None
        add_481: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_62, 1);  exp_62 = None
        div_66: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_850, add_481);  convert_element_type_850 = add_481 = None
        convert_element_type_851: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_66, torch.bfloat16);  div_66 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        convert_element_type_852: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_306, torch.bfloat16);  primals_306 = None
        permute_191: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(convert_element_type_852, [1, 0]);  convert_element_type_852 = None
        mm_120: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_851, permute_191)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_503: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_120, [10, -1, 64]);  mm_120 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        expand_40: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.expand.default(view_503, [10, 32, 64]);  view_503 = None
        view_504: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_40, [10, 32, 64]);  expand_40 = None
        expand_41: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.expand.default(permute_186, [10, 64, 256])
        view_505: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.view.default(expand_41, [10, 64, 256]);  expand_41 = None
        bmm_14: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_504, view_505)
        view_506: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_14, [10, 32, 256]);  bmm_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        permute_192: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(permute_186, [0, 2, 1]);  permute_186 = None
        expand_42: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_506, [10, 32, 256]);  view_506 = None
        view_507: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_42, [10, 32, 256]);  expand_42 = None
        expand_43: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.expand.default(permute_192, [10, 256, 64]);  permute_192 = None
        view_508: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(expand_43, [10, 256, 64]);  expand_43 = None
        bmm_15: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_507, view_508)
        view_509: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_15, [10, 32, 64]);  bmm_15 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_510: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_509, [10, -1]);  view_509 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        clone_47: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.clone.default(getitem_251, memory_format = torch.contiguous_format);  getitem_251 = None
        view_511: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(clone_47, [10, 4096]);  clone_47 = None
        cat_40: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.aten.cat.default([view_510, view_511], 1);  view_510 = view_511 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        cat_41: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.cat.default([cat_40, view_48], 1);  cat_40 = view_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_512: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(cat_41, [-1, 8472]);  cat_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:992 in forward, code: y = torch.empty_like(x)
        empty_148: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:993 in forward, code: rstd = torch.empty((N,), dtype=torch.float32, device=x.device)
        empty_149: "f32[10][1]cuda:0" = torch.ops.aten.empty.memory_format([10], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1010 in forward, code: _weighted_rms_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_70 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 4, constant_args_idx = 184, grid = [(10, 1, 1), (10, 1, 1), (3, 1, 1), (3, 1, 1), (1, 1, 1), (1, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_512, 'Y': empty_148, 'W': primals_307, 'Rstd': empty_149}, tensors_to_clone = ['Y', 'Rstd']);  empty_148 = empty_149 = None
        getitem_252: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_70['Y']
        getitem_253: "f32[10][1]cuda:0" = triton_kernel_wrapper_functional_proxy_70['Rstd'];  triton_kernel_wrapper_functional_proxy_70 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_514: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_252, [10, 8472]);  getitem_252 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_859: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_514, torch.bfloat16);  view_514 = None
        convert_element_type_860: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_308, torch.bfloat16);  primals_308 = None
        permute_193: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(convert_element_type_860, [1, 0]);  convert_element_type_860 = None
        mm_121: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_859, permute_193)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_515: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(mm_121, [-1, 2048]);  mm_121 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_863: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_515, torch.float32);  view_515 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_54: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_863, 2)
        mean_53: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_54, [1], True);  pow_54 = None
        add_482: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_53, 1e-05);  mean_53 = None
        rsqrt_61: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_482);  add_482 = None
        mul_335: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_863, rsqrt_61)
        mul_336: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_335, primals_309);  mul_335 = None
        alias_55: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_61);  rsqrt_61 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_864: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_336, torch.bfloat16);  mul_336 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_516: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_864, [10, 2048]);  convert_element_type_864 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_865: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_516, torch.float32)
        neg_63: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_865)
        exp_63: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_63);  neg_63 = None
        add_483: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_63, 1);  exp_63 = None
        div_67: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_865, add_483);  convert_element_type_865 = add_483 = None
        convert_element_type_866: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_67, torch.bfloat16);  div_67 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_867: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_310, torch.bfloat16);  primals_310 = None
        permute_194: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_867, [1, 0]);  convert_element_type_867 = None
        mm_122: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_866, permute_194)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_517: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_122, [-1, 1024]);  mm_122 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_870: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_517, torch.float32);  view_517 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_55: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_870, 2)
        mean_54: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_55, [1], True);  pow_55 = None
        add_484: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_54, 1e-05);  mean_54 = None
        rsqrt_62: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_484);  add_484 = None
        mul_337: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_870, rsqrt_62)
        mul_338: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_337, primals_311);  mul_337 = None
        alias_56: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_62);  rsqrt_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_871: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_338, torch.bfloat16);  mul_338 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_518: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_871, [10, 1024]);  convert_element_type_871 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_872: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_518, torch.float32)
        neg_64: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_872)
        exp_64: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_64);  neg_64 = None
        add_485: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_64, 1);  exp_64 = None
        div_68: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_872, add_485);  convert_element_type_872 = add_485 = None
        convert_element_type_873: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_68, torch.bfloat16);  div_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_874: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_312, torch.bfloat16);  primals_312 = None
        permute_195: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_874, [1, 0]);  convert_element_type_874 = None
        mm_123: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_873, permute_195)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_486: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_866, mm_123);  mm_123 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_519: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_486, [-1, 2048]);  add_486 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_877: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_519, torch.float32);  view_519 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_56: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_877, 2)
        mean_55: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_56, [1], True);  pow_56 = None
        add_487: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_55, 1e-05);  mean_55 = None
        rsqrt_63: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_487);  add_487 = None
        mul_339: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_877, rsqrt_63)
        mul_340: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_339, primals_313);  mul_339 = None
        alias_57: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_63);  rsqrt_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_878: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_340, torch.bfloat16);  mul_340 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_520: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_878, [10, 2048]);  convert_element_type_878 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_879: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_520, torch.float32)
        neg_65: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_879)
        exp_65: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_65);  neg_65 = None
        add_488: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_65, 1);  exp_65 = None
        div_69: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_879, add_488);  convert_element_type_879 = add_488 = None
        convert_element_type_880: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_69, torch.bfloat16);  div_69 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_881: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_314, torch.bfloat16);  primals_314 = None
        permute_196: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(convert_element_type_881, [1, 0]);  convert_element_type_881 = None
        mm_124: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_880, permute_196)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_521: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(mm_124, [-1, 1024]);  mm_124 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_884: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_521, torch.float32);  view_521 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_57: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_884, 2)
        mean_56: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_57, [1], True);  pow_57 = None
        add_489: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_56, 1e-05);  mean_56 = None
        rsqrt_64: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_489);  add_489 = None
        mul_341: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_884, rsqrt_64)
        mul_342: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_341, primals_315);  mul_341 = None
        alias_58: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_64);  rsqrt_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_885: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_342, torch.bfloat16);  mul_342 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_522: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_885, [10, 1024]);  convert_element_type_885 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_886: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_522, torch.float32)
        neg_66: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_886)
        exp_66: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_66);  neg_66 = None
        add_490: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_66, 1);  exp_66 = None
        div_70: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_886, add_490);  convert_element_type_886 = add_490 = None
        convert_element_type_887: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_70, torch.bfloat16);  div_70 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        convert_element_type_888: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(primals_316, torch.bfloat16);  primals_316 = None
        permute_197: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_888, [1, 0]);  convert_element_type_888 = None
        mm_125: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_887, permute_197)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:172 in forward, code: residual_tensor = residual_tensor + output
        add_491: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_880, mm_125);  mm_125 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_523: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(add_491, [-1, 2048]);  add_491 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_891: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_523, torch.float32);  view_523 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        pow_58: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_891, 2)
        mean_57: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_58, [1], True);  pow_58 = None
        add_492: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_57, 1e-05);  mean_57 = None
        rsqrt_65: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_492);  add_492 = None
        mul_343: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_891, rsqrt_65)
        mul_344: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_343, primals_317);  mul_343 = None
        alias_59: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_65);  rsqrt_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_892: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_344, torch.bfloat16);  mul_344 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_524: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_892, [10, 2048]);  convert_element_type_892 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_893: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_524, torch.float32)
        neg_67: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_893)
        exp_67: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_67);  neg_67 = None
        add_493: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_67, 1);  exp_67 = None
        div_71: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(convert_element_type_893, add_493);  convert_element_type_893 = add_493 = None
        convert_element_type_894: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(div_71, torch.bfloat16);  div_71 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        convert_element_type_895: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_894, torch.float32);  convert_element_type_894 = None
        pow_59: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_895, 2)
        mean_58: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mean.dim(pow_59, [1], True);  pow_59 = None
        add_494: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.add.Scalar(mean_58, 1.1920928955078125e-07);  mean_58 = None
        rsqrt_66: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_494);  add_494 = None
        alias_60: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(rsqrt_66)
        mul_345: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_895, rsqrt_66)
        mul_346: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_345, primals_318)
        convert_element_type_896: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_346, torch.bfloat16);  mul_346 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:482 in forward, code: return inter_arch_out.to(dense_proj.dtype)  # this would be task arch input
        convert_element_type_897: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_896, torch.float32);  convert_element_type_896 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:702 in forward, code: x = input_tensor.unsqueeze(1).expand(-1, self._num_channels, -1).contiguous()
        unsqueeze_8: "f32[10, 1, 2048][2048, 2048, 1]cuda:0" = torch.ops.aten.unsqueeze.default(convert_element_type_897, 1);  convert_element_type_897 = None
        expand_44: "f32[10, 18, 2048][2048, 0, 1]cuda:0" = torch.ops.aten.expand.default(unsqueeze_8, [-1, 18, -1]);  unsqueeze_8 = None
        clone_48: "f32[10, 18, 2048][36864, 2048, 1]cuda:0" = torch.ops.aten.clone.default(expand_44, memory_format = torch.contiguous_format);  expand_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        unsqueeze_9: "f32[10, 18, 2048, 1][36864, 2048, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(clone_48, 3);  clone_48 = None
        permute_198: "f32[10, 18, 1, 2048][36864, 2048, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_9, [0, 1, 3, 2]);  unsqueeze_9 = None
        unsqueeze_10: "f32[18, 2048, 1024, 1][2097152, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(primals_319, 3);  primals_319 = None
        permute_199: "f32[1, 18, 1024, 2048][1, 2097152, 1, 1024]cuda:0" = torch.ops.aten.permute.default(unsqueeze_10, [3, 0, 2, 1]);  unsqueeze_10 = None
        permute_200: "f32[18, 10, 2048, 1][2048, 36864, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_198, [1, 0, 3, 2]);  permute_198 = None
        view_525: "f32[18, 10, 2048][2048, 36864, 1]cuda:0" = torch.ops.aten.view.default(permute_200, [18, 10, 2048]);  permute_200 = None
        permute_201: "f32[18, 2048, 1024, 1][2097152, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_199, [1, 3, 2, 0]);  permute_199 = None
        view_526: "f32[18, 2048, 1024][2097152, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_201, [18, 2048, 1024]);  permute_201 = None
        bmm_16: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_525, view_526)
        view_527: "f32[18, 10, 1, 1024][10240, 1024, 1024, 1]cuda:0" = torch.ops.aten.view.default(bmm_16, [18, 10, 1, 1024]);  bmm_16 = None
        permute_202: "f32[10, 18, 1024, 1][1024, 10240, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_527, [1, 0, 3, 2]);  view_527 = None
        view_528: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.view.default(permute_202, [10, 18, 1024]);  permute_202 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        clone_49: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.clone.default(view_528, memory_format = torch.contiguous_format)
        var_mean_8 = torch.ops.aten.var_mean.correction(clone_49, [1, 2], correction = 0, keepdim = True)
        getitem_254: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_8[0]
        getitem_255: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_8[1];  var_mean_8 = None
        add_495: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_254, 1e-05);  getitem_254 = None
        rsqrt_67: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_495);  add_495 = None
        sub_164: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(clone_49, getitem_255);  clone_49 = None
        mul_347: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_164, rsqrt_67);  sub_164 = None
        mul_348: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_347, primals_320);  mul_347 = None
        add_496: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_348, primals_321);  mul_348 = primals_321 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_68: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_496)
        exp_68: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_68);  neg_68 = None
        add_497: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_68, 1);  exp_68 = None
        div_72: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.div.Tensor(add_496, add_497);  add_497 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        unsqueeze_11: "f32[10, 18, 1024, 1][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div_72, 3)
        permute_203: "f32[10, 18, 1, 1024][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_11, [0, 1, 3, 2]);  unsqueeze_11 = None
        unsqueeze_12: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(primals_322, 3);  primals_322 = None
        permute_204: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(unsqueeze_12, [3, 0, 2, 1]);  unsqueeze_12 = None
        permute_205: "f32[18, 10, 1024, 1][1024, 18432, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_203, [1, 0, 3, 2]);  permute_203 = None
        view_529: "f32[18, 10, 1024][1024, 18432, 1]cuda:0" = torch.ops.aten.view.default(permute_205, [18, 10, 1024]);  permute_205 = None
        permute_206: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_204, [1, 3, 2, 0]);  permute_204 = None
        view_530: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_206, [18, 1024, 1024]);  permute_206 = None
        bmm_17: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_529, view_530)
        view_531: "f32[18, 10, 1, 1024][10240, 1024, 1024, 1]cuda:0" = torch.ops.aten.view.default(bmm_17, [18, 10, 1, 1024]);  bmm_17 = None
        permute_207: "f32[10, 18, 1024, 1][1024, 10240, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_531, [1, 0, 3, 2]);  view_531 = None
        view_532: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.view.default(permute_207, [10, 18, 1024]);  permute_207 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        clone_50: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.clone.default(view_532, memory_format = torch.contiguous_format)
        var_mean_9 = torch.ops.aten.var_mean.correction(clone_50, [1, 2], correction = 0, keepdim = True)
        getitem_256: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_9[0]
        getitem_257: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_9[1];  var_mean_9 = None
        add_498: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_256, 1e-05);  getitem_256 = None
        rsqrt_68: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_498);  add_498 = None
        sub_165: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(clone_50, getitem_257);  clone_50 = None
        mul_349: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_165, rsqrt_68);  sub_165 = None
        mul_350: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_349, primals_323);  mul_349 = None
        add_499: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_350, primals_324);  mul_350 = primals_324 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_69: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_499)
        exp_69: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_69);  neg_69 = None
        add_500: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_69, 1);  exp_69 = None
        div_73: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.div.Tensor(add_499, add_500);  add_500 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        unsqueeze_13: "f32[10, 18, 1024, 1][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div_73, 3);  div_73 = None
        permute_208: "f32[10, 18, 1, 1024][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_13, [0, 1, 3, 2]);  unsqueeze_13 = None
        unsqueeze_14: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(primals_325, 3);  primals_325 = None
        permute_209: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(unsqueeze_14, [3, 0, 2, 1]);  unsqueeze_14 = None
        permute_210: "f32[18, 10, 1024, 1][1024, 18432, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_208, [1, 0, 3, 2]);  permute_208 = None
        view_533: "f32[18, 10, 1024][1024, 18432, 1]cuda:0" = torch.ops.aten.view.default(permute_210, [18, 10, 1024]);  permute_210 = None
        permute_211: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_209, [1, 3, 2, 0]);  permute_209 = None
        view_534: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_211, [18, 1024, 1024]);  permute_211 = None
        bmm_18: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_533, view_534)
        view_535: "f32[18, 10, 1, 1024][10240, 1024, 1024, 1]cuda:0" = torch.ops.aten.view.default(bmm_18, [18, 10, 1, 1024]);  bmm_18 = None
        permute_212: "f32[10, 18, 1024, 1][1024, 10240, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_535, [1, 0, 3, 2]);  view_535 = None
        view_536: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.view.default(permute_212, [10, 18, 1024]);  permute_212 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        clone_51: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.clone.default(view_536, memory_format = torch.contiguous_format)
        var_mean_10 = torch.ops.aten.var_mean.correction(clone_51, [1, 2], correction = 0, keepdim = True)
        getitem_258: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_10[0]
        getitem_259: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_10[1];  var_mean_10 = None
        add_501: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_258, 1e-05);  getitem_258 = None
        rsqrt_69: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_501);  add_501 = None
        sub_166: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(clone_51, getitem_259);  clone_51 = None
        mul_351: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_166, rsqrt_69);  sub_166 = None
        mul_352: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_351, primals_326);  mul_351 = None
        add_502: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_352, primals_327);  mul_352 = primals_327 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_70: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_502)
        exp_70: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_70);  neg_70 = None
        add_503: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_70, 1);  exp_70 = None
        div_74: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.div.Tensor(add_502, add_503);  add_503 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:726 in forward, code: x = residual_tensor + x
        add_504: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(div_72, div_74);  div_72 = div_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        unsqueeze_15: "f32[10, 18, 1024, 1][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(add_504, 3);  add_504 = None
        permute_213: "f32[10, 18, 1, 1024][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_15, [0, 1, 3, 2]);  unsqueeze_15 = None
        unsqueeze_16: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(primals_328, 3);  primals_328 = None
        permute_214: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(unsqueeze_16, [3, 0, 2, 1]);  unsqueeze_16 = None
        permute_215: "f32[18, 10, 1024, 1][1024, 18432, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_213, [1, 0, 3, 2]);  permute_213 = None
        view_537: "f32[18, 10, 1024][1024, 18432, 1]cuda:0" = torch.ops.aten.view.default(permute_215, [18, 10, 1024]);  permute_215 = None
        permute_216: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_214, [1, 3, 2, 0]);  permute_214 = None
        view_538: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_216, [18, 1024, 1024]);  permute_216 = None
        bmm_19: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_537, view_538)
        view_539: "f32[18, 10, 1, 1024][10240, 1024, 1024, 1]cuda:0" = torch.ops.aten.view.default(bmm_19, [18, 10, 1, 1024]);  bmm_19 = None
        permute_217: "f32[10, 18, 1024, 1][1024, 10240, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_539, [1, 0, 3, 2]);  view_539 = None
        view_540: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.view.default(permute_217, [10, 18, 1024]);  permute_217 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        clone_52: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.clone.default(view_540, memory_format = torch.contiguous_format)
        var_mean_11 = torch.ops.aten.var_mean.correction(clone_52, [1, 2], correction = 0, keepdim = True)
        getitem_260: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_11[0]
        getitem_261: "f32[10, 1, 1][1, 1, 1]cuda:0" = var_mean_11[1];  var_mean_11 = None
        add_505: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_260, 1e-05);  getitem_260 = None
        rsqrt_70: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.rsqrt.default(add_505);  add_505 = None
        sub_167: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(clone_52, getitem_261);  clone_52 = None
        mul_353: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_167, rsqrt_70);  sub_167 = None
        mul_354: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_353, primals_329);  mul_353 = None
        add_506: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_354, primals_330);  mul_354 = primals_330 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_71: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_506)
        exp_71: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_71);  neg_71 = None
        add_507: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_71, 1);  exp_71 = None
        div_75: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.div.Tensor(add_506, add_507);  add_507 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:732 in forward, code: x = torch.einsum("bci,cio->bco", x, self._output_weight)
        unsqueeze_17: "f32[10, 18, 1024, 1][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(div_75, 3);  div_75 = None
        permute_218: "f32[10, 18, 1, 1024][18432, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_17, [0, 1, 3, 2]);  unsqueeze_17 = None
        unsqueeze_18: "f32[18, 1024, 1, 1][1024, 1, 1, 1]cuda:0" = torch.ops.aten.unsqueeze.default(primals_331, 3);  primals_331 = None
        permute_219: "f32[1, 18, 1, 1024][1, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(unsqueeze_18, [3, 0, 2, 1]);  unsqueeze_18 = None
        permute_220: "f32[18, 10, 1024, 1][1024, 18432, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_218, [1, 0, 3, 2]);  permute_218 = None
        view_541: "f32[18, 10, 1024][1024, 18432, 1]cuda:0" = torch.ops.aten.view.default(permute_220, [18, 10, 1024]);  permute_220 = None
        permute_221: "f32[18, 1024, 1, 1][1024, 1, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_219, [1, 3, 2, 0]);  permute_219 = None
        view_542: "f32[18, 1024, 1][1024, 1, 1]cuda:0" = torch.ops.aten.view.default(permute_221, [18, 1024, 1]);  permute_221 = None
        bmm_20: "f32[18, 10, 1][10, 1, 1]cuda:0" = torch.ops.aten.bmm.default(view_541, view_542)
        view_543: "f32[18, 10, 1, 1][10, 1, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_20, [18, 10, 1, 1])
        permute_222: "f32[10, 18, 1, 1][1, 10, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_543, [1, 0, 3, 2]);  view_543 = None
        view_544: "f32[10, 18, 1][1, 10, 1]cuda:0" = torch.ops.aten.view.default(permute_222, [10, 18, 1]);  permute_222 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:745 in forward, code: output = output.squeeze(-1)  # [B, C]
        squeeze: "f32[10, 18][1, 10]cuda:0" = torch.ops.aten.squeeze.dim(view_544, -1);  view_544 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:192 in <listcomp>, code: logits = [task_outputs[:, i] for i in range(self._num_tasks)]
        select: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 0)
        select_1: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 1)
        select_2: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 2)
        select_3: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 3)
        select_4: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 4)
        select_5: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 5)
        select_6: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 6)
        select_7: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 7)
        select_8: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 8)
        select_9: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 9)
        select_10: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 10)
        select_11: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 11)
        select_12: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 12)
        select_13: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 13)
        select_14: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 14)
        select_15: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 15)
        select_16: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 16)
        select_17: "f32[10][1]cuda:0" = torch.ops.aten.select.int(squeeze, 1, 17);  squeeze = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:732 in forward, code: x = torch.einsum("bci,cio->bco", x, self._output_weight)
        permute_223: "f32[18, 1024, 10][1024, 1, 18432]cuda:0" = torch.ops.aten.permute.default(view_541, [0, 2, 1]);  view_541 = None
        bmm_21: "f32[18, 1024, 1][1024, 1, 1]cuda:0" = torch.ops.aten.bmm.default(permute_223, tangents_3);  permute_223 = None
        permute_224: "f32[18, 1, 1024][1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_542, [0, 2, 1]);  view_542 = None
        bmm_22: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(tangents_3, permute_224);  tangents_3 = permute_224 = None
        view_545: "f32[18, 1024, 1, 1][1024, 1, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_21, [18, 1024, 1, 1]);  bmm_21 = None
        permute_225: "f32[1, 18, 1, 1024][1, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_545, [3, 0, 2, 1]);  view_545 = None
        view_546: "f32[18, 10, 1024, 1][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_22, [18, 10, 1024, 1]);  bmm_22 = None
        permute_226: "f32[10, 18, 1, 1024][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_546, [1, 0, 3, 2]);  view_546 = None
        permute_227: "f32[18, 1024, 1, 1][1024, 1, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_225, [1, 3, 2, 0]);  permute_225 = None
        squeeze_1: "f32[18, 1024, 1][1024, 1, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_227, 3);  permute_227 = None
        permute_228: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_226, [0, 1, 3, 2]);  permute_226 = None
        squeeze_2: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_228, 3);  permute_228 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_72: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_506)
        exp_72: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_72);  neg_72 = None
        add_508: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_72, 1);  exp_72 = None
        reciprocal_16: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_508);  add_508 = None
        mul_355: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_16, 1);  reciprocal_16 = None
        mul_356: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(squeeze_2, mul_355);  squeeze_2 = None
        sub_168: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_355);  mul_355 = None
        mul_357: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_506, sub_168);  add_506 = sub_168 = None
        add_509: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_357, 1);  mul_357 = None
        mul_358: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_356, add_509);  mul_356 = add_509 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        sub_169: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_540, getitem_261);  view_540 = getitem_261 = None
        mul_359: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_169, rsqrt_70);  sub_169 = None
        mul_360: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_358, primals_329);  primals_329 = None
        mul_361: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_360, 18432)
        sum_1: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_360, [1, 2], True)
        mul_362: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_360, mul_359);  mul_360 = None
        sum_2: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_362, [1, 2], True);  mul_362 = None
        mul_363: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_359, sum_2);  sum_2 = None
        sub_170: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_361, sum_1);  mul_361 = sum_1 = None
        sub_171: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_170, mul_363);  sub_170 = mul_363 = None
        div_76: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_70, 18432);  rsqrt_70 = None
        mul_364: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_76, sub_171);  div_76 = sub_171 = None
        mul_365: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_358, mul_359);  mul_359 = None
        sum_3: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_365, [0]);  mul_365 = None
        sum_4: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_358, [0]);  mul_358 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        view_547: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.view.default(mul_364, [10, 18, 1024, 1]);  mul_364 = None
        permute_229: "f32[18, 10, 1, 1024][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_547, [1, 0, 3, 2]);  view_547 = None
        view_548: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_229, [18, 10, 1024]);  permute_229 = None
        permute_230: "f32[18, 1024, 10][1024, 1, 18432]cuda:0" = torch.ops.aten.permute.default(view_537, [0, 2, 1]);  view_537 = None
        bmm_23: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(permute_230, view_548);  permute_230 = None
        permute_231: "f32[18, 1024, 1024][1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_538, [0, 2, 1]);  view_538 = None
        bmm_24: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_548, permute_231);  view_548 = permute_231 = None
        view_549: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_23, [18, 1024, 1024, 1]);  bmm_23 = None
        permute_232: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_549, [3, 0, 2, 1]);  view_549 = None
        view_550: "f32[18, 10, 1024, 1][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_24, [18, 10, 1024, 1]);  bmm_24 = None
        permute_233: "f32[10, 18, 1, 1024][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_550, [1, 0, 3, 2]);  view_550 = None
        permute_234: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_232, [1, 3, 2, 0]);  permute_232 = None
        squeeze_3: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_234, 3);  permute_234 = None
        permute_235: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_233, [0, 1, 3, 2]);  permute_233 = None
        squeeze_4: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_235, 3);  permute_235 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_73: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_502)
        exp_73: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_73);  neg_73 = None
        add_510: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_73, 1);  exp_73 = None
        reciprocal_17: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_510);  add_510 = None
        mul_366: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_17, 1);  reciprocal_17 = None
        mul_367: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(squeeze_4, mul_366)
        sub_172: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_366);  mul_366 = None
        mul_368: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_502, sub_172);  add_502 = sub_172 = None
        add_511: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_368, 1);  mul_368 = None
        mul_369: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_367, add_511);  mul_367 = add_511 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        sub_173: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_536, getitem_259);  view_536 = getitem_259 = None
        mul_370: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_173, rsqrt_69);  sub_173 = None
        mul_371: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_369, primals_326);  primals_326 = None
        mul_372: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_371, 18432)
        sum_5: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_371, [1, 2], True)
        mul_373: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_371, mul_370);  mul_371 = None
        sum_6: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_373, [1, 2], True);  mul_373 = None
        mul_374: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_370, sum_6);  sum_6 = None
        sub_174: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_372, sum_5);  mul_372 = sum_5 = None
        sub_175: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_174, mul_374);  sub_174 = mul_374 = None
        div_77: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_69, 18432);  rsqrt_69 = None
        mul_375: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_77, sub_175);  div_77 = sub_175 = None
        mul_376: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_369, mul_370);  mul_370 = None
        sum_7: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_376, [0]);  mul_376 = None
        sum_8: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_369, [0]);  mul_369 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        view_551: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.view.default(mul_375, [10, 18, 1024, 1]);  mul_375 = None
        permute_236: "f32[18, 10, 1, 1024][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_551, [1, 0, 3, 2]);  view_551 = None
        view_552: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_236, [18, 10, 1024]);  permute_236 = None
        permute_237: "f32[18, 1024, 10][1024, 1, 18432]cuda:0" = torch.ops.aten.permute.default(view_533, [0, 2, 1]);  view_533 = None
        bmm_25: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(permute_237, view_552);  permute_237 = None
        permute_238: "f32[18, 1024, 1024][1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_534, [0, 2, 1]);  view_534 = None
        bmm_26: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_552, permute_238);  view_552 = permute_238 = None
        view_553: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_25, [18, 1024, 1024, 1]);  bmm_25 = None
        permute_239: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_553, [3, 0, 2, 1]);  view_553 = None
        view_554: "f32[18, 10, 1024, 1][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_26, [18, 10, 1024, 1]);  bmm_26 = None
        permute_240: "f32[10, 18, 1, 1024][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_554, [1, 0, 3, 2]);  view_554 = None
        permute_241: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_239, [1, 3, 2, 0]);  permute_239 = None
        squeeze_5: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_241, 3);  permute_241 = None
        permute_242: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_240, [0, 1, 3, 2]);  permute_240 = None
        squeeze_6: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_242, 3);  permute_242 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_74: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_499)
        exp_74: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_74);  neg_74 = None
        add_512: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_74, 1);  exp_74 = None
        reciprocal_18: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_512);  add_512 = None
        mul_377: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_18, 1);  reciprocal_18 = None
        mul_378: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(squeeze_6, mul_377);  squeeze_6 = None
        sub_176: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_377);  mul_377 = None
        mul_379: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_499, sub_176);  add_499 = sub_176 = None
        add_513: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_379, 1);  mul_379 = None
        mul_380: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_378, add_513);  mul_378 = add_513 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        sub_177: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_532, getitem_257);  view_532 = getitem_257 = None
        mul_381: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_177, rsqrt_68);  sub_177 = None
        mul_382: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_380, primals_323);  primals_323 = None
        mul_383: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_382, 18432)
        sum_9: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_382, [1, 2], True)
        mul_384: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_382, mul_381);  mul_382 = None
        sum_10: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_384, [1, 2], True);  mul_384 = None
        mul_385: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_381, sum_10);  sum_10 = None
        sub_178: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_383, sum_9);  mul_383 = sum_9 = None
        sub_179: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_178, mul_385);  sub_178 = mul_385 = None
        div_78: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_68, 18432);  rsqrt_68 = None
        mul_386: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_78, sub_179);  div_78 = sub_179 = None
        mul_387: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_380, mul_381);  mul_381 = None
        sum_11: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_387, [0]);  mul_387 = None
        sum_12: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_380, [0]);  mul_380 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        view_555: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.view.default(mul_386, [10, 18, 1024, 1]);  mul_386 = None
        permute_243: "f32[18, 10, 1, 1024][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_555, [1, 0, 3, 2]);  view_555 = None
        view_556: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_243, [18, 10, 1024]);  permute_243 = None
        permute_244: "f32[18, 1024, 10][1024, 1, 18432]cuda:0" = torch.ops.aten.permute.default(view_529, [0, 2, 1]);  view_529 = None
        bmm_27: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(permute_244, view_556);  permute_244 = None
        permute_245: "f32[18, 1024, 1024][1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_530, [0, 2, 1]);  view_530 = None
        bmm_28: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(view_556, permute_245);  view_556 = permute_245 = None
        view_557: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_27, [18, 1024, 1024, 1]);  bmm_27 = None
        permute_246: "f32[1, 18, 1024, 1024][1, 1048576, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_557, [3, 0, 2, 1]);  view_557 = None
        view_558: "f32[18, 10, 1024, 1][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_28, [18, 10, 1024, 1]);  bmm_28 = None
        permute_247: "f32[10, 18, 1, 1024][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_558, [1, 0, 3, 2]);  view_558 = None
        permute_248: "f32[18, 1024, 1024, 1][1048576, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_246, [1, 3, 2, 0]);  permute_246 = None
        squeeze_7: "f32[18, 1024, 1024][1048576, 1024, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_248, 3);  permute_248 = None
        permute_249: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_247, [0, 1, 3, 2]);  permute_247 = None
        squeeze_8: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_249, 3);  permute_249 = None
        add_514: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.add.Tensor(squeeze_4, squeeze_8);  squeeze_4 = squeeze_8 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:719 in forward, code: x = torch.nn.functional.silu(x)
        neg_75: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.neg.default(add_496)
        exp_75: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_75);  neg_75 = None
        add_515: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_75, 1);  exp_75 = None
        reciprocal_19: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_515);  add_515 = None
        mul_388: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_19, 1);  reciprocal_19 = None
        mul_389: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_514, mul_388);  add_514 = None
        sub_180: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_388);  mul_388 = None
        mul_390: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(add_496, sub_180);  add_496 = sub_180 = None
        add_516: "f32[10, 18, 1024][18432, 1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_390, 1);  mul_390 = None
        mul_391: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_389, add_516);  mul_389 = add_516 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:32 in pytorch_layer_norm, code: return torch.nn.functional.layer_norm(
        sub_181: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(view_528, getitem_255);  view_528 = getitem_255 = None
        mul_392: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_181, rsqrt_67);  sub_181 = None
        mul_393: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_391, primals_320);  primals_320 = None
        mul_394: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_393, 18432)
        sum_13: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_393, [1, 2], True)
        mul_395: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_393, mul_392);  mul_393 = None
        sum_14: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_395, [1, 2], True);  mul_395 = None
        mul_396: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_392, sum_14);  sum_14 = None
        sub_182: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_394, sum_13);  mul_394 = sum_13 = None
        sub_183: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_182, mul_396);  sub_182 = mul_396 = None
        div_79: "f32[10, 1, 1][1, 1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_67, 18432);  rsqrt_67 = None
        mul_397: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_79, sub_183);  div_79 = sub_183 = None
        mul_398: "f32[10, 18, 1024][1024, 10240, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_391, mul_392);  mul_392 = None
        sum_15: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_398, [0]);  mul_398 = None
        sum_16: "f32[18, 1024][1024, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_391, [0]);  mul_391 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:711 in forward, code: x = torch.einsum("bci,cio->bco", x, weight)
        view_559: "f32[10, 18, 1024, 1][1024, 10240, 1, 1]cuda:0" = torch.ops.aten.view.default(mul_397, [10, 18, 1024, 1]);  mul_397 = None
        permute_250: "f32[18, 10, 1, 1024][10240, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_559, [1, 0, 3, 2]);  view_559 = None
        view_560: "f32[18, 10, 1024][10240, 1024, 1]cuda:0" = torch.ops.aten.view.default(permute_250, [18, 10, 1024]);  permute_250 = None
        permute_251: "f32[18, 2048, 10][2048, 1, 36864]cuda:0" = torch.ops.aten.permute.default(view_525, [0, 2, 1]);  view_525 = None
        bmm_29: "f32[18, 2048, 1024][2097152, 1024, 1]cuda:0" = torch.ops.aten.bmm.default(permute_251, view_560);  permute_251 = None
        permute_252: "f32[18, 1024, 2048][2097152, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_526, [0, 2, 1]);  view_526 = None
        bmm_30: "f32[18, 10, 2048][20480, 2048, 1]cuda:0" = torch.ops.aten.bmm.default(view_560, permute_252);  view_560 = permute_252 = None
        view_561: "f32[18, 2048, 1024, 1][2097152, 1024, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_29, [18, 2048, 1024, 1]);  bmm_29 = None
        permute_253: "f32[1, 18, 1024, 2048][1, 2097152, 1, 1024]cuda:0" = torch.ops.aten.permute.default(view_561, [3, 0, 2, 1]);  view_561 = None
        view_562: "f32[18, 10, 2048, 1][20480, 2048, 1, 1]cuda:0" = torch.ops.aten.view.default(bmm_30, [18, 10, 2048, 1]);  bmm_30 = None
        permute_254: "f32[10, 18, 1, 2048][2048, 20480, 1, 1]cuda:0" = torch.ops.aten.permute.default(view_562, [1, 0, 3, 2]);  view_562 = None
        permute_255: "f32[18, 2048, 1024, 1][2097152, 1024, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_253, [1, 3, 2, 0]);  permute_253 = None
        squeeze_9: "f32[18, 2048, 1024][2097152, 1024, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_255, 3);  permute_255 = None
        permute_256: "f32[10, 18, 2048, 1][2048, 20480, 1, 1]cuda:0" = torch.ops.aten.permute.default(permute_254, [0, 1, 3, 2]);  permute_254 = None
        squeeze_10: "f32[10, 18, 2048][2048, 20480, 1]cuda:0" = torch.ops.aten.squeeze.dim(permute_256, 3);  permute_256 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:702 in forward, code: x = input_tensor.unsqueeze(1).expand(-1, self._num_channels, -1).contiguous()
        sum_17: "f32[10, 1, 2048][2048, 2048, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(squeeze_10, [1], True);  squeeze_10 = None
        squeeze_11: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.squeeze.dim(sum_17, 1);  sum_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:482 in forward, code: return inter_arch_out.to(dense_proj.dtype)  # this would be task arch input
        convert_element_type_898: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(squeeze_11, torch.bfloat16);  squeeze_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        convert_element_type_899: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_898, torch.float32);  convert_element_type_898 = None
        mul_399: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_899, mul_345);  mul_345 = None
        mul_400: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_899, primals_318);  convert_element_type_899 = primals_318 = None
        sum_18: "f32[1, 2048][2048, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_399, [0], True);  mul_399 = None
        view_563: "f32[2048][1]cuda:0" = torch.ops.aten.view.default(sum_18, [2048]);  sum_18 = None
        mul_401: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_400, convert_element_type_895)
        mul_402: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_400, rsqrt_66);  mul_400 = rsqrt_66 = None
        sum_19: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_401, [1], True);  mul_401 = None
        alias_61: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_60);  alias_60 = None
        mul_403: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mul.Scalar(sum_19, -0.5);  sum_19 = None
        pow_60: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(alias_61, 3);  alias_61 = None
        mul_404: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_403, pow_60);  mul_403 = pow_60 = None
        expand_45: "f32[10, 2048][1, 0]cuda:0" = torch.ops.aten.expand.default(mul_404, [10, 2048]);  mul_404 = None
        div_80: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Scalar(expand_45, 2048);  expand_45 = None
        pow_61: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.pow.Tensor_Scalar(convert_element_type_895, 1.0);  convert_element_type_895 = None
        mul_405: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Scalar(pow_61, 2.0);  pow_61 = None
        mul_406: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_80, mul_405);  div_80 = mul_405 = None
        add_517: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_402, mul_406);  mul_402 = mul_406 = None
        convert_element_type_900: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_517, torch.bfloat16);  add_517 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_901: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_900, torch.float32);  convert_element_type_900 = None
        convert_element_type_902: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_524, torch.float32);  view_524 = None
        neg_76: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_902)
        exp_76: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_76);  neg_76 = None
        add_518: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_76, 1);  exp_76 = None
        reciprocal_20: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_518);  add_518 = None
        mul_407: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_20, 1);  reciprocal_20 = None
        mul_408: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_901, mul_407);  convert_element_type_901 = None
        sub_184: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_407);  mul_407 = None
        mul_409: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_902, sub_184);  convert_element_type_902 = sub_184 = None
        add_519: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_409, 1);  mul_409 = None
        mul_410: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_408, add_519);  mul_408 = add_519 = None
        convert_element_type_903: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_410, torch.bfloat16);  mul_410 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_564: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_903, [10, 2048]);  convert_element_type_903 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_904: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_564, torch.float32);  view_564 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_62: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_59);  alias_59 = None
        mul_411: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_904, primals_317);  primals_317 = None
        mul_412: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_891, alias_62);  convert_element_type_891 = None
        mul_413: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_412, mul_411)
        sum_20: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_413, [1], True);  mul_413 = None
        div_81: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_412, 2048)
        mul_414: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_81, sum_20);  div_81 = sum_20 = None
        sub_185: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_411, mul_414);  mul_411 = mul_414 = None
        mul_415: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_185, alias_62);  sub_185 = alias_62 = None
        mul_416: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_904, mul_412);  convert_element_type_904 = mul_412 = None
        sum_21: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_416, [0]);  mul_416 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_905: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_415, torch.bfloat16);  mul_415 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_565: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_905, [10, 2048]);  convert_element_type_905 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_257: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_565, [1, 0])
        mm_126: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_257, convert_element_type_887);  permute_257 = convert_element_type_887 = None
        permute_258: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_126, [1, 0]);  mm_126 = None
        permute_259: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_197, [1, 0]);  permute_197 = None
        mm_127: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_565, permute_259);  permute_259 = None
        permute_260: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_258, [1, 0]);  permute_258 = None
        convert_element_type_910: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_260, torch.float32);  permute_260 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_911: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_127, torch.float32);  mm_127 = None
        convert_element_type_912: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_522, torch.float32);  view_522 = None
        neg_77: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_912)
        exp_77: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_77);  neg_77 = None
        add_520: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_77, 1);  exp_77 = None
        reciprocal_21: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_520);  add_520 = None
        mul_417: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_21, 1);  reciprocal_21 = None
        mul_418: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_911, mul_417);  convert_element_type_911 = None
        sub_186: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_417);  mul_417 = None
        mul_419: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_912, sub_186);  convert_element_type_912 = sub_186 = None
        add_521: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_419, 1);  mul_419 = None
        mul_420: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_418, add_521);  mul_418 = add_521 = None
        convert_element_type_913: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_420, torch.bfloat16);  mul_420 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_566: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_913, [10, 1024]);  convert_element_type_913 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_914: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_566, torch.float32);  view_566 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_63: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_58);  alias_58 = None
        mul_421: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_914, primals_315);  primals_315 = None
        mul_422: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_884, alias_63);  convert_element_type_884 = None
        mul_423: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_422, mul_421)
        sum_22: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_423, [1], True);  mul_423 = None
        div_82: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_422, 1024)
        mul_424: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_82, sum_22);  div_82 = sum_22 = None
        sub_187: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_421, mul_424);  mul_421 = mul_424 = None
        mul_425: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_187, alias_63);  sub_187 = alias_63 = None
        mul_426: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_914, mul_422);  convert_element_type_914 = mul_422 = None
        sum_23: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_426, [0]);  mul_426 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_915: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_425, torch.bfloat16);  mul_425 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_567: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_915, [10, 1024]);  convert_element_type_915 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_261: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_567, [1, 0])
        mm_128: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_261, convert_element_type_880);  permute_261 = convert_element_type_880 = None
        permute_262: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_128, [1, 0]);  mm_128 = None
        permute_263: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_196, [1, 0]);  permute_196 = None
        mm_129: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_567, permute_263);  view_567 = permute_263 = None
        add_522: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_565, mm_129);  view_565 = mm_129 = None
        permute_264: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_262, [1, 0]);  permute_262 = None
        convert_element_type_920: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_264, torch.float32);  permute_264 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_921: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_522, torch.float32);  add_522 = None
        convert_element_type_922: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_520, torch.float32);  view_520 = None
        neg_78: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_922)
        exp_78: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_78);  neg_78 = None
        add_523: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_78, 1);  exp_78 = None
        reciprocal_22: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_523);  add_523 = None
        mul_427: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_22, 1);  reciprocal_22 = None
        mul_428: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_921, mul_427);  convert_element_type_921 = None
        sub_188: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_427);  mul_427 = None
        mul_429: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_922, sub_188);  convert_element_type_922 = sub_188 = None
        add_524: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_429, 1);  mul_429 = None
        mul_430: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_428, add_524);  mul_428 = add_524 = None
        convert_element_type_923: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_430, torch.bfloat16);  mul_430 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_568: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_923, [10, 2048]);  convert_element_type_923 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_924: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_568, torch.float32);  view_568 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_64: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_57);  alias_57 = None
        mul_431: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_924, primals_313);  primals_313 = None
        mul_432: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_877, alias_64);  convert_element_type_877 = None
        mul_433: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_432, mul_431)
        sum_24: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_433, [1], True);  mul_433 = None
        div_83: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_432, 2048)
        mul_434: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_83, sum_24);  div_83 = sum_24 = None
        sub_189: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_431, mul_434);  mul_431 = mul_434 = None
        mul_435: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_189, alias_64);  sub_189 = alias_64 = None
        mul_436: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_924, mul_432);  convert_element_type_924 = mul_432 = None
        sum_25: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_436, [0]);  mul_436 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_925: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_435, torch.bfloat16);  mul_435 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_569: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_925, [10, 2048]);  convert_element_type_925 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_265: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_569, [1, 0])
        mm_130: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_265, convert_element_type_873);  permute_265 = convert_element_type_873 = None
        permute_266: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_130, [1, 0]);  mm_130 = None
        permute_267: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_195, [1, 0]);  permute_195 = None
        mm_131: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_569, permute_267);  permute_267 = None
        permute_268: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_266, [1, 0]);  permute_266 = None
        convert_element_type_930: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_268, torch.float32);  permute_268 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_931: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_131, torch.float32);  mm_131 = None
        convert_element_type_932: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_518, torch.float32);  view_518 = None
        neg_79: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_932)
        exp_79: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_79);  neg_79 = None
        add_525: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_79, 1);  exp_79 = None
        reciprocal_23: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_525);  add_525 = None
        mul_437: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_23, 1);  reciprocal_23 = None
        mul_438: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_931, mul_437);  convert_element_type_931 = None
        sub_190: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_437);  mul_437 = None
        mul_439: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_932, sub_190);  convert_element_type_932 = sub_190 = None
        add_526: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_439, 1);  mul_439 = None
        mul_440: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_438, add_526);  mul_438 = add_526 = None
        convert_element_type_933: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_440, torch.bfloat16);  mul_440 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_570: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_933, [10, 1024]);  convert_element_type_933 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_934: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_570, torch.float32);  view_570 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_65: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_56);  alias_56 = None
        mul_441: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_934, primals_311);  primals_311 = None
        mul_442: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_870, alias_65);  convert_element_type_870 = None
        mul_443: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_442, mul_441)
        sum_26: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_443, [1], True);  mul_443 = None
        div_84: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_442, 1024)
        mul_444: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_84, sum_26);  div_84 = sum_26 = None
        sub_191: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_441, mul_444);  mul_441 = mul_444 = None
        mul_445: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_191, alias_65);  sub_191 = alias_65 = None
        mul_446: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_934, mul_442);  convert_element_type_934 = mul_442 = None
        sum_27: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_446, [0]);  mul_446 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_935: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_445, torch.bfloat16);  mul_445 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_571: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_935, [10, 1024]);  convert_element_type_935 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_269: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_571, [1, 0])
        mm_132: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_269, convert_element_type_866);  permute_269 = convert_element_type_866 = None
        permute_270: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_132, [1, 0]);  mm_132 = None
        permute_271: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_194, [1, 0]);  permute_194 = None
        mm_133: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_571, permute_271);  view_571 = permute_271 = None
        add_527: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_569, mm_133);  view_569 = mm_133 = None
        permute_272: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_270, [1, 0]);  permute_270 = None
        convert_element_type_940: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_272, torch.float32);  permute_272 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_941: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_527, torch.float32);  add_527 = None
        convert_element_type_942: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_516, torch.float32);  view_516 = None
        neg_80: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_942)
        exp_80: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_80);  neg_80 = None
        add_528: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_80, 1);  exp_80 = None
        reciprocal_24: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_528);  add_528 = None
        mul_447: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_24, 1);  reciprocal_24 = None
        mul_448: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_941, mul_447);  convert_element_type_941 = None
        sub_192: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_447);  mul_447 = None
        mul_449: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_942, sub_192);  convert_element_type_942 = sub_192 = None
        add_529: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_449, 1);  mul_449 = None
        mul_450: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_448, add_529);  mul_448 = add_529 = None
        convert_element_type_943: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_450, torch.bfloat16);  mul_450 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_572: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_943, [10, 2048]);  convert_element_type_943 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_944: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_572, torch.float32);  view_572 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_66: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_55);  alias_55 = None
        mul_451: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_944, primals_309);  primals_309 = None
        mul_452: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_863, alias_66);  convert_element_type_863 = None
        mul_453: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_452, mul_451)
        sum_28: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_453, [1], True);  mul_453 = None
        div_85: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_452, 2048)
        mul_454: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_85, sum_28);  div_85 = sum_28 = None
        sub_193: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_451, mul_454);  mul_451 = mul_454 = None
        mul_455: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_193, alias_66);  sub_193 = alias_66 = None
        mul_456: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_944, mul_452);  convert_element_type_944 = mul_452 = None
        sum_29: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_456, [0]);  mul_456 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_945: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_455, torch.bfloat16);  mul_455 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_573: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_945, [10, 2048]);  convert_element_type_945 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_273: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_573, [1, 0])
        mm_134: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_273, convert_element_type_859);  permute_273 = convert_element_type_859 = None
        permute_274: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_134, [1, 0]);  mm_134 = None
        permute_275: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_193, [1, 0]);  permute_193 = None
        mm_135: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_573, permute_275);  view_573 = permute_275 = None
        permute_276: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_274, [1, 0]);  permute_274 = None
        convert_element_type_950: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_276, torch.float32);  permute_276 = None
        convert_element_type_951: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_135, torch.float32);  mm_135 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_574: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_951, [10, 8472]);  convert_element_type_951 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_150: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_10: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_71 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 185, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_150, 'DY': view_574, 'DW': full_10, 'X': view_512, 'W': primals_307, 'Rstd': getitem_253}, tensors_to_clone = ['DX', 'DW']);  empty_150 = view_574 = full_10 = view_512 = primals_307 = getitem_253 = None
        getitem_262: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_71['DX']
        getitem_263: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_71['DW'];  triton_kernel_wrapper_functional_proxy_71 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_577: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_262, [10, 8472])
        slice_39: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_577, 1, 0, 6144);  view_577 = None
        convert_element_type_952: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_39, torch.bfloat16);  slice_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_40: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_952, 1, 0, 2048)
        slice_41: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_952, 1, 2048, 6144);  convert_element_type_952 = None
        view_578: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_41, [10, 16, 256]);  slice_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_579: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_40, [10, 32, 64]);  slice_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_580: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_579, [10, 32, 64]);  view_579 = None
        permute_277: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_507, [0, 2, 1]);  view_507 = None
        bmm_31: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_277, view_580);  permute_277 = None
        permute_278: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_508, [0, 2, 1]);  view_508 = None
        bmm_32: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_580, permute_278);  view_580 = permute_278 = None
        view_581: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_31, [10, 256, 64]);  bmm_31 = None
        view_582: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_32, [10, 32, 256]);  bmm_32 = None
        permute_279: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_581, [0, 2, 1]);  view_581 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_583: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_582, [10, 32, 256]);  view_582 = None
        permute_280: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_504, [0, 2, 1]);  view_504 = None
        bmm_33: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_280, view_583);  permute_280 = None
        permute_281: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_505, [0, 2, 1]);  view_505 = None
        bmm_34: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_583, permute_281);  view_583 = permute_281 = None
        view_584: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_33, [10, 64, 256]);  bmm_33 = None
        add_530: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(permute_279, view_584);  permute_279 = view_584 = None
        view_585: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_34, [10, 32, 64]);  bmm_34 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_586: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_585, [10, 2048]);  view_585 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_282: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_586, [1, 0])
        mm_136: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_282, convert_element_type_851);  permute_282 = convert_element_type_851 = None
        permute_283: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_136, [1, 0]);  mm_136 = None
        permute_284: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_191, [1, 0]);  permute_191 = None
        mm_137: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_586, permute_284);  view_586 = permute_284 = None
        permute_285: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_283, [1, 0]);  permute_283 = None
        convert_element_type_965: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_285, torch.float32);  permute_285 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_966: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_137, torch.float32);  mm_137 = None
        convert_element_type_967: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_502, torch.float32);  view_502 = None
        neg_81: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_967)
        exp_81: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_81);  neg_81 = None
        add_531: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_81, 1);  exp_81 = None
        reciprocal_25: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_531);  add_531 = None
        mul_457: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_25, 1);  reciprocal_25 = None
        mul_458: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_966, mul_457);  convert_element_type_966 = None
        sub_194: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_457);  mul_457 = None
        mul_459: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_967, sub_194);  convert_element_type_967 = sub_194 = None
        add_532: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_459, 1);  mul_459 = None
        mul_460: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_458, add_532);  mul_458 = add_532 = None
        convert_element_type_968: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_460, torch.bfloat16);  mul_460 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_587: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_968, [10, 512]);  convert_element_type_968 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_969: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_587, torch.float32);  view_587 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_67: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_54);  alias_54 = None
        mul_461: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_969, primals_305);  primals_305 = None
        mul_462: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_848, alias_67);  convert_element_type_848 = None
        mul_463: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_462, mul_461)
        sum_30: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_463, [1], True);  mul_463 = None
        div_86: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_462, 512)
        mul_464: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_86, sum_30);  div_86 = sum_30 = None
        sub_195: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_461, mul_464);  mul_461 = mul_464 = None
        mul_465: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_195, alias_67);  sub_195 = alias_67 = None
        mul_466: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_969, mul_462);  convert_element_type_969 = mul_462 = None
        sum_31: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_466, [0]);  mul_466 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_970: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_465, torch.bfloat16);  mul_465 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_588: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_970, [10, 512]);  convert_element_type_970 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_286: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_588, [1, 0])
        mm_138: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_286, view_500);  permute_286 = view_500 = None
        permute_287: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_138, [1, 0]);  mm_138 = None
        permute_288: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_190, [1, 0]);  permute_190 = None
        mm_139: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_588, permute_288);  view_588 = permute_288 = None
        permute_289: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_287, [1, 0]);  permute_287 = None
        convert_element_type_975: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_289, torch.float32);  permute_289 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_589: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_139, [10, 32, 256]);  mm_139 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_42: "bf16[10, 48, 256][12288, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_589, view_578], 1);  view_589 = view_578 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_290: "bf16[10, 256, 48][12288, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_42, [0, 2, 1]);  cat_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_53: "bf16[10, 256, 48][12288, 48, 1]cuda:0" = torch.ops.aten.clone.default(permute_290, memory_format = torch.contiguous_format);  permute_290 = None
        view_590: "bf16[2560, 48][48, 1]cuda:0" = torch.ops.aten.view.default(clone_53, [2560, 48]);  clone_53 = None
        permute_291: "bf16[48, 2560][1, 48]cuda:0" = torch.ops.aten.permute.default(view_590, [1, 0])
        mm_140: "bf16[48, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_291, view_498);  permute_291 = view_498 = None
        permute_292: "bf16[64, 48][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_140, [1, 0]);  mm_140 = None
        permute_293: "bf16[48, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_188, [1, 0]);  permute_188 = None
        mm_141: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_590, permute_293);  view_590 = permute_293 = None
        view_591: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_141, [10, 256, 64]);  mm_141 = None
        permute_294: "bf16[48, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_292, [1, 0]);  permute_292 = None
        convert_element_type_980: "f32[48, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_294, torch.float32);  permute_294 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_295: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_591, [0, 2, 1]);  view_591 = None
        add_533: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(add_530, permute_295);  add_530 = permute_295 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_296: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(add_533, [0, 2, 1]);  add_533 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        view_592: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(permute_296, [2560, 64]);  permute_296 = None
        permute_297: "bf16[64, 2560][1, 64]cuda:0" = torch.ops.aten.permute.default(view_592, [1, 0])
        mm_142: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(permute_297, view_496);  permute_297 = view_496 = None
        permute_298: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(mm_142, [1, 0]);  mm_142 = None
        permute_299: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_185, [1, 0]);  permute_185 = None
        mm_143: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(view_592, permute_299);  view_592 = permute_299 = None
        view_593: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.view.default(mm_143, [10, 256, 112]);  mm_143 = None
        permute_300: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_298, [1, 0]);  permute_298 = None
        convert_element_type_985: "f32[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_300, torch.float32);  permute_300 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_301: "bf16[10, 112, 256][28672, 1, 112]cuda:0" = torch.ops.aten.permute.default(view_593, [0, 2, 1]);  view_593 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        slice_42: "bf16[10, 64, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 1, 0, 64)
        slice_43: "bf16[10, 32, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 1, 64, 96)
        slice_44: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_301, 1, 96, 112);  permute_301 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_594: "bf16[10, 1, 16, 256][28672, 16, 1, 112]cuda:0" = torch.ops.aten.view.default(slice_44, [10, 1, 16, 256]);  slice_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        mul_467: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_594, index_14);  index_14 = None
        mul_468: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_594, index_15);  view_594 = index_15 = None
        full_11: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_11, [None, getitem_249], mul_467, True);  full_11 = getitem_249 = mul_467 = None
        full_12: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_1: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_12, [None, getitem_248], mul_468, True);  full_12 = getitem_248 = mul_468 = None
        add_534: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put, index_put_1);  index_put = index_put_1 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        select_18: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_534, 1, 0)
        select_19: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_534, 1, 1);  add_534 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        slice_45: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_43, 1, 0, 16)
        slice_46: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_43, 1, 16, 32);  slice_43 = None
        add_535: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_18, slice_45);  select_18 = slice_45 = None
        add_536: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_19, slice_46);  select_19 = slice_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_595: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_536, [160, 256]);  add_536 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_151: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_15: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_72 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 183, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_151, 'DY': view_595, 'DW': full_15, 'X': view_489, 'W': primals_301, 'Rstd': getitem_247}, tensors_to_clone = ['DX', 'DW']);  empty_151 = view_595 = full_15 = view_489 = primals_301 = getitem_247 = None
        getitem_264: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_72['DX']
        getitem_265: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_72['DW'];  triton_kernel_wrapper_functional_proxy_72 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_599: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_264, [10, 16, 256])
        view_600: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_599, [160, 256]);  view_599 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_602: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_264, [10, 16, 256])
        view_603: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_602, [160, 256]);  view_602 = None
        view_604: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_603, [10, 4096]);  view_603 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_605: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_264, [10, 16, 256]);  getitem_264 = None
        view_606: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_605, [160, 256]);  view_605 = None
        view_607: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_606, [10, 4096]);  view_606 = None
        permute_303: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_607, [1, 0]);  view_607 = None
        mm_144: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_303, mul_327);  permute_303 = mul_327 = None
        permute_304: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_144, [1, 0]);  mm_144 = None
        permute_305: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_183, [1, 0]);  permute_183 = None
        mm_145: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_604, permute_305);  view_604 = permute_305 = None
        permute_306: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_304, [1, 0]);  permute_304 = None
        convert_element_type_990: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_306, torch.float32);  permute_306 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_469: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_145, convert_element_type_828);  convert_element_type_828 = None
        mul_470: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_145, mm_115);  mm_145 = mm_115 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_307: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_469, [1, 0])
        mm_146: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_307, convert_element_type_829);  permute_307 = convert_element_type_829 = None
        permute_308: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_146, [1, 0]);  mm_146 = None
        permute_309: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_182, [1, 0]);  permute_182 = None
        mm_147: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_469, permute_309);  mul_469 = permute_309 = None
        permute_310: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_308, [1, 0]);  permute_308 = None
        convert_element_type_995: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_310, torch.float32);  permute_310 = None
        convert_element_type_996: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_147, torch.float32);  mm_147 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_997: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_470, torch.float32);  mul_470 = None
        convert_element_type_998: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_114, torch.float32);  mm_114 = None
        neg_82: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_998)
        exp_82: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_82);  neg_82 = None
        add_537: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_82, 1);  exp_82 = None
        reciprocal_26: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_537);  add_537 = None
        mul_471: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_26, 1);  reciprocal_26 = None
        mul_472: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_997, mul_471);  convert_element_type_997 = None
        sub_196: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_471);  mul_471 = None
        mul_473: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_998, sub_196);  convert_element_type_998 = sub_196 = None
        add_538: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_473, 1);  mul_473 = None
        mul_474: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_472, add_538);  mul_472 = add_538 = None
        convert_element_type_999: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_474, torch.bfloat16);  mul_474 = None
        permute_311: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_999, [1, 0])
        mm_148: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_311, convert_element_type_823);  permute_311 = convert_element_type_823 = None
        permute_312: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_148, [1, 0]);  mm_148 = None
        permute_313: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_181, [1, 0]);  permute_181 = None
        mm_149: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_999, permute_313);  convert_element_type_999 = permute_313 = None
        permute_314: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_312, [1, 0]);  permute_312 = None
        convert_element_type_1004: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_314, torch.float32);  permute_314 = None
        convert_element_type_1005: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_149, torch.float32);  mm_149 = None
        add_539: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_996, convert_element_type_1005);  convert_element_type_996 = convert_element_type_1005 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_608: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_539, [160, 256]);  add_539 = None
        sub_197: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_822, getitem_245);  convert_element_type_822 = getitem_245 = None
        mul_475: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_197, rsqrt_59);  sub_197 = None
        mul_476: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_608, primals_296);  primals_296 = None
        mul_477: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_476, 256)
        sum_32: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_476, [1], True)
        mul_478: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_476, mul_475);  mul_476 = None
        sum_33: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_478, [1], True);  mul_478 = None
        mul_479: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_475, sum_33);  sum_33 = None
        sub_198: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_477, sum_32);  mul_477 = sum_32 = None
        sub_199: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_198, mul_479);  sub_198 = mul_479 = None
        div_87: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_59, 256);  rsqrt_59 = None
        mul_480: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_87, sub_199);  div_87 = sub_199 = None
        mul_481: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_608, mul_475);  mul_475 = None
        sum_34: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_481, [0]);  mul_481 = None
        sum_35: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_608, [0]);  view_608 = None
        convert_element_type_1006: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_480, torch.bfloat16);  mul_480 = None
        add_540: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_600, convert_element_type_1006);  view_600 = convert_element_type_1006 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_609: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_540, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_17: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_18: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_23, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_23 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_152: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_73 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 181, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_152, 'Q': view_482, 'K': view_483, 'V': view_483, 'DO': view_609, 'total_seq_len_kv': _local_scalar_dense_15, 'seq_offsets': getitem_243, 'seq_offsets_q': getitem_242, 'DQ': full_17, 'DK': full_18, 'DV': full_18, 'max_seq_len': _local_scalar_dense_13, 'attn_scale': reciprocal_15, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_13}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_152 = view_482 = view_483 = view_609 = getitem_243 = getitem_242 = full_17 = full_18 = reciprocal_15 = None
        getitem_266: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_73['seq_offsets']
        getitem_268: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_73['DQ']
        getitem_269: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_73['DK'];  triton_kernel_wrapper_functional_proxy_73 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_611: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_269, [_local_scalar_dense_15, 256]);  getitem_269 = None
        add_541: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_1, view_611);  view_611 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_613: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_268, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_614: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_268, [160, 256]);  getitem_268 = None
        permute_316: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_614, [1, 0]);  view_614 = None
        mm_150: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_316, view_481);  permute_316 = view_481 = None
        permute_317: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_150, [1, 0]);  mm_150 = None
        permute_318: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_180, [1, 0]);  permute_180 = None
        mm_151: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_613, permute_318);  view_613 = permute_318 = None
        permute_319: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_317, [1, 0]);  permute_317 = None
        convert_element_type_1011: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_319, torch.float32);  permute_319 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_615: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_151, [160, 256]);  mm_151 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_153: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_21: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_74 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 179, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_153, 'DY': view_615, 'DW': full_21, 'X': view_479, 'W': primals_294, 'Rstd': getitem_240}, tensors_to_clone = ['DX', 'DW']);  empty_153 = view_615 = full_21 = view_479 = primals_294 = getitem_240 = None
        getitem_270: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_74['DX']
        getitem_271: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_74['DW'];  triton_kernel_wrapper_functional_proxy_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_617: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_270, [160, 256]);  getitem_270 = None
        add_542: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_540, view_617);  add_540 = view_617 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_618: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_542, [10, 16, 256]);  add_542 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_320: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_618, [0, 2, 1]);  view_618 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_55: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_320, memory_format = torch.contiguous_format);  permute_320 = None
        view_619: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_55, [2560, 16]);  clone_55 = None
        permute_321: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_619, [1, 0])
        mm_152: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_321, view_476);  permute_321 = view_476 = None
        permute_322: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_152, [1, 0]);  mm_152 = None
        permute_323: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_177, [1, 0]);  permute_177 = None
        mm_153: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_619, permute_323);  view_619 = permute_323 = None
        view_620: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_153, [10, 256, 64]);  mm_153 = None
        permute_324: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_322, [1, 0]);  permute_322 = None
        convert_element_type_1016: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_324, torch.float32);  permute_324 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_325: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_620, [0, 2, 1]);  view_620 = None
        add_543: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(slice_42, permute_325);  slice_42 = permute_325 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_621: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_535, [160, 256]);  add_535 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_154: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_24: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_75 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 177, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_154, 'DY': view_621, 'DW': full_24, 'X': view_473, 'W': primals_292, 'Rstd': getitem_238}, tensors_to_clone = ['DX', 'DW']);  empty_154 = view_621 = full_24 = view_473 = primals_292 = getitem_238 = None
        getitem_272: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_75['DX']
        getitem_273: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_75['DW'];  triton_kernel_wrapper_functional_proxy_75 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_625: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_272, [10, 16, 256])
        view_626: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_625, [160, 256]);  view_625 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_628: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_272, [10, 16, 256])
        view_629: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_628, [160, 256]);  view_628 = None
        view_630: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_629, [10, 4096]);  view_629 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_631: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_272, [10, 16, 256]);  getitem_272 = None
        view_632: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_631, [160, 256]);  view_631 = None
        view_633: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_632, [10, 4096]);  view_632 = None
        permute_327: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_633, [1, 0]);  view_633 = None
        mm_154: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_327, mul_321);  permute_327 = mul_321 = None
        permute_328: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_154, [1, 0]);  mm_154 = None
        permute_329: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_175, [1, 0]);  permute_175 = None
        mm_155: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_630, permute_329);  view_630 = permute_329 = None
        permute_330: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_328, [1, 0]);  permute_328 = None
        convert_element_type_1021: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_330, torch.float32);  permute_330 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_482: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_155, convert_element_type_804);  convert_element_type_804 = None
        mul_483: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_155, mm_110);  mm_155 = mm_110 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_331: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_482, [1, 0])
        mm_156: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_331, convert_element_type_805);  permute_331 = convert_element_type_805 = None
        permute_332: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_156, [1, 0]);  mm_156 = None
        permute_333: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_174, [1, 0]);  permute_174 = None
        mm_157: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_482, permute_333);  mul_482 = permute_333 = None
        permute_334: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_332, [1, 0]);  permute_332 = None
        convert_element_type_1026: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_334, torch.float32);  permute_334 = None
        convert_element_type_1027: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_157, torch.float32);  mm_157 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1028: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_483, torch.float32);  mul_483 = None
        convert_element_type_1029: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_109, torch.float32);  mm_109 = None
        neg_83: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1029)
        exp_83: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_83);  neg_83 = None
        add_544: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_83, 1);  exp_83 = None
        reciprocal_27: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_544);  add_544 = None
        mul_484: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_27, 1);  reciprocal_27 = None
        mul_485: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1028, mul_484);  convert_element_type_1028 = None
        sub_200: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_484);  mul_484 = None
        mul_486: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1029, sub_200);  convert_element_type_1029 = sub_200 = None
        add_545: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_486, 1);  mul_486 = None
        mul_487: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_485, add_545);  mul_485 = add_545 = None
        convert_element_type_1030: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_487, torch.bfloat16);  mul_487 = None
        permute_335: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1030, [1, 0])
        mm_158: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_335, convert_element_type_799);  permute_335 = convert_element_type_799 = None
        permute_336: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_158, [1, 0]);  mm_158 = None
        permute_337: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_173, [1, 0]);  permute_173 = None
        mm_159: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1030, permute_337);  convert_element_type_1030 = permute_337 = None
        permute_338: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_336, [1, 0]);  permute_336 = None
        convert_element_type_1035: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_338, torch.float32);  permute_338 = None
        convert_element_type_1036: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_159, torch.float32);  mm_159 = None
        add_546: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1027, convert_element_type_1036);  convert_element_type_1027 = convert_element_type_1036 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_634: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_546, [160, 256]);  add_546 = None
        sub_201: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_798, getitem_236);  convert_element_type_798 = getitem_236 = None
        mul_488: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_201, rsqrt_58);  sub_201 = None
        mul_489: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_634, primals_287);  primals_287 = None
        mul_490: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_489, 256)
        sum_36: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_489, [1], True)
        mul_491: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_489, mul_488);  mul_489 = None
        sum_37: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_491, [1], True);  mul_491 = None
        mul_492: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_488, sum_37);  sum_37 = None
        sub_202: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_490, sum_36);  mul_490 = sum_36 = None
        sub_203: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_202, mul_492);  sub_202 = mul_492 = None
        div_88: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_58, 256);  rsqrt_58 = None
        mul_493: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_88, sub_203);  div_88 = sub_203 = None
        mul_494: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_634, mul_488);  mul_488 = None
        sum_38: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_494, [0]);  mul_494 = None
        sum_39: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_634, [0]);  view_634 = None
        convert_element_type_1037: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_493, torch.bfloat16);  mul_493 = None
        add_547: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_626, convert_element_type_1037);  view_626 = convert_element_type_1037 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_635: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_547, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_26: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_27: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_22, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_22 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_155: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_76 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 175, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_155, 'Q': view_466, 'K': view_467, 'V': view_467, 'DO': view_635, 'total_seq_len_kv': _local_scalar_dense_11, 'seq_offsets': getitem_234, 'seq_offsets_q': getitem_233, 'DQ': full_26, 'DK': full_27, 'DV': full_27, 'max_seq_len': _local_scalar_dense_9, 'attn_scale': reciprocal_14, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_9}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_155 = view_466 = view_467 = view_635 = getitem_234 = getitem_233 = full_26 = full_27 = reciprocal_14 = None
        getitem_274: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_76['seq_offsets']
        getitem_276: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_76['DQ']
        getitem_277: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_76['DK'];  triton_kernel_wrapper_functional_proxy_76 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_637: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_277, [_local_scalar_dense_11, 256]);  getitem_277 = None
        add_548: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(tangents_2, view_637);  view_637 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_639: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_276, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_640: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_276, [160, 256]);  getitem_276 = None
        permute_340: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_640, [1, 0]);  view_640 = None
        mm_160: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_340, view_465);  permute_340 = view_465 = None
        permute_341: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_160, [1, 0]);  mm_160 = None
        permute_342: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_172, [1, 0]);  permute_172 = None
        mm_161: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_639, permute_342);  view_639 = permute_342 = None
        permute_343: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_341, [1, 0]);  permute_341 = None
        convert_element_type_1042: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_343, torch.float32);  permute_343 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_641: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_161, [160, 256]);  mm_161 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_156: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_30: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_77 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 173, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_156, 'DY': view_641, 'DW': full_30, 'X': view_463, 'W': primals_285, 'Rstd': getitem_231}, tensors_to_clone = ['DX', 'DW']);  empty_156 = view_641 = full_30 = view_463 = primals_285 = getitem_231 = None
        getitem_278: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_77['DX']
        getitem_279: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_77['DW'];  triton_kernel_wrapper_functional_proxy_77 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_643: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_278, [160, 256]);  getitem_278 = None
        add_549: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_547, view_643);  add_547 = view_643 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_644: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_549, [10, 16, 256]);  add_549 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_344: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_644, [0, 2, 1]);  view_644 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_57: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_344, memory_format = torch.contiguous_format);  permute_344 = None
        view_645: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_57, [2560, 16]);  clone_57 = None
        permute_345: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_645, [1, 0])
        mm_162: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_345, view_460);  permute_345 = view_460 = None
        permute_346: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_162, [1, 0]);  mm_162 = None
        permute_347: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_169, [1, 0]);  permute_169 = None
        mm_163: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_645, permute_347);  view_645 = permute_347 = None
        view_646: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_163, [10, 256, 64]);  mm_163 = None
        permute_348: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_346, [1, 0]);  permute_346 = None
        convert_element_type_1047: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_348, torch.float32);  permute_348 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_349: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_646, [0, 2, 1]);  view_646 = None
        add_550: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(add_543, permute_349);  add_543 = permute_349 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_350: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_784, [1, 0]);  convert_element_type_784 = None
        mm_164: "bf16[u31, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_541, permute_350);  permute_350 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        sym_size_int_24: "Sym(u31)" = torch.ops.aten.sym_size.int(tangents_1, 0);  tangents_1 = None
        empty_157: "bf16[u31, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_24, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        sym_size_int_25: "Sym(u31)" = torch.ops.aten.sym_size.int(view_457, 0)
        empty_158: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_25, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_25 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_159: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_34: "Sym((u31//4))" = sym_size_int_24 // 4
        sym_min: "Sym(Min(9472, (u31//4)))" = torch.sym_min(floordiv_34, 9472);  floordiv_34 = None
        sym_max_8: "Sym(Max(1, Min(9472, (u31//4))))" = torch.sym_max(sym_min, 1);  sym_min = None
        empty_160: "f32[Max(1, Min(9472, (u31//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_8, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_161: "f32[Max(1, Min(9472, (u31//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_8, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_162: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_163: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_78 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 31, constant_args_idx = 170, grid = [(sym_max_8, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_158, 'DU': empty_159, 'DY': mm_164, 'DW': empty_160, 'DB': empty_161, 'X': view_457, 'U': convert_element_type_781, 'Y': empty_157, 'W': convert_element_type_782, 'B': convert_element_type_783, 'Mean': getitem_228, 'Rstd': getitem_229, 'seed': primals_283, 'N': sym_size_int_24}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_158 = empty_159 = mm_164 = empty_160 = empty_161 = view_457 = convert_element_type_781 = empty_157 = convert_element_type_782 = convert_element_type_783 = getitem_228 = getitem_229 = primals_283 = sym_size_int_24 = None
        getitem_280: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_78['DX']
        getitem_281: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_78['DU']
        getitem_282: "f32[Max(1, Min(9472, (u31//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_78['DW']
        getitem_283: "f32[Max(1, Min(9472, (u31//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_78['DB']
        getitem_284: "bf16[u31, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_78['Y'];  triton_kernel_wrapper_functional_proxy_78 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_79 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 171, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_282, 'DB': getitem_283, 'FINAL_DW': empty_162, 'FINAL_DB': empty_163, 'N': sym_max_8}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_282 = getitem_283 = empty_162 = empty_163 = sym_max_8 = None
        getitem_285: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_79['FINAL_DW']
        getitem_286: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_79['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_352: "bf16[768, u31][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_284, [1, 0]);  getitem_284 = None
        mm_165: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_352, add_541);  permute_352 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1052: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_165, torch.float32);  mm_165 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1053: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_286, torch.float32);  getitem_286 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1054: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_285, torch.float32);  getitem_285 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_164: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        add_551: "Sym(u31 + 4)" = _local_scalar_dense_15 + 4
        sub_204: "Sym(u31 + 3)" = add_551 - 1;  add_551 = None
        floordiv_35: "Sym(((u31 + 3)//4))" = sub_204 // 4;  sub_204 = None
        add_552: "Sym(u31 + 8)" = _local_scalar_dense_15 + 8
        sub_205: "Sym(u31 + 7)" = add_552 - 1;  add_552 = None
        floordiv_36: "Sym(((u31 + 7)//8))" = sub_205 // 8;  sub_205 = None
        add_553: "Sym(u31 + 16)" = _local_scalar_dense_15 + 16
        sub_206: "Sym(u31 + 15)" = add_553 - 1;  add_553 = None
        floordiv_37: "Sym(((u31 + 15)//16))" = sub_206 // 16;  sub_206 = None
        triton_kernel_wrapper_functional_proxy_80 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 165, grid = [(floordiv_35, 1, 1), (floordiv_35, 1, 1), (floordiv_35, 1, 1), (floordiv_36, 1, 1), (floordiv_36, 1, 1), (floordiv_36, 1, 1), (floordiv_37, 1, 1), (floordiv_37, 1, 1), (floordiv_37, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_203, 'Y': empty_164, 'W': convert_element_type_773, 'B': convert_element_type_774, 'Mean': getitem_218, 'Rstd': getitem_219, 'N': _local_scalar_dense_15}, tensors_to_clone = ['Y']);  floordiv_35 = floordiv_36 = floordiv_37 = empty_164 = None
        getitem_287: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_80['Y'];  triton_kernel_wrapper_functional_proxy_80 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_18: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_776, getitem_287, convert_element_type_775);  convert_element_type_776 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_16 = torch.ops.aten.split_with_sizes.default(addmm_18, [256, 256, 256, 256], 1);  addmm_18 = None
        getitem_288: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_16[0]
        getitem_289: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_16[1]
        getitem_290: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_16[2]
        getitem_291: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_16[3];  split_with_sizes_16 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_165: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:473 in backward, code: du, dv, dq, dk = duvqk.split(
        split_with_sizes_17 = torch.ops.aten.split_with_sizes.default(empty_165, [256, 256, 256, 256], 1)
        getitem_294: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_17[2];  split_with_sizes_17 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_648: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_290, [-1, 2, 128]);  getitem_290 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_649: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_291, [-1, 2, 128]);  getitem_291 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_650: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_289, [-1, 2, 128]);  getitem_289 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:485 in backward, code: dq = dq.view(-1, ctx.num_heads, ctx.attn_dim)
        view_651: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_294, [-1, 2, 128]);  getitem_294 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_166: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        add_554: "Sym(u25 + 16)" = _local_scalar_dense_13 + 16
        sub_207: "Sym(u25 + 15)" = add_554 - 1;  add_554 = None
        floordiv_38: "Sym(((u25 + 15)//16))" = sub_207 // 16;  sub_207 = None
        full_53: "i32[4, ((u25 + 15)//16)][Max(1, ((u25 + 15)//16)), 1]cuda:0" = torch.ops.aten.full.default([4, floordiv_38], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  floordiv_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        sym_size_int_26: "Sym(u31)" = torch.ops.aten.sym_size.int(view_651, 0);  view_651 = None
        full_54: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_26, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_26 = None
        view_654: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_54, [_local_scalar_dense_15, 256]);  full_54 = None
        slice_scatter: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_165, view_654, 1, 512, 768);  empty_165 = view_654 = None
        split_with_sizes_19 = torch.ops.aten.split_with_sizes.default(slice_scatter, [256, 256, 256, 256], 1)
        getitem_302: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_19[2];  split_with_sizes_19 = None
        view_655: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_302, [-1, 2, 128]);  getitem_302 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_656: "bf16[u31, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_280, [sym_size_int_19, 2, 128]);  getitem_280 = sym_size_int_19 = None
        split_with_sizes_20 = torch.ops.aten.split_with_sizes.default(slice_scatter, [256, 256, 256, 256], 1)
        getitem_307: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_20[3];  split_with_sizes_20 = None
        view_657: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_307, [-1, 2, 128]);  getitem_307 = None
        split_with_sizes_21 = torch.ops.aten.split_with_sizes.default(slice_scatter, [256, 256, 256, 256], 1)
        getitem_309: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_21[1];  split_with_sizes_21 = None
        view_658: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_309, [-1, 2, 128]);  getitem_309 = None
        add_555: "Sym(u25 + 32)" = _local_scalar_dense_13 + 32
        sub_208: "Sym(u25 + 31)" = add_555 - 1;  add_555 = None
        floordiv_39: "Sym(((u25 + 31)//32))" = sub_208 // 32;  sub_208 = None
        triton_kernel_wrapper_functional_proxy_81 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 166, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, floordiv_39, 1), (4, floordiv_39, 1), (4, floordiv_39, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_648, 'K': view_649, 'V': view_650, 'sort_by_length_indices': getitem_225, 'seq_offsets': getitem_266, 'attn_scale': reciprocal_13, 'DOut': view_656, 'DQ': view_655, 'DK': view_657, 'DV': view_658, 'LOCK': full_53, 'M': empty_166, 'MAX_SEQ_LEN': _local_scalar_dense_13, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_13}, tensors_to_clone = ['DQ', 'DK', 'DV']);  floordiv_39 = view_648 = view_649 = view_650 = getitem_225 = reciprocal_13 = view_656 = view_655 = view_657 = view_658 = full_53 = empty_166 = None
        getitem_312: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_81['DQ']
        getitem_313: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_81['DK']
        getitem_314: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_81['DV'];  triton_kernel_wrapper_functional_proxy_81 = None
        view_659: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_312, [_local_scalar_dense_15, 256]);  getitem_312 = None
        slice_scatter_1: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter, view_659, 1, 512, 768);  slice_scatter = view_659 = None
        view_661: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_313, [_local_scalar_dense_15, 256]);  getitem_313 = None
        slice_scatter_2: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_1, view_661, 1, 768, 1024);  slice_scatter_1 = view_661 = None
        view_663: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_314, [_local_scalar_dense_15, 256]);  getitem_314 = None
        slice_scatter_3: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_2, view_663, 1, 256, 512);  slice_scatter_2 = view_663 = None
        split_with_sizes_28 = torch.ops.aten.split_with_sizes.default(slice_scatter_3, [256, 256, 256, 256], 1)
        getitem_341: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_28[2];  split_with_sizes_28 = None
        view_665: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_341, [-1, 2, 128]);  getitem_341 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_665, view_665);  view_665 = None
        view_666: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy, [_local_scalar_dense_15, 256]);  copy = None
        slice_scatter_4: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_3, view_666, 1, 512, 768);  slice_scatter_3 = view_666 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_31 = torch.ops.aten.split_with_sizes.default(slice_scatter_4, [256, 256, 256, 256], 1)
        getitem_354: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_31[3];  split_with_sizes_31 = None
        view_668: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_354, [-1, 2, 128]);  getitem_354 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_1: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_668, view_668);  view_668 = None
        view_669: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_1, [_local_scalar_dense_15, 256]);  copy_1 = None
        slice_scatter_5: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_4, view_669, 1, 768, 1024);  slice_scatter_4 = view_669 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_34 = torch.ops.aten.split_with_sizes.default(slice_scatter_5, [256, 256, 256, 256], 1)
        getitem_364: "bf16[u31, 256][1024, 1]cuda:0" = split_with_sizes_34[1];  split_with_sizes_34 = None
        view_671: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_364, [-1, 2, 128]);  getitem_364 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_2: "bf16[u31, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_671, view_671);  view_671 = None
        view_672: "bf16[u31, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_2, [_local_scalar_dense_15, 256]);  copy_2 = None
        slice_scatter_6: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_5, view_672, 1, 256, 512);  slice_scatter_5 = view_672 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1058: "f32[u31, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_281, torch.float32);  getitem_281 = None
        convert_element_type_1059: "f32[u31, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_288, torch.float32);  getitem_288 = None
        neg_84: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1059)
        exp_84: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_84);  neg_84 = None
        add_556: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_84, 1);  exp_84 = None
        reciprocal_28: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_556);  add_556 = None
        mul_495: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_28, 1);  reciprocal_28 = None
        mul_496: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1058, mul_495);  convert_element_type_1058 = None
        sub_209: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_495);  mul_495 = None
        mul_497: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1059, sub_209);  convert_element_type_1059 = sub_209 = None
        add_557: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_497, 1);  mul_497 = None
        mul_498: "f32[u31, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_496, add_557);  mul_496 = add_557 = None
        convert_element_type_1060: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_498, torch.bfloat16);  mul_498 = None
        slice_scatter_7: "bf16[u31, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_6, convert_element_type_1060, 1, 0, 256);  slice_scatter_6 = convert_element_type_1060 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_40: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_7, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_354: "bf16[256, u31][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_287, [1, 0]);  getitem_287 = None
        mm_166: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_354, slice_scatter_7);  permute_354 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_355: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_775, [1, 0]);  convert_element_type_775 = None
        mm_167: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_7, permute_355);  slice_scatter_7 = permute_355 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_167: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_15, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_40: "Sym((u31//4))" = _local_scalar_dense_15 // 4
        sym_min_1: "Sym(Min(1184, (u31//4)))" = torch.sym_min(floordiv_40, 1184);  floordiv_40 = None
        sym_max_9: "Sym(Max(1, Min(1184, (u31//4))))" = torch.sym_max(sym_min_1, 1);  sym_min_1 = None
        empty_168: "f32[Max(1, Min(1184, (u31//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_9, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_169: "f32[Max(1, Min(1184, (u31//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_9, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_170: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_171: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_82 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 167, grid = [(sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1), (sym_max_9, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_167, 'DY': mm_167, 'DW': empty_168, 'DB': empty_169, 'X': getitem_203, 'W': convert_element_type_773, 'B': convert_element_type_774, 'Mean': getitem_218, 'Rstd': getitem_219, 'N': _local_scalar_dense_15}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_167 = mm_167 = empty_168 = empty_169 = getitem_203 = convert_element_type_773 = convert_element_type_774 = getitem_218 = getitem_219 = _local_scalar_dense_15 = None
        getitem_383: "bf16[u31, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_82['DX']
        getitem_384: "f32[Max(1, Min(1184, (u31//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_82['DW']
        getitem_385: "f32[Max(1, Min(1184, (u31//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_82['DB'];  triton_kernel_wrapper_functional_proxy_82 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_83 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 168, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_384, 'DB': getitem_385, 'FINAL_DW': empty_170, 'FINAL_DB': empty_171, 'N': sym_max_9}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_384 = getitem_385 = empty_170 = empty_171 = sym_max_9 = None
        getitem_386: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_83['FINAL_DW']
        getitem_387: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_83['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_83 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_558: "bf16[u31, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_541, getitem_383);  add_541 = getitem_383 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1065: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_40, torch.float32);  sum_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1066: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_166, torch.float32);  mm_166 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1067: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_387, torch.float32);  getitem_387 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1068: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_386, torch.float32);  getitem_386 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_356: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_765, [1, 0]);  convert_element_type_765 = None
        mm_168: "bf16[u23, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_548, permute_356);  permute_356 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        sym_size_int_27: "Sym(u23)" = torch.ops.aten.sym_size.int(tangents_2, 0);  tangents_2 = None
        empty_172: "bf16[u23, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_27, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        sym_size_int_28: "Sym(u23)" = torch.ops.aten.sym_size.int(view_449, 0)
        empty_173: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_28, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_28 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_174: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_41: "Sym((u23//4))" = sym_size_int_27 // 4
        sym_min_2: "Sym(Min(9472, (u23//4)))" = torch.sym_min(floordiv_41, 9472);  floordiv_41 = None
        sym_max_10: "Sym(Max(1, Min(9472, (u23//4))))" = torch.sym_max(sym_min_2, 1);  sym_min_2 = None
        empty_175: "f32[Max(1, Min(9472, (u23//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_10, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_176: "f32[Max(1, Min(9472, (u23//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_10, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_177: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_178: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_84 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 29, constant_args_idx = 161, grid = [(sym_max_10, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_173, 'DU': empty_174, 'DY': mm_168, 'DW': empty_175, 'DB': empty_176, 'X': view_449, 'U': convert_element_type_762, 'Y': empty_172, 'W': convert_element_type_763, 'B': convert_element_type_764, 'Mean': getitem_215, 'Rstd': getitem_216, 'seed': primals_275, 'N': sym_size_int_27}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_173 = empty_174 = mm_168 = empty_175 = empty_176 = view_449 = convert_element_type_762 = empty_172 = convert_element_type_763 = convert_element_type_764 = getitem_215 = getitem_216 = primals_275 = sym_size_int_27 = None
        getitem_388: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_84['DX']
        getitem_389: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_84['DU']
        getitem_390: "f32[Max(1, Min(9472, (u23//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_84['DW']
        getitem_391: "f32[Max(1, Min(9472, (u23//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_84['DB']
        getitem_392: "bf16[u23, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_84['Y'];  triton_kernel_wrapper_functional_proxy_84 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_85 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 162, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_390, 'DB': getitem_391, 'FINAL_DW': empty_177, 'FINAL_DB': empty_178, 'N': sym_max_10}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_390 = getitem_391 = empty_177 = empty_178 = sym_max_10 = None
        getitem_393: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_85['FINAL_DW']
        getitem_394: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_85['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_358: "bf16[768, u23][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_392, [1, 0]);  getitem_392 = None
        mm_169: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_358, add_548);  permute_358 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1073: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_169, torch.float32);  mm_169 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1074: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_394, torch.float32);  getitem_394 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1075: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_393, torch.float32);  getitem_393 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_179: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        add_559: "Sym(u23 + 4)" = _local_scalar_dense_11 + 4
        sub_210: "Sym(u23 + 3)" = add_559 - 1;  add_559 = None
        floordiv_42: "Sym(((u23 + 3)//4))" = sub_210 // 4;  sub_210 = None
        add_560: "Sym(u23 + 8)" = _local_scalar_dense_11 + 8
        sub_211: "Sym(u23 + 7)" = add_560 - 1;  add_560 = None
        floordiv_43: "Sym(((u23 + 7)//8))" = sub_211 // 8;  sub_211 = None
        add_561: "Sym(u23 + 16)" = _local_scalar_dense_11 + 16
        sub_212: "Sym(u23 + 15)" = add_561 - 1;  add_561 = None
        floordiv_44: "Sym(((u23 + 15)//16))" = sub_212 // 16;  sub_212 = None
        triton_kernel_wrapper_functional_proxy_86 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 156, grid = [(floordiv_42, 1, 1), (floordiv_42, 1, 1), (floordiv_42, 1, 1), (floordiv_43, 1, 1), (floordiv_43, 1, 1), (floordiv_43, 1, 1), (floordiv_44, 1, 1), (floordiv_44, 1, 1), (floordiv_44, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_201, 'Y': empty_179, 'W': convert_element_type_754, 'B': convert_element_type_755, 'Mean': getitem_205, 'Rstd': getitem_206, 'N': _local_scalar_dense_11}, tensors_to_clone = ['Y']);  floordiv_42 = floordiv_43 = floordiv_44 = empty_179 = None
        getitem_395: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_86['Y'];  triton_kernel_wrapper_functional_proxy_86 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_19: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_757, getitem_395, convert_element_type_756);  convert_element_type_757 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_39 = torch.ops.aten.split_with_sizes.default(addmm_19, [256, 256, 256, 256], 1);  addmm_19 = None
        getitem_396: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_39[0]
        getitem_397: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_39[1]
        getitem_398: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_39[2]
        getitem_399: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_39[3];  split_with_sizes_39 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_180: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:473 in backward, code: du, dv, dq, dk = duvqk.split(
        split_with_sizes_40 = torch.ops.aten.split_with_sizes.default(empty_180, [256, 256, 256, 256], 1)
        getitem_402: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_40[2];  split_with_sizes_40 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_675: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_398, [-1, 2, 128]);  getitem_398 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_676: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_399, [-1, 2, 128]);  getitem_399 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_677: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_397, [-1, 2, 128]);  getitem_397 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:485 in backward, code: dq = dq.view(-1, ctx.num_heads, ctx.attn_dim)
        view_678: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_402, [-1, 2, 128]);  getitem_402 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_181: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        add_562: "Sym(u17 + 16)" = _local_scalar_dense_9 + 16
        sub_213: "Sym(u17 + 15)" = add_562 - 1;  add_562 = None
        floordiv_45: "Sym(((u17 + 15)//16))" = sub_213 // 16;  sub_213 = None
        full_77: "i32[4, ((u17 + 15)//16)][Max(1, ((u17 + 15)//16)), 1]cuda:0" = torch.ops.aten.full.default([4, floordiv_45], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  floordiv_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        sym_size_int_29: "Sym(u23)" = torch.ops.aten.sym_size.int(view_678, 0);  view_678 = None
        full_78: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_29, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_29 = None
        view_681: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_78, [_local_scalar_dense_11, 256]);  full_78 = None
        slice_scatter_8: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_180, view_681, 1, 512, 768);  empty_180 = view_681 = None
        split_with_sizes_42 = torch.ops.aten.split_with_sizes.default(slice_scatter_8, [256, 256, 256, 256], 1)
        getitem_410: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_42[2];  split_with_sizes_42 = None
        view_682: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_410, [-1, 2, 128]);  getitem_410 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_683: "bf16[u23, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_388, [sym_size_int_14, 2, 128]);  getitem_388 = sym_size_int_14 = None
        split_with_sizes_43 = torch.ops.aten.split_with_sizes.default(slice_scatter_8, [256, 256, 256, 256], 1)
        getitem_415: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_43[3];  split_with_sizes_43 = None
        view_684: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_415, [-1, 2, 128]);  getitem_415 = None
        split_with_sizes_44 = torch.ops.aten.split_with_sizes.default(slice_scatter_8, [256, 256, 256, 256], 1)
        getitem_417: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_44[1];  split_with_sizes_44 = None
        view_685: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_417, [-1, 2, 128]);  getitem_417 = None
        add_563: "Sym(u17 + 32)" = _local_scalar_dense_9 + 32
        sub_214: "Sym(u17 + 31)" = add_563 - 1;  add_563 = None
        floordiv_46: "Sym(((u17 + 31)//32))" = sub_214 // 32;  sub_214 = None
        triton_kernel_wrapper_functional_proxy_87 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 157, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, floordiv_46, 1), (4, floordiv_46, 1), (4, floordiv_46, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_675, 'K': view_676, 'V': view_677, 'sort_by_length_indices': getitem_212, 'seq_offsets': getitem_274, 'attn_scale': reciprocal_12, 'DOut': view_683, 'DQ': view_682, 'DK': view_684, 'DV': view_685, 'LOCK': full_77, 'M': empty_181, 'MAX_SEQ_LEN': _local_scalar_dense_9, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_9}, tensors_to_clone = ['DQ', 'DK', 'DV']);  floordiv_46 = view_675 = view_676 = view_677 = getitem_212 = reciprocal_12 = view_683 = view_682 = view_684 = view_685 = full_77 = empty_181 = None
        getitem_420: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_87['DQ']
        getitem_421: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_87['DK']
        getitem_422: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_87['DV'];  triton_kernel_wrapper_functional_proxy_87 = None
        view_686: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_420, [_local_scalar_dense_11, 256]);  getitem_420 = None
        slice_scatter_9: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_8, view_686, 1, 512, 768);  slice_scatter_8 = view_686 = None
        view_688: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_421, [_local_scalar_dense_11, 256]);  getitem_421 = None
        slice_scatter_10: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_9, view_688, 1, 768, 1024);  slice_scatter_9 = view_688 = None
        view_690: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_422, [_local_scalar_dense_11, 256]);  getitem_422 = None
        slice_scatter_11: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_10, view_690, 1, 256, 512);  slice_scatter_10 = view_690 = None
        split_with_sizes_51 = torch.ops.aten.split_with_sizes.default(slice_scatter_11, [256, 256, 256, 256], 1)
        getitem_449: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_51[2];  split_with_sizes_51 = None
        view_692: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_449, [-1, 2, 128]);  getitem_449 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_3: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_692, view_692);  view_692 = None
        view_693: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_3, [_local_scalar_dense_11, 256]);  copy_3 = None
        slice_scatter_12: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_11, view_693, 1, 512, 768);  slice_scatter_11 = view_693 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_54 = torch.ops.aten.split_with_sizes.default(slice_scatter_12, [256, 256, 256, 256], 1)
        getitem_462: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_54[3];  split_with_sizes_54 = None
        view_695: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_462, [-1, 2, 128]);  getitem_462 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_4: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_695, view_695);  view_695 = None
        view_696: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_4, [_local_scalar_dense_11, 256]);  copy_4 = None
        slice_scatter_13: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_12, view_696, 1, 768, 1024);  slice_scatter_12 = view_696 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_57 = torch.ops.aten.split_with_sizes.default(slice_scatter_13, [256, 256, 256, 256], 1)
        getitem_472: "bf16[u23, 256][1024, 1]cuda:0" = split_with_sizes_57[1];  split_with_sizes_57 = None
        view_698: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_472, [-1, 2, 128]);  getitem_472 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_5: "bf16[u23, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_698, view_698);  view_698 = None
        view_699: "bf16[u23, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_5, [_local_scalar_dense_11, 256]);  copy_5 = None
        slice_scatter_14: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_13, view_699, 1, 256, 512);  slice_scatter_13 = view_699 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1079: "f32[u23, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_389, torch.float32);  getitem_389 = None
        convert_element_type_1080: "f32[u23, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_396, torch.float32);  getitem_396 = None
        neg_85: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1080)
        exp_85: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_85);  neg_85 = None
        add_564: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_85, 1);  exp_85 = None
        reciprocal_29: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_564);  add_564 = None
        mul_499: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_29, 1);  reciprocal_29 = None
        mul_500: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1079, mul_499);  convert_element_type_1079 = None
        sub_215: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_499);  mul_499 = None
        mul_501: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1080, sub_215);  convert_element_type_1080 = sub_215 = None
        add_565: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_501, 1);  mul_501 = None
        mul_502: "f32[u23, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_500, add_565);  mul_500 = add_565 = None
        convert_element_type_1081: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_502, torch.bfloat16);  mul_502 = None
        slice_scatter_15: "bf16[u23, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_14, convert_element_type_1081, 1, 0, 256);  slice_scatter_14 = convert_element_type_1081 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_41: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_15, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_360: "bf16[256, u23][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_395, [1, 0]);  getitem_395 = None
        mm_170: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_360, slice_scatter_15);  permute_360 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_361: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_756, [1, 0]);  convert_element_type_756 = None
        mm_171: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_15, permute_361);  slice_scatter_15 = permute_361 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_182: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_11, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_47: "Sym((u23//4))" = _local_scalar_dense_11 // 4
        sym_min_3: "Sym(Min(1184, (u23//4)))" = torch.sym_min(floordiv_47, 1184);  floordiv_47 = None
        sym_max_11: "Sym(Max(1, Min(1184, (u23//4))))" = torch.sym_max(sym_min_3, 1);  sym_min_3 = None
        empty_183: "f32[Max(1, Min(1184, (u23//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_11, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_184: "f32[Max(1, Min(1184, (u23//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_11, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_185: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_186: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_88 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 158, grid = [(sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1), (sym_max_11, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_182, 'DY': mm_171, 'DW': empty_183, 'DB': empty_184, 'X': getitem_201, 'W': convert_element_type_754, 'B': convert_element_type_755, 'Mean': getitem_205, 'Rstd': getitem_206, 'N': _local_scalar_dense_11}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_182 = mm_171 = empty_183 = empty_184 = getitem_201 = convert_element_type_754 = convert_element_type_755 = getitem_205 = getitem_206 = _local_scalar_dense_11 = None
        getitem_491: "bf16[u23, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_88['DX']
        getitem_492: "f32[Max(1, Min(1184, (u23//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_88['DW']
        getitem_493: "f32[Max(1, Min(1184, (u23//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_88['DB'];  triton_kernel_wrapper_functional_proxy_88 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_89 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 159, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_492, 'DB': getitem_493, 'FINAL_DW': empty_185, 'FINAL_DB': empty_186, 'N': sym_max_11}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_492 = getitem_493 = empty_185 = empty_186 = sym_max_11 = None
        getitem_494: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_89['FINAL_DW']
        getitem_495: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_89['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_89 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_566: "bf16[u23, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_548, getitem_491);  add_548 = getitem_491 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1086: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_41, torch.float32);  sum_41 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1087: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_170, torch.float32);  mm_170 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1088: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_495, torch.float32);  getitem_495 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1089: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_494, torch.float32);  getitem_494 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        full_79: "bf16[u30, 256][256, 1]cuda:0" = torch.ops.aten.full.default([_local_scalar_dense_14, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  _local_scalar_dense_14 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1515 in backward, code: (ctx.seq_len_a + ctx.seq_len_b, ctx.D),
        add_567: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(index_12, index_13);  index_12 = index_13 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1514 in backward, code: dvalues = torch.empty(
        _local_scalar_dense_16: "Sym(u64)" = torch.ops.aten._local_scalar_dense.default(add_567);  add_567 = None
        empty_187: "bf16[u64, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_16, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_32: "Sym(u64 >= 0)" = _local_scalar_dense_16 >= 0
        _assert_scalar_60 = torch.ops.aten._assert_scalar.default(ge_32, "Runtime assertion failed for expression u29 >= 0 on node 'ge'");  ge_32 = _assert_scalar_60 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:107 in _triton_concat_2D_jagged_internal, code: concat_2D_jagged_multirow[grid](
        add_568: "Sym(u9 + 1)" = _local_scalar_dense_5 + 1
        floordiv_48: "Sym(((u9 + 1)//2))" = add_568 // 2;  add_568 = None
        add_569: "Sym(u9 + 4)" = _local_scalar_dense_5 + 4
        sub_216: "Sym(u9 + 3)" = add_569 - 1;  add_569 = None
        floordiv_49: "Sym(((u9 + 3)//4))" = sub_216 // 4;  sub_216 = None
        add_570: "Sym(u9 + 8)" = _local_scalar_dense_5 + 8
        sub_217: "Sym(u9 + 7)" = add_570 - 1;  add_570 = None
        floordiv_50: "Sym(((u9 + 7)//8))" = sub_217 // 8;  sub_217 = None
        triton_kernel_wrapper_functional_proxy_90 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 2, constant_args_idx = 153, grid = [(_local_scalar_dense_5, 2, 1), (_local_scalar_dense_5, 2, 1), (_local_scalar_dense_5, 2, 1), (floordiv_48, 2, 1), (floordiv_48, 2, 1), (floordiv_48, 2, 1), (floordiv_49, 2, 1), (floordiv_49, 2, 1), (floordiv_49, 2, 1), (floordiv_50, 2, 1), (floordiv_50, 2, 1), (floordiv_50, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'OffsetsA': asynchronous_complete_cumsum_6, 'ValuesA': full_79, 'OffsetsB': getitem_266, 'ValuesB': add_558, 'Out': empty_187}, tensors_to_clone = ['Out']);  floordiv_48 = floordiv_49 = floordiv_50 = asynchronous_complete_cumsum_6 = full_79 = add_558 = empty_187 = None
        getitem_496: "bf16[u64, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_90['Out'];  triton_kernel_wrapper_functional_proxy_90 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        full_86: "bf16[u22, 256][256, 1]cuda:0" = torch.ops.aten.full.default([_local_scalar_dense_10, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  _local_scalar_dense_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1515 in backward, code: (ctx.seq_len_a + ctx.seq_len_b, ctx.D),
        add_571: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(index_10, index_11);  index_10 = index_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1514 in backward, code: dvalues = torch.empty(
        _local_scalar_dense_17: "Sym(u65)" = torch.ops.aten._local_scalar_dense.default(add_571);  add_571 = None
        empty_188: "bf16[u65, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_17, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_33: "Sym(u65 >= 0)" = _local_scalar_dense_17 >= 0
        _assert_scalar_61 = torch.ops.aten._assert_scalar.default(ge_33, "Runtime assertion failed for expression u21 >= 0 on node 'ge'");  ge_33 = _assert_scalar_61 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:107 in _triton_concat_2D_jagged_internal, code: concat_2D_jagged_multirow[grid](
        add_572: "Sym(u1 + 1)" = _local_scalar_dense_1 + 1
        floordiv_51: "Sym(((u1 + 1)//2))" = add_572 // 2;  add_572 = None
        add_573: "Sym(u1 + 4)" = _local_scalar_dense_1 + 4
        sub_218: "Sym(u1 + 3)" = add_573 - 1;  add_573 = None
        floordiv_52: "Sym(((u1 + 3)//4))" = sub_218 // 4;  sub_218 = None
        add_574: "Sym(u1 + 8)" = _local_scalar_dense_1 + 8
        sub_219: "Sym(u1 + 7)" = add_574 - 1;  add_574 = None
        floordiv_53: "Sym(((u1 + 7)//8))" = sub_219 // 8;  sub_219 = None
        triton_kernel_wrapper_functional_proxy_91 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 2, constant_args_idx = 151, grid = [(_local_scalar_dense_1, 2, 1), (_local_scalar_dense_1, 2, 1), (_local_scalar_dense_1, 2, 1), (floordiv_51, 2, 1), (floordiv_51, 2, 1), (floordiv_51, 2, 1), (floordiv_52, 2, 1), (floordiv_52, 2, 1), (floordiv_52, 2, 1), (floordiv_53, 2, 1), (floordiv_53, 2, 1), (floordiv_53, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'OffsetsA': asynchronous_complete_cumsum_4, 'ValuesA': full_86, 'OffsetsB': getitem_274, 'ValuesB': add_566, 'Out': empty_188}, tensors_to_clone = ['Out']);  floordiv_51 = floordiv_52 = floordiv_53 = asynchronous_complete_cumsum_4 = full_86 = add_566 = empty_188 = None
        getitem_497: "bf16[u65, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_91['Out'];  triton_kernel_wrapper_functional_proxy_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        clone_58: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.clone.default(add_550, memory_format = torch.contiguous_format);  add_550 = None
        view_701: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_58, [640, 256]);  clone_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_189: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_95: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_92 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 149, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_189, 'DY': view_701, 'DW': full_95, 'X': view_440, 'W': primals_267, 'Rstd': getitem_199}, tensors_to_clone = ['DX', 'DW']);  empty_189 = view_701 = full_95 = view_440 = primals_267 = getitem_199 = None
        getitem_498: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_92['DX']
        getitem_499: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_92['DW'];  triton_kernel_wrapper_functional_proxy_92 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_703: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_498, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_706: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_498, [10, 64, 256])
        slice_50: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_706, 1, 0, 32);  view_706 = None
        view_707: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_50, [10, 8192]);  slice_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_708: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_498, [10, 64, 256])
        slice_51: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_708, 1, 0, 32);  view_708 = None
        view_709: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_51, [10, 8192]);  slice_51 = None
        permute_363: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_709, [1, 0]);  view_709 = None
        mm_172: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_363, convert_element_type_742);  permute_363 = convert_element_type_742 = None
        permute_364: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_172, [1, 0]);  mm_172 = None
        permute_365: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_167, [1, 0]);  permute_167 = None
        mm_173: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_707, permute_365);  view_707 = permute_365 = None
        permute_366: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_364, [1, 0]);  permute_364 = None
        convert_element_type_1094: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_366, torch.float32);  permute_366 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1095: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_173, torch.float32);  mm_173 = None
        convert_element_type_1096: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_438, torch.float32);  view_438 = None
        neg_86: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1096)
        exp_86: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_86);  neg_86 = None
        add_575: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_86, 1);  exp_86 = None
        reciprocal_30: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_575);  add_575 = None
        mul_503: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_30, 1);  reciprocal_30 = None
        mul_504: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1095, mul_503);  convert_element_type_1095 = None
        sub_220: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_503);  mul_503 = None
        mul_505: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1096, sub_220);  convert_element_type_1096 = sub_220 = None
        add_576: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_505, 1);  mul_505 = None
        mul_506: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_504, add_576);  mul_504 = add_576 = None
        convert_element_type_1097: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_506, torch.bfloat16);  mul_506 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_710: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1097, [10, 2048]);  convert_element_type_1097 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1098: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_710, torch.float32);  view_710 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_68: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_53);  alias_53 = None
        mul_507: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1098, primals_265);  primals_265 = None
        mul_508: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_739, alias_68);  convert_element_type_739 = None
        mul_509: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_508, mul_507)
        sum_42: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_509, [1], True);  mul_509 = None
        div_89: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_508, 2048)
        mul_510: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_89, sum_42);  div_89 = sum_42 = None
        sub_221: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_507, mul_510);  mul_507 = mul_510 = None
        mul_511: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_221, alias_68);  sub_221 = alias_68 = None
        mul_512: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1098, mul_508);  convert_element_type_1098 = mul_508 = None
        sum_43: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_512, [0]);  mul_512 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1099: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_511, torch.bfloat16);  mul_511 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_711: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1099, [10, 2048]);  convert_element_type_1099 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_367: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_711, [1, 0])
        mm_174: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_367, convert_element_type_735);  permute_367 = convert_element_type_735 = None
        permute_368: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_174, [1, 0]);  mm_174 = None
        permute_369: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_166, [1, 0]);  permute_166 = None
        mm_175: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_711, permute_369);  permute_369 = None
        permute_370: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_368, [1, 0]);  permute_368 = None
        convert_element_type_1104: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_370, torch.float32);  permute_370 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1105: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_175, torch.float32);  mm_175 = None
        convert_element_type_1106: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_436, torch.float32);  view_436 = None
        neg_87: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1106)
        exp_87: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_87);  neg_87 = None
        add_577: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_87, 1);  exp_87 = None
        reciprocal_31: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_577);  add_577 = None
        mul_513: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_31, 1);  reciprocal_31 = None
        mul_514: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1105, mul_513);  convert_element_type_1105 = None
        sub_222: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_513);  mul_513 = None
        mul_515: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1106, sub_222);  convert_element_type_1106 = sub_222 = None
        add_578: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_515, 1);  mul_515 = None
        mul_516: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_514, add_578);  mul_514 = add_578 = None
        convert_element_type_1107: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_516, torch.bfloat16);  mul_516 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_712: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1107, [10, 1024]);  convert_element_type_1107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1108: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_712, torch.float32);  view_712 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_69: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_52);  alias_52 = None
        mul_517: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1108, primals_263);  primals_263 = None
        mul_518: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_732, alias_69);  convert_element_type_732 = None
        mul_519: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_518, mul_517)
        sum_44: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_519, [1], True);  mul_519 = None
        div_90: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_518, 1024)
        mul_520: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_90, sum_44);  div_90 = sum_44 = None
        sub_223: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_517, mul_520);  mul_517 = mul_520 = None
        mul_521: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_223, alias_69);  sub_223 = alias_69 = None
        mul_522: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1108, mul_518);  convert_element_type_1108 = mul_518 = None
        sum_45: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_522, [0]);  mul_522 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1109: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_521, torch.bfloat16);  mul_521 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_713: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1109, [10, 1024]);  convert_element_type_1109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_371: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_713, [1, 0])
        mm_176: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_371, convert_element_type_728);  permute_371 = convert_element_type_728 = None
        permute_372: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_176, [1, 0]);  mm_176 = None
        permute_373: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_165, [1, 0]);  permute_165 = None
        mm_177: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_713, permute_373);  view_713 = permute_373 = None
        add_579: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_711, mm_177);  view_711 = mm_177 = None
        permute_374: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_372, [1, 0]);  permute_372 = None
        convert_element_type_1114: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_374, torch.float32);  permute_374 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1115: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_579, torch.float32);  add_579 = None
        convert_element_type_1116: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_434, torch.float32);  view_434 = None
        neg_88: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1116)
        exp_88: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_88);  neg_88 = None
        add_580: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_88, 1);  exp_88 = None
        reciprocal_32: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_580);  add_580 = None
        mul_523: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_32, 1);  reciprocal_32 = None
        mul_524: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1115, mul_523);  convert_element_type_1115 = None
        sub_224: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_523);  mul_523 = None
        mul_525: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1116, sub_224);  convert_element_type_1116 = sub_224 = None
        add_581: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_525, 1);  mul_525 = None
        mul_526: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_524, add_581);  mul_524 = add_581 = None
        convert_element_type_1117: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_526, torch.bfloat16);  mul_526 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_714: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1117, [10, 2048]);  convert_element_type_1117 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1118: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_714, torch.float32);  view_714 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_70: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_51);  alias_51 = None
        mul_527: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1118, primals_261);  primals_261 = None
        mul_528: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_725, alias_70);  convert_element_type_725 = None
        mul_529: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_528, mul_527)
        sum_46: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_529, [1], True);  mul_529 = None
        div_91: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_528, 2048)
        mul_530: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_91, sum_46);  div_91 = sum_46 = None
        sub_225: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_527, mul_530);  mul_527 = mul_530 = None
        mul_531: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_225, alias_70);  sub_225 = alias_70 = None
        mul_532: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1118, mul_528);  convert_element_type_1118 = mul_528 = None
        sum_47: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_532, [0]);  mul_532 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1119: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_531, torch.bfloat16);  mul_531 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_715: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1119, [10, 2048]);  convert_element_type_1119 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_375: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_715, [1, 0])
        mm_178: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_375, convert_element_type_721);  permute_375 = convert_element_type_721 = None
        permute_376: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_178, [1, 0]);  mm_178 = None
        permute_377: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_164, [1, 0]);  permute_164 = None
        mm_179: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_715, permute_377);  permute_377 = None
        permute_378: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_376, [1, 0]);  permute_376 = None
        convert_element_type_1124: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_378, torch.float32);  permute_378 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1125: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_179, torch.float32);  mm_179 = None
        convert_element_type_1126: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_432, torch.float32);  view_432 = None
        neg_89: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1126)
        exp_89: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_89);  neg_89 = None
        add_582: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_89, 1);  exp_89 = None
        reciprocal_33: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_582);  add_582 = None
        mul_533: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_33, 1);  reciprocal_33 = None
        mul_534: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1125, mul_533);  convert_element_type_1125 = None
        sub_226: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_533);  mul_533 = None
        mul_535: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1126, sub_226);  convert_element_type_1126 = sub_226 = None
        add_583: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_535, 1);  mul_535 = None
        mul_536: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_534, add_583);  mul_534 = add_583 = None
        convert_element_type_1127: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_536, torch.bfloat16);  mul_536 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_716: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1127, [10, 1024]);  convert_element_type_1127 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1128: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_716, torch.float32);  view_716 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_71: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_50);  alias_50 = None
        mul_537: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1128, primals_259);  primals_259 = None
        mul_538: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_718, alias_71);  convert_element_type_718 = None
        mul_539: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_538, mul_537)
        sum_48: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_539, [1], True);  mul_539 = None
        div_92: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_538, 1024)
        mul_540: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_92, sum_48);  div_92 = sum_48 = None
        sub_227: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_537, mul_540);  mul_537 = mul_540 = None
        mul_541: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_227, alias_71);  sub_227 = alias_71 = None
        mul_542: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1128, mul_538);  convert_element_type_1128 = mul_538 = None
        sum_49: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_542, [0]);  mul_542 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1129: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_541, torch.bfloat16);  mul_541 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_717: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1129, [10, 1024]);  convert_element_type_1129 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_379: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_717, [1, 0])
        mm_180: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_379, convert_element_type_714);  permute_379 = convert_element_type_714 = None
        permute_380: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_180, [1, 0]);  mm_180 = None
        permute_381: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_163, [1, 0]);  permute_163 = None
        mm_181: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_717, permute_381);  view_717 = permute_381 = None
        add_584: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_715, mm_181);  view_715 = mm_181 = None
        permute_382: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_380, [1, 0]);  permute_380 = None
        convert_element_type_1134: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_382, torch.float32);  permute_382 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1135: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_584, torch.float32);  add_584 = None
        convert_element_type_1136: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_430, torch.float32);  view_430 = None
        neg_90: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1136)
        exp_90: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_90);  neg_90 = None
        add_585: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_90, 1);  exp_90 = None
        reciprocal_34: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_585);  add_585 = None
        mul_543: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_34, 1);  reciprocal_34 = None
        mul_544: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1135, mul_543);  convert_element_type_1135 = None
        sub_228: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_543);  mul_543 = None
        mul_545: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1136, sub_228);  convert_element_type_1136 = sub_228 = None
        add_586: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_545, 1);  mul_545 = None
        mul_546: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_544, add_586);  mul_544 = add_586 = None
        convert_element_type_1137: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_546, torch.bfloat16);  mul_546 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_718: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1137, [10, 2048]);  convert_element_type_1137 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1138: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_718, torch.float32);  view_718 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_72: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_49);  alias_49 = None
        mul_547: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1138, primals_257);  primals_257 = None
        mul_548: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_711, alias_72);  convert_element_type_711 = None
        mul_549: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_548, mul_547)
        sum_50: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_549, [1], True);  mul_549 = None
        div_93: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_548, 2048)
        mul_550: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_93, sum_50);  div_93 = sum_50 = None
        sub_229: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_547, mul_550);  mul_547 = mul_550 = None
        mul_551: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_229, alias_72);  sub_229 = alias_72 = None
        mul_552: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1138, mul_548);  convert_element_type_1138 = mul_548 = None
        sum_51: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_552, [0]);  mul_552 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1139: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_551, torch.bfloat16);  mul_551 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_719: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1139, [10, 2048]);  convert_element_type_1139 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_383: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_719, [1, 0])
        mm_182: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_383, convert_element_type_707);  permute_383 = convert_element_type_707 = None
        permute_384: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_182, [1, 0]);  mm_182 = None
        permute_385: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_162, [1, 0]);  permute_162 = None
        mm_183: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_719, permute_385);  view_719 = permute_385 = None
        permute_386: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_384, [1, 0]);  permute_384 = None
        convert_element_type_1144: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_386, torch.float32);  permute_386 = None
        convert_element_type_1145: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_183, torch.float32);  mm_183 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_720: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1145, [10, 8472]);  convert_element_type_1145 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_190: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_98: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_93 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 147, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_190, 'DY': view_720, 'DW': full_98, 'X': view_426, 'W': primals_255, 'Rstd': getitem_197}, tensors_to_clone = ['DX', 'DW']);  empty_190 = view_720 = full_98 = view_426 = primals_255 = getitem_197 = None
        getitem_500: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_93['DX']
        getitem_501: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_93['DW'];  triton_kernel_wrapper_functional_proxy_93 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_723: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_500, [10, 8472])
        slice_54: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_723, 1, 0, 6144);  view_723 = None
        convert_element_type_1146: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_54, torch.bfloat16);  slice_54 = None
        view_724: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_262, [10, 8472]);  getitem_262 = None
        slice_55: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_724, 1, 6144, 8472);  view_724 = None
        view_725: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_500, [10, 8472]);  getitem_500 = None
        slice_56: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_725, 1, 6144, 8472);  view_725 = None
        add_587: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(slice_55, slice_56);  slice_55 = slice_56 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_57: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1146, 1, 0, 2048)
        slice_58: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1146, 1, 2048, 6144);  convert_element_type_1146 = None
        view_726: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_58, [10, 16, 256]);  slice_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_727: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_57, [10, 32, 64]);  slice_57 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_728: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_727, [10, 32, 64]);  view_727 = None
        permute_387: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_421, [0, 2, 1]);  view_421 = None
        bmm_35: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_387, view_728);  permute_387 = None
        permute_388: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_422, [0, 2, 1]);  view_422 = None
        bmm_36: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_728, permute_388);  view_728 = permute_388 = None
        view_729: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_35, [10, 256, 64]);  bmm_35 = None
        view_730: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_36, [10, 32, 256]);  bmm_36 = None
        permute_389: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_729, [0, 2, 1]);  view_729 = None
        add_588: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_703, permute_389);  view_703 = permute_389 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_731: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_730, [10, 32, 256]);  view_730 = None
        permute_390: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_418, [0, 2, 1]);  view_418 = None
        bmm_37: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_390, view_731);  permute_390 = None
        permute_391: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_419, [0, 2, 1]);  view_419 = None
        bmm_38: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_731, permute_391);  view_731 = permute_391 = None
        view_732: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_37, [10, 64, 256]);  bmm_37 = None
        add_589: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_588, view_732);  add_588 = view_732 = None
        view_733: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_38, [10, 32, 64]);  bmm_38 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_734: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_733, [10, 2048]);  view_733 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_392: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_734, [1, 0])
        mm_184: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_392, convert_element_type_699);  permute_392 = convert_element_type_699 = None
        permute_393: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_184, [1, 0]);  mm_184 = None
        permute_394: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_160, [1, 0]);  permute_160 = None
        mm_185: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_734, permute_394);  view_734 = permute_394 = None
        permute_395: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_393, [1, 0]);  permute_393 = None
        convert_element_type_1159: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_395, torch.float32);  permute_395 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1160: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_185, torch.float32);  mm_185 = None
        convert_element_type_1161: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_416, torch.float32);  view_416 = None
        neg_91: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1161)
        exp_91: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_91);  neg_91 = None
        add_590: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_91, 1);  exp_91 = None
        reciprocal_35: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_590);  add_590 = None
        mul_553: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_35, 1);  reciprocal_35 = None
        mul_554: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1160, mul_553);  convert_element_type_1160 = None
        sub_230: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_553);  mul_553 = None
        mul_555: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1161, sub_230);  convert_element_type_1161 = sub_230 = None
        add_591: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_555, 1);  mul_555 = None
        mul_556: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_554, add_591);  mul_554 = add_591 = None
        convert_element_type_1162: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_556, torch.bfloat16);  mul_556 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_735: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1162, [10, 512]);  convert_element_type_1162 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1163: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_735, torch.float32);  view_735 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_73: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_48);  alias_48 = None
        mul_557: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1163, primals_253);  primals_253 = None
        mul_558: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_696, alias_73);  convert_element_type_696 = None
        mul_559: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_558, mul_557)
        sum_52: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_559, [1], True);  mul_559 = None
        div_94: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_558, 512)
        mul_560: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_94, sum_52);  div_94 = sum_52 = None
        sub_231: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_557, mul_560);  mul_557 = mul_560 = None
        mul_561: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_231, alias_73);  sub_231 = alias_73 = None
        mul_562: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1163, mul_558);  convert_element_type_1163 = mul_558 = None
        sum_53: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_562, [0]);  mul_562 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1164: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_561, torch.bfloat16);  mul_561 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_736: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1164, [10, 512]);  convert_element_type_1164 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_396: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_736, [1, 0])
        mm_186: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_396, view_414);  permute_396 = view_414 = None
        permute_397: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_186, [1, 0]);  mm_186 = None
        permute_398: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_159, [1, 0]);  permute_159 = None
        mm_187: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_736, permute_398);  view_736 = permute_398 = None
        permute_399: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_397, [1, 0]);  permute_397 = None
        convert_element_type_1169: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_399, torch.float32);  permute_399 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_737: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_187, [10, 32, 256]);  mm_187 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_738: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_498, [10, 64, 256]);  getitem_498 = None
        slice_59: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_738, 1, 32, 64);  view_738 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_43: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_59, view_737, view_726], 1);  slice_59 = view_737 = view_726 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_400: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_43, [0, 2, 1]);  cat_43 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_59: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_400, memory_format = torch.contiguous_format);  permute_400 = None
        view_739: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_59, [2560, 80]);  clone_59 = None
        permute_401: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_739, [1, 0])
        mm_188: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_401, view_412);  permute_401 = view_412 = None
        permute_402: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_188, [1, 0]);  mm_188 = None
        permute_403: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_157, [1, 0]);  permute_157 = None
        mm_189: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_739, permute_403);  view_739 = permute_403 = None
        view_740: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_189, [10, 256, 64]);  mm_189 = None
        permute_404: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_402, [1, 0]);  permute_402 = None
        convert_element_type_1174: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_404, torch.float32);  permute_404 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_405: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_740, [0, 2, 1]);  view_740 = None
        add_592: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_589, permute_405);  add_589 = permute_405 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_406: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(add_592, [0, 2, 1]);  add_592 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_60: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_406, memory_format = torch.contiguous_format);  permute_406 = None
        view_741: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_60, [2560, 64]);  clone_60 = None
        permute_407: "bf16[64, 2560][1, 64]cuda:0" = torch.ops.aten.permute.default(view_741, [1, 0])
        mm_190: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(permute_407, view_410);  permute_407 = view_410 = None
        permute_408: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(mm_190, [1, 0]);  mm_190 = None
        permute_409: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_154, [1, 0]);  permute_154 = None
        mm_191: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(view_741, permute_409);  view_741 = permute_409 = None
        view_742: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.view.default(mm_191, [10, 256, 112]);  mm_191 = None
        permute_410: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_408, [1, 0]);  permute_408 = None
        convert_element_type_1179: "f32[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_410, torch.float32);  permute_410 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_411: "bf16[10, 112, 256][28672, 1, 112]cuda:0" = torch.ops.aten.permute.default(view_742, [0, 2, 1]);  view_742 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        slice_60: "bf16[10, 64, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_411, 1, 0, 64)
        slice_61: "bf16[10, 32, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_411, 1, 64, 96)
        slice_62: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_411, 1, 96, 112);  permute_411 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_743: "bf16[10, 1, 16, 256][28672, 16, 1, 112]cuda:0" = torch.ops.aten.view.default(slice_62, [10, 1, 16, 256]);  slice_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        mul_563: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_743, index_8);  index_8 = None
        mul_564: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_743, index_9);  view_743 = index_9 = None
        full_99: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_2: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_99, [None, getitem_192], mul_563, True);  full_99 = getitem_192 = mul_563 = None
        full_100: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_3: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_100, [None, getitem_191], mul_564, True);  full_100 = getitem_191 = mul_564 = None
        add_593: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_2, index_put_3);  index_put_2 = index_put_3 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        select_20: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_593, 1, 0)
        select_21: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_593, 1, 1);  add_593 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        slice_63: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_61, 1, 0, 16)
        slice_64: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_61, 1, 16, 32);  slice_61 = None
        add_594: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_20, slice_63);  select_20 = slice_63 = None
        add_595: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_21, slice_64);  select_21 = slice_64 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_744: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_595, [160, 256]);  add_595 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_191: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_103: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_94 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 145, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_191, 'DY': view_744, 'DW': full_103, 'X': view_403, 'W': primals_249, 'Rstd': getitem_190}, tensors_to_clone = ['DX', 'DW']);  empty_191 = view_744 = full_103 = view_403 = primals_249 = getitem_190 = None
        getitem_502: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_94['DX']
        getitem_503: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_94['DW'];  triton_kernel_wrapper_functional_proxy_94 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_748: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_502, [10, 16, 256])
        view_749: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_748, [160, 256]);  view_748 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_751: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_502, [10, 16, 256])
        view_752: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_751, [160, 256]);  view_751 = None
        view_753: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_752, [10, 4096]);  view_752 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_754: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_502, [10, 16, 256]);  getitem_502 = None
        view_755: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_754, [160, 256]);  view_754 = None
        view_756: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_755, [10, 4096]);  view_755 = None
        permute_413: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_756, [1, 0]);  view_756 = None
        mm_192: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_413, mul_214);  permute_413 = mul_214 = None
        permute_414: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_192, [1, 0]);  mm_192 = None
        permute_415: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_152, [1, 0]);  permute_152 = None
        mm_193: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_753, permute_415);  view_753 = permute_415 = None
        permute_416: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_414, [1, 0]);  permute_414 = None
        convert_element_type_1184: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_416, torch.float32);  permute_416 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_565: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_193, convert_element_type_676);  convert_element_type_676 = None
        mul_566: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_193, mm_95);  mm_193 = mm_95 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_417: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_565, [1, 0])
        mm_194: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_417, convert_element_type_677);  permute_417 = convert_element_type_677 = None
        permute_418: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_194, [1, 0]);  mm_194 = None
        permute_419: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_151, [1, 0]);  permute_151 = None
        mm_195: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_565, permute_419);  mul_565 = permute_419 = None
        permute_420: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_418, [1, 0]);  permute_418 = None
        convert_element_type_1189: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_420, torch.float32);  permute_420 = None
        convert_element_type_1190: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_195, torch.float32);  mm_195 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1191: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_566, torch.float32);  mul_566 = None
        convert_element_type_1192: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_94, torch.float32);  mm_94 = None
        neg_92: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1192)
        exp_92: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_92);  neg_92 = None
        add_596: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_92, 1);  exp_92 = None
        reciprocal_36: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_596);  add_596 = None
        mul_567: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_36, 1);  reciprocal_36 = None
        mul_568: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1191, mul_567);  convert_element_type_1191 = None
        sub_232: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_567);  mul_567 = None
        mul_569: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1192, sub_232);  convert_element_type_1192 = sub_232 = None
        add_597: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_569, 1);  mul_569 = None
        mul_570: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_568, add_597);  mul_568 = add_597 = None
        convert_element_type_1193: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_570, torch.bfloat16);  mul_570 = None
        permute_421: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1193, [1, 0])
        mm_196: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_421, convert_element_type_671);  permute_421 = convert_element_type_671 = None
        permute_422: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_196, [1, 0]);  mm_196 = None
        permute_423: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_150, [1, 0]);  permute_150 = None
        mm_197: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1193, permute_423);  convert_element_type_1193 = permute_423 = None
        permute_424: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_422, [1, 0]);  permute_422 = None
        convert_element_type_1198: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_424, torch.float32);  permute_424 = None
        convert_element_type_1199: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_197, torch.float32);  mm_197 = None
        add_598: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1190, convert_element_type_1199);  convert_element_type_1190 = convert_element_type_1199 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_757: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_598, [160, 256]);  add_598 = None
        sub_233: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_670, getitem_188);  convert_element_type_670 = getitem_188 = None
        mul_571: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_233, rsqrt_51);  sub_233 = None
        mul_572: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_757, primals_244);  primals_244 = None
        mul_573: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_572, 256)
        sum_54: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_572, [1], True)
        mul_574: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_572, mul_571);  mul_572 = None
        sum_55: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_574, [1], True);  mul_574 = None
        mul_575: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_571, sum_55);  sum_55 = None
        sub_234: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_573, sum_54);  mul_573 = sum_54 = None
        sub_235: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_234, mul_575);  sub_234 = mul_575 = None
        div_95: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_51, 256);  rsqrt_51 = None
        mul_576: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_95, sub_235);  div_95 = sub_235 = None
        mul_577: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_757, mul_571);  mul_571 = None
        sum_56: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_577, [0]);  mul_577 = None
        sum_57: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_757, [0]);  view_757 = None
        convert_element_type_1200: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_576, torch.bfloat16);  mul_576 = None
        add_599: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_749, convert_element_type_1200);  view_749 = convert_element_type_1200 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_758: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_599, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_105: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_106: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_11, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_11 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_192: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_95 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 143, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_192, 'Q': view_396, 'K': view_397, 'V': view_397, 'DO': view_758, 'total_seq_len_kv': _local_scalar_dense_7, 'seq_offsets': getitem_186, 'seq_offsets_q': getitem_185, 'DQ': full_105, 'DK': full_106, 'DV': full_106, 'max_seq_len': _local_scalar_dense_5, 'attn_scale': reciprocal_11, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_5}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_192 = view_396 = view_397 = view_758 = getitem_186 = getitem_185 = full_105 = full_106 = reciprocal_11 = None
        getitem_504: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_95['seq_offsets']
        getitem_506: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_95['DQ']
        getitem_507: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_95['DK'];  triton_kernel_wrapper_functional_proxy_95 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_760: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_507, [_local_scalar_dense_7, 256]);  getitem_507 = None
        add_600: "bf16[u64, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_496, view_760);  getitem_496 = view_760 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_762: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_506, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_763: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_506, [160, 256]);  getitem_506 = None
        permute_426: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_763, [1, 0]);  view_763 = None
        mm_198: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_426, view_395);  permute_426 = view_395 = None
        permute_427: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_198, [1, 0]);  mm_198 = None
        permute_428: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_149, [1, 0]);  permute_149 = None
        mm_199: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_762, permute_428);  view_762 = permute_428 = None
        permute_429: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_427, [1, 0]);  permute_427 = None
        convert_element_type_1205: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_429, torch.float32);  permute_429 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_764: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_199, [160, 256]);  mm_199 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_193: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_109: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_96 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 141, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_193, 'DY': view_764, 'DW': full_109, 'X': view_393, 'W': primals_242, 'Rstd': getitem_183}, tensors_to_clone = ['DX', 'DW']);  empty_193 = view_764 = full_109 = view_393 = primals_242 = getitem_183 = None
        getitem_508: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_96['DX']
        getitem_509: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_96['DW'];  triton_kernel_wrapper_functional_proxy_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_766: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_508, [160, 256]);  getitem_508 = None
        add_601: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_599, view_766);  add_599 = view_766 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_767: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_601, [10, 16, 256]);  add_601 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_430: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_767, [0, 2, 1]);  view_767 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_62: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_430, memory_format = torch.contiguous_format);  permute_430 = None
        view_768: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_62, [2560, 16]);  clone_62 = None
        permute_431: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_768, [1, 0])
        mm_200: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_431, view_390);  permute_431 = view_390 = None
        permute_432: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_200, [1, 0]);  mm_200 = None
        permute_433: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_146, [1, 0]);  permute_146 = None
        mm_201: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_768, permute_433);  view_768 = permute_433 = None
        view_769: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_201, [10, 256, 64]);  mm_201 = None
        permute_434: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_432, [1, 0]);  permute_432 = None
        convert_element_type_1210: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_434, torch.float32);  permute_434 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_435: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_769, [0, 2, 1]);  view_769 = None
        add_602: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(slice_60, permute_435);  slice_60 = permute_435 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_770: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_594, [160, 256]);  add_594 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_194: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_112: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_97 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 139, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_194, 'DY': view_770, 'DW': full_112, 'X': view_387, 'W': primals_240, 'Rstd': getitem_181}, tensors_to_clone = ['DX', 'DW']);  empty_194 = view_770 = full_112 = view_387 = primals_240 = getitem_181 = None
        getitem_510: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_97['DX']
        getitem_511: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_97['DW'];  triton_kernel_wrapper_functional_proxy_97 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_774: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_510, [10, 16, 256])
        view_775: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_774, [160, 256]);  view_774 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_777: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_510, [10, 16, 256])
        view_778: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_777, [160, 256]);  view_777 = None
        view_779: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_778, [10, 4096]);  view_778 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_780: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_510, [10, 16, 256]);  getitem_510 = None
        view_781: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_780, [160, 256]);  view_780 = None
        view_782: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_781, [10, 4096]);  view_781 = None
        permute_437: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_782, [1, 0]);  view_782 = None
        mm_202: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_437, mul_208);  permute_437 = mul_208 = None
        permute_438: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_202, [1, 0]);  mm_202 = None
        permute_439: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_144, [1, 0]);  permute_144 = None
        mm_203: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_779, permute_439);  view_779 = permute_439 = None
        permute_440: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_438, [1, 0]);  permute_438 = None
        convert_element_type_1215: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_440, torch.float32);  permute_440 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_578: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_203, convert_element_type_652);  convert_element_type_652 = None
        mul_579: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_203, mm_90);  mm_203 = mm_90 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_441: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_578, [1, 0])
        mm_204: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_441, convert_element_type_653);  permute_441 = convert_element_type_653 = None
        permute_442: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_204, [1, 0]);  mm_204 = None
        permute_443: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_143, [1, 0]);  permute_143 = None
        mm_205: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_578, permute_443);  mul_578 = permute_443 = None
        permute_444: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_442, [1, 0]);  permute_442 = None
        convert_element_type_1220: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_444, torch.float32);  permute_444 = None
        convert_element_type_1221: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_205, torch.float32);  mm_205 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1222: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_579, torch.float32);  mul_579 = None
        convert_element_type_1223: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_89, torch.float32);  mm_89 = None
        neg_93: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1223)
        exp_93: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_93);  neg_93 = None
        add_603: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_93, 1);  exp_93 = None
        reciprocal_37: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_603);  add_603 = None
        mul_580: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_37, 1);  reciprocal_37 = None
        mul_581: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1222, mul_580);  convert_element_type_1222 = None
        sub_236: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_580);  mul_580 = None
        mul_582: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1223, sub_236);  convert_element_type_1223 = sub_236 = None
        add_604: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_582, 1);  mul_582 = None
        mul_583: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_581, add_604);  mul_581 = add_604 = None
        convert_element_type_1224: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_583, torch.bfloat16);  mul_583 = None
        permute_445: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1224, [1, 0])
        mm_206: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_445, convert_element_type_647);  permute_445 = convert_element_type_647 = None
        permute_446: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_206, [1, 0]);  mm_206 = None
        permute_447: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_142, [1, 0]);  permute_142 = None
        mm_207: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1224, permute_447);  convert_element_type_1224 = permute_447 = None
        permute_448: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_446, [1, 0]);  permute_446 = None
        convert_element_type_1229: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_448, torch.float32);  permute_448 = None
        convert_element_type_1230: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_207, torch.float32);  mm_207 = None
        add_605: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1221, convert_element_type_1230);  convert_element_type_1221 = convert_element_type_1230 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_783: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_605, [160, 256]);  add_605 = None
        sub_237: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_646, getitem_179);  convert_element_type_646 = getitem_179 = None
        mul_584: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_237, rsqrt_50);  sub_237 = None
        mul_585: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_783, primals_235);  primals_235 = None
        mul_586: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_585, 256)
        sum_58: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_585, [1], True)
        mul_587: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_585, mul_584);  mul_585 = None
        sum_59: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_587, [1], True);  mul_587 = None
        mul_588: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_584, sum_59);  sum_59 = None
        sub_238: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_586, sum_58);  mul_586 = sum_58 = None
        sub_239: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_238, mul_588);  sub_238 = mul_588 = None
        div_96: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_50, 256);  rsqrt_50 = None
        mul_589: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_96, sub_239);  div_96 = sub_239 = None
        mul_590: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_783, mul_584);  mul_584 = None
        sum_60: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_590, [0]);  mul_590 = None
        sum_61: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_783, [0]);  view_783 = None
        convert_element_type_1231: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_589, torch.bfloat16);  mul_589 = None
        add_606: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_775, convert_element_type_1231);  view_775 = convert_element_type_1231 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_784: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_606, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_114: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_115: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_10, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_10 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_195: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_98 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 137, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_195, 'Q': view_380, 'K': view_381, 'V': view_381, 'DO': view_784, 'total_seq_len_kv': _local_scalar_dense_3, 'seq_offsets': getitem_177, 'seq_offsets_q': getitem_176, 'DQ': full_114, 'DK': full_115, 'DV': full_115, 'max_seq_len': _local_scalar_dense_1, 'attn_scale': reciprocal_10, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_1}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_195 = view_380 = view_381 = view_784 = getitem_177 = getitem_176 = full_114 = full_115 = reciprocal_10 = None
        getitem_512: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_98['seq_offsets']
        getitem_514: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_98['DQ']
        getitem_515: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_98['DK'];  triton_kernel_wrapper_functional_proxy_98 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_786: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_515, [_local_scalar_dense_3, 256]);  getitem_515 = None
        add_607: "bf16[u65, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_497, view_786);  getitem_497 = view_786 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_788: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_514, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_789: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_514, [160, 256]);  getitem_514 = None
        permute_450: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_789, [1, 0]);  view_789 = None
        mm_208: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_450, view_379);  permute_450 = view_379 = None
        permute_451: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_208, [1, 0]);  mm_208 = None
        permute_452: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_141, [1, 0]);  permute_141 = None
        mm_209: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_788, permute_452);  view_788 = permute_452 = None
        permute_453: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_451, [1, 0]);  permute_451 = None
        convert_element_type_1236: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_453, torch.float32);  permute_453 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_790: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_209, [160, 256]);  mm_209 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_196: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_118: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_99 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 135, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_196, 'DY': view_790, 'DW': full_118, 'X': view_377, 'W': primals_233, 'Rstd': getitem_174}, tensors_to_clone = ['DX', 'DW']);  empty_196 = view_790 = full_118 = view_377 = primals_233 = getitem_174 = None
        getitem_516: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_99['DX']
        getitem_517: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_99['DW'];  triton_kernel_wrapper_functional_proxy_99 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_792: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_516, [160, 256]);  getitem_516 = None
        add_608: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_606, view_792);  add_606 = view_792 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_793: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_608, [10, 16, 256]);  add_608 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_454: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_793, [0, 2, 1]);  view_793 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_64: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_454, memory_format = torch.contiguous_format);  permute_454 = None
        view_794: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_64, [2560, 16]);  clone_64 = None
        permute_455: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_794, [1, 0])
        mm_210: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_455, view_374);  permute_455 = view_374 = None
        permute_456: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_210, [1, 0]);  mm_210 = None
        permute_457: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_138, [1, 0]);  permute_138 = None
        mm_211: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_794, permute_457);  view_794 = permute_457 = None
        view_795: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_211, [10, 256, 64]);  mm_211 = None
        permute_458: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_456, [1, 0]);  permute_456 = None
        convert_element_type_1241: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_458, torch.float32);  permute_458 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_459: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_795, [0, 2, 1]);  view_795 = None
        add_609: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(add_602, permute_459);  add_602 = permute_459 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_460: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_632, [1, 0]);  convert_element_type_632 = None
        mm_212: "bf16[u64, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_600, permute_460);  permute_460 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_197: "bf16[u64, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_16, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        sym_size_int_30: "Sym(u15)" = torch.ops.aten.sym_size.int(view_371, 0)
        empty_198: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_30, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_30 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_199: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_54: "Sym((u64//4))" = _local_scalar_dense_16 // 4
        sym_min_4: "Sym(Min(9472, (u64//4)))" = torch.sym_min(floordiv_54, 9472);  floordiv_54 = None
        sym_max_12: "Sym(Max(1, Min(9472, (u64//4))))" = torch.sym_max(sym_min_4, 1);  sym_min_4 = None
        empty_200: "f32[Max(1, Min(9472, (u64//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_12, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_201: "f32[Max(1, Min(9472, (u64//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_12, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_202: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_203: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_100 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 27, constant_args_idx = 132, grid = [(sym_max_12, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_198, 'DU': empty_199, 'DY': mm_212, 'DW': empty_200, 'DB': empty_201, 'X': view_371, 'U': convert_element_type_629, 'Y': empty_197, 'W': convert_element_type_630, 'B': convert_element_type_631, 'Mean': getitem_171, 'Rstd': getitem_172, 'seed': primals_231, 'N': _local_scalar_dense_16}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_198 = empty_199 = mm_212 = empty_200 = empty_201 = view_371 = convert_element_type_629 = empty_197 = convert_element_type_630 = convert_element_type_631 = getitem_171 = getitem_172 = primals_231 = _local_scalar_dense_16 = None
        getitem_518: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_100['DX']
        getitem_519: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_100['DU']
        getitem_520: "f32[Max(1, Min(9472, (u64//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_100['DW']
        getitem_521: "f32[Max(1, Min(9472, (u64//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_100['DB']
        getitem_522: "bf16[u64, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_100['Y'];  triton_kernel_wrapper_functional_proxy_100 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_101 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 133, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_520, 'DB': getitem_521, 'FINAL_DW': empty_202, 'FINAL_DB': empty_203, 'N': sym_max_12}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_520 = getitem_521 = empty_202 = empty_203 = sym_max_12 = None
        getitem_523: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_101['FINAL_DW']
        getitem_524: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_101['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_101 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_462: "bf16[768, u64][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_522, [1, 0]);  getitem_522 = None
        mm_213: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_462, add_600);  permute_462 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1246: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_213, torch.float32);  mm_213 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1247: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_524, torch.float32);  getitem_524 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1248: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_523, torch.float32);  getitem_523 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_204: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        add_610: "Sym(u15 + 4)" = _local_scalar_dense_7 + 4
        sub_240: "Sym(u15 + 3)" = add_610 - 1;  add_610 = None
        floordiv_55: "Sym(((u15 + 3)//4))" = sub_240 // 4;  sub_240 = None
        add_611: "Sym(u15 + 8)" = _local_scalar_dense_7 + 8
        sub_241: "Sym(u15 + 7)" = add_611 - 1;  add_611 = None
        floordiv_56: "Sym(((u15 + 7)//8))" = sub_241 // 8;  sub_241 = None
        add_612: "Sym(u15 + 16)" = _local_scalar_dense_7 + 16
        sub_242: "Sym(u15 + 15)" = add_612 - 1;  add_612 = None
        floordiv_57: "Sym(((u15 + 15)//16))" = sub_242 // 16;  sub_242 = None
        triton_kernel_wrapper_functional_proxy_102 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 127, grid = [(floordiv_55, 1, 1), (floordiv_55, 1, 1), (floordiv_55, 1, 1), (floordiv_56, 1, 1), (floordiv_56, 1, 1), (floordiv_56, 1, 1), (floordiv_57, 1, 1), (floordiv_57, 1, 1), (floordiv_57, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_146, 'Y': empty_204, 'W': convert_element_type_621, 'B': convert_element_type_622, 'Mean': getitem_161, 'Rstd': getitem_162, 'N': _local_scalar_dense_7}, tensors_to_clone = ['Y']);  floordiv_55 = floordiv_56 = floordiv_57 = empty_204 = None
        getitem_525: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_102['Y'];  triton_kernel_wrapper_functional_proxy_102 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_20: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_624, getitem_525, convert_element_type_623);  convert_element_type_624 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_62 = torch.ops.aten.split_with_sizes.default(addmm_20, [256, 256, 256, 256], 1);  addmm_20 = None
        getitem_526: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_62[0]
        getitem_527: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_62[1]
        getitem_528: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_62[2]
        getitem_529: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_62[3];  split_with_sizes_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_205: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:473 in backward, code: du, dv, dq, dk = duvqk.split(
        split_with_sizes_63 = torch.ops.aten.split_with_sizes.default(empty_205, [256, 256, 256, 256], 1)
        getitem_532: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_63[2];  split_with_sizes_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_797: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_528, [-1, 2, 128]);  getitem_528 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_798: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_529, [-1, 2, 128]);  getitem_529 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_799: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_527, [-1, 2, 128]);  getitem_527 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:485 in backward, code: dq = dq.view(-1, ctx.num_heads, ctx.attn_dim)
        view_800: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_532, [-1, 2, 128]);  getitem_532 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_206: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        add_613: "Sym(u9 + 16)" = _local_scalar_dense_5 + 16
        sub_243: "Sym(u9 + 15)" = add_613 - 1;  add_613 = None
        floordiv_58: "Sym(((u9 + 15)//16))" = sub_243 // 16;  sub_243 = None
        full_141: "i32[4, ((u9 + 15)//16)][Max(1, ((u9 + 15)//16)), 1]cuda:0" = torch.ops.aten.full.default([4, floordiv_58], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  floordiv_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        sym_size_int_31: "Sym(u15)" = torch.ops.aten.sym_size.int(view_800, 0);  view_800 = None
        full_142: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_31, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_31 = None
        view_803: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_142, [_local_scalar_dense_7, 256]);  full_142 = None
        slice_scatter_16: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_205, view_803, 1, 512, 768);  empty_205 = view_803 = None
        split_with_sizes_65 = torch.ops.aten.split_with_sizes.default(slice_scatter_16, [256, 256, 256, 256], 1)
        getitem_540: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_65[2];  split_with_sizes_65 = None
        view_804: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_540, [-1, 2, 128]);  getitem_540 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_805: "bf16[u15, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_518, [sym_size_int_7, 2, 128]);  getitem_518 = sym_size_int_7 = None
        split_with_sizes_66 = torch.ops.aten.split_with_sizes.default(slice_scatter_16, [256, 256, 256, 256], 1)
        getitem_545: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_66[3];  split_with_sizes_66 = None
        view_806: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_545, [-1, 2, 128]);  getitem_545 = None
        split_with_sizes_67 = torch.ops.aten.split_with_sizes.default(slice_scatter_16, [256, 256, 256, 256], 1)
        getitem_547: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_67[1];  split_with_sizes_67 = None
        view_807: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_547, [-1, 2, 128]);  getitem_547 = None
        add_614: "Sym(u9 + 32)" = _local_scalar_dense_5 + 32
        sub_244: "Sym(u9 + 31)" = add_614 - 1;  add_614 = None
        floordiv_59: "Sym(((u9 + 31)//32))" = sub_244 // 32;  sub_244 = None
        triton_kernel_wrapper_functional_proxy_103 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 128, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, floordiv_59, 1), (4, floordiv_59, 1), (4, floordiv_59, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_797, 'K': view_798, 'V': view_799, 'sort_by_length_indices': getitem_168, 'seq_offsets': getitem_504, 'attn_scale': reciprocal_9, 'DOut': view_805, 'DQ': view_804, 'DK': view_806, 'DV': view_807, 'LOCK': full_141, 'M': empty_206, 'MAX_SEQ_LEN': _local_scalar_dense_5, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_5}, tensors_to_clone = ['DQ', 'DK', 'DV']);  floordiv_59 = view_797 = view_798 = view_799 = getitem_168 = reciprocal_9 = view_805 = view_804 = view_806 = view_807 = full_141 = empty_206 = _local_scalar_dense_5 = None
        getitem_550: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_103['DQ']
        getitem_551: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_103['DK']
        getitem_552: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_103['DV'];  triton_kernel_wrapper_functional_proxy_103 = None
        view_808: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_550, [_local_scalar_dense_7, 256]);  getitem_550 = None
        slice_scatter_17: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_16, view_808, 1, 512, 768);  slice_scatter_16 = view_808 = None
        view_810: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_551, [_local_scalar_dense_7, 256]);  getitem_551 = None
        slice_scatter_18: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_17, view_810, 1, 768, 1024);  slice_scatter_17 = view_810 = None
        view_812: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_552, [_local_scalar_dense_7, 256]);  getitem_552 = None
        slice_scatter_19: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_18, view_812, 1, 256, 512);  slice_scatter_18 = view_812 = None
        split_with_sizes_74 = torch.ops.aten.split_with_sizes.default(slice_scatter_19, [256, 256, 256, 256], 1)
        getitem_579: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_74[2];  split_with_sizes_74 = None
        view_814: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_579, [-1, 2, 128]);  getitem_579 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_6: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_814, view_814);  view_814 = None
        view_815: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_6, [_local_scalar_dense_7, 256]);  copy_6 = None
        slice_scatter_20: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_19, view_815, 1, 512, 768);  slice_scatter_19 = view_815 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_77 = torch.ops.aten.split_with_sizes.default(slice_scatter_20, [256, 256, 256, 256], 1)
        getitem_592: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_77[3];  split_with_sizes_77 = None
        view_817: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_592, [-1, 2, 128]);  getitem_592 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_7: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_817, view_817);  view_817 = None
        view_818: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_7, [_local_scalar_dense_7, 256]);  copy_7 = None
        slice_scatter_21: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_20, view_818, 1, 768, 1024);  slice_scatter_20 = view_818 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_80 = torch.ops.aten.split_with_sizes.default(slice_scatter_21, [256, 256, 256, 256], 1)
        getitem_602: "bf16[u15, 256][1024, 1]cuda:0" = split_with_sizes_80[1];  split_with_sizes_80 = None
        view_820: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_602, [-1, 2, 128]);  getitem_602 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_8: "bf16[u15, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_820, view_820);  view_820 = None
        view_821: "bf16[u15, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_8, [_local_scalar_dense_7, 256]);  copy_8 = None
        slice_scatter_22: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_21, view_821, 1, 256, 512);  slice_scatter_21 = view_821 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1252: "f32[u15, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_519, torch.float32);  getitem_519 = None
        convert_element_type_1253: "f32[u15, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_526, torch.float32);  getitem_526 = None
        neg_94: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1253)
        exp_94: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_94);  neg_94 = None
        add_615: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_94, 1);  exp_94 = None
        reciprocal_38: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_615);  add_615 = None
        mul_591: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_38, 1);  reciprocal_38 = None
        mul_592: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1252, mul_591);  convert_element_type_1252 = None
        sub_245: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_591);  mul_591 = None
        mul_593: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1253, sub_245);  convert_element_type_1253 = sub_245 = None
        add_616: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_593, 1);  mul_593 = None
        mul_594: "f32[u15, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_592, add_616);  mul_592 = add_616 = None
        convert_element_type_1254: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_594, torch.bfloat16);  mul_594 = None
        slice_scatter_23: "bf16[u15, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_22, convert_element_type_1254, 1, 0, 256);  slice_scatter_22 = convert_element_type_1254 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_62: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_23, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_464: "bf16[256, u15][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_525, [1, 0]);  getitem_525 = None
        mm_214: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_464, slice_scatter_23);  permute_464 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_465: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_623, [1, 0]);  convert_element_type_623 = None
        mm_215: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_23, permute_465);  slice_scatter_23 = permute_465 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_207: "bf16[u15, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_7, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_60: "Sym((u15//4))" = _local_scalar_dense_7 // 4
        sym_min_5: "Sym(Min(1184, (u15//4)))" = torch.sym_min(floordiv_60, 1184);  floordiv_60 = None
        sym_max_13: "Sym(Max(1, Min(1184, (u15//4))))" = torch.sym_max(sym_min_5, 1);  sym_min_5 = None
        empty_208: "f32[Max(1, Min(1184, (u15//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_13, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_209: "f32[Max(1, Min(1184, (u15//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_13, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_210: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_211: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_104 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 129, grid = [(sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1), (sym_max_13, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_207, 'DY': mm_215, 'DW': empty_208, 'DB': empty_209, 'X': getitem_146, 'W': convert_element_type_621, 'B': convert_element_type_622, 'Mean': getitem_161, 'Rstd': getitem_162, 'N': _local_scalar_dense_7}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_207 = mm_215 = empty_208 = empty_209 = getitem_146 = convert_element_type_621 = convert_element_type_622 = getitem_161 = getitem_162 = _local_scalar_dense_7 = None
        getitem_621: "bf16[u15, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_104['DX']
        getitem_622: "f32[Max(1, Min(1184, (u15//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_104['DW']
        getitem_623: "f32[Max(1, Min(1184, (u15//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_104['DB'];  triton_kernel_wrapper_functional_proxy_104 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_105 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 130, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_622, 'DB': getitem_623, 'FINAL_DW': empty_210, 'FINAL_DB': empty_211, 'N': sym_max_13}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_622 = getitem_623 = empty_210 = empty_211 = sym_max_13 = None
        getitem_624: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_105['FINAL_DW']
        getitem_625: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_105['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_105 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_617: "bf16[u64, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_600, getitem_621);  add_600 = getitem_621 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1259: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_62, torch.float32);  sum_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1260: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_214, torch.float32);  mm_214 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1261: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_625, torch.float32);  getitem_625 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1262: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_624, torch.float32);  getitem_624 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_466: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_613, [1, 0]);  convert_element_type_613 = None
        mm_216: "bf16[u65, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_607, permute_466);  permute_466 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_212: "bf16[u65, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_17, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        sym_size_int_32: "Sym(u7)" = torch.ops.aten.sym_size.int(view_363, 0)
        empty_213: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_size_int_32, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_32 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_214: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_61: "Sym((u65//4))" = _local_scalar_dense_17 // 4
        sym_min_6: "Sym(Min(9472, (u65//4)))" = torch.sym_min(floordiv_61, 9472);  floordiv_61 = None
        sym_max_14: "Sym(Max(1, Min(9472, (u65//4))))" = torch.sym_max(sym_min_6, 1);  sym_min_6 = None
        empty_215: "f32[Max(1, Min(9472, (u65//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_14, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_216: "f32[Max(1, Min(9472, (u65//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_14, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_217: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_218: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_106 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 25, constant_args_idx = 123, grid = [(sym_max_14, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_213, 'DU': empty_214, 'DY': mm_216, 'DW': empty_215, 'DB': empty_216, 'X': view_363, 'U': convert_element_type_610, 'Y': empty_212, 'W': convert_element_type_611, 'B': convert_element_type_612, 'Mean': getitem_158, 'Rstd': getitem_159, 'seed': primals_223, 'N': _local_scalar_dense_17}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_213 = empty_214 = mm_216 = empty_215 = empty_216 = view_363 = convert_element_type_610 = empty_212 = convert_element_type_611 = convert_element_type_612 = getitem_158 = getitem_159 = primals_223 = _local_scalar_dense_17 = None
        getitem_626: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_106['DX']
        getitem_627: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_106['DU']
        getitem_628: "f32[Max(1, Min(9472, (u65//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_106['DW']
        getitem_629: "f32[Max(1, Min(9472, (u65//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_106['DB']
        getitem_630: "bf16[u65, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_106['Y'];  triton_kernel_wrapper_functional_proxy_106 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_107 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 124, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_628, 'DB': getitem_629, 'FINAL_DW': empty_217, 'FINAL_DB': empty_218, 'N': sym_max_14}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_628 = getitem_629 = empty_217 = empty_218 = sym_max_14 = None
        getitem_631: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_107['FINAL_DW']
        getitem_632: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_107['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_468: "bf16[768, u65][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_630, [1, 0]);  getitem_630 = None
        mm_217: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_468, add_607);  permute_468 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1267: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_217, torch.float32);  mm_217 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1268: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_632, torch.float32);  getitem_632 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1269: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_631, torch.float32);  getitem_631 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_219: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        add_618: "Sym(u7 + 4)" = _local_scalar_dense_3 + 4
        sub_246: "Sym(u7 + 3)" = add_618 - 1;  add_618 = None
        floordiv_62: "Sym(((u7 + 3)//4))" = sub_246 // 4;  sub_246 = None
        add_619: "Sym(u7 + 8)" = _local_scalar_dense_3 + 8
        sub_247: "Sym(u7 + 7)" = add_619 - 1;  add_619 = None
        floordiv_63: "Sym(((u7 + 7)//8))" = sub_247 // 8;  sub_247 = None
        add_620: "Sym(u7 + 16)" = _local_scalar_dense_3 + 16
        sub_248: "Sym(u7 + 15)" = add_620 - 1;  add_620 = None
        floordiv_64: "Sym(((u7 + 15)//16))" = sub_248 // 16;  sub_248 = None
        triton_kernel_wrapper_functional_proxy_108 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 118, grid = [(floordiv_62, 1, 1), (floordiv_62, 1, 1), (floordiv_62, 1, 1), (floordiv_63, 1, 1), (floordiv_63, 1, 1), (floordiv_63, 1, 1), (floordiv_64, 1, 1), (floordiv_64, 1, 1), (floordiv_64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': getitem_144, 'Y': empty_219, 'W': convert_element_type_602, 'B': convert_element_type_603, 'Mean': getitem_148, 'Rstd': getitem_149, 'N': _local_scalar_dense_3}, tensors_to_clone = ['Y']);  floordiv_62 = floordiv_63 = floordiv_64 = empty_219 = None
        getitem_633: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_108['Y'];  triton_kernel_wrapper_functional_proxy_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_21: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_605, getitem_633, convert_element_type_604);  convert_element_type_605 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_85 = torch.ops.aten.split_with_sizes.default(addmm_21, [256, 256, 256, 256], 1);  addmm_21 = None
        getitem_634: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_85[0]
        getitem_635: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_85[1]
        getitem_636: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_85[2]
        getitem_637: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_85[3];  split_with_sizes_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_220: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:473 in backward, code: du, dv, dq, dk = duvqk.split(
        split_with_sizes_86 = torch.ops.aten.split_with_sizes.default(empty_220, [256, 256, 256, 256], 1)
        getitem_640: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_86[2];  split_with_sizes_86 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_824: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_636, [-1, 2, 128]);  getitem_636 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_825: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_637, [-1, 2, 128]);  getitem_637 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_826: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_635, [-1, 2, 128]);  getitem_635 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:485 in backward, code: dq = dq.view(-1, ctx.num_heads, ctx.attn_dim)
        view_827: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_640, [-1, 2, 128]);  getitem_640 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_221: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        add_621: "Sym(u1 + 16)" = _local_scalar_dense_1 + 16
        sub_249: "Sym(u1 + 15)" = add_621 - 1;  add_621 = None
        floordiv_65: "Sym(((u1 + 15)//16))" = sub_249 // 16;  sub_249 = None
        full_165: "i32[4, ((u1 + 15)//16)][Max(1, ((u1 + 15)//16)), 1]cuda:0" = torch.ops.aten.full.default([4, floordiv_65], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  floordiv_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        sym_size_int_33: "Sym(u7)" = torch.ops.aten.sym_size.int(view_827, 0);  view_827 = None
        full_166: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([sym_size_int_33, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  sym_size_int_33 = None
        view_830: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_166, [_local_scalar_dense_3, 256]);  full_166 = None
        slice_scatter_24: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_220, view_830, 1, 512, 768);  empty_220 = view_830 = None
        split_with_sizes_88 = torch.ops.aten.split_with_sizes.default(slice_scatter_24, [256, 256, 256, 256], 1)
        getitem_648: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_88[2];  split_with_sizes_88 = None
        view_831: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_648, [-1, 2, 128]);  getitem_648 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_832: "bf16[u7, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_626, [sym_size_int_2, 2, 128]);  getitem_626 = sym_size_int_2 = None
        split_with_sizes_89 = torch.ops.aten.split_with_sizes.default(slice_scatter_24, [256, 256, 256, 256], 1)
        getitem_653: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_89[3];  split_with_sizes_89 = None
        view_833: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_653, [-1, 2, 128]);  getitem_653 = None
        split_with_sizes_90 = torch.ops.aten.split_with_sizes.default(slice_scatter_24, [256, 256, 256, 256], 1)
        getitem_655: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_90[1];  split_with_sizes_90 = None
        view_834: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_655, [-1, 2, 128]);  getitem_655 = None
        add_622: "Sym(u1 + 32)" = _local_scalar_dense_1 + 32
        sub_250: "Sym(u1 + 31)" = add_622 - 1;  add_622 = None
        floordiv_66: "Sym(((u1 + 31)//32))" = sub_250 // 32;  sub_250 = None
        triton_kernel_wrapper_functional_proxy_109 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 119, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, floordiv_66, 1), (4, floordiv_66, 1), (4, floordiv_66, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_824, 'K': view_825, 'V': view_826, 'sort_by_length_indices': getitem_155, 'seq_offsets': getitem_512, 'attn_scale': reciprocal_8, 'DOut': view_832, 'DQ': view_831, 'DK': view_833, 'DV': view_834, 'LOCK': full_165, 'M': empty_221, 'MAX_SEQ_LEN': _local_scalar_dense_1, 'AUTOTUNE_MAX_SEQ_LEN': _local_scalar_dense_1}, tensors_to_clone = ['DQ', 'DK', 'DV']);  floordiv_66 = view_824 = view_825 = view_826 = getitem_155 = reciprocal_8 = view_832 = view_831 = view_833 = view_834 = full_165 = empty_221 = _local_scalar_dense_1 = None
        getitem_658: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_109['DQ']
        getitem_659: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_109['DK']
        getitem_660: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_109['DV'];  triton_kernel_wrapper_functional_proxy_109 = None
        view_835: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_658, [_local_scalar_dense_3, 256]);  getitem_658 = None
        slice_scatter_25: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_24, view_835, 1, 512, 768);  slice_scatter_24 = view_835 = None
        view_837: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_659, [_local_scalar_dense_3, 256]);  getitem_659 = None
        slice_scatter_26: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_25, view_837, 1, 768, 1024);  slice_scatter_25 = view_837 = None
        view_839: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_660, [_local_scalar_dense_3, 256]);  getitem_660 = None
        slice_scatter_27: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_26, view_839, 1, 256, 512);  slice_scatter_26 = view_839 = None
        split_with_sizes_97 = torch.ops.aten.split_with_sizes.default(slice_scatter_27, [256, 256, 256, 256], 1)
        getitem_687: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_97[2];  split_with_sizes_97 = None
        view_841: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_687, [-1, 2, 128]);  getitem_687 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_9: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_841, view_841);  view_841 = None
        view_842: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_9, [_local_scalar_dense_3, 256]);  copy_9 = None
        slice_scatter_28: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_27, view_842, 1, 512, 768);  slice_scatter_27 = view_842 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_100 = torch.ops.aten.split_with_sizes.default(slice_scatter_28, [256, 256, 256, 256], 1)
        getitem_700: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_100[3];  split_with_sizes_100 = None
        view_844: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_700, [-1, 2, 128]);  getitem_700 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_10: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_844, view_844);  view_844 = None
        view_845: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_10, [_local_scalar_dense_3, 256]);  copy_10 = None
        slice_scatter_29: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_28, view_845, 1, 768, 1024);  slice_scatter_28 = view_845 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_103 = torch.ops.aten.split_with_sizes.default(slice_scatter_29, [256, 256, 256, 256], 1)
        getitem_710: "bf16[u7, 256][1024, 1]cuda:0" = split_with_sizes_103[1];  split_with_sizes_103 = None
        view_847: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_710, [-1, 2, 128]);  getitem_710 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_11: "bf16[u7, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_847, view_847);  view_847 = None
        view_848: "bf16[u7, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_11, [_local_scalar_dense_3, 256]);  copy_11 = None
        slice_scatter_30: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_29, view_848, 1, 256, 512);  slice_scatter_29 = view_848 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1273: "f32[u7, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_627, torch.float32);  getitem_627 = None
        convert_element_type_1274: "f32[u7, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_634, torch.float32);  getitem_634 = None
        neg_95: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1274)
        exp_95: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_95);  neg_95 = None
        add_623: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_95, 1);  exp_95 = None
        reciprocal_39: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_623);  add_623 = None
        mul_595: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_39, 1);  reciprocal_39 = None
        mul_596: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1273, mul_595);  convert_element_type_1273 = None
        sub_251: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_595);  mul_595 = None
        mul_597: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1274, sub_251);  convert_element_type_1274 = sub_251 = None
        add_624: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_597, 1);  mul_597 = None
        mul_598: "f32[u7, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_596, add_624);  mul_596 = add_624 = None
        convert_element_type_1275: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_598, torch.bfloat16);  mul_598 = None
        slice_scatter_31: "bf16[u7, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_30, convert_element_type_1275, 1, 0, 256);  slice_scatter_30 = convert_element_type_1275 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_63: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_31, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_470: "bf16[256, u7][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_633, [1, 0]);  getitem_633 = None
        mm_218: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_470, slice_scatter_31);  permute_470 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_471: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_604, [1, 0]);  convert_element_type_604 = None
        mm_219: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_31, permute_471);  slice_scatter_31 = permute_471 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_222: "bf16[u7, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_3, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        floordiv_67: "Sym((u7//4))" = _local_scalar_dense_3 // 4
        sym_min_7: "Sym(Min(1184, (u7//4)))" = torch.sym_min(floordiv_67, 1184);  floordiv_67 = None
        sym_max_15: "Sym(Max(1, Min(1184, (u7//4))))" = torch.sym_max(sym_min_7, 1);  sym_min_7 = None
        empty_223: "f32[Max(1, Min(1184, (u7//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_15, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_224: "f32[Max(1, Min(1184, (u7//4))), 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([sym_max_15, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_225: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_226: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_110 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 120, grid = [(sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1), (sym_max_15, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_222, 'DY': mm_219, 'DW': empty_223, 'DB': empty_224, 'X': getitem_144, 'W': convert_element_type_602, 'B': convert_element_type_603, 'Mean': getitem_148, 'Rstd': getitem_149, 'N': _local_scalar_dense_3}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_222 = mm_219 = empty_223 = empty_224 = getitem_144 = convert_element_type_602 = convert_element_type_603 = getitem_148 = getitem_149 = _local_scalar_dense_3 = None
        getitem_729: "bf16[u7, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_110['DX']
        getitem_730: "f32[Max(1, Min(1184, (u7//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_110['DW']
        getitem_731: "f32[Max(1, Min(1184, (u7//4))), 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_110['DB'];  triton_kernel_wrapper_functional_proxy_110 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_111 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 121, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_730, 'DB': getitem_731, 'FINAL_DW': empty_225, 'FINAL_DB': empty_226, 'N': sym_max_15}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_730 = getitem_731 = empty_225 = empty_226 = sym_max_15 = None
        getitem_732: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_111['FINAL_DW']
        getitem_733: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_111['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_625: "bf16[u65, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_607, getitem_729);  add_607 = getitem_729 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1280: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_63, torch.float32);  sum_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1281: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_218, torch.float32);  mm_218 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1282: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_733, torch.float32);  getitem_733 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1283: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_732, torch.float32);  getitem_732 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        full_167: "bf16[u14, 256][256, 1]cuda:0" = torch.ops.aten.full.default([_local_scalar_dense_6, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  _local_scalar_dense_6 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1515 in backward, code: (ctx.seq_len_a + ctx.seq_len_b, ctx.D),
        add_626: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(index_6, index_7);  index_6 = index_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1514 in backward, code: dvalues = torch.empty(
        _local_scalar_dense_18: "Sym(514)" = torch.ops.aten._local_scalar_dense.default(add_626);  add_626 = None
        empty_227: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_18, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_34: "Sym(True)" = _local_scalar_dense_18 >= 0;  _local_scalar_dense_18 = None
        _assert_scalar_62 = torch.ops.aten._assert_scalar.default(ge_34, "Runtime assertion failed for expression u13 >= 0 on node 'ge'");  ge_34 = _assert_scalar_62 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:107 in _triton_concat_2D_jagged_internal, code: concat_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_112 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 2, constant_args_idx = 115, grid = [(257, 2, 1), (257, 2, 1), (257, 2, 1), (129, 2, 1), (129, 2, 1), (129, 2, 1), (65, 2, 1), (65, 2, 1), (65, 2, 1), (33, 2, 1), (33, 2, 1), (33, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'OffsetsA': asynchronous_complete_cumsum_2, 'ValuesA': full_167, 'OffsetsB': getitem_504, 'ValuesB': add_617, 'Out': empty_227}, tensors_to_clone = ['Out']);  asynchronous_complete_cumsum_2 = full_167 = getitem_504 = add_617 = empty_227 = None
        getitem_734: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_112['Out'];  triton_kernel_wrapper_functional_proxy_112 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1641 in triton_split_2D_jagged, code: return _Split2DJaggedFunction.apply(
        full_174: "bf16[u6, 256][256, 1]cuda:0" = torch.ops.aten.full.default([_local_scalar_dense_2, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False);  _local_scalar_dense_2 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1515 in backward, code: (ctx.seq_len_a + ctx.seq_len_b, ctx.D),
        add_627: "i64[1][1]cuda:0" = torch.ops.aten.add.Tensor(index_4, index_5);  index_4 = index_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:1514 in backward, code: dvalues = torch.empty(
        _local_scalar_dense_19: "Sym(514)" = torch.ops.aten._local_scalar_dense.default(add_627);  add_627 = None
        empty_228: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([_local_scalar_dense_19, 256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        ge_35: "Sym(True)" = _local_scalar_dense_19 >= 0;  _local_scalar_dense_19 = None
        _assert_scalar_63 = torch.ops.aten._assert_scalar.default(ge_35, "Runtime assertion failed for expression u5 >= 0 on node 'ge'");  ge_35 = _assert_scalar_63 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_jagged.py:107 in _triton_concat_2D_jagged_internal, code: concat_2D_jagged_multirow[grid](
        triton_kernel_wrapper_functional_proxy_113 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 2, constant_args_idx = 113, grid = [(257, 2, 1), (257, 2, 1), (257, 2, 1), (129, 2, 1), (129, 2, 1), (129, 2, 1), (65, 2, 1), (65, 2, 1), (65, 2, 1), (33, 2, 1), (33, 2, 1), (33, 2, 1)], tma_descriptor_metadata = {}, kwargs = {'OffsetsA': asynchronous_complete_cumsum, 'ValuesA': full_174, 'OffsetsB': getitem_512, 'ValuesB': add_625, 'Out': empty_228}, tensors_to_clone = ['Out']);  asynchronous_complete_cumsum = full_174 = getitem_512 = add_625 = empty_228 = None
        getitem_735: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_113['Out'];  triton_kernel_wrapper_functional_proxy_113 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        clone_65: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.clone.default(add_609, memory_format = torch.contiguous_format);  add_609 = None
        view_850: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_65, [640, 256]);  clone_65 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_229: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_183: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_114 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 111, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_229, 'DY': view_850, 'DW': full_183, 'X': view_354, 'W': primals_215, 'Rstd': getitem_142}, tensors_to_clone = ['DX', 'DW']);  empty_229 = view_850 = full_183 = view_354 = primals_215 = getitem_142 = None
        getitem_736: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_114['DX']
        getitem_737: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_114['DW'];  triton_kernel_wrapper_functional_proxy_114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_852: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_736, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_855: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_736, [10, 64, 256])
        slice_68: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_855, 1, 0, 32);  view_855 = None
        view_856: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_68, [10, 8192]);  slice_68 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_857: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_736, [10, 64, 256])
        slice_69: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_857, 1, 0, 32);  view_857 = None
        view_858: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_69, [10, 8192]);  slice_69 = None
        permute_473: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_858, [1, 0]);  view_858 = None
        mm_220: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_473, convert_element_type_590);  permute_473 = convert_element_type_590 = None
        permute_474: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_220, [1, 0]);  mm_220 = None
        permute_475: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_136, [1, 0]);  permute_136 = None
        mm_221: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_856, permute_475);  view_856 = permute_475 = None
        permute_476: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_474, [1, 0]);  permute_474 = None
        convert_element_type_1288: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_476, torch.float32);  permute_476 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1289: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_221, torch.float32);  mm_221 = None
        convert_element_type_1290: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_352, torch.float32);  view_352 = None
        neg_96: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1290)
        exp_96: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_96);  neg_96 = None
        add_628: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_96, 1);  exp_96 = None
        reciprocal_40: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_628);  add_628 = None
        mul_599: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_40, 1);  reciprocal_40 = None
        mul_600: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1289, mul_599);  convert_element_type_1289 = None
        sub_252: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_599);  mul_599 = None
        mul_601: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1290, sub_252);  convert_element_type_1290 = sub_252 = None
        add_629: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_601, 1);  mul_601 = None
        mul_602: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_600, add_629);  mul_600 = add_629 = None
        convert_element_type_1291: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_602, torch.bfloat16);  mul_602 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_859: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1291, [10, 2048]);  convert_element_type_1291 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1292: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_859, torch.float32);  view_859 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_74: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_47);  alias_47 = None
        mul_603: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1292, primals_213);  primals_213 = None
        mul_604: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_587, alias_74);  convert_element_type_587 = None
        mul_605: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_604, mul_603)
        sum_64: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_605, [1], True);  mul_605 = None
        div_97: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_604, 2048)
        mul_606: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_97, sum_64);  div_97 = sum_64 = None
        sub_253: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_603, mul_606);  mul_603 = mul_606 = None
        mul_607: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_253, alias_74);  sub_253 = alias_74 = None
        mul_608: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1292, mul_604);  convert_element_type_1292 = mul_604 = None
        sum_65: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_608, [0]);  mul_608 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1293: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_607, torch.bfloat16);  mul_607 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_860: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1293, [10, 2048]);  convert_element_type_1293 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_477: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_860, [1, 0])
        mm_222: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_477, convert_element_type_583);  permute_477 = convert_element_type_583 = None
        permute_478: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_222, [1, 0]);  mm_222 = None
        permute_479: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_135, [1, 0]);  permute_135 = None
        mm_223: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_860, permute_479);  permute_479 = None
        permute_480: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_478, [1, 0]);  permute_478 = None
        convert_element_type_1298: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_480, torch.float32);  permute_480 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1299: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_223, torch.float32);  mm_223 = None
        convert_element_type_1300: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_350, torch.float32);  view_350 = None
        neg_97: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1300)
        exp_97: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_97);  neg_97 = None
        add_630: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_97, 1);  exp_97 = None
        reciprocal_41: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_630);  add_630 = None
        mul_609: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_41, 1);  reciprocal_41 = None
        mul_610: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1299, mul_609);  convert_element_type_1299 = None
        sub_254: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_609);  mul_609 = None
        mul_611: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1300, sub_254);  convert_element_type_1300 = sub_254 = None
        add_631: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_611, 1);  mul_611 = None
        mul_612: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_610, add_631);  mul_610 = add_631 = None
        convert_element_type_1301: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_612, torch.bfloat16);  mul_612 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_861: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1301, [10, 1024]);  convert_element_type_1301 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1302: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_861, torch.float32);  view_861 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_75: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_46);  alias_46 = None
        mul_613: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1302, primals_211);  primals_211 = None
        mul_614: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_580, alias_75);  convert_element_type_580 = None
        mul_615: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_614, mul_613)
        sum_66: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_615, [1], True);  mul_615 = None
        div_98: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_614, 1024)
        mul_616: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_98, sum_66);  div_98 = sum_66 = None
        sub_255: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_613, mul_616);  mul_613 = mul_616 = None
        mul_617: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_255, alias_75);  sub_255 = alias_75 = None
        mul_618: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1302, mul_614);  convert_element_type_1302 = mul_614 = None
        sum_67: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_618, [0]);  mul_618 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1303: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_617, torch.bfloat16);  mul_617 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_862: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1303, [10, 1024]);  convert_element_type_1303 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_481: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_862, [1, 0])
        mm_224: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_481, convert_element_type_576);  permute_481 = convert_element_type_576 = None
        permute_482: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_224, [1, 0]);  mm_224 = None
        permute_483: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_134, [1, 0]);  permute_134 = None
        mm_225: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_862, permute_483);  view_862 = permute_483 = None
        add_632: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_860, mm_225);  view_860 = mm_225 = None
        permute_484: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_482, [1, 0]);  permute_482 = None
        convert_element_type_1308: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_484, torch.float32);  permute_484 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1309: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_632, torch.float32);  add_632 = None
        convert_element_type_1310: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_348, torch.float32);  view_348 = None
        neg_98: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1310)
        exp_98: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_98);  neg_98 = None
        add_633: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_98, 1);  exp_98 = None
        reciprocal_42: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_633);  add_633 = None
        mul_619: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_42, 1);  reciprocal_42 = None
        mul_620: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1309, mul_619);  convert_element_type_1309 = None
        sub_256: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_619);  mul_619 = None
        mul_621: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1310, sub_256);  convert_element_type_1310 = sub_256 = None
        add_634: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_621, 1);  mul_621 = None
        mul_622: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_620, add_634);  mul_620 = add_634 = None
        convert_element_type_1311: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_622, torch.bfloat16);  mul_622 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_863: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1311, [10, 2048]);  convert_element_type_1311 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1312: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_863, torch.float32);  view_863 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_76: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_45);  alias_45 = None
        mul_623: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1312, primals_209);  primals_209 = None
        mul_624: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_573, alias_76);  convert_element_type_573 = None
        mul_625: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_624, mul_623)
        sum_68: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_625, [1], True);  mul_625 = None
        div_99: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_624, 2048)
        mul_626: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_99, sum_68);  div_99 = sum_68 = None
        sub_257: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_623, mul_626);  mul_623 = mul_626 = None
        mul_627: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_257, alias_76);  sub_257 = alias_76 = None
        mul_628: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1312, mul_624);  convert_element_type_1312 = mul_624 = None
        sum_69: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_628, [0]);  mul_628 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1313: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_627, torch.bfloat16);  mul_627 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_864: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1313, [10, 2048]);  convert_element_type_1313 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_485: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_864, [1, 0])
        mm_226: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_485, convert_element_type_569);  permute_485 = convert_element_type_569 = None
        permute_486: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_226, [1, 0]);  mm_226 = None
        permute_487: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_133, [1, 0]);  permute_133 = None
        mm_227: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_864, permute_487);  permute_487 = None
        permute_488: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_486, [1, 0]);  permute_486 = None
        convert_element_type_1318: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_488, torch.float32);  permute_488 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1319: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_227, torch.float32);  mm_227 = None
        convert_element_type_1320: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_346, torch.float32);  view_346 = None
        neg_99: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1320)
        exp_99: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_99);  neg_99 = None
        add_635: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_99, 1);  exp_99 = None
        reciprocal_43: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_635);  add_635 = None
        mul_629: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_43, 1);  reciprocal_43 = None
        mul_630: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1319, mul_629);  convert_element_type_1319 = None
        sub_258: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_629);  mul_629 = None
        mul_631: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1320, sub_258);  convert_element_type_1320 = sub_258 = None
        add_636: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_631, 1);  mul_631 = None
        mul_632: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_630, add_636);  mul_630 = add_636 = None
        convert_element_type_1321: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_632, torch.bfloat16);  mul_632 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_865: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1321, [10, 1024]);  convert_element_type_1321 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1322: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_865, torch.float32);  view_865 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_77: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_44);  alias_44 = None
        mul_633: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1322, primals_207);  primals_207 = None
        mul_634: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_566, alias_77);  convert_element_type_566 = None
        mul_635: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_634, mul_633)
        sum_70: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_635, [1], True);  mul_635 = None
        div_100: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_634, 1024)
        mul_636: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_100, sum_70);  div_100 = sum_70 = None
        sub_259: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_633, mul_636);  mul_633 = mul_636 = None
        mul_637: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_259, alias_77);  sub_259 = alias_77 = None
        mul_638: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1322, mul_634);  convert_element_type_1322 = mul_634 = None
        sum_71: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_638, [0]);  mul_638 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1323: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_637, torch.bfloat16);  mul_637 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_866: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1323, [10, 1024]);  convert_element_type_1323 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_489: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_866, [1, 0])
        mm_228: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_489, convert_element_type_562);  permute_489 = convert_element_type_562 = None
        permute_490: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_228, [1, 0]);  mm_228 = None
        permute_491: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_132, [1, 0]);  permute_132 = None
        mm_229: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_866, permute_491);  view_866 = permute_491 = None
        add_637: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_864, mm_229);  view_864 = mm_229 = None
        permute_492: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_490, [1, 0]);  permute_490 = None
        convert_element_type_1328: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_492, torch.float32);  permute_492 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1329: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_637, torch.float32);  add_637 = None
        convert_element_type_1330: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_344, torch.float32);  view_344 = None
        neg_100: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1330)
        exp_100: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_100);  neg_100 = None
        add_638: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_100, 1);  exp_100 = None
        reciprocal_44: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_638);  add_638 = None
        mul_639: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_44, 1);  reciprocal_44 = None
        mul_640: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1329, mul_639);  convert_element_type_1329 = None
        sub_260: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_639);  mul_639 = None
        mul_641: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1330, sub_260);  convert_element_type_1330 = sub_260 = None
        add_639: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_641, 1);  mul_641 = None
        mul_642: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_640, add_639);  mul_640 = add_639 = None
        convert_element_type_1331: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_642, torch.bfloat16);  mul_642 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_867: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1331, [10, 2048]);  convert_element_type_1331 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1332: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_867, torch.float32);  view_867 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_78: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_43);  alias_43 = None
        mul_643: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1332, primals_205);  primals_205 = None
        mul_644: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_559, alias_78);  convert_element_type_559 = None
        mul_645: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_644, mul_643)
        sum_72: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_645, [1], True);  mul_645 = None
        div_101: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_644, 2048)
        mul_646: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_101, sum_72);  div_101 = sum_72 = None
        sub_261: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_643, mul_646);  mul_643 = mul_646 = None
        mul_647: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_261, alias_78);  sub_261 = alias_78 = None
        mul_648: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1332, mul_644);  convert_element_type_1332 = mul_644 = None
        sum_73: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_648, [0]);  mul_648 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1333: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_647, torch.bfloat16);  mul_647 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_868: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1333, [10, 2048]);  convert_element_type_1333 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_493: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_868, [1, 0])
        mm_230: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_493, convert_element_type_555);  permute_493 = convert_element_type_555 = None
        permute_494: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_230, [1, 0]);  mm_230 = None
        permute_495: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_131, [1, 0]);  permute_131 = None
        mm_231: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_868, permute_495);  view_868 = permute_495 = None
        permute_496: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_494, [1, 0]);  permute_494 = None
        convert_element_type_1338: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_496, torch.float32);  permute_496 = None
        convert_element_type_1339: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_231, torch.float32);  mm_231 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_869: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1339, [10, 8472]);  convert_element_type_1339 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_230: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_186: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_115 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 109, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_230, 'DY': view_869, 'DW': full_186, 'X': view_340, 'W': primals_203, 'Rstd': getitem_140}, tensors_to_clone = ['DX', 'DW']);  empty_230 = view_869 = full_186 = view_340 = primals_203 = getitem_140 = None
        getitem_738: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_115['DX']
        getitem_739: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_115['DW'];  triton_kernel_wrapper_functional_proxy_115 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_872: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_738, [10, 8472])
        slice_72: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_872, 1, 0, 6144);  view_872 = None
        convert_element_type_1340: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_72, torch.bfloat16);  slice_72 = None
        view_873: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_738, [10, 8472]);  getitem_738 = None
        slice_73: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_873, 1, 6144, 8472);  view_873 = None
        add_640: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_587, slice_73);  add_587 = slice_73 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_74: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1340, 1, 0, 2048)
        slice_75: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1340, 1, 2048, 6144);  convert_element_type_1340 = None
        view_874: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_75, [10, 16, 256]);  slice_75 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_875: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_74, [10, 32, 64]);  slice_74 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_876: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_875, [10, 32, 64]);  view_875 = None
        permute_497: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_335, [0, 2, 1]);  view_335 = None
        bmm_39: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_497, view_876);  permute_497 = None
        permute_498: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_336, [0, 2, 1]);  view_336 = None
        bmm_40: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_876, permute_498);  view_876 = permute_498 = None
        view_877: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_39, [10, 256, 64]);  bmm_39 = None
        view_878: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_40, [10, 32, 256]);  bmm_40 = None
        permute_499: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_877, [0, 2, 1]);  view_877 = None
        add_641: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_852, permute_499);  view_852 = permute_499 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_879: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_878, [10, 32, 256]);  view_878 = None
        permute_500: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_332, [0, 2, 1]);  view_332 = None
        bmm_41: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_500, view_879);  permute_500 = None
        permute_501: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_333, [0, 2, 1]);  view_333 = None
        bmm_42: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_879, permute_501);  view_879 = permute_501 = None
        view_880: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_41, [10, 64, 256]);  bmm_41 = None
        add_642: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_641, view_880);  add_641 = view_880 = None
        view_881: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_42, [10, 32, 64]);  bmm_42 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_882: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_881, [10, 2048]);  view_881 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_502: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_882, [1, 0])
        mm_232: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_502, convert_element_type_547);  permute_502 = convert_element_type_547 = None
        permute_503: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_232, [1, 0]);  mm_232 = None
        permute_504: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_129, [1, 0]);  permute_129 = None
        mm_233: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_882, permute_504);  view_882 = permute_504 = None
        permute_505: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_503, [1, 0]);  permute_503 = None
        convert_element_type_1353: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_505, torch.float32);  permute_505 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1354: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_233, torch.float32);  mm_233 = None
        convert_element_type_1355: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_330, torch.float32);  view_330 = None
        neg_101: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1355)
        exp_101: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_101);  neg_101 = None
        add_643: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_101, 1);  exp_101 = None
        reciprocal_45: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_643);  add_643 = None
        mul_649: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_45, 1);  reciprocal_45 = None
        mul_650: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1354, mul_649);  convert_element_type_1354 = None
        sub_262: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_649);  mul_649 = None
        mul_651: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1355, sub_262);  convert_element_type_1355 = sub_262 = None
        add_644: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_651, 1);  mul_651 = None
        mul_652: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_650, add_644);  mul_650 = add_644 = None
        convert_element_type_1356: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_652, torch.bfloat16);  mul_652 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_883: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1356, [10, 512]);  convert_element_type_1356 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1357: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_883, torch.float32);  view_883 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_79: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_42);  alias_42 = None
        mul_653: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1357, primals_201);  primals_201 = None
        mul_654: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_544, alias_79);  convert_element_type_544 = None
        mul_655: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_654, mul_653)
        sum_74: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_655, [1], True);  mul_655 = None
        div_102: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_654, 512)
        mul_656: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_102, sum_74);  div_102 = sum_74 = None
        sub_263: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_653, mul_656);  mul_653 = mul_656 = None
        mul_657: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_263, alias_79);  sub_263 = alias_79 = None
        mul_658: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1357, mul_654);  convert_element_type_1357 = mul_654 = None
        sum_75: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_658, [0]);  mul_658 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1358: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_657, torch.bfloat16);  mul_657 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_884: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1358, [10, 512]);  convert_element_type_1358 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_506: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_884, [1, 0])
        mm_234: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_506, view_328);  permute_506 = view_328 = None
        permute_507: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_234, [1, 0]);  mm_234 = None
        permute_508: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_128, [1, 0]);  permute_128 = None
        mm_235: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_884, permute_508);  view_884 = permute_508 = None
        permute_509: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_507, [1, 0]);  permute_507 = None
        convert_element_type_1363: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_509, torch.float32);  permute_509 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_885: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_235, [10, 32, 256]);  mm_235 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_886: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_736, [10, 64, 256]);  getitem_736 = None
        slice_76: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_886, 1, 32, 64);  view_886 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_44: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_76, view_885, view_874], 1);  slice_76 = view_885 = view_874 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_510: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_44, [0, 2, 1]);  cat_44 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_66: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_510, memory_format = torch.contiguous_format);  permute_510 = None
        view_887: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_66, [2560, 80]);  clone_66 = None
        permute_511: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_887, [1, 0])
        mm_236: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_511, view_326);  permute_511 = view_326 = None
        permute_512: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_236, [1, 0]);  mm_236 = None
        permute_513: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_126, [1, 0]);  permute_126 = None
        mm_237: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_887, permute_513);  view_887 = permute_513 = None
        view_888: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_237, [10, 256, 64]);  mm_237 = None
        permute_514: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_512, [1, 0]);  permute_512 = None
        convert_element_type_1368: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_514, torch.float32);  permute_514 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_515: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_888, [0, 2, 1]);  view_888 = None
        add_645: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_642, permute_515);  add_642 = permute_515 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_516: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(add_645, [0, 2, 1]);  add_645 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_67: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_516, memory_format = torch.contiguous_format);  permute_516 = None
        view_889: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_67, [2560, 64]);  clone_67 = None
        permute_517: "bf16[64, 2560][1, 64]cuda:0" = torch.ops.aten.permute.default(view_889, [1, 0])
        mm_238: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(permute_517, view_324);  permute_517 = view_324 = None
        permute_518: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(mm_238, [1, 0]);  mm_238 = None
        permute_519: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_123, [1, 0]);  permute_123 = None
        mm_239: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(view_889, permute_519);  view_889 = permute_519 = None
        view_890: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.view.default(mm_239, [10, 256, 112]);  mm_239 = None
        permute_520: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_518, [1, 0]);  permute_518 = None
        convert_element_type_1373: "f32[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_520, torch.float32);  permute_520 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_521: "bf16[10, 112, 256][28672, 1, 112]cuda:0" = torch.ops.aten.permute.default(view_890, [0, 2, 1]);  view_890 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        slice_77: "bf16[10, 64, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_521, 1, 0, 64)
        slice_78: "bf16[10, 32, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_521, 1, 64, 96)
        slice_79: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_521, 1, 96, 112);  permute_521 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_891: "bf16[10, 1, 16, 256][28672, 16, 1, 112]cuda:0" = torch.ops.aten.view.default(slice_79, [10, 1, 16, 256]);  slice_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        mul_659: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_891, index_2);  index_2 = None
        mul_660: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_891, index_3);  view_891 = index_3 = None
        full_187: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_4: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_187, [None, getitem_135], mul_659, True);  full_187 = getitem_135 = mul_659 = None
        full_188: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_5: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_188, [None, getitem_134], mul_660, True);  full_188 = getitem_134 = mul_660 = None
        add_646: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_4, index_put_5);  index_put_4 = index_put_5 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        select_22: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_646, 1, 0)
        select_23: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_646, 1, 1);  add_646 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        slice_80: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_78, 1, 0, 16)
        slice_81: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_78, 1, 16, 32);  slice_78 = None
        add_647: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_22, slice_80);  select_22 = slice_80 = None
        add_648: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_23, slice_81);  select_23 = slice_81 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_892: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_648, [160, 256]);  add_648 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_231: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_191: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_116 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 107, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_231, 'DY': view_892, 'DW': full_191, 'X': view_317, 'W': primals_197, 'Rstd': getitem_133}, tensors_to_clone = ['DX', 'DW']);  empty_231 = view_892 = full_191 = view_317 = primals_197 = getitem_133 = None
        getitem_740: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_116['DX']
        getitem_741: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_116['DW'];  triton_kernel_wrapper_functional_proxy_116 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_896: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_740, [10, 16, 256])
        view_897: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_896, [160, 256]);  view_896 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_899: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_740, [10, 16, 256])
        view_900: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_899, [160, 256]);  view_899 = None
        view_901: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_900, [10, 4096]);  view_900 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_902: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_740, [10, 16, 256]);  getitem_740 = None
        view_903: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_902, [160, 256]);  view_902 = None
        view_904: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_903, [10, 4096]);  view_903 = None
        permute_523: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_904, [1, 0]);  view_904 = None
        mm_240: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_523, mul_101);  permute_523 = mul_101 = None
        permute_524: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_240, [1, 0]);  mm_240 = None
        permute_525: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_121, [1, 0]);  permute_121 = None
        mm_241: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_901, permute_525);  view_901 = permute_525 = None
        permute_526: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_524, [1, 0]);  permute_524 = None
        convert_element_type_1378: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_526, torch.float32);  permute_526 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_661: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_241, convert_element_type_524);  convert_element_type_524 = None
        mul_662: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_241, mm_75);  mm_241 = mm_75 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_527: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_661, [1, 0])
        mm_242: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_527, convert_element_type_525);  permute_527 = convert_element_type_525 = None
        permute_528: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_242, [1, 0]);  mm_242 = None
        permute_529: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_120, [1, 0]);  permute_120 = None
        mm_243: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_661, permute_529);  mul_661 = permute_529 = None
        permute_530: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_528, [1, 0]);  permute_528 = None
        convert_element_type_1383: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_530, torch.float32);  permute_530 = None
        convert_element_type_1384: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_243, torch.float32);  mm_243 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1385: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_662, torch.float32);  mul_662 = None
        convert_element_type_1386: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_74, torch.float32);  mm_74 = None
        neg_102: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1386)
        exp_102: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_102);  neg_102 = None
        add_649: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_102, 1);  exp_102 = None
        reciprocal_46: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_649);  add_649 = None
        mul_663: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_46, 1);  reciprocal_46 = None
        mul_664: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1385, mul_663);  convert_element_type_1385 = None
        sub_264: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_663);  mul_663 = None
        mul_665: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1386, sub_264);  convert_element_type_1386 = sub_264 = None
        add_650: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_665, 1);  mul_665 = None
        mul_666: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_664, add_650);  mul_664 = add_650 = None
        convert_element_type_1387: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_666, torch.bfloat16);  mul_666 = None
        permute_531: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1387, [1, 0])
        mm_244: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_531, convert_element_type_519);  permute_531 = convert_element_type_519 = None
        permute_532: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_244, [1, 0]);  mm_244 = None
        permute_533: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_119, [1, 0]);  permute_119 = None
        mm_245: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1387, permute_533);  convert_element_type_1387 = permute_533 = None
        permute_534: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_532, [1, 0]);  permute_532 = None
        convert_element_type_1392: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_534, torch.float32);  permute_534 = None
        convert_element_type_1393: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_245, torch.float32);  mm_245 = None
        add_651: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1384, convert_element_type_1393);  convert_element_type_1384 = convert_element_type_1393 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_905: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_651, [160, 256]);  add_651 = None
        sub_265: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_518, getitem_131);  convert_element_type_518 = getitem_131 = None
        mul_667: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_265, rsqrt_43);  sub_265 = None
        mul_668: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_905, primals_192);  primals_192 = None
        mul_669: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_668, 256)
        sum_76: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_668, [1], True)
        mul_670: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_668, mul_667);  mul_668 = None
        sum_77: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_670, [1], True);  mul_670 = None
        mul_671: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_667, sum_77);  sum_77 = None
        sub_266: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_669, sum_76);  mul_669 = sum_76 = None
        sub_267: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_266, mul_671);  sub_266 = mul_671 = None
        div_103: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_43, 256);  rsqrt_43 = None
        mul_672: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_103, sub_267);  div_103 = sub_267 = None
        mul_673: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_905, mul_667);  mul_667 = None
        sum_78: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_673, [0]);  mul_673 = None
        sum_79: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_905, [0]);  view_905 = None
        convert_element_type_1394: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_672, torch.bfloat16);  mul_672 = None
        add_652: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_897, convert_element_type_1394);  view_897 = convert_element_type_1394 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_906: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_652, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_193: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_194: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_232: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_117 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 105, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_232, 'Q': view_310, 'K': view_311, 'V': view_311, 'DO': view_906, 'seq_offsets': getitem_129, 'seq_offsets_q': getitem_128, 'DQ': full_193, 'DK': full_194, 'DV': full_194, 'attn_scale': reciprocal_7}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_232 = view_310 = view_311 = view_906 = getitem_128 = full_193 = full_194 = reciprocal_7 = None
        getitem_742: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_117['seq_offsets']
        getitem_744: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_117['DQ']
        getitem_745: "bf16[514, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_117['DK'];  triton_kernel_wrapper_functional_proxy_117 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_908: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_745, [514, 256]);  getitem_745 = None
        add_653: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_734, view_908);  getitem_734 = view_908 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_910: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_744, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_911: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_744, [160, 256]);  getitem_744 = None
        permute_536: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_911, [1, 0]);  view_911 = None
        mm_246: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_536, view_309);  permute_536 = view_309 = None
        permute_537: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_246, [1, 0]);  mm_246 = None
        permute_538: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_118, [1, 0]);  permute_118 = None
        mm_247: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_910, permute_538);  view_910 = permute_538 = None
        permute_539: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_537, [1, 0]);  permute_537 = None
        convert_element_type_1399: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_539, torch.float32);  permute_539 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_912: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_247, [160, 256]);  mm_247 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_233: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_197: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_118 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 103, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_233, 'DY': view_912, 'DW': full_197, 'X': view_307, 'W': primals_190, 'Rstd': getitem_126}, tensors_to_clone = ['DX', 'DW']);  empty_233 = view_912 = full_197 = view_307 = primals_190 = getitem_126 = None
        getitem_746: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_118['DX']
        getitem_747: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_118['DW'];  triton_kernel_wrapper_functional_proxy_118 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_914: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_746, [160, 256]);  getitem_746 = None
        add_654: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_652, view_914);  add_652 = view_914 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_915: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_654, [10, 16, 256]);  add_654 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_540: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_915, [0, 2, 1]);  view_915 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_69: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_540, memory_format = torch.contiguous_format);  permute_540 = None
        view_916: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_69, [2560, 16]);  clone_69 = None
        permute_541: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_916, [1, 0])
        mm_248: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_541, view_304);  permute_541 = view_304 = None
        permute_542: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_248, [1, 0]);  mm_248 = None
        permute_543: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_115, [1, 0]);  permute_115 = None
        mm_249: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_916, permute_543);  view_916 = permute_543 = None
        view_917: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_249, [10, 256, 64]);  mm_249 = None
        permute_544: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_542, [1, 0]);  permute_542 = None
        convert_element_type_1404: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_544, torch.float32);  permute_544 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_545: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_917, [0, 2, 1]);  view_917 = None
        add_655: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(slice_77, permute_545);  slice_77 = permute_545 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_918: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_647, [160, 256]);  add_647 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_234: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_200: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_119 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 101, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_234, 'DY': view_918, 'DW': full_200, 'X': view_301, 'W': primals_188, 'Rstd': getitem_124}, tensors_to_clone = ['DX', 'DW']);  empty_234 = view_918 = full_200 = view_301 = primals_188 = getitem_124 = None
        getitem_748: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_119['DX']
        getitem_749: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_119['DW'];  triton_kernel_wrapper_functional_proxy_119 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_922: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_748, [10, 16, 256])
        view_923: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_922, [160, 256]);  view_922 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_925: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_748, [10, 16, 256])
        view_926: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_925, [160, 256]);  view_925 = None
        view_927: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_926, [10, 4096]);  view_926 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_928: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_748, [10, 16, 256]);  getitem_748 = None
        view_929: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_928, [160, 256]);  view_928 = None
        view_930: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_929, [10, 4096]);  view_929 = None
        permute_547: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_930, [1, 0]);  view_930 = None
        mm_250: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_547, mul_97);  permute_547 = mul_97 = None
        permute_548: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_250, [1, 0]);  mm_250 = None
        permute_549: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_113, [1, 0]);  permute_113 = None
        mm_251: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_927, permute_549);  view_927 = permute_549 = None
        permute_550: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_548, [1, 0]);  permute_548 = None
        convert_element_type_1409: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_550, torch.float32);  permute_550 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_674: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_251, convert_element_type_501);  convert_element_type_501 = None
        mul_675: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_251, mm_70);  mm_251 = mm_70 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_551: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_674, [1, 0])
        mm_252: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_551, convert_element_type_502);  permute_551 = convert_element_type_502 = None
        permute_552: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_252, [1, 0]);  mm_252 = None
        permute_553: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_112, [1, 0]);  permute_112 = None
        mm_253: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_674, permute_553);  mul_674 = permute_553 = None
        permute_554: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_552, [1, 0]);  permute_552 = None
        convert_element_type_1414: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_554, torch.float32);  permute_554 = None
        convert_element_type_1415: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_253, torch.float32);  mm_253 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1416: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_675, torch.float32);  mul_675 = None
        convert_element_type_1417: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_69, torch.float32);  mm_69 = None
        neg_103: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1417)
        exp_103: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_103);  neg_103 = None
        add_656: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_103, 1);  exp_103 = None
        reciprocal_47: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_656);  add_656 = None
        mul_676: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_47, 1);  reciprocal_47 = None
        mul_677: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1416, mul_676);  convert_element_type_1416 = None
        sub_268: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_676);  mul_676 = None
        mul_678: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1417, sub_268);  convert_element_type_1417 = sub_268 = None
        add_657: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_678, 1);  mul_678 = None
        mul_679: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_677, add_657);  mul_677 = add_657 = None
        convert_element_type_1418: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_679, torch.bfloat16);  mul_679 = None
        permute_555: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1418, [1, 0])
        mm_254: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_555, convert_element_type_496);  permute_555 = convert_element_type_496 = None
        permute_556: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_254, [1, 0]);  mm_254 = None
        permute_557: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_111, [1, 0]);  permute_111 = None
        mm_255: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1418, permute_557);  convert_element_type_1418 = permute_557 = None
        permute_558: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_556, [1, 0]);  permute_556 = None
        convert_element_type_1423: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_558, torch.float32);  permute_558 = None
        convert_element_type_1424: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_255, torch.float32);  mm_255 = None
        add_658: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1415, convert_element_type_1424);  convert_element_type_1415 = convert_element_type_1424 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_931: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_658, [160, 256]);  add_658 = None
        sub_269: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_495, getitem_122);  convert_element_type_495 = getitem_122 = None
        mul_680: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_269, rsqrt_42);  sub_269 = None
        mul_681: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_931, primals_183);  primals_183 = None
        mul_682: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_681, 256)
        sum_80: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_681, [1], True)
        mul_683: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_681, mul_680);  mul_681 = None
        sum_81: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_683, [1], True);  mul_683 = None
        mul_684: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_680, sum_81);  sum_81 = None
        sub_270: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_682, sum_80);  mul_682 = sum_80 = None
        sub_271: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_270, mul_684);  sub_270 = mul_684 = None
        div_104: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_42, 256);  rsqrt_42 = None
        mul_685: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_104, sub_271);  div_104 = sub_271 = None
        mul_686: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_931, mul_680);  mul_680 = None
        sum_82: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_686, [0]);  mul_686 = None
        sum_83: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_931, [0]);  view_931 = None
        convert_element_type_1425: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_685, torch.bfloat16);  mul_685 = None
        add_659: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_923, convert_element_type_1425);  view_923 = convert_element_type_1425 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_932: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_659, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_202: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_203: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_235: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_120 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 99, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_235, 'Q': view_294, 'K': view_295, 'V': view_295, 'DO': view_932, 'seq_offsets': getitem_120, 'seq_offsets_q': getitem_119, 'DQ': full_202, 'DK': full_203, 'DV': full_203, 'attn_scale': reciprocal_6}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_235 = view_294 = view_295 = view_932 = getitem_119 = full_202 = full_203 = reciprocal_6 = None
        getitem_750: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_120['seq_offsets']
        getitem_752: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_120['DQ']
        getitem_753: "bf16[514, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_120['DK'];  triton_kernel_wrapper_functional_proxy_120 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_934: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_753, [514, 256]);  getitem_753 = None
        add_660: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(getitem_735, view_934);  getitem_735 = view_934 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_936: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_752, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_937: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_752, [160, 256]);  getitem_752 = None
        permute_560: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_937, [1, 0]);  view_937 = None
        mm_256: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_560, view_293);  permute_560 = view_293 = None
        permute_561: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_256, [1, 0]);  mm_256 = None
        permute_562: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_110, [1, 0]);  permute_110 = None
        mm_257: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_936, permute_562);  view_936 = permute_562 = None
        permute_563: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_561, [1, 0]);  permute_561 = None
        convert_element_type_1430: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_563, torch.float32);  permute_563 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_938: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_257, [160, 256]);  mm_257 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_236: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_206: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_121 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 97, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_236, 'DY': view_938, 'DW': full_206, 'X': view_291, 'W': primals_181, 'Rstd': getitem_117}, tensors_to_clone = ['DX', 'DW']);  empty_236 = view_938 = full_206 = view_291 = primals_181 = getitem_117 = None
        getitem_754: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_121['DX']
        getitem_755: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_121['DW'];  triton_kernel_wrapper_functional_proxy_121 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_940: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_754, [160, 256]);  getitem_754 = None
        add_661: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_659, view_940);  add_659 = view_940 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_941: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_661, [10, 16, 256]);  add_661 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_564: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_941, [0, 2, 1]);  view_941 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_71: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_564, memory_format = torch.contiguous_format);  permute_564 = None
        view_942: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_71, [2560, 16]);  clone_71 = None
        permute_565: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_942, [1, 0])
        mm_258: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_565, view_288);  permute_565 = view_288 = None
        permute_566: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_258, [1, 0]);  mm_258 = None
        permute_567: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_107, [1, 0]);  permute_107 = None
        mm_259: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_942, permute_567);  view_942 = permute_567 = None
        view_943: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_259, [10, 256, 64]);  mm_259 = None
        permute_568: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_566, [1, 0]);  permute_566 = None
        convert_element_type_1435: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_568, torch.float32);  permute_568 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_569: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_943, [0, 2, 1]);  view_943 = None
        add_662: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(add_655, permute_569);  add_655 = permute_569 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_570: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_482, [1, 0]);  convert_element_type_482 = None
        mm_260: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_653, permute_570);  permute_570 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_237: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        empty_238: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_239: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_240: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_241: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_242: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_243: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_122 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 23, constant_args_idx = 94, grid = [(128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_238, 'DU': empty_239, 'DY': mm_260, 'DW': empty_240, 'DB': empty_241, 'X': view_285, 'U': convert_element_type_479, 'Y': empty_237, 'W': convert_element_type_480, 'B': convert_element_type_481, 'Mean': getitem_114, 'Rstd': getitem_115, 'seed': primals_179}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_238 = empty_239 = mm_260 = empty_240 = empty_241 = view_285 = convert_element_type_479 = empty_237 = convert_element_type_480 = convert_element_type_481 = getitem_114 = getitem_115 = primals_179 = None
        getitem_756: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_122['DX']
        getitem_757: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_122['DU']
        getitem_758: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_122['DW']
        getitem_759: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_122['DB']
        getitem_760: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_122['Y'];  triton_kernel_wrapper_functional_proxy_122 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_123 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 95, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_758, 'DB': getitem_759, 'FINAL_DW': empty_242, 'FINAL_DB': empty_243}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_758 = getitem_759 = empty_242 = empty_243 = None
        getitem_761: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_123['FINAL_DW']
        getitem_762: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_123['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_123 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_572: "bf16[768, 514][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_760, [1, 0]);  getitem_760 = None
        mm_261: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_572, add_653);  permute_572 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1440: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_261, torch.float32);  mm_261 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1441: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_762, torch.float32);  getitem_762 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1442: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_761, torch.float32);  getitem_761 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_244: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_124 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 89, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': addmm_5, 'Y': empty_244, 'W': convert_element_type_471, 'B': convert_element_type_472, 'Mean': getitem_104, 'Rstd': getitem_105}, tensors_to_clone = ['Y']);  empty_244 = None
        getitem_763: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_124['Y'];  triton_kernel_wrapper_functional_proxy_124 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_22: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_474, getitem_763, convert_element_type_473);  convert_element_type_474 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_108 = torch.ops.aten.split_with_sizes.default(addmm_22, [256, 256, 256, 256], 1);  addmm_22 = None
        getitem_764: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_108[0]
        getitem_765: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_108[1]
        getitem_766: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_108[2]
        getitem_767: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_108[3];  split_with_sizes_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_245: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_945: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_766, [-1, 2, 128]);  getitem_766 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_946: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_767, [-1, 2, 128]);  getitem_767 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_947: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_765, [-1, 2, 128]);  getitem_765 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_246: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        full_229: "i32[4, 17][17, 1]cuda:0" = torch.ops.aten.full.default([4, 17], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        full_230: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        view_951: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_230, [514, 256]);  full_230 = None
        slice_scatter_32: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_245, view_951, 1, 512, 768);  empty_245 = view_951 = None
        split_with_sizes_111 = torch.ops.aten.split_with_sizes.default(slice_scatter_32, [256, 256, 256, 256], 1)
        getitem_778: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_111[2];  split_with_sizes_111 = None
        view_952: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_778, [-1, 2, 128]);  getitem_778 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_953: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_756, [514, 2, 128]);  getitem_756 = None
        split_with_sizes_112 = torch.ops.aten.split_with_sizes.default(slice_scatter_32, [256, 256, 256, 256], 1)
        getitem_783: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_112[3];  split_with_sizes_112 = None
        view_954: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_783, [-1, 2, 128]);  getitem_783 = None
        split_with_sizes_113 = torch.ops.aten.split_with_sizes.default(slice_scatter_32, [256, 256, 256, 256], 1)
        getitem_785: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_113[1];  split_with_sizes_113 = None
        view_955: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_785, [-1, 2, 128]);  getitem_785 = None
        triton_kernel_wrapper_functional_proxy_125 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 90, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 9, 1), (4, 9, 1), (4, 9, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_945, 'K': view_946, 'V': view_947, 'sort_by_length_indices': getitem_111, 'seq_offsets': getitem_742, 'attn_scale': reciprocal_5, 'DOut': view_953, 'DQ': view_952, 'DK': view_954, 'DV': view_955, 'LOCK': full_229, 'M': empty_246}, tensors_to_clone = ['DQ', 'DK', 'DV']);  view_945 = view_946 = view_947 = getitem_111 = reciprocal_5 = view_953 = view_952 = view_954 = view_955 = full_229 = empty_246 = None
        getitem_788: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_125['DQ']
        getitem_789: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_125['DK']
        getitem_790: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_125['DV'];  triton_kernel_wrapper_functional_proxy_125 = None
        view_956: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_788, [514, 256]);  getitem_788 = None
        slice_scatter_33: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_32, view_956, 1, 512, 768);  slice_scatter_32 = view_956 = None
        view_958: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_789, [514, 256]);  getitem_789 = None
        slice_scatter_34: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_33, view_958, 1, 768, 1024);  slice_scatter_33 = view_958 = None
        view_960: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_790, [514, 256]);  getitem_790 = None
        slice_scatter_35: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_34, view_960, 1, 256, 512);  slice_scatter_34 = view_960 = None
        split_with_sizes_120 = torch.ops.aten.split_with_sizes.default(slice_scatter_35, [256, 256, 256, 256], 1)
        getitem_817: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_120[2];  split_with_sizes_120 = None
        view_962: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_817, [-1, 2, 128]);  getitem_817 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_12: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_962, view_962);  view_962 = None
        view_963: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_12, [514, 256]);  copy_12 = None
        slice_scatter_36: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_35, view_963, 1, 512, 768);  slice_scatter_35 = view_963 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_123 = torch.ops.aten.split_with_sizes.default(slice_scatter_36, [256, 256, 256, 256], 1)
        getitem_830: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_123[3];  split_with_sizes_123 = None
        view_965: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_830, [-1, 2, 128]);  getitem_830 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_13: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_965, view_965);  view_965 = None
        view_966: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_13, [514, 256]);  copy_13 = None
        slice_scatter_37: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_36, view_966, 1, 768, 1024);  slice_scatter_36 = view_966 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_126 = torch.ops.aten.split_with_sizes.default(slice_scatter_37, [256, 256, 256, 256], 1)
        getitem_840: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_126[1];  split_with_sizes_126 = None
        view_968: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_840, [-1, 2, 128]);  getitem_840 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_14: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_968, view_968);  view_968 = None
        view_969: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_14, [514, 256]);  copy_14 = None
        slice_scatter_38: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_37, view_969, 1, 256, 512);  slice_scatter_37 = view_969 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1446: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_757, torch.float32);  getitem_757 = None
        convert_element_type_1447: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_764, torch.float32);  getitem_764 = None
        neg_104: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1447)
        exp_104: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_104);  neg_104 = None
        add_663: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_104, 1);  exp_104 = None
        reciprocal_48: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_663);  add_663 = None
        mul_687: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_48, 1);  reciprocal_48 = None
        mul_688: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1446, mul_687);  convert_element_type_1446 = None
        sub_272: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_687);  mul_687 = None
        mul_689: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1447, sub_272);  convert_element_type_1447 = sub_272 = None
        add_664: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_689, 1);  mul_689 = None
        mul_690: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_688, add_664);  mul_688 = add_664 = None
        convert_element_type_1448: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_690, torch.bfloat16);  mul_690 = None
        slice_scatter_39: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_38, convert_element_type_1448, 1, 0, 256);  slice_scatter_38 = convert_element_type_1448 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_84: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_39, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_574: "bf16[256, 514][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_763, [1, 0]);  getitem_763 = None
        mm_262: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_574, slice_scatter_39);  permute_574 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_575: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_473, [1, 0]);  convert_element_type_473 = None
        mm_263: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_39, permute_575);  slice_scatter_39 = permute_575 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_247: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_248: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_249: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_250: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_251: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_126 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 91, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_247, 'DY': mm_263, 'DW': empty_248, 'DB': empty_249, 'X': addmm_5, 'W': convert_element_type_471, 'B': convert_element_type_472, 'Mean': getitem_104, 'Rstd': getitem_105}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_247 = mm_263 = empty_248 = empty_249 = addmm_5 = convert_element_type_471 = convert_element_type_472 = getitem_104 = getitem_105 = None
        getitem_859: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_126['DX']
        getitem_860: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_126['DW']
        getitem_861: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_126['DB'];  triton_kernel_wrapper_functional_proxy_126 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_127 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 92, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_860, 'DB': getitem_861, 'FINAL_DW': empty_250, 'FINAL_DB': empty_251}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_860 = getitem_861 = empty_250 = empty_251 = None
        getitem_862: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_127['FINAL_DW']
        getitem_863: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_127['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_127 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_665: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_653, getitem_859);  add_653 = getitem_859 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1453: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_84, torch.float32);  sum_84 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1454: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_262, torch.float32);  mm_262 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1455: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_863, torch.float32);  getitem_863 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1456: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_862, torch.float32);  getitem_862 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_576: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_464, [1, 0]);  convert_element_type_464 = None
        mm_264: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_660, permute_576);  permute_576 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_252: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        empty_253: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_254: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_255: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_256: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_257: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_258: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_128 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 21, constant_args_idx = 85, grid = [(128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_253, 'DU': empty_254, 'DY': mm_264, 'DW': empty_255, 'DB': empty_256, 'X': view_277, 'U': convert_element_type_461, 'Y': empty_252, 'W': convert_element_type_462, 'B': convert_element_type_463, 'Mean': getitem_101, 'Rstd': getitem_102, 'seed': primals_171}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_253 = empty_254 = mm_264 = empty_255 = empty_256 = view_277 = convert_element_type_461 = empty_252 = convert_element_type_462 = convert_element_type_463 = getitem_101 = getitem_102 = primals_171 = None
        getitem_864: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_128['DX']
        getitem_865: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_128['DU']
        getitem_866: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_128['DW']
        getitem_867: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_128['DB']
        getitem_868: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_128['Y'];  triton_kernel_wrapper_functional_proxy_128 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_129 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 86, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_866, 'DB': getitem_867, 'FINAL_DW': empty_257, 'FINAL_DB': empty_258}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_866 = getitem_867 = empty_257 = empty_258 = None
        getitem_869: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_129['FINAL_DW']
        getitem_870: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_129['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_129 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_578: "bf16[768, 514][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_868, [1, 0]);  getitem_868 = None
        mm_265: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_578, add_660);  permute_578 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1461: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_265, torch.float32);  mm_265 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1462: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_870, torch.float32);  getitem_870 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1463: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_869, torch.float32);  getitem_869 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_259: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_130 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 80, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': addmm_3, 'Y': empty_259, 'W': convert_element_type_453, 'B': convert_element_type_454, 'Mean': getitem_91, 'Rstd': getitem_92}, tensors_to_clone = ['Y']);  empty_259 = None
        getitem_871: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_130['Y'];  triton_kernel_wrapper_functional_proxy_130 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_23: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_456, getitem_871, convert_element_type_455);  convert_element_type_456 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_131 = torch.ops.aten.split_with_sizes.default(addmm_23, [256, 256, 256, 256], 1);  addmm_23 = None
        getitem_872: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_131[0]
        getitem_873: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_131[1]
        getitem_874: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_131[2]
        getitem_875: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_131[3];  split_with_sizes_131 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_260: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_972: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_874, [-1, 2, 128]);  getitem_874 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_973: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_875, [-1, 2, 128]);  getitem_875 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_974: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_873, [-1, 2, 128]);  getitem_873 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_261: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        full_253: "i32[4, 17][17, 1]cuda:0" = torch.ops.aten.full.default([4, 17], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        full_254: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        view_978: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_254, [514, 256]);  full_254 = None
        slice_scatter_40: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_260, view_978, 1, 512, 768);  empty_260 = view_978 = None
        split_with_sizes_134 = torch.ops.aten.split_with_sizes.default(slice_scatter_40, [256, 256, 256, 256], 1)
        getitem_886: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_134[2];  split_with_sizes_134 = None
        view_979: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_886, [-1, 2, 128]);  getitem_886 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_980: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_864, [514, 2, 128]);  getitem_864 = None
        split_with_sizes_135 = torch.ops.aten.split_with_sizes.default(slice_scatter_40, [256, 256, 256, 256], 1)
        getitem_891: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_135[3];  split_with_sizes_135 = None
        view_981: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_891, [-1, 2, 128]);  getitem_891 = None
        split_with_sizes_136 = torch.ops.aten.split_with_sizes.default(slice_scatter_40, [256, 256, 256, 256], 1)
        getitem_893: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_136[1];  split_with_sizes_136 = None
        view_982: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_893, [-1, 2, 128]);  getitem_893 = None
        triton_kernel_wrapper_functional_proxy_131 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 81, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 9, 1), (4, 9, 1), (4, 9, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_972, 'K': view_973, 'V': view_974, 'sort_by_length_indices': getitem_98, 'seq_offsets': getitem_750, 'attn_scale': reciprocal_4, 'DOut': view_980, 'DQ': view_979, 'DK': view_981, 'DV': view_982, 'LOCK': full_253, 'M': empty_261}, tensors_to_clone = ['DQ', 'DK', 'DV']);  view_972 = view_973 = view_974 = getitem_98 = reciprocal_4 = view_980 = view_979 = view_981 = view_982 = full_253 = empty_261 = None
        getitem_896: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_131['DQ']
        getitem_897: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_131['DK']
        getitem_898: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_131['DV'];  triton_kernel_wrapper_functional_proxy_131 = None
        view_983: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_896, [514, 256]);  getitem_896 = None
        slice_scatter_41: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_40, view_983, 1, 512, 768);  slice_scatter_40 = view_983 = None
        view_985: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_897, [514, 256]);  getitem_897 = None
        slice_scatter_42: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_41, view_985, 1, 768, 1024);  slice_scatter_41 = view_985 = None
        view_987: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_898, [514, 256]);  getitem_898 = None
        slice_scatter_43: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_42, view_987, 1, 256, 512);  slice_scatter_42 = view_987 = None
        split_with_sizes_143 = torch.ops.aten.split_with_sizes.default(slice_scatter_43, [256, 256, 256, 256], 1)
        getitem_925: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_143[2];  split_with_sizes_143 = None
        view_989: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_925, [-1, 2, 128]);  getitem_925 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_15: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_989, view_989);  view_989 = None
        view_990: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_15, [514, 256]);  copy_15 = None
        slice_scatter_44: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_43, view_990, 1, 512, 768);  slice_scatter_43 = view_990 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_146 = torch.ops.aten.split_with_sizes.default(slice_scatter_44, [256, 256, 256, 256], 1)
        getitem_938: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_146[3];  split_with_sizes_146 = None
        view_992: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_938, [-1, 2, 128]);  getitem_938 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_16: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_992, view_992);  view_992 = None
        view_993: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_16, [514, 256]);  copy_16 = None
        slice_scatter_45: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_44, view_993, 1, 768, 1024);  slice_scatter_44 = view_993 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_149 = torch.ops.aten.split_with_sizes.default(slice_scatter_45, [256, 256, 256, 256], 1)
        getitem_948: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_149[1];  split_with_sizes_149 = None
        view_995: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_948, [-1, 2, 128]);  getitem_948 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_17: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_995, view_995);  view_995 = None
        view_996: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_17, [514, 256]);  copy_17 = None
        slice_scatter_46: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_45, view_996, 1, 256, 512);  slice_scatter_45 = view_996 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1467: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_865, torch.float32);  getitem_865 = None
        convert_element_type_1468: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_872, torch.float32);  getitem_872 = None
        neg_105: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1468)
        exp_105: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_105);  neg_105 = None
        add_666: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_105, 1);  exp_105 = None
        reciprocal_49: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_666);  add_666 = None
        mul_691: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_49, 1);  reciprocal_49 = None
        mul_692: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1467, mul_691);  convert_element_type_1467 = None
        sub_273: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_691);  mul_691 = None
        mul_693: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1468, sub_273);  convert_element_type_1468 = sub_273 = None
        add_667: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_693, 1);  mul_693 = None
        mul_694: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_692, add_667);  mul_692 = add_667 = None
        convert_element_type_1469: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_694, torch.bfloat16);  mul_694 = None
        slice_scatter_47: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_46, convert_element_type_1469, 1, 0, 256);  slice_scatter_46 = convert_element_type_1469 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_85: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_47, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_580: "bf16[256, 514][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_871, [1, 0]);  getitem_871 = None
        mm_266: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_580, slice_scatter_47);  permute_580 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_581: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_455, [1, 0]);  convert_element_type_455 = None
        mm_267: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_47, permute_581);  slice_scatter_47 = permute_581 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_262: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_263: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_264: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_265: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_266: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_132 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 82, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_262, 'DY': mm_267, 'DW': empty_263, 'DB': empty_264, 'X': addmm_3, 'W': convert_element_type_453, 'B': convert_element_type_454, 'Mean': getitem_91, 'Rstd': getitem_92}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_262 = mm_267 = empty_263 = empty_264 = addmm_3 = convert_element_type_453 = convert_element_type_454 = getitem_91 = getitem_92 = None
        getitem_967: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_132['DX']
        getitem_968: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_132['DW']
        getitem_969: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_132['DB'];  triton_kernel_wrapper_functional_proxy_132 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_133 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 83, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_968, 'DB': getitem_969, 'FINAL_DW': empty_265, 'FINAL_DB': empty_266}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_968 = getitem_969 = empty_265 = empty_266 = None
        getitem_970: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_133['FINAL_DW']
        getitem_971: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_133['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_133 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_668: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_660, getitem_967);  add_660 = getitem_967 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1474: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_85, torch.float32);  sum_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1475: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_266, torch.float32);  mm_266 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1476: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_971, torch.float32);  getitem_971 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1477: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_970, torch.float32);  getitem_970 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        clone_72: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.clone.default(add_662, memory_format = torch.contiguous_format);  add_662 = None
        view_998: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_72, [640, 256]);  clone_72 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_267: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_257: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_134 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 77, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_267, 'DY': view_998, 'DW': full_257, 'X': view_268, 'W': primals_163, 'Rstd': getitem_89}, tensors_to_clone = ['DX', 'DW']);  empty_267 = view_998 = full_257 = view_268 = primals_163 = getitem_89 = None
        getitem_972: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_134['DX']
        getitem_973: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_134['DW'];  triton_kernel_wrapper_functional_proxy_134 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1000: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_972, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_1003: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_972, [10, 64, 256])
        slice_85: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1003, 1, 0, 32);  view_1003 = None
        view_1004: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_85, [10, 8192]);  slice_85 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_1005: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_972, [10, 64, 256])
        slice_86: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1005, 1, 0, 32);  view_1005 = None
        view_1006: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_86, [10, 8192]);  slice_86 = None
        permute_583: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_1006, [1, 0]);  view_1006 = None
        mm_268: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_583, convert_element_type_446);  permute_583 = convert_element_type_446 = None
        permute_584: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_268, [1, 0]);  mm_268 = None
        permute_585: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_105, [1, 0]);  permute_105 = None
        mm_269: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1004, permute_585);  view_1004 = permute_585 = None
        permute_586: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_584, [1, 0]);  permute_584 = None
        convert_element_type_1482: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_586, torch.float32);  permute_586 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1483: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_269, torch.float32);  mm_269 = None
        convert_element_type_1484: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_266, torch.float32);  view_266 = None
        neg_106: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1484)
        exp_106: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_106);  neg_106 = None
        add_669: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_106, 1);  exp_106 = None
        reciprocal_50: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_669);  add_669 = None
        mul_695: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_50, 1);  reciprocal_50 = None
        mul_696: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1483, mul_695);  convert_element_type_1483 = None
        sub_274: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_695);  mul_695 = None
        mul_697: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1484, sub_274);  convert_element_type_1484 = sub_274 = None
        add_670: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_697, 1);  mul_697 = None
        mul_698: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_696, add_670);  mul_696 = add_670 = None
        convert_element_type_1485: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_698, torch.bfloat16);  mul_698 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1007: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1485, [10, 2048]);  convert_element_type_1485 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1486: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1007, torch.float32);  view_1007 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_80: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_41);  alias_41 = None
        mul_699: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1486, primals_161);  primals_161 = None
        mul_700: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_443, alias_80);  convert_element_type_443 = None
        mul_701: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_700, mul_699)
        sum_86: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_701, [1], True);  mul_701 = None
        div_105: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_700, 2048)
        mul_702: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_105, sum_86);  div_105 = sum_86 = None
        sub_275: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_699, mul_702);  mul_699 = mul_702 = None
        mul_703: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_275, alias_80);  sub_275 = alias_80 = None
        mul_704: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1486, mul_700);  convert_element_type_1486 = mul_700 = None
        sum_87: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_704, [0]);  mul_704 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1487: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_703, torch.bfloat16);  mul_703 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1008: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1487, [10, 2048]);  convert_element_type_1487 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_587: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1008, [1, 0])
        mm_270: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_587, convert_element_type_439);  permute_587 = convert_element_type_439 = None
        permute_588: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_270, [1, 0]);  mm_270 = None
        permute_589: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_104, [1, 0]);  permute_104 = None
        mm_271: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1008, permute_589);  permute_589 = None
        permute_590: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_588, [1, 0]);  permute_588 = None
        convert_element_type_1492: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_590, torch.float32);  permute_590 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1493: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_271, torch.float32);  mm_271 = None
        convert_element_type_1494: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_264, torch.float32);  view_264 = None
        neg_107: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1494)
        exp_107: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_107);  neg_107 = None
        add_671: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_107, 1);  exp_107 = None
        reciprocal_51: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_671);  add_671 = None
        mul_705: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_51, 1);  reciprocal_51 = None
        mul_706: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1493, mul_705);  convert_element_type_1493 = None
        sub_276: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_705);  mul_705 = None
        mul_707: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1494, sub_276);  convert_element_type_1494 = sub_276 = None
        add_672: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_707, 1);  mul_707 = None
        mul_708: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_706, add_672);  mul_706 = add_672 = None
        convert_element_type_1495: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_708, torch.bfloat16);  mul_708 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1009: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1495, [10, 1024]);  convert_element_type_1495 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1496: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1009, torch.float32);  view_1009 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_81: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_40);  alias_40 = None
        mul_709: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1496, primals_159);  primals_159 = None
        mul_710: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_436, alias_81);  convert_element_type_436 = None
        mul_711: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_710, mul_709)
        sum_88: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_711, [1], True);  mul_711 = None
        div_106: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_710, 1024)
        mul_712: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_106, sum_88);  div_106 = sum_88 = None
        sub_277: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_709, mul_712);  mul_709 = mul_712 = None
        mul_713: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_277, alias_81);  sub_277 = alias_81 = None
        mul_714: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1496, mul_710);  convert_element_type_1496 = mul_710 = None
        sum_89: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_714, [0]);  mul_714 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1497: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_713, torch.bfloat16);  mul_713 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1010: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1497, [10, 1024]);  convert_element_type_1497 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_591: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1010, [1, 0])
        mm_272: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_591, convert_element_type_432);  permute_591 = convert_element_type_432 = None
        permute_592: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_272, [1, 0]);  mm_272 = None
        permute_593: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_103, [1, 0]);  permute_103 = None
        mm_273: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1010, permute_593);  view_1010 = permute_593 = None
        add_673: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1008, mm_273);  view_1008 = mm_273 = None
        permute_594: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_592, [1, 0]);  permute_592 = None
        convert_element_type_1502: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_594, torch.float32);  permute_594 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1503: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_673, torch.float32);  add_673 = None
        convert_element_type_1504: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_262, torch.float32);  view_262 = None
        neg_108: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1504)
        exp_108: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_108);  neg_108 = None
        add_674: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_108, 1);  exp_108 = None
        reciprocal_52: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_674);  add_674 = None
        mul_715: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_52, 1);  reciprocal_52 = None
        mul_716: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1503, mul_715);  convert_element_type_1503 = None
        sub_278: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_715);  mul_715 = None
        mul_717: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1504, sub_278);  convert_element_type_1504 = sub_278 = None
        add_675: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_717, 1);  mul_717 = None
        mul_718: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_716, add_675);  mul_716 = add_675 = None
        convert_element_type_1505: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_718, torch.bfloat16);  mul_718 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1011: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1505, [10, 2048]);  convert_element_type_1505 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1506: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1011, torch.float32);  view_1011 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_82: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_39);  alias_39 = None
        mul_719: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1506, primals_157);  primals_157 = None
        mul_720: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_429, alias_82);  convert_element_type_429 = None
        mul_721: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_720, mul_719)
        sum_90: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_721, [1], True);  mul_721 = None
        div_107: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_720, 2048)
        mul_722: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_107, sum_90);  div_107 = sum_90 = None
        sub_279: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_719, mul_722);  mul_719 = mul_722 = None
        mul_723: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_279, alias_82);  sub_279 = alias_82 = None
        mul_724: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1506, mul_720);  convert_element_type_1506 = mul_720 = None
        sum_91: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_724, [0]);  mul_724 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1507: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_723, torch.bfloat16);  mul_723 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1012: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1507, [10, 2048]);  convert_element_type_1507 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_595: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1012, [1, 0])
        mm_274: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_595, convert_element_type_425);  permute_595 = convert_element_type_425 = None
        permute_596: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_274, [1, 0]);  mm_274 = None
        permute_597: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_102, [1, 0]);  permute_102 = None
        mm_275: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1012, permute_597);  permute_597 = None
        permute_598: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_596, [1, 0]);  permute_596 = None
        convert_element_type_1512: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_598, torch.float32);  permute_598 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1513: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_275, torch.float32);  mm_275 = None
        convert_element_type_1514: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_260, torch.float32);  view_260 = None
        neg_109: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1514)
        exp_109: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_109);  neg_109 = None
        add_676: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_109, 1);  exp_109 = None
        reciprocal_53: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_676);  add_676 = None
        mul_725: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_53, 1);  reciprocal_53 = None
        mul_726: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1513, mul_725);  convert_element_type_1513 = None
        sub_280: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_725);  mul_725 = None
        mul_727: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1514, sub_280);  convert_element_type_1514 = sub_280 = None
        add_677: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_727, 1);  mul_727 = None
        mul_728: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_726, add_677);  mul_726 = add_677 = None
        convert_element_type_1515: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_728, torch.bfloat16);  mul_728 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1013: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1515, [10, 1024]);  convert_element_type_1515 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1516: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1013, torch.float32);  view_1013 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_83: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_38);  alias_38 = None
        mul_729: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1516, primals_155);  primals_155 = None
        mul_730: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_422, alias_83);  convert_element_type_422 = None
        mul_731: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_730, mul_729)
        sum_92: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_731, [1], True);  mul_731 = None
        div_108: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_730, 1024)
        mul_732: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_108, sum_92);  div_108 = sum_92 = None
        sub_281: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_729, mul_732);  mul_729 = mul_732 = None
        mul_733: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_281, alias_83);  sub_281 = alias_83 = None
        mul_734: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1516, mul_730);  convert_element_type_1516 = mul_730 = None
        sum_93: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_734, [0]);  mul_734 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1517: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_733, torch.bfloat16);  mul_733 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1014: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1517, [10, 1024]);  convert_element_type_1517 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_599: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1014, [1, 0])
        mm_276: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_599, convert_element_type_418);  permute_599 = convert_element_type_418 = None
        permute_600: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_276, [1, 0]);  mm_276 = None
        permute_601: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_101, [1, 0]);  permute_101 = None
        mm_277: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1014, permute_601);  view_1014 = permute_601 = None
        add_678: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1012, mm_277);  view_1012 = mm_277 = None
        permute_602: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_600, [1, 0]);  permute_600 = None
        convert_element_type_1522: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_602, torch.float32);  permute_602 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1523: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_678, torch.float32);  add_678 = None
        convert_element_type_1524: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_258, torch.float32);  view_258 = None
        neg_110: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1524)
        exp_110: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_110);  neg_110 = None
        add_679: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_110, 1);  exp_110 = None
        reciprocal_54: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_679);  add_679 = None
        mul_735: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_54, 1);  reciprocal_54 = None
        mul_736: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1523, mul_735);  convert_element_type_1523 = None
        sub_282: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_735);  mul_735 = None
        mul_737: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1524, sub_282);  convert_element_type_1524 = sub_282 = None
        add_680: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_737, 1);  mul_737 = None
        mul_738: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_736, add_680);  mul_736 = add_680 = None
        convert_element_type_1525: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_738, torch.bfloat16);  mul_738 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1015: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1525, [10, 2048]);  convert_element_type_1525 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1526: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1015, torch.float32);  view_1015 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_84: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_37);  alias_37 = None
        mul_739: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1526, primals_153);  primals_153 = None
        mul_740: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_415, alias_84);  convert_element_type_415 = None
        mul_741: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_740, mul_739)
        sum_94: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_741, [1], True);  mul_741 = None
        div_109: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_740, 2048)
        mul_742: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_109, sum_94);  div_109 = sum_94 = None
        sub_283: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_739, mul_742);  mul_739 = mul_742 = None
        mul_743: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_283, alias_84);  sub_283 = alias_84 = None
        mul_744: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1526, mul_740);  convert_element_type_1526 = mul_740 = None
        sum_95: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_744, [0]);  mul_744 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1527: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_743, torch.bfloat16);  mul_743 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1016: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1527, [10, 2048]);  convert_element_type_1527 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_603: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1016, [1, 0])
        mm_278: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_603, convert_element_type_411);  permute_603 = convert_element_type_411 = None
        permute_604: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_278, [1, 0]);  mm_278 = None
        permute_605: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_100, [1, 0]);  permute_100 = None
        mm_279: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_1016, permute_605);  view_1016 = permute_605 = None
        permute_606: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_604, [1, 0]);  permute_604 = None
        convert_element_type_1532: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_606, torch.float32);  permute_606 = None
        convert_element_type_1533: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_279, torch.float32);  mm_279 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1017: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1533, [10, 8472]);  convert_element_type_1533 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_268: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_260: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_135 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 75, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_268, 'DY': view_1017, 'DW': full_260, 'X': view_254, 'W': primals_151, 'Rstd': getitem_87}, tensors_to_clone = ['DX', 'DW']);  empty_268 = view_1017 = full_260 = view_254 = primals_151 = getitem_87 = None
        getitem_974: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_135['DX']
        getitem_975: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_135['DW'];  triton_kernel_wrapper_functional_proxy_135 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_1020: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_974, [10, 8472])
        slice_89: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1020, 1, 0, 6144);  view_1020 = None
        convert_element_type_1534: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_89, torch.bfloat16);  slice_89 = None
        view_1021: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_974, [10, 8472]);  getitem_974 = None
        slice_90: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1021, 1, 6144, 8472);  view_1021 = None
        add_681: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_640, slice_90);  add_640 = slice_90 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_91: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1534, 1, 0, 2048)
        slice_92: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1534, 1, 2048, 6144);  convert_element_type_1534 = None
        view_1022: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_92, [10, 16, 256]);  slice_92 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_1023: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_91, [10, 32, 64]);  slice_91 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_1024: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_1023, [10, 32, 64]);  view_1023 = None
        permute_607: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_249, [0, 2, 1]);  view_249 = None
        bmm_43: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_607, view_1024);  permute_607 = None
        permute_608: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_250, [0, 2, 1]);  view_250 = None
        bmm_44: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_1024, permute_608);  view_1024 = permute_608 = None
        view_1025: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_43, [10, 256, 64]);  bmm_43 = None
        view_1026: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_44, [10, 32, 256]);  bmm_44 = None
        permute_609: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1025, [0, 2, 1]);  view_1025 = None
        add_682: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1000, permute_609);  view_1000 = permute_609 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_1027: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_1026, [10, 32, 256]);  view_1026 = None
        permute_610: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_246, [0, 2, 1]);  view_246 = None
        bmm_45: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_610, view_1027);  permute_610 = None
        permute_611: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.permute.default(view_247, [0, 2, 1]);  view_247 = None
        bmm_46: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_1027, permute_611);  view_1027 = permute_611 = None
        view_1028: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_45, [10, 64, 256]);  bmm_45 = None
        add_683: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_682, view_1028);  add_682 = view_1028 = None
        view_1029: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_46, [10, 32, 64]);  bmm_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_1030: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_1029, [10, 2048]);  view_1029 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_612: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1030, [1, 0])
        mm_280: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_612, convert_element_type_403);  permute_612 = convert_element_type_403 = None
        permute_613: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_280, [1, 0]);  mm_280 = None
        permute_614: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_98, [1, 0]);  permute_98 = None
        mm_281: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_1030, permute_614);  view_1030 = permute_614 = None
        permute_615: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_613, [1, 0]);  permute_613 = None
        convert_element_type_1547: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_615, torch.float32);  permute_615 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1548: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_281, torch.float32);  mm_281 = None
        convert_element_type_1549: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_244, torch.float32);  view_244 = None
        neg_111: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1549)
        exp_111: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_111);  neg_111 = None
        add_684: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_111, 1);  exp_111 = None
        reciprocal_55: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_684);  add_684 = None
        mul_745: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_55, 1);  reciprocal_55 = None
        mul_746: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1548, mul_745);  convert_element_type_1548 = None
        sub_284: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_745);  mul_745 = None
        mul_747: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1549, sub_284);  convert_element_type_1549 = sub_284 = None
        add_685: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_747, 1);  mul_747 = None
        mul_748: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_746, add_685);  mul_746 = add_685 = None
        convert_element_type_1550: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_748, torch.bfloat16);  mul_748 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1031: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1550, [10, 512]);  convert_element_type_1550 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1551: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1031, torch.float32);  view_1031 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_85: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_36);  alias_36 = None
        mul_749: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1551, primals_149);  primals_149 = None
        mul_750: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_400, alias_85);  convert_element_type_400 = None
        mul_751: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_750, mul_749)
        sum_96: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_751, [1], True);  mul_751 = None
        div_110: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_750, 512)
        mul_752: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_110, sum_96);  div_110 = sum_96 = None
        sub_285: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_749, mul_752);  mul_749 = mul_752 = None
        mul_753: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_285, alias_85);  sub_285 = alias_85 = None
        mul_754: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1551, mul_750);  convert_element_type_1551 = mul_750 = None
        sum_97: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_754, [0]);  mul_754 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1552: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_753, torch.bfloat16);  mul_753 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1032: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1552, [10, 512]);  convert_element_type_1552 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_616: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_1032, [1, 0])
        mm_282: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_616, view_242);  permute_616 = view_242 = None
        permute_617: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_282, [1, 0]);  mm_282 = None
        permute_618: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_97, [1, 0]);  permute_97 = None
        mm_283: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1032, permute_618);  view_1032 = permute_618 = None
        permute_619: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_617, [1, 0]);  permute_617 = None
        convert_element_type_1557: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_619, torch.float32);  permute_619 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_1033: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_283, [10, 32, 256]);  mm_283 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_1034: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_972, [10, 64, 256]);  getitem_972 = None
        slice_93: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1034, 1, 32, 64);  view_1034 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_45: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_93, view_1033, view_1022], 1);  slice_93 = view_1033 = view_1022 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_620: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_45, [0, 2, 1]);  cat_45 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_73: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_620, memory_format = torch.contiguous_format);  permute_620 = None
        view_1035: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_73, [2560, 80]);  clone_73 = None
        permute_621: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_1035, [1, 0])
        mm_284: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_621, view_240);  permute_621 = view_240 = None
        permute_622: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_284, [1, 0]);  mm_284 = None
        permute_623: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_95, [1, 0]);  permute_95 = None
        mm_285: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1035, permute_623);  view_1035 = permute_623 = None
        view_1036: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_285, [10, 256, 64]);  mm_285 = None
        permute_624: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_622, [1, 0]);  permute_622 = None
        convert_element_type_1562: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_624, torch.float32);  permute_624 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_625: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1036, [0, 2, 1]);  view_1036 = None
        add_686: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_683, permute_625);  add_683 = permute_625 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_626: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(add_686, [0, 2, 1]);  add_686 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_74: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.clone.default(permute_626, memory_format = torch.contiguous_format);  permute_626 = None
        view_1037: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.view.default(clone_74, [2560, 64]);  clone_74 = None
        permute_627: "bf16[64, 2560][1, 64]cuda:0" = torch.ops.aten.permute.default(view_1037, [1, 0])
        mm_286: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(permute_627, view_238);  permute_627 = view_238 = None
        permute_628: "bf16[112, 64][1, 112]cuda:0" = torch.ops.aten.permute.default(mm_286, [1, 0]);  mm_286 = None
        permute_629: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_92, [1, 0]);  permute_92 = None
        mm_287: "bf16[2560, 112][112, 1]cuda:0" = torch.ops.aten.mm.default(view_1037, permute_629);  view_1037 = permute_629 = None
        view_1038: "bf16[10, 256, 112][28672, 112, 1]cuda:0" = torch.ops.aten.view.default(mm_287, [10, 256, 112]);  mm_287 = None
        permute_630: "bf16[64, 112][112, 1]cuda:0" = torch.ops.aten.permute.default(permute_628, [1, 0]);  permute_628 = None
        convert_element_type_1567: "f32[64, 112][112, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_630, torch.float32);  permute_630 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_631: "bf16[10, 112, 256][28672, 1, 112]cuda:0" = torch.ops.aten.permute.default(view_1038, [0, 2, 1]);  view_1038 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:306 in forward, code: torch.cat([x_pooled, uih_sum_output, uih_pairwise_products], dim=1)
        slice_94: "bf16[10, 64, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_631, 1, 0, 64)
        slice_95: "bf16[10, 32, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_631, 1, 64, 96)
        slice_96: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(permute_631, 1, 96, 112);  permute_631 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:302 in forward, code: uih_pairwise_products = pairwise_tensor.reshape(
        view_1039: "bf16[10, 1, 16, 256][28672, 16, 1, 112]cuda:0" = torch.ops.aten.view.default(slice_96, [10, 1, 16, 256]);  slice_96 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:300 in forward, code: pairwise_tensor = stacked_uih[:, i_indices] * stacked_uih[:, j_indices]
        mul_755: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_1039, index);  index = None
        mul_756: "bf16[10, 1, 16, 256][4096, 16, 1, 16]cuda:0" = torch.ops.aten.mul.Tensor(view_1039, index_1);  view_1039 = index_1 = None
        full_261: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_6: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_261, [None, getitem_82], mul_755, True);  full_261 = getitem_82 = mul_755 = None
        full_262: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.full.default([10, 2, 16, 256], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        index_put_7: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.index_put.default(full_262, [None, getitem_81], mul_756, True);  full_262 = getitem_81 = mul_756 = None
        add_687: "bf16[10, 2, 16, 256][8192, 4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(index_put_6, index_put_7);  index_put_6 = index_put_7 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:291 in forward, code: stacked_uih = torch.stack(uih_sum_union, dim=1)  # [B, N, n_query, D]
        select_24: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_687, 1, 0)
        select_25: "bf16[10, 16, 256][8192, 256, 1]cuda:0" = torch.ops.aten.select.int(add_687, 1, 1);  add_687 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:287 in forward, code: uih_sum_output = torch.cat(uih_sum_union, dim=1)
        slice_97: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_95, 1, 0, 16)
        slice_98: "bf16[10, 16, 256][28672, 1, 112]cuda:0" = torch.ops.aten.slice.Tensor(slice_95, 1, 16, 32);  slice_95 = None
        add_688: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_24, slice_97);  select_24 = slice_97 = None
        add_689: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(select_25, slice_98);  select_25 = slice_98 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1040: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_689, [160, 256]);  add_689 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_269: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_265: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_136 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 73, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_269, 'DY': view_1040, 'DW': full_265, 'X': view_231, 'W': primals_145, 'Rstd': getitem_80}, tensors_to_clone = ['DX', 'DW']);  empty_269 = view_1040 = full_265 = view_231 = primals_145 = getitem_80 = None
        getitem_976: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_136['DX']
        getitem_977: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_136['DW'];  triton_kernel_wrapper_functional_proxy_136 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_1044: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_976, [10, 16, 256])
        view_1045: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1044, [160, 256]);  view_1044 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_1047: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_976, [10, 16, 256])
        view_1048: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1047, [160, 256]);  view_1047 = None
        view_1049: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1048, [10, 4096]);  view_1048 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_1050: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_976, [10, 16, 256]);  getitem_976 = None
        view_1051: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1050, [160, 256]);  view_1050 = None
        view_1052: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1051, [10, 4096]);  view_1051 = None
        permute_633: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_1052, [1, 0]);  view_1052 = None
        mm_288: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_633, mul_76);  permute_633 = mul_76 = None
        permute_634: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_288, [1, 0]);  mm_288 = None
        permute_635: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_90, [1, 0]);  permute_90 = None
        mm_289: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1049, permute_635);  view_1049 = permute_635 = None
        permute_636: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_634, [1, 0]);  permute_634 = None
        convert_element_type_1572: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_636, torch.float32);  permute_636 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_757: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_289, convert_element_type_380);  convert_element_type_380 = None
        mul_758: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_289, mm_55);  mm_289 = mm_55 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_637: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_757, [1, 0])
        mm_290: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_637, convert_element_type_381);  permute_637 = convert_element_type_381 = None
        permute_638: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_290, [1, 0]);  mm_290 = None
        permute_639: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_89, [1, 0]);  permute_89 = None
        mm_291: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_757, permute_639);  mul_757 = permute_639 = None
        permute_640: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_638, [1, 0]);  permute_638 = None
        convert_element_type_1577: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_640, torch.float32);  permute_640 = None
        convert_element_type_1578: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_291, torch.float32);  mm_291 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1579: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_758, torch.float32);  mul_758 = None
        convert_element_type_1580: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_54, torch.float32);  mm_54 = None
        neg_112: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1580)
        exp_112: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_112);  neg_112 = None
        add_690: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_112, 1);  exp_112 = None
        reciprocal_56: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_690);  add_690 = None
        mul_759: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_56, 1);  reciprocal_56 = None
        mul_760: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1579, mul_759);  convert_element_type_1579 = None
        sub_286: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_759);  mul_759 = None
        mul_761: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1580, sub_286);  convert_element_type_1580 = sub_286 = None
        add_691: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_761, 1);  mul_761 = None
        mul_762: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_760, add_691);  mul_760 = add_691 = None
        convert_element_type_1581: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_762, torch.bfloat16);  mul_762 = None
        permute_641: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1581, [1, 0])
        mm_292: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_641, convert_element_type_375);  permute_641 = convert_element_type_375 = None
        permute_642: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_292, [1, 0]);  mm_292 = None
        permute_643: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_88, [1, 0]);  permute_88 = None
        mm_293: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1581, permute_643);  convert_element_type_1581 = permute_643 = None
        permute_644: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_642, [1, 0]);  permute_642 = None
        convert_element_type_1586: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_644, torch.float32);  permute_644 = None
        convert_element_type_1587: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_293, torch.float32);  mm_293 = None
        add_692: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1578, convert_element_type_1587);  convert_element_type_1578 = convert_element_type_1587 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_1053: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_692, [160, 256]);  add_692 = None
        sub_287: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_374, getitem_78);  convert_element_type_374 = getitem_78 = None
        mul_763: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_287, rsqrt_35);  sub_287 = None
        mul_764: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_1053, primals_140);  primals_140 = None
        mul_765: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_764, 256)
        sum_98: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_764, [1], True)
        mul_766: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_764, mul_763);  mul_764 = None
        sum_99: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_766, [1], True);  mul_766 = None
        mul_767: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_763, sum_99);  sum_99 = None
        sub_288: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_765, sum_98);  mul_765 = sum_98 = None
        sub_289: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_288, mul_767);  sub_288 = mul_767 = None
        div_111: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_35, 256);  rsqrt_35 = None
        mul_768: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_111, sub_289);  div_111 = sub_289 = None
        mul_769: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_1053, mul_763);  mul_763 = None
        sum_100: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_769, [0]);  mul_769 = None
        sum_101: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_1053, [0]);  view_1053 = None
        convert_element_type_1588: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_768, torch.bfloat16);  mul_768 = None
        add_693: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1045, convert_element_type_1588);  view_1045 = convert_element_type_1588 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_1054: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_693, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_267: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_268: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_270: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_137 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 71, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_270, 'Q': view_224, 'K': view_225, 'V': view_225, 'DO': view_1054, 'seq_offsets': getitem_742, 'seq_offsets_q': getitem_75, 'DQ': full_267, 'DK': full_268, 'DV': full_268, 'attn_scale': reciprocal_3}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_270 = view_224 = view_225 = view_1054 = getitem_742 = getitem_75 = full_267 = full_268 = reciprocal_3 = None
        getitem_978: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_137['seq_offsets']
        getitem_980: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_137['DQ']
        getitem_981: "bf16[514, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_137['DK'];  triton_kernel_wrapper_functional_proxy_137 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_1056: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_981, [514, 256]);  getitem_981 = None
        add_694: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_665, view_1056);  add_665 = view_1056 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_1058: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_980, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_1059: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_980, [160, 256]);  getitem_980 = None
        permute_646: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1059, [1, 0]);  view_1059 = None
        mm_294: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_646, view_223);  permute_646 = view_223 = None
        permute_647: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_294, [1, 0]);  mm_294 = None
        permute_648: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_87, [1, 0]);  permute_87 = None
        mm_295: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_1058, permute_648);  view_1058 = permute_648 = None
        permute_649: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_647, [1, 0]);  permute_647 = None
        convert_element_type_1593: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_649, torch.float32);  permute_649 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1060: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_295, [160, 256]);  mm_295 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_271: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_271: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_138 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 69, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_271, 'DY': view_1060, 'DW': full_271, 'X': view_221, 'W': primals_138, 'Rstd': getitem_73}, tensors_to_clone = ['DX', 'DW']);  empty_271 = view_1060 = full_271 = view_221 = primals_138 = getitem_73 = None
        getitem_982: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_138['DX']
        getitem_983: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_138['DW'];  triton_kernel_wrapper_functional_proxy_138 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1062: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_982, [160, 256]);  getitem_982 = None
        add_695: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_693, view_1062);  add_693 = view_1062 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_1063: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_695, [10, 16, 256]);  add_695 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_650: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1063, [0, 2, 1]);  view_1063 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_76: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_650, memory_format = torch.contiguous_format);  permute_650 = None
        view_1064: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_76, [2560, 16]);  clone_76 = None
        permute_651: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_1064, [1, 0])
        mm_296: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_651, view_218);  permute_651 = view_218 = None
        permute_652: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_296, [1, 0]);  mm_296 = None
        permute_653: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_84, [1, 0]);  permute_84 = None
        mm_297: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1064, permute_653);  view_1064 = permute_653 = None
        view_1065: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_297, [10, 256, 64]);  mm_297 = None
        permute_654: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_652, [1, 0]);  permute_652 = None
        convert_element_type_1598: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_654, torch.float32);  permute_654 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_655: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1065, [0, 2, 1]);  view_1065 = None
        add_696: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(slice_94, permute_655);  slice_94 = permute_655 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1066: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_688, [160, 256]);  add_688 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_272: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_274: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_139 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 67, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_272, 'DY': view_1066, 'DW': full_274, 'X': view_215, 'W': primals_136, 'Rstd': getitem_71}, tensors_to_clone = ['DX', 'DW']);  empty_272 = view_1066 = full_274 = view_215 = primals_136 = getitem_71 = None
        getitem_984: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_139['DX']
        getitem_985: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_139['DW'];  triton_kernel_wrapper_functional_proxy_139 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:282 in forward, code: uih_sum = uih_sum.reshape(B_NRO, n_query, n_dim)
        view_1070: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_984, [10, 16, 256])
        view_1071: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1070, [160, 256]);  view_1070 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:217 in forward, code: ffn_output = self.ffn(ffn_input).reshape(q_batch_size * self.n_q, self.d_model)
        view_1073: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_984, [10, 16, 256])
        view_1074: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1073, [160, 256]);  view_1073 = None
        view_1075: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1074, [10, 4096]);  view_1074 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:483 in forward, code: output = self.w_down(hidden)
        view_1076: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_984, [10, 16, 256]);  getitem_984 = None
        view_1077: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(view_1076, [160, 256]);  view_1076 = None
        view_1078: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(view_1077, [10, 4096]);  view_1077 = None
        permute_657: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_1078, [1, 0]);  view_1078 = None
        mm_298: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_657, mul_72);  permute_657 = mul_72 = None
        permute_658: "bf16[8192, 4096][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_298, [1, 0]);  mm_298 = None
        permute_659: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_82, [1, 0]);  permute_82 = None
        mm_299: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1075, permute_659);  view_1075 = permute_659 = None
        permute_660: "bf16[4096, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_658, [1, 0]);  permute_658 = None
        convert_element_type_1603: "f32[4096, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_660, torch.float32);  permute_660 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:482 in forward, code: hidden = gate * up
        mul_770: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_299, convert_element_type_357);  convert_element_type_357 = None
        mul_771: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mm_299, mm_50);  mm_299 = mm_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:481 in forward, code: up = self.w_up(x)
        permute_661: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(mul_770, [1, 0])
        mm_300: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_661, convert_element_type_358);  permute_661 = convert_element_type_358 = None
        permute_662: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_300, [1, 0]);  mm_300 = None
        permute_663: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_81, [1, 0]);  permute_81 = None
        mm_301: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(mul_770, permute_663);  mul_770 = permute_663 = None
        permute_664: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_662, [1, 0]);  permute_662 = None
        convert_element_type_1608: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_664, torch.float32);  permute_664 = None
        convert_element_type_1609: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_301, torch.float32);  mm_301 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:480 in forward, code: gate = self.silu(self.w_gate(x))
        convert_element_type_1610: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_771, torch.float32);  mul_771 = None
        convert_element_type_1611: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_49, torch.float32);  mm_49 = None
        neg_113: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1611)
        exp_113: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.exp.default(neg_113);  neg_113 = None
        add_697: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_113, 1);  exp_113 = None
        reciprocal_57: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_697);  add_697 = None
        mul_772: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_57, 1);  reciprocal_57 = None
        mul_773: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1610, mul_772);  convert_element_type_1610 = None
        sub_290: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_772);  mul_772 = None
        mul_774: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1611, sub_290);  convert_element_type_1611 = sub_290 = None
        add_698: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_774, 1);  mul_774 = None
        mul_775: "f32[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_773, add_698);  mul_773 = add_698 = None
        convert_element_type_1612: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_775, torch.bfloat16);  mul_775 = None
        permute_665: "bf16[8192, 10][1, 8192]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1612, [1, 0])
        mm_302: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_665, convert_element_type_352);  permute_665 = convert_element_type_352 = None
        permute_666: "bf16[4096, 8192][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_302, [1, 0]);  mm_302 = None
        permute_667: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_80, [1, 0]);  permute_80 = None
        mm_303: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_1612, permute_667);  convert_element_type_1612 = permute_667 = None
        permute_668: "bf16[8192, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_666, [1, 0]);  permute_666 = None
        convert_element_type_1617: "f32[8192, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_668, torch.float32);  permute_668 = None
        convert_element_type_1618: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_303, torch.float32);  mm_303 = None
        add_699: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_1609, convert_element_type_1618);  convert_element_type_1609 = convert_element_type_1618 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:215 in forward, code: ffn_input = self.ffn_ln(attn_res).reshape(q_batch_size, self.d_model * self.n_q)
        view_1079: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_699, [160, 256]);  add_699 = None
        sub_291: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(convert_element_type_351, getitem_69);  convert_element_type_351 = getitem_69 = None
        mul_776: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_291, rsqrt_34);  sub_291 = None
        mul_777: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_1079, primals_131);  primals_131 = None
        mul_778: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_777, 256)
        sum_102: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_777, [1], True)
        mul_779: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_777, mul_776);  mul_777 = None
        sum_103: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_779, [1], True);  mul_779 = None
        mul_780: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_776, sum_103);  sum_103 = None
        sub_292: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_778, sum_102);  mul_778 = sum_102 = None
        sub_293: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(sub_292, mul_780);  sub_292 = mul_780 = None
        div_112: "f32[160, 1][1, 1]cuda:0" = torch.ops.aten.div.Tensor(rsqrt_34, 256);  rsqrt_34 = None
        mul_781: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_112, sub_293);  div_112 = sub_293 = None
        mul_782: "f32[160, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(view_1079, mul_776);  mul_776 = None
        sum_104: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_782, [0]);  mul_782 = None
        sum_105: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(view_1079, [0]);  view_1079 = None
        convert_element_type_1619: "bf16[160, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_781, torch.bfloat16);  mul_781 = None
        add_700: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1071, convert_element_type_1619);  view_1071 = convert_element_type_1619 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:211 in forward, code: attn_res = attn_res.reshape(-1, self.d_head * self.n_heads)
        view_1080: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(add_700, [160, 2, 128])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1921 in triton_hstu_attention_bwd, code: dq = torch.zeros_like(q)
        full_276: "bf16[160, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([160, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1922 in triton_hstu_attention_bwd, code: dk = torch.zeros_like(k)
        full_277: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:658 in get_bwd_tma_workspace, code: return torch.empty(
        empty_273: "u8[16384][1]cuda:0" = torch.ops.aten.empty.memory_format([16384], dtype = torch.uint8, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/template/triton_bw_cross_attention.py:1968 in triton_hstu_attention_bwd, code: _hstu_attn_bwd_redkv[grid](
        triton_kernel_wrapper_functional_proxy_140 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 19, constant_args_idx = 65, grid = [(1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (1, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1), (2, 4, 1)], tma_descriptor_metadata = {}, kwargs = {'workspace_ptr': empty_273, 'Q': view_208, 'K': view_209, 'V': view_209, 'DO': view_1080, 'seq_offsets': getitem_750, 'seq_offsets_q': getitem_66, 'DQ': full_276, 'DK': full_277, 'DV': full_277, 'attn_scale': reciprocal_2}, tensors_to_clone = ['seq_offsets', 'seq_offsets_q', 'DQ', 'DK']);  empty_273 = view_208 = view_209 = view_1080 = getitem_750 = getitem_66 = full_276 = full_277 = reciprocal_2 = None
        getitem_986: "i64[3][1]cuda:0" = triton_kernel_wrapper_functional_proxy_140['seq_offsets']
        getitem_988: "bf16[160, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_140['DQ']
        getitem_989: "bf16[514, 2, 128][256, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_140['DK'];  triton_kernel_wrapper_functional_proxy_140 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:178 in forward, code: kv = kv.view(
        view_1082: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_989, [514, 256]);  getitem_989 = None
        add_701: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_668, view_1082);  add_668 = view_1082 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:169 in forward, code: q = q.view(-1, self.n_heads, self.d_head)  # size: (jagged_len, n_heads, d_head)
        view_1084: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_988, [160, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:158 in forward, code: q = self.q_proj(x)
        view_1085: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_988, [160, 256]);  getitem_988 = None
        permute_670: "bf16[256, 160][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1085, [1, 0]);  view_1085 = None
        mm_304: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_670, view_207);  permute_670 = view_207 = None
        permute_671: "bf16[256, 256][1, 256]cuda:0" = torch.ops.aten.permute.default(mm_304, [1, 0]);  mm_304 = None
        permute_672: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_79, [1, 0]);  permute_79 = None
        mm_305: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(view_1084, permute_672);  view_1084 = permute_672 = None
        permute_673: "bf16[256, 256][256, 1]cuda:0" = torch.ops.aten.permute.default(permute_671, [1, 0]);  permute_671 = None
        convert_element_type_1624: "f32[256, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_673, torch.float32);  permute_673 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1086: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(mm_305, [160, 256]);  mm_305 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_274: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([160, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_280: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_141 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 63, grid = [(40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1), (40, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_274, 'DY': view_1086, 'DW': full_280, 'X': view_205, 'W': primals_129, 'Rstd': getitem_64}, tensors_to_clone = ['DX', 'DW']);  empty_274 = view_1086 = full_280 = view_205 = primals_129 = getitem_64 = None
        getitem_990: "bf16[160, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_141['DX']
        getitem_991: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_141['DW'];  triton_kernel_wrapper_functional_proxy_141 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1088: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_990, [160, 256]);  getitem_990 = None
        add_702: "bf16[160, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_700, view_1088);  add_700 = view_1088 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:273 in forward, code: x=uihsum_q.reshape(B_NRO * n_query, n_dim),
        view_1089: "bf16[10, 16, 256][4096, 256, 1]cuda:0" = torch.ops.aten.view.default(add_702, [10, 16, 256]);  add_702 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_674: "bf16[10, 256, 16][4096, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1089, [0, 2, 1]);  view_1089 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_78: "bf16[10, 256, 16][4096, 16, 1]cuda:0" = torch.ops.aten.clone.default(permute_674, memory_format = torch.contiguous_format);  permute_674 = None
        view_1090: "bf16[2560, 16][16, 1]cuda:0" = torch.ops.aten.view.default(clone_78, [2560, 16]);  clone_78 = None
        permute_675: "bf16[16, 2560][1, 16]cuda:0" = torch.ops.aten.permute.default(view_1090, [1, 0])
        mm_306: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_675, view_202);  permute_675 = view_202 = None
        permute_676: "bf16[64, 16][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_306, [1, 0]);  mm_306 = None
        permute_677: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_76, [1, 0]);  permute_76 = None
        mm_307: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1090, permute_677);  view_1090 = permute_677 = None
        view_1091: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_307, [10, 256, 64]);  mm_307 = None
        permute_678: "bf16[16, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_676, [1, 0]);  permute_676 = None
        convert_element_type_1629: "f32[16, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_678, torch.float32);  permute_678 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_679: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1091, [0, 2, 1]);  view_1091 = None
        add_703: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.add.Tensor(add_696, permute_679);  add_696 = permute_679 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_680: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_338, [1, 0]);  convert_element_type_338 = None
        mm_308: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_694, permute_680);  permute_680 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_275: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        empty_276: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_277: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_278: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_279: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_280: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_281: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_142 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 17, constant_args_idx = 60, grid = [(128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_276, 'DU': empty_277, 'DY': mm_308, 'DW': empty_278, 'DB': empty_279, 'X': view_199, 'U': convert_element_type_335, 'Y': empty_275, 'W': convert_element_type_336, 'B': convert_element_type_337, 'Mean': getitem_61, 'Rstd': getitem_62, 'seed': primals_126}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_276 = empty_277 = mm_308 = empty_278 = empty_279 = view_199 = convert_element_type_335 = empty_275 = convert_element_type_336 = convert_element_type_337 = getitem_61 = getitem_62 = primals_126 = None
        getitem_992: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_142['DX']
        getitem_993: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_142['DU']
        getitem_994: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_142['DW']
        getitem_995: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_142['DB']
        getitem_996: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_142['Y'];  triton_kernel_wrapper_functional_proxy_142 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_143 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 61, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_994, 'DB': getitem_995, 'FINAL_DW': empty_280, 'FINAL_DB': empty_281}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_994 = getitem_995 = empty_280 = empty_281 = None
        getitem_997: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_143['FINAL_DW']
        getitem_998: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_143['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_143 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_682: "bf16[768, 514][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_996, [1, 0]);  getitem_996 = None
        mm_309: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_682, add_694);  permute_682 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1634: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_309, torch.float32);  mm_309 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1635: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_998, torch.float32);  getitem_998 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1636: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_997, torch.float32);  getitem_997 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_282: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_144 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 55, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_191, 'Y': empty_282, 'W': convert_element_type_327, 'B': convert_element_type_328, 'Mean': getitem_51, 'Rstd': getitem_52}, tensors_to_clone = ['Y']);  empty_282 = None
        getitem_999: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_144['Y'];  triton_kernel_wrapper_functional_proxy_144 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_24: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_330, getitem_999, convert_element_type_329);  convert_element_type_330 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_154 = torch.ops.aten.split_with_sizes.default(addmm_24, [256, 256, 256, 256], 1);  addmm_24 = None
        getitem_1000: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_154[0]
        getitem_1001: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_154[1]
        getitem_1002: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_154[2]
        getitem_1003: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_154[3];  split_with_sizes_154 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_283: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_1093: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1002, [-1, 2, 128]);  getitem_1002 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_1094: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1003, [-1, 2, 128]);  getitem_1003 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_1095: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1001, [-1, 2, 128]);  getitem_1001 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_284: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        full_303: "i32[4, 17][17, 1]cuda:0" = torch.ops.aten.full.default([4, 17], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        full_304: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        view_1099: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_304, [514, 256]);  full_304 = None
        slice_scatter_48: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_283, view_1099, 1, 512, 768);  empty_283 = view_1099 = None
        split_with_sizes_157 = torch.ops.aten.split_with_sizes.default(slice_scatter_48, [256, 256, 256, 256], 1)
        getitem_1014: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_157[2];  split_with_sizes_157 = None
        view_1100: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1014, [-1, 2, 128]);  getitem_1014 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_1101: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_992, [514, 2, 128]);  getitem_992 = None
        split_with_sizes_158 = torch.ops.aten.split_with_sizes.default(slice_scatter_48, [256, 256, 256, 256], 1)
        getitem_1019: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_158[3];  split_with_sizes_158 = None
        view_1102: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1019, [-1, 2, 128]);  getitem_1019 = None
        split_with_sizes_159 = torch.ops.aten.split_with_sizes.default(slice_scatter_48, [256, 256, 256, 256], 1)
        getitem_1021: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_159[1];  split_with_sizes_159 = None
        view_1103: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1021, [-1, 2, 128]);  getitem_1021 = None
        triton_kernel_wrapper_functional_proxy_145 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 56, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 9, 1), (4, 9, 1), (4, 9, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_1093, 'K': view_1094, 'V': view_1095, 'sort_by_length_indices': getitem_58, 'seq_offsets': getitem_978, 'attn_scale': reciprocal_1, 'DOut': view_1101, 'DQ': view_1100, 'DK': view_1102, 'DV': view_1103, 'LOCK': full_303, 'M': empty_284}, tensors_to_clone = ['DQ', 'DK', 'DV']);  view_1093 = view_1094 = view_1095 = getitem_58 = reciprocal_1 = view_1101 = view_1100 = view_1102 = view_1103 = full_303 = empty_284 = None
        getitem_1024: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_145['DQ']
        getitem_1025: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_145['DK']
        getitem_1026: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_145['DV'];  triton_kernel_wrapper_functional_proxy_145 = None
        view_1104: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1024, [514, 256]);  getitem_1024 = None
        slice_scatter_49: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_48, view_1104, 1, 512, 768);  slice_scatter_48 = view_1104 = None
        view_1106: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1025, [514, 256]);  getitem_1025 = None
        slice_scatter_50: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_49, view_1106, 1, 768, 1024);  slice_scatter_49 = view_1106 = None
        view_1108: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1026, [514, 256]);  getitem_1026 = None
        slice_scatter_51: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_50, view_1108, 1, 256, 512);  slice_scatter_50 = view_1108 = None
        split_with_sizes_166 = torch.ops.aten.split_with_sizes.default(slice_scatter_51, [256, 256, 256, 256], 1)
        getitem_1053: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_166[2];  split_with_sizes_166 = None
        view_1110: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1053, [-1, 2, 128]);  getitem_1053 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_18: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1110, view_1110);  view_1110 = None
        view_1111: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_18, [514, 256]);  copy_18 = None
        slice_scatter_52: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_51, view_1111, 1, 512, 768);  slice_scatter_51 = view_1111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_169 = torch.ops.aten.split_with_sizes.default(slice_scatter_52, [256, 256, 256, 256], 1)
        getitem_1066: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_169[3];  split_with_sizes_169 = None
        view_1113: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1066, [-1, 2, 128]);  getitem_1066 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_19: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1113, view_1113);  view_1113 = None
        view_1114: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_19, [514, 256]);  copy_19 = None
        slice_scatter_53: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_52, view_1114, 1, 768, 1024);  slice_scatter_52 = view_1114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_172 = torch.ops.aten.split_with_sizes.default(slice_scatter_53, [256, 256, 256, 256], 1)
        getitem_1076: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_172[1];  split_with_sizes_172 = None
        view_1116: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1076, [-1, 2, 128]);  getitem_1076 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_20: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1116, view_1116);  view_1116 = None
        view_1117: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_20, [514, 256]);  copy_20 = None
        slice_scatter_54: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_53, view_1117, 1, 256, 512);  slice_scatter_53 = view_1117 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1640: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_993, torch.float32);  getitem_993 = None
        convert_element_type_1641: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1000, torch.float32);  getitem_1000 = None
        neg_114: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1641)
        exp_114: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_114);  neg_114 = None
        add_704: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_114, 1);  exp_114 = None
        reciprocal_58: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_704);  add_704 = None
        mul_783: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_58, 1);  reciprocal_58 = None
        mul_784: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1640, mul_783);  convert_element_type_1640 = None
        sub_294: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_783);  mul_783 = None
        mul_785: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1641, sub_294);  convert_element_type_1641 = sub_294 = None
        add_705: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_785, 1);  mul_785 = None
        mul_786: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_784, add_705);  mul_784 = add_705 = None
        convert_element_type_1642: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_786, torch.bfloat16);  mul_786 = None
        slice_scatter_55: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_54, convert_element_type_1642, 1, 0, 256);  slice_scatter_54 = convert_element_type_1642 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_106: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_55, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_684: "bf16[256, 514][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_999, [1, 0]);  getitem_999 = None
        mm_310: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_684, slice_scatter_55);  permute_684 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_685: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_329, [1, 0]);  convert_element_type_329 = None
        mm_311: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_55, permute_685);  slice_scatter_55 = permute_685 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_285: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_286: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_287: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_288: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_289: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_146 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 57, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_285, 'DY': mm_311, 'DW': empty_286, 'DB': empty_287, 'X': view_191, 'W': convert_element_type_327, 'B': convert_element_type_328, 'Mean': getitem_51, 'Rstd': getitem_52}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_285 = mm_311 = empty_286 = empty_287 = view_191 = convert_element_type_327 = convert_element_type_328 = getitem_51 = getitem_52 = None
        getitem_1095: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_146['DX']
        getitem_1096: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_146['DW']
        getitem_1097: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_146['DB'];  triton_kernel_wrapper_functional_proxy_146 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_147 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 58, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_1096, 'DB': getitem_1097, 'FINAL_DW': empty_288, 'FINAL_DB': empty_289}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_1096 = getitem_1097 = empty_288 = empty_289 = None
        getitem_1098: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_147['FINAL_DW']
        getitem_1099: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_147['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_147 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_706: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_694, getitem_1095);  add_694 = getitem_1095 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1647: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_106, torch.float32);  sum_106 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1648: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_310, torch.float32);  mm_310 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1649: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1099, torch.float32);  getitem_1099 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1650: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1098, torch.float32);  getitem_1098 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:284 in backward, code: dy = torch.mm(dout, output_weight.t())
        permute_686: "bf16[256, 768][1, 256]cuda:0" = torch.ops.aten.permute.default(convert_element_type_320, [1, 0]);  convert_element_type_320 = None
        mm_312: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.mm.default(add_701, permute_686);  permute_686 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:903 in triton_layer_norm_mul_dropout_bwd, code: y = torch.empty((N, 3 * D), dtype=x.dtype, device=x.device)
        empty_290: "bf16[514, 768][768, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 768], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:914 in triton_layer_norm_mul_dropout_bwd, code: dx = torch.empty_like(x)
        empty_291: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:915 in triton_layer_norm_mul_dropout_bwd, code: du = torch.empty_like(u)
        empty_292: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:918 in triton_layer_norm_mul_dropout_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_293: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:919 in triton_layer_norm_mul_dropout_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_294: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:920 in triton_layer_norm_mul_dropout_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_295: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:921 in triton_layer_norm_mul_dropout_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_296: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:977 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dx_du[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_148 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 14, constant_args_idx = 51, grid = [(128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_291, 'DU': empty_292, 'DY': mm_312, 'DW': empty_293, 'DB': empty_294, 'X': view_190, 'U': convert_element_type_317, 'Y': empty_290, 'W': convert_element_type_318, 'B': convert_element_type_319, 'Mean': getitem_48, 'Rstd': getitem_49, 'seed': primals_117}, tensors_to_clone = ['DX', 'DU', 'DW', 'DB', 'Y']);  empty_291 = empty_292 = mm_312 = empty_293 = empty_294 = view_190 = convert_element_type_317 = empty_290 = convert_element_type_318 = convert_element_type_319 = getitem_48 = getitem_49 = primals_117 = None
        getitem_1100: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_148['DX']
        getitem_1101: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_148['DU']
        getitem_1102: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_148['DW']
        getitem_1103: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_148['DB']
        getitem_1104: "bf16[514, 768][768, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_148['Y'];  triton_kernel_wrapper_functional_proxy_148 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_hstu_linear.py:1016 in triton_layer_norm_mul_dropout_bwd, code: _ln_mul_dropout_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_149 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 15, constant_args_idx = 52, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_1102, 'DB': getitem_1103, 'FINAL_DW': empty_295, 'FINAL_DB': empty_296}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_1102 = getitem_1103 = empty_295 = empty_296 = None
        getitem_1105: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_149['FINAL_DW']
        getitem_1106: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_149['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_149 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/fb/ultra/ops/hstu_linear.py:333 in backward, code: d_output_weight = torch.mm(y.t(), dout)
        permute_688: "bf16[768, 514][1, 768]cuda:0" = torch.ops.aten.permute.default(getitem_1104, [1, 0]);  getitem_1104 = None
        mm_313: "bf16[768, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(permute_688, add_701);  permute_688 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:363 in _forward_impl, code: output_weight=self._output_weight.to(x.dtype),
        convert_element_type_1655: "f32[768, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_313, torch.float32);  mm_313 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:360 in _forward_impl, code: norm_bias=self._output_norm_bias.to(x.dtype),
        convert_element_type_1656: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1106, torch.float32);  getitem_1106 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/fused_hstu.py:359 in _forward_impl, code: norm_weight=self._output_norm_weight.to(x.dtype),
        convert_element_type_1657: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1105, torch.float32);  getitem_1105 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:491 in triton_weighted_layer_norm_fwd, code: y = torch.empty_like(x)
        empty_297: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:512 in triton_weighted_layer_norm_fwd, code: _weighted_layer_norm_fwd[grid](
        triton_kernel_wrapper_functional_proxy_150 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 6, constant_args_idx = 46, grid = [(129, 1, 1), (129, 1, 1), (129, 1, 1), (65, 1, 1), (65, 1, 1), (65, 1, 1), (33, 1, 1), (33, 1, 1), (33, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'X': view_182, 'Y': empty_297, 'W': convert_element_type_309, 'B': convert_element_type_310, 'Mean': getitem_38, 'Rstd': getitem_39}, tensors_to_clone = ['Y']);  empty_297 = None
        getitem_1107: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_150['Y'];  triton_kernel_wrapper_functional_proxy_150 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1218 in maybe_triton_addmm_fwd, code: return torch.addmm(y, x, w)
        addmm_25: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.addmm.default(convert_element_type_312, getitem_1107, convert_element_type_311);  convert_element_type_312 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:425 in backward, code: u, v, q, k = uvqk.split(
        split_with_sizes_177 = torch.ops.aten.split_with_sizes.default(addmm_25, [256, 256, 256, 256], 1);  addmm_25 = None
        getitem_1108: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_177[0]
        getitem_1109: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_177[1]
        getitem_1110: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_177[2]
        getitem_1111: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_177[3];  split_with_sizes_177 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:465 in backward, code: duvqk = torch.empty(
        empty_298: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 1024], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:482 in backward, code: q = q.view(-1, ctx.num_heads, ctx.attn_dim)
        view_1120: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1110, [-1, 2, 128]);  getitem_1110 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:483 in backward, code: k = k.view(-1, ctx.num_heads, ctx.attn_dim)
        view_1121: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1111, [-1, 2, 128]);  getitem_1111 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:484 in backward, code: v = v.view(-1, ctx.num_heads, ctx.hidden_dim)
        view_1122: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1109, [-1, 2, 128]);  getitem_1109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4248 in triton_ragged_attention_bwd, code: M = torch.empty(1, dtype=torch.float32, device=q.device)
        empty_299: "f32[1][1]cuda:0" = torch.ops.aten.empty.memory_format([1], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4257 in triton_ragged_attention_bwd, code: lock = torch.zeros(
        full_327: "i32[4, 17][17, 1]cuda:0" = torch.ops.aten.full.default([4, 17], 0, dtype = torch.int32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4286 in triton_ragged_attention_bwd, code: dq.zero_()
        full_328: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.full.default([514, 2, 128], 0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        view_1126: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(full_328, [514, 256]);  full_328 = None
        slice_scatter_56: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(empty_298, view_1126, 1, 512, 768);  empty_298 = view_1126 = None
        split_with_sizes_180 = torch.ops.aten.split_with_sizes.default(slice_scatter_56, [256, 256, 256, 256], 1)
        getitem_1122: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_180[2];  split_with_sizes_180 = None
        view_1127: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1122, [-1, 2, 128]);  getitem_1122 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        view_1128: "bf16[514, 2, 128][256, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1100, [514, 2, 128]);  getitem_1100 = None
        split_with_sizes_181 = torch.ops.aten.split_with_sizes.default(slice_scatter_56, [256, 256, 256, 256], 1)
        getitem_1127: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_181[3];  split_with_sizes_181 = None
        view_1129: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1127, [-1, 2, 128]);  getitem_1127 = None
        split_with_sizes_182 = torch.ops.aten.split_with_sizes.default(slice_scatter_56, [256, 256, 256, 256], 1)
        getitem_1129: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_182[1];  split_with_sizes_182 = None
        view_1130: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1129, [-1, 2, 128]);  getitem_1129 = None
        triton_kernel_wrapper_functional_proxy_151 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 8, constant_args_idx = 47, grid = [(4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 1, 1), (4, 9, 1), (4, 9, 1), (4, 9, 1)], tma_descriptor_metadata = {}, kwargs = {'Q': view_1120, 'K': view_1121, 'V': view_1122, 'sort_by_length_indices': getitem_45, 'seq_offsets': getitem_986, 'attn_scale': reciprocal, 'DOut': view_1128, 'DQ': view_1127, 'DK': view_1129, 'DV': view_1130, 'LOCK': full_327, 'M': empty_299}, tensors_to_clone = ['DQ', 'DK', 'DV']);  view_1120 = view_1121 = view_1122 = getitem_45 = reciprocal = view_1128 = view_1127 = view_1129 = view_1130 = full_327 = empty_299 = None
        getitem_1132: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_151['DQ']
        getitem_1133: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_151['DK']
        getitem_1134: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_151['DV'];  triton_kernel_wrapper_functional_proxy_151 = None
        view_1131: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1132, [514, 256]);  getitem_1132 = None
        slice_scatter_57: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_56, view_1131, 1, 512, 768);  slice_scatter_56 = view_1131 = None
        view_1133: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1133, [514, 256]);  getitem_1133 = None
        slice_scatter_58: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_57, view_1133, 1, 768, 1024);  slice_scatter_57 = view_1133 = None
        view_1135: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(getitem_1134, [514, 256]);  getitem_1134 = None
        slice_scatter_59: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_58, view_1135, 1, 256, 512);  slice_scatter_58 = view_1135 = None
        split_with_sizes_189 = torch.ops.aten.split_with_sizes.default(slice_scatter_59, [256, 256, 256, 256], 1)
        getitem_1161: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_189[2];  split_with_sizes_189 = None
        view_1137: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1161, [-1, 2, 128]);  getitem_1161 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:563 in backward, code: dq.copy_(_dq)
        copy_21: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1137, view_1137);  view_1137 = None
        view_1138: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_21, [514, 256]);  copy_21 = None
        slice_scatter_60: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_59, view_1138, 1, 512, 768);  slice_scatter_59 = view_1138 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_192 = torch.ops.aten.split_with_sizes.default(slice_scatter_60, [256, 256, 256, 256], 1)
        getitem_1174: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_192[3];  split_with_sizes_192 = None
        view_1140: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1174, [-1, 2, 128]);  getitem_1174 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:564 in backward, code: dk.copy_(_dk)
        copy_22: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1140, view_1140);  view_1140 = None
        view_1141: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_22, [514, 256]);  copy_22 = None
        slice_scatter_61: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_60, view_1141, 1, 768, 1024);  slice_scatter_60 = view_1141 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/triton/triton_ragged_hstu_attention.py:4287 in triton_ragged_attention_bwd, code: _ragged_hstu_attn_bwd[grid](
        split_with_sizes_195 = torch.ops.aten.split_with_sizes.default(slice_scatter_61, [256, 256, 256, 256], 1)
        getitem_1184: "bf16[514, 256][1024, 1]cuda:0" = split_with_sizes_195[1];  split_with_sizes_195 = None
        view_1143: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.view.default(getitem_1184, [-1, 2, 128]);  getitem_1184 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:565 in backward, code: dv.copy_(_dv)
        copy_23: "bf16[514, 2, 128][1024, 128, 1]cuda:0" = torch.ops.aten.copy.default(view_1143, view_1143);  view_1143 = None
        view_1144: "bf16[514, 256][1024, 1]cuda:0" = torch.ops.aten.view.default(copy_23, [514, 256]);  copy_23 = None
        slice_scatter_62: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_61, view_1144, 1, 256, 512);  slice_scatter_61 = view_1144 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:567 in backward, code: torch.ops.aten.silu_backward(_du, u, grad_input=du)
        convert_element_type_1661: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1101, torch.float32);  getitem_1101 = None
        convert_element_type_1662: "f32[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1108, torch.float32);  getitem_1108 = None
        neg_115: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1662)
        exp_115: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.exp.default(neg_115);  neg_115 = None
        add_707: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_115, 1);  exp_115 = None
        reciprocal_59: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_707);  add_707 = None
        mul_787: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_59, 1);  reciprocal_59 = None
        mul_788: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1661, mul_787);  convert_element_type_1661 = None
        sub_295: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_787);  mul_787 = None
        mul_789: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1662, sub_295);  convert_element_type_1662 = sub_295 = None
        add_708: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_789, 1);  mul_789 = None
        mul_790: "f32[514, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_788, add_708);  mul_788 = add_708 = None
        convert_element_type_1663: "bf16[514, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_790, torch.bfloat16);  mul_790 = None
        slice_scatter_63: "bf16[514, 1024][1024, 1]cuda:0" = torch.ops.aten.slice_scatter.default(slice_scatter_62, convert_element_type_1663, 1, 0, 256);  slice_scatter_62 = convert_element_type_1663 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1200 in triton_addmm_bwd, code: dy = torch.sum(dz, dim=0)
        sum_107: "bf16[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(slice_scatter_63, [0])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1203 in triton_addmm_bwd, code: dw = torch.mm(x.t(), dz)
        permute_690: "bf16[256, 514][1, 256]cuda:0" = torch.ops.aten.permute.default(getitem_1107, [1, 0]);  getitem_1107 = None
        mm_314: "bf16[256, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_690, slice_scatter_63);  permute_690 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_addmm.py:1204 in triton_addmm_bwd, code: dx = torch.mm(dz, w.t())
        permute_691: "bf16[1024, 256][1, 1024]cuda:0" = torch.ops.aten.permute.default(convert_element_type_311, [1, 0]);  convert_element_type_311 = None
        mm_315: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.mm.default(slice_scatter_63, permute_691);  slice_scatter_63 = permute_691 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:563 in triton_weighted_layer_norm_bwd, code: dx = torch.empty_like(x)
        empty_300: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:566 in triton_weighted_layer_norm_bwd, code: _dweight = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_301: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:567 in triton_weighted_layer_norm_bwd, code: _dbias = torch.empty((tile_num, D), dtype=torch.float32, device=x.device)
        empty_302: "f32[128, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([128, 256], dtype = torch.float32, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:568 in triton_weighted_layer_norm_bwd, code: dweight = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_303: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:569 in triton_weighted_layer_norm_bwd, code: dbias = torch.empty((D,), dtype=weight.dtype, device=x.device)
        empty_304: "bf16[256][1]cuda:0" = torch.ops.aten.empty.memory_format([256], dtype = torch.bfloat16, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:575 in triton_weighted_layer_norm_bwd, code: _weighted_layer_norm_bwd_dx[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_152 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 9, constant_args_idx = 48, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_300, 'DY': mm_315, 'DW': empty_301, 'DB': empty_302, 'X': view_182, 'W': convert_element_type_309, 'B': convert_element_type_310, 'Mean': getitem_38, 'Rstd': getitem_39}, tensors_to_clone = ['DX', 'DW', 'DB']);  empty_300 = mm_315 = empty_301 = empty_302 = view_182 = convert_element_type_309 = convert_element_type_310 = getitem_38 = getitem_39 = None
        getitem_1203: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_152['DX']
        getitem_1204: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_152['DW']
        getitem_1205: "f32[128, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_152['DB'];  triton_kernel_wrapper_functional_proxy_152 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:601 in triton_weighted_layer_norm_bwd, code: _layer_norm_bwd_dwdb[grid](
        triton_kernel_wrapper_functional_proxy_153 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 10, constant_args_idx = 49, grid = [(64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1), (64, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DW': getitem_1204, 'DB': getitem_1205, 'FINAL_DW': empty_303, 'FINAL_DB': empty_304}, tensors_to_clone = ['FINAL_DW', 'FINAL_DB']);  getitem_1204 = getitem_1205 = empty_303 = empty_304 = None
        getitem_1206: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_153['FINAL_DW']
        getitem_1207: "bf16[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_153['FINAL_DB'];  triton_kernel_wrapper_functional_proxy_153 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/triton/triton_hstu_preprocess_and_attention.py:1250 in triton_hstu_preprocess_and_attention, code: return _HSTUPreprocessAndAttentionFunction.apply(
        add_709: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_701, getitem_1203);  add_701 = getitem_1203 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:999 in hstu_preprocess_and_attention, code: uvqk_bias=uvqk_bias.to(x.dtype),
        convert_element_type_1668: "f32[1024][1]cuda:0" = torch.ops.prims.convert_element_type.default(sum_107, torch.float32);  sum_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:998 in hstu_preprocess_and_attention, code: uvqk_weight=uvqk_weight.to(x.dtype),
        convert_element_type_1669: "f32[256, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_314, torch.float32);  mm_314 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:996 in hstu_preprocess_and_attention, code: norm_bias=norm_bias.to(x.dtype),
        convert_element_type_1670: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1207, torch.float32);  getitem_1207 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/ops/hstu_preprocess_and_attention.py:995 in hstu_preprocess_and_attention, code: norm_weight=norm_weight.to(x.dtype),
        convert_element_type_1671: "f32[256][1]cuda:0" = torch.ops.prims.convert_element_type.default(getitem_1206, torch.float32);  getitem_1206 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        clone_79: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.clone.default(add_703, memory_format = torch.contiguous_format);  add_703 = None
        view_1146: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_79, [640, 256]);  clone_79 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_305: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_331: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_154 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 43, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_305, 'DY': view_1146, 'DW': full_331, 'X': view_180, 'W': primals_108, 'Rstd': getitem_36}, tensors_to_clone = ['DX', 'DW']);  empty_305 = view_1146 = full_331 = view_180 = primals_108 = getitem_36 = None
        getitem_1208: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_154['DX']
        getitem_1209: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_154['DW'];  triton_kernel_wrapper_functional_proxy_154 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1148: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1208, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_1151: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1208, [10, 64, 256])
        slice_102: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1151, 1, 0, 32);  view_1151 = None
        view_1152: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_102, [10, 8192]);  slice_102 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_1153: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1208, [10, 64, 256])
        slice_103: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1153, 1, 0, 32);  view_1153 = None
        view_1154: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_103, [10, 8192]);  slice_103 = None
        permute_693: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_1154, [1, 0]);  view_1154 = None
        mm_316: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_693, convert_element_type_302);  permute_693 = convert_element_type_302 = None
        permute_694: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_316, [1, 0]);  mm_316 = None
        permute_695: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_74, [1, 0]);  permute_74 = None
        mm_317: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1152, permute_695);  view_1152 = permute_695 = None
        permute_696: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_694, [1, 0]);  permute_694 = None
        convert_element_type_1676: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_696, torch.float32);  permute_696 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1677: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_317, torch.float32);  mm_317 = None
        convert_element_type_1678: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_178, torch.float32);  view_178 = None
        neg_116: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1678)
        exp_116: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_116);  neg_116 = None
        add_710: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_116, 1);  exp_116 = None
        reciprocal_60: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_710);  add_710 = None
        mul_791: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_60, 1);  reciprocal_60 = None
        mul_792: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1677, mul_791);  convert_element_type_1677 = None
        sub_296: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_791);  mul_791 = None
        mul_793: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1678, sub_296);  convert_element_type_1678 = sub_296 = None
        add_711: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_793, 1);  mul_793 = None
        mul_794: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_792, add_711);  mul_792 = add_711 = None
        convert_element_type_1679: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_794, torch.bfloat16);  mul_794 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1155: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1679, [10, 2048]);  convert_element_type_1679 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1680: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1155, torch.float32);  view_1155 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_86: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_35);  alias_35 = None
        mul_795: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1680, primals_106);  primals_106 = None
        mul_796: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_299, alias_86);  convert_element_type_299 = None
        mul_797: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_796, mul_795)
        sum_108: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_797, [1], True);  mul_797 = None
        div_113: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_796, 2048)
        mul_798: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_113, sum_108);  div_113 = sum_108 = None
        sub_297: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_795, mul_798);  mul_795 = mul_798 = None
        mul_799: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_297, alias_86);  sub_297 = alias_86 = None
        mul_800: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1680, mul_796);  convert_element_type_1680 = mul_796 = None
        sum_109: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_800, [0]);  mul_800 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1681: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_799, torch.bfloat16);  mul_799 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1156: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1681, [10, 2048]);  convert_element_type_1681 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_697: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1156, [1, 0])
        mm_318: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_697, convert_element_type_295);  permute_697 = convert_element_type_295 = None
        permute_698: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_318, [1, 0]);  mm_318 = None
        permute_699: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_73, [1, 0]);  permute_73 = None
        mm_319: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1156, permute_699);  permute_699 = None
        permute_700: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_698, [1, 0]);  permute_698 = None
        convert_element_type_1686: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_700, torch.float32);  permute_700 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1687: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_319, torch.float32);  mm_319 = None
        convert_element_type_1688: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_176, torch.float32);  view_176 = None
        neg_117: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1688)
        exp_117: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_117);  neg_117 = None
        add_712: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_117, 1);  exp_117 = None
        reciprocal_61: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_712);  add_712 = None
        mul_801: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_61, 1);  reciprocal_61 = None
        mul_802: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1687, mul_801);  convert_element_type_1687 = None
        sub_298: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_801);  mul_801 = None
        mul_803: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1688, sub_298);  convert_element_type_1688 = sub_298 = None
        add_713: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_803, 1);  mul_803 = None
        mul_804: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_802, add_713);  mul_802 = add_713 = None
        convert_element_type_1689: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_804, torch.bfloat16);  mul_804 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1157: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1689, [10, 1024]);  convert_element_type_1689 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1690: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1157, torch.float32);  view_1157 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_87: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_34);  alias_34 = None
        mul_805: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1690, primals_104);  primals_104 = None
        mul_806: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_292, alias_87);  convert_element_type_292 = None
        mul_807: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_806, mul_805)
        sum_110: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_807, [1], True);  mul_807 = None
        div_114: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_806, 1024)
        mul_808: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_114, sum_110);  div_114 = sum_110 = None
        sub_299: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_805, mul_808);  mul_805 = mul_808 = None
        mul_809: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_299, alias_87);  sub_299 = alias_87 = None
        mul_810: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1690, mul_806);  convert_element_type_1690 = mul_806 = None
        sum_111: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_810, [0]);  mul_810 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1691: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_809, torch.bfloat16);  mul_809 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1158: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1691, [10, 1024]);  convert_element_type_1691 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_701: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1158, [1, 0])
        mm_320: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_701, convert_element_type_288);  permute_701 = convert_element_type_288 = None
        permute_702: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_320, [1, 0]);  mm_320 = None
        permute_703: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_72, [1, 0]);  permute_72 = None
        mm_321: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1158, permute_703);  view_1158 = permute_703 = None
        add_714: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1156, mm_321);  view_1156 = mm_321 = None
        permute_704: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_702, [1, 0]);  permute_702 = None
        convert_element_type_1696: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_704, torch.float32);  permute_704 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1697: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_714, torch.float32);  add_714 = None
        convert_element_type_1698: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_174, torch.float32);  view_174 = None
        neg_118: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1698)
        exp_118: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_118);  neg_118 = None
        add_715: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_118, 1);  exp_118 = None
        reciprocal_62: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_715);  add_715 = None
        mul_811: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_62, 1);  reciprocal_62 = None
        mul_812: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1697, mul_811);  convert_element_type_1697 = None
        sub_300: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_811);  mul_811 = None
        mul_813: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1698, sub_300);  convert_element_type_1698 = sub_300 = None
        add_716: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_813, 1);  mul_813 = None
        mul_814: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_812, add_716);  mul_812 = add_716 = None
        convert_element_type_1699: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_814, torch.bfloat16);  mul_814 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1159: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1699, [10, 2048]);  convert_element_type_1699 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1700: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1159, torch.float32);  view_1159 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_88: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_33);  alias_33 = None
        mul_815: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1700, primals_102);  primals_102 = None
        mul_816: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_285, alias_88);  convert_element_type_285 = None
        mul_817: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_816, mul_815)
        sum_112: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_817, [1], True);  mul_817 = None
        div_115: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_816, 2048)
        mul_818: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_115, sum_112);  div_115 = sum_112 = None
        sub_301: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_815, mul_818);  mul_815 = mul_818 = None
        mul_819: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_301, alias_88);  sub_301 = alias_88 = None
        mul_820: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1700, mul_816);  convert_element_type_1700 = mul_816 = None
        sum_113: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_820, [0]);  mul_820 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1701: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_819, torch.bfloat16);  mul_819 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1160: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1701, [10, 2048]);  convert_element_type_1701 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_705: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1160, [1, 0])
        mm_322: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_705, convert_element_type_281);  permute_705 = convert_element_type_281 = None
        permute_706: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_322, [1, 0]);  mm_322 = None
        permute_707: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_71, [1, 0]);  permute_71 = None
        mm_323: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1160, permute_707);  permute_707 = None
        permute_708: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_706, [1, 0]);  permute_706 = None
        convert_element_type_1706: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_708, torch.float32);  permute_708 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1707: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_323, torch.float32);  mm_323 = None
        convert_element_type_1708: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_172, torch.float32);  view_172 = None
        neg_119: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1708)
        exp_119: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_119);  neg_119 = None
        add_717: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_119, 1);  exp_119 = None
        reciprocal_63: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_717);  add_717 = None
        mul_821: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_63, 1);  reciprocal_63 = None
        mul_822: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1707, mul_821);  convert_element_type_1707 = None
        sub_302: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_821);  mul_821 = None
        mul_823: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1708, sub_302);  convert_element_type_1708 = sub_302 = None
        add_718: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_823, 1);  mul_823 = None
        mul_824: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_822, add_718);  mul_822 = add_718 = None
        convert_element_type_1709: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_824, torch.bfloat16);  mul_824 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1161: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1709, [10, 1024]);  convert_element_type_1709 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1710: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1161, torch.float32);  view_1161 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_89: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_32);  alias_32 = None
        mul_825: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1710, primals_100);  primals_100 = None
        mul_826: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_278, alias_89);  convert_element_type_278 = None
        mul_827: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_826, mul_825)
        sum_114: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_827, [1], True);  mul_827 = None
        div_116: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_826, 1024)
        mul_828: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_116, sum_114);  div_116 = sum_114 = None
        sub_303: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_825, mul_828);  mul_825 = mul_828 = None
        mul_829: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_303, alias_89);  sub_303 = alias_89 = None
        mul_830: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1710, mul_826);  convert_element_type_1710 = mul_826 = None
        sum_115: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_830, [0]);  mul_830 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1711: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_829, torch.bfloat16);  mul_829 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1162: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1711, [10, 1024]);  convert_element_type_1711 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_709: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1162, [1, 0])
        mm_324: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_709, convert_element_type_274);  permute_709 = convert_element_type_274 = None
        permute_710: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_324, [1, 0]);  mm_324 = None
        permute_711: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_70, [1, 0]);  permute_70 = None
        mm_325: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1162, permute_711);  view_1162 = permute_711 = None
        add_719: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1160, mm_325);  view_1160 = mm_325 = None
        permute_712: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_710, [1, 0]);  permute_710 = None
        convert_element_type_1716: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_712, torch.float32);  permute_712 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1717: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_719, torch.float32);  add_719 = None
        convert_element_type_1718: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_170, torch.float32);  view_170 = None
        neg_120: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1718)
        exp_120: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_120);  neg_120 = None
        add_720: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_120, 1);  exp_120 = None
        reciprocal_64: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_720);  add_720 = None
        mul_831: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_64, 1);  reciprocal_64 = None
        mul_832: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1717, mul_831);  convert_element_type_1717 = None
        sub_304: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_831);  mul_831 = None
        mul_833: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1718, sub_304);  convert_element_type_1718 = sub_304 = None
        add_721: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_833, 1);  mul_833 = None
        mul_834: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_832, add_721);  mul_832 = add_721 = None
        convert_element_type_1719: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_834, torch.bfloat16);  mul_834 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1163: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1719, [10, 2048]);  convert_element_type_1719 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1720: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1163, torch.float32);  view_1163 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_90: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_31);  alias_31 = None
        mul_835: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1720, primals_98);  primals_98 = None
        mul_836: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_271, alias_90);  convert_element_type_271 = None
        mul_837: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_836, mul_835)
        sum_116: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_837, [1], True);  mul_837 = None
        div_117: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_836, 2048)
        mul_838: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_117, sum_116);  div_117 = sum_116 = None
        sub_305: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_835, mul_838);  mul_835 = mul_838 = None
        mul_839: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_305, alias_90);  sub_305 = alias_90 = None
        mul_840: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1720, mul_836);  convert_element_type_1720 = mul_836 = None
        sum_117: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_840, [0]);  mul_840 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1721: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_839, torch.bfloat16);  mul_839 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1164: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1721, [10, 2048]);  convert_element_type_1721 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_713: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1164, [1, 0])
        mm_326: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_713, convert_element_type_267);  permute_713 = convert_element_type_267 = None
        permute_714: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_326, [1, 0]);  mm_326 = None
        permute_715: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_69, [1, 0]);  permute_69 = None
        mm_327: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_1164, permute_715);  view_1164 = permute_715 = None
        permute_716: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_714, [1, 0]);  permute_714 = None
        convert_element_type_1726: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_716, torch.float32);  permute_716 = None
        convert_element_type_1727: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_327, torch.float32);  mm_327 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1165: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1727, [10, 8472]);  convert_element_type_1727 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_306: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_334: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_155 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 41, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_306, 'DY': view_1165, 'DW': full_334, 'X': view_166, 'W': primals_96, 'Rstd': getitem_34}, tensors_to_clone = ['DX', 'DW']);  empty_306 = view_1165 = full_334 = view_166 = primals_96 = getitem_34 = None
        getitem_1210: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_155['DX']
        getitem_1211: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_155['DW'];  triton_kernel_wrapper_functional_proxy_155 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_1168: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1210, [10, 8472])
        slice_106: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1168, 1, 0, 6144);  view_1168 = None
        convert_element_type_1728: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_106, torch.bfloat16);  slice_106 = None
        view_1169: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1210, [10, 8472]);  getitem_1210 = None
        slice_107: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1169, 1, 6144, 8472);  view_1169 = None
        add_722: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_681, slice_107);  add_681 = slice_107 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_108: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1728, 1, 0, 2048)
        slice_109: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1728, 1, 2048, 6144);  convert_element_type_1728 = None
        view_1170: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_109, [10, 16, 256]);  slice_109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_1171: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_108, [10, 32, 64]);  slice_108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_1172: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_1171, [10, 32, 64]);  view_1171 = None
        permute_717: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_157, [0, 2, 1]);  view_157 = None
        bmm_47: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_717, view_1172);  permute_717 = None
        view_1173: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256])
        permute_719: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1173, [0, 2, 1]);  view_1173 = None
        expand_46: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_719, [10, 256, 64]);  permute_719 = None
        view_1174: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_46, [10, 256, 64]);  expand_46 = None
        permute_720: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.permute.default(view_1174, [0, 2, 1]);  view_1174 = None
        bmm_48: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_1172, permute_720);  view_1172 = permute_720 = None
        view_1175: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_47, [10, 256, 64]);  bmm_47 = None
        view_1176: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_48, [10, 32, 256]);  bmm_48 = None
        permute_721: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1175, [0, 2, 1]);  view_1175 = None
        add_723: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1148, permute_721);  view_1148 = permute_721 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_1177: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_1176, [10, 32, 256]);  view_1176 = None
        permute_722: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_151, [0, 2, 1]);  view_151 = None
        bmm_49: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_722, view_1177);  permute_722 = None
        view_1178: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_28, [10, 64, 256]);  getitem_28 = None
        expand_47: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_1178, [10, 64, 256]);  view_1178 = None
        view_1179: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_47, [10, 64, 256]);  expand_47 = None
        permute_724: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1179, [0, 2, 1]);  view_1179 = None
        bmm_50: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_1177, permute_724);  view_1177 = permute_724 = None
        view_1180: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_49, [10, 64, 256]);  bmm_49 = None
        add_724: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_723, view_1180);  add_723 = view_1180 = None
        view_1181: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_50, [10, 32, 64]);  bmm_50 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_1182: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_1181, [10, 2048]);  view_1181 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_725: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1182, [1, 0])
        mm_328: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_725, convert_element_type_259);  permute_725 = convert_element_type_259 = None
        permute_726: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_328, [1, 0]);  mm_328 = None
        permute_727: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_64, [1, 0]);  permute_64 = None
        mm_329: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_1182, permute_727);  view_1182 = permute_727 = None
        permute_728: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_726, [1, 0]);  permute_726 = None
        convert_element_type_1741: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_728, torch.float32);  permute_728 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1742: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_329, torch.float32);  mm_329 = None
        convert_element_type_1743: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_149, torch.float32);  view_149 = None
        neg_121: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1743)
        exp_121: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_121);  neg_121 = None
        add_725: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_121, 1);  exp_121 = None
        reciprocal_65: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_725);  add_725 = None
        mul_841: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_65, 1);  reciprocal_65 = None
        mul_842: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1742, mul_841);  convert_element_type_1742 = None
        sub_306: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_841);  mul_841 = None
        mul_843: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1743, sub_306);  convert_element_type_1743 = sub_306 = None
        add_726: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_843, 1);  mul_843 = None
        mul_844: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_842, add_726);  mul_842 = add_726 = None
        convert_element_type_1744: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_844, torch.bfloat16);  mul_844 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1183: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1744, [10, 512]);  convert_element_type_1744 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1745: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1183, torch.float32);  view_1183 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_91: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_30);  alias_30 = None
        mul_845: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1745, primals_94);  primals_94 = None
        mul_846: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_256, alias_91);  convert_element_type_256 = None
        mul_847: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_846, mul_845)
        sum_118: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_847, [1], True);  mul_847 = None
        div_118: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_846, 512)
        mul_848: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_118, sum_118);  div_118 = sum_118 = None
        sub_307: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_845, mul_848);  mul_845 = mul_848 = None
        mul_849: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_307, alias_91);  sub_307 = alias_91 = None
        mul_850: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1745, mul_846);  convert_element_type_1745 = mul_846 = None
        sum_119: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_850, [0]);  mul_850 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1746: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_849, torch.bfloat16);  mul_849 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1184: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1746, [10, 512]);  convert_element_type_1746 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_729: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_1184, [1, 0])
        mm_330: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_729, view_147);  permute_729 = view_147 = None
        permute_730: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_330, [1, 0]);  mm_330 = None
        permute_731: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_63, [1, 0]);  permute_63 = None
        mm_331: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1184, permute_731);  view_1184 = permute_731 = None
        permute_732: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_730, [1, 0]);  permute_730 = None
        convert_element_type_1751: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_732, torch.float32);  permute_732 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_1185: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_331, [10, 32, 256]);  mm_331 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_1186: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1208, [10, 64, 256]);  getitem_1208 = None
        slice_110: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1186, 1, 32, 64);  view_1186 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_46: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_110, view_1185, view_1170], 1);  slice_110 = view_1185 = view_1170 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_733: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_46, [0, 2, 1]);  cat_46 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_80: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_733, memory_format = torch.contiguous_format);  permute_733 = None
        view_1187: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_80, [2560, 80]);  clone_80 = None
        permute_734: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_1187, [1, 0])
        mm_332: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_734, view_145);  permute_734 = view_145 = None
        permute_735: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_332, [1, 0]);  mm_332 = None
        permute_736: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_60, [1, 0]);  permute_60 = None
        mm_333: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1187, permute_736);  view_1187 = permute_736 = None
        view_1188: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_333, [10, 256, 64]);  mm_333 = None
        permute_737: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_735, [1, 0]);  permute_735 = None
        convert_element_type_1756: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_737, torch.float32);  permute_737 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_738: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1188, [0, 2, 1]);  view_1188 = None
        add_727: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_724, permute_738);  add_724 = permute_738 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1189: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_727, [640, 256]);  add_727 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_307: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_337: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_156 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 39, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_307, 'DY': view_1189, 'DW': full_337, 'X': view_141, 'W': primals_91, 'Rstd': getitem_29}, tensors_to_clone = ['DX', 'DW']);  empty_307 = view_1189 = full_337 = view_141 = primals_91 = getitem_29 = None
        getitem_1212: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_156['DX']
        getitem_1213: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_156['DW'];  triton_kernel_wrapper_functional_proxy_156 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1191: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1212, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_1194: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1212, [10, 64, 256])
        slice_114: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1194, 1, 0, 32);  view_1194 = None
        view_1195: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_114, [10, 8192]);  slice_114 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_1196: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1212, [10, 64, 256])
        slice_115: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1196, 1, 0, 32);  view_1196 = None
        view_1197: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_115, [10, 8192]);  slice_115 = None
        permute_740: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_1197, [1, 0]);  view_1197 = None
        mm_334: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_740, convert_element_type_246);  permute_740 = convert_element_type_246 = None
        permute_741: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_334, [1, 0]);  mm_334 = None
        permute_742: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_58, [1, 0]);  permute_58 = None
        mm_335: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1195, permute_742);  view_1195 = permute_742 = None
        permute_743: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_741, [1, 0]);  permute_741 = None
        convert_element_type_1761: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_743, torch.float32);  permute_743 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1762: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_335, torch.float32);  mm_335 = None
        convert_element_type_1763: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_139, torch.float32);  view_139 = None
        neg_122: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1763)
        exp_122: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_122);  neg_122 = None
        add_728: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_122, 1);  exp_122 = None
        reciprocal_66: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_728);  add_728 = None
        mul_851: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_66, 1);  reciprocal_66 = None
        mul_852: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1762, mul_851);  convert_element_type_1762 = None
        sub_308: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_851);  mul_851 = None
        mul_853: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1763, sub_308);  convert_element_type_1763 = sub_308 = None
        add_729: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_853, 1);  mul_853 = None
        mul_854: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_852, add_729);  mul_852 = add_729 = None
        convert_element_type_1764: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_854, torch.bfloat16);  mul_854 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1198: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1764, [10, 2048]);  convert_element_type_1764 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1765: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1198, torch.float32);  view_1198 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_92: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_29);  alias_29 = None
        mul_855: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1765, primals_89);  primals_89 = None
        mul_856: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_243, alias_92);  convert_element_type_243 = None
        mul_857: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_856, mul_855)
        sum_120: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_857, [1], True);  mul_857 = None
        div_119: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_856, 2048)
        mul_858: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_119, sum_120);  div_119 = sum_120 = None
        sub_309: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_855, mul_858);  mul_855 = mul_858 = None
        mul_859: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_309, alias_92);  sub_309 = alias_92 = None
        mul_860: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1765, mul_856);  convert_element_type_1765 = mul_856 = None
        sum_121: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_860, [0]);  mul_860 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1766: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_859, torch.bfloat16);  mul_859 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1199: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1766, [10, 2048]);  convert_element_type_1766 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_744: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1199, [1, 0])
        mm_336: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_744, convert_element_type_239);  permute_744 = convert_element_type_239 = None
        permute_745: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_336, [1, 0]);  mm_336 = None
        permute_746: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_57, [1, 0]);  permute_57 = None
        mm_337: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1199, permute_746);  permute_746 = None
        permute_747: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_745, [1, 0]);  permute_745 = None
        convert_element_type_1771: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_747, torch.float32);  permute_747 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1772: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_337, torch.float32);  mm_337 = None
        convert_element_type_1773: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_137, torch.float32);  view_137 = None
        neg_123: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1773)
        exp_123: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_123);  neg_123 = None
        add_730: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_123, 1);  exp_123 = None
        reciprocal_67: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_730);  add_730 = None
        mul_861: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_67, 1);  reciprocal_67 = None
        mul_862: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1772, mul_861);  convert_element_type_1772 = None
        sub_310: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_861);  mul_861 = None
        mul_863: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1773, sub_310);  convert_element_type_1773 = sub_310 = None
        add_731: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_863, 1);  mul_863 = None
        mul_864: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_862, add_731);  mul_862 = add_731 = None
        convert_element_type_1774: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_864, torch.bfloat16);  mul_864 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1200: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1774, [10, 1024]);  convert_element_type_1774 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1775: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1200, torch.float32);  view_1200 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_93: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_28);  alias_28 = None
        mul_865: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1775, primals_87);  primals_87 = None
        mul_866: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_236, alias_93);  convert_element_type_236 = None
        mul_867: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_866, mul_865)
        sum_122: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_867, [1], True);  mul_867 = None
        div_120: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_866, 1024)
        mul_868: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_120, sum_122);  div_120 = sum_122 = None
        sub_311: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_865, mul_868);  mul_865 = mul_868 = None
        mul_869: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_311, alias_93);  sub_311 = alias_93 = None
        mul_870: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1775, mul_866);  convert_element_type_1775 = mul_866 = None
        sum_123: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_870, [0]);  mul_870 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1776: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_869, torch.bfloat16);  mul_869 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1201: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1776, [10, 1024]);  convert_element_type_1776 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_748: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1201, [1, 0])
        mm_338: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_748, convert_element_type_232);  permute_748 = convert_element_type_232 = None
        permute_749: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_338, [1, 0]);  mm_338 = None
        permute_750: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_56, [1, 0]);  permute_56 = None
        mm_339: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1201, permute_750);  view_1201 = permute_750 = None
        add_732: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1199, mm_339);  view_1199 = mm_339 = None
        permute_751: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_749, [1, 0]);  permute_749 = None
        convert_element_type_1781: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_751, torch.float32);  permute_751 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1782: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_732, torch.float32);  add_732 = None
        convert_element_type_1783: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_135, torch.float32);  view_135 = None
        neg_124: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1783)
        exp_124: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_124);  neg_124 = None
        add_733: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_124, 1);  exp_124 = None
        reciprocal_68: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_733);  add_733 = None
        mul_871: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_68, 1);  reciprocal_68 = None
        mul_872: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1782, mul_871);  convert_element_type_1782 = None
        sub_312: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_871);  mul_871 = None
        mul_873: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1783, sub_312);  convert_element_type_1783 = sub_312 = None
        add_734: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_873, 1);  mul_873 = None
        mul_874: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_872, add_734);  mul_872 = add_734 = None
        convert_element_type_1784: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_874, torch.bfloat16);  mul_874 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1202: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1784, [10, 2048]);  convert_element_type_1784 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1785: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1202, torch.float32);  view_1202 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_94: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_27);  alias_27 = None
        mul_875: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1785, primals_85);  primals_85 = None
        mul_876: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_229, alias_94);  convert_element_type_229 = None
        mul_877: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_876, mul_875)
        sum_124: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_877, [1], True);  mul_877 = None
        div_121: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_876, 2048)
        mul_878: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_121, sum_124);  div_121 = sum_124 = None
        sub_313: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_875, mul_878);  mul_875 = mul_878 = None
        mul_879: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_313, alias_94);  sub_313 = alias_94 = None
        mul_880: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1785, mul_876);  convert_element_type_1785 = mul_876 = None
        sum_125: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_880, [0]);  mul_880 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1786: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_879, torch.bfloat16);  mul_879 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1203: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1786, [10, 2048]);  convert_element_type_1786 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_752: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1203, [1, 0])
        mm_340: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_752, convert_element_type_225);  permute_752 = convert_element_type_225 = None
        permute_753: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_340, [1, 0]);  mm_340 = None
        permute_754: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_55, [1, 0]);  permute_55 = None
        mm_341: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1203, permute_754);  permute_754 = None
        permute_755: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_753, [1, 0]);  permute_753 = None
        convert_element_type_1791: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_755, torch.float32);  permute_755 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1792: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_341, torch.float32);  mm_341 = None
        convert_element_type_1793: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_133, torch.float32);  view_133 = None
        neg_125: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1793)
        exp_125: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_125);  neg_125 = None
        add_735: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_125, 1);  exp_125 = None
        reciprocal_69: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_735);  add_735 = None
        mul_881: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_69, 1);  reciprocal_69 = None
        mul_882: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1792, mul_881);  convert_element_type_1792 = None
        sub_314: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_881);  mul_881 = None
        mul_883: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1793, sub_314);  convert_element_type_1793 = sub_314 = None
        add_736: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_883, 1);  mul_883 = None
        mul_884: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_882, add_736);  mul_882 = add_736 = None
        convert_element_type_1794: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_884, torch.bfloat16);  mul_884 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1204: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1794, [10, 1024]);  convert_element_type_1794 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1795: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1204, torch.float32);  view_1204 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_95: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_26);  alias_26 = None
        mul_885: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1795, primals_83);  primals_83 = None
        mul_886: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_222, alias_95);  convert_element_type_222 = None
        mul_887: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_886, mul_885)
        sum_126: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_887, [1], True);  mul_887 = None
        div_122: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_886, 1024)
        mul_888: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_122, sum_126);  div_122 = sum_126 = None
        sub_315: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_885, mul_888);  mul_885 = mul_888 = None
        mul_889: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_315, alias_95);  sub_315 = alias_95 = None
        mul_890: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1795, mul_886);  convert_element_type_1795 = mul_886 = None
        sum_127: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_890, [0]);  mul_890 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1796: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_889, torch.bfloat16);  mul_889 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1205: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1796, [10, 1024]);  convert_element_type_1796 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_756: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1205, [1, 0])
        mm_342: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_756, convert_element_type_218);  permute_756 = convert_element_type_218 = None
        permute_757: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_342, [1, 0]);  mm_342 = None
        permute_758: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_54, [1, 0]);  permute_54 = None
        mm_343: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1205, permute_758);  view_1205 = permute_758 = None
        add_737: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1203, mm_343);  view_1203 = mm_343 = None
        permute_759: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_757, [1, 0]);  permute_757 = None
        convert_element_type_1801: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_759, torch.float32);  permute_759 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1802: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_737, torch.float32);  add_737 = None
        convert_element_type_1803: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_131, torch.float32);  view_131 = None
        neg_126: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1803)
        exp_126: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_126);  neg_126 = None
        add_738: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_126, 1);  exp_126 = None
        reciprocal_70: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_738);  add_738 = None
        mul_891: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_70, 1);  reciprocal_70 = None
        mul_892: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1802, mul_891);  convert_element_type_1802 = None
        sub_316: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_891);  mul_891 = None
        mul_893: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1803, sub_316);  convert_element_type_1803 = sub_316 = None
        add_739: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_893, 1);  mul_893 = None
        mul_894: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_892, add_739);  mul_892 = add_739 = None
        convert_element_type_1804: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_894, torch.bfloat16);  mul_894 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1206: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1804, [10, 2048]);  convert_element_type_1804 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1805: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1206, torch.float32);  view_1206 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_96: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_25);  alias_25 = None
        mul_895: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1805, primals_81);  primals_81 = None
        mul_896: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_215, alias_96);  convert_element_type_215 = None
        mul_897: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_896, mul_895)
        sum_128: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_897, [1], True);  mul_897 = None
        div_123: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_896, 2048)
        mul_898: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_123, sum_128);  div_123 = sum_128 = None
        sub_317: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_895, mul_898);  mul_895 = mul_898 = None
        mul_899: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_317, alias_96);  sub_317 = alias_96 = None
        mul_900: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1805, mul_896);  convert_element_type_1805 = mul_896 = None
        sum_129: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_900, [0]);  mul_900 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1806: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_899, torch.bfloat16);  mul_899 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1207: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1806, [10, 2048]);  convert_element_type_1806 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_760: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1207, [1, 0])
        mm_344: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_760, convert_element_type_211);  permute_760 = convert_element_type_211 = None
        permute_761: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_344, [1, 0]);  mm_344 = None
        permute_762: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_53, [1, 0]);  permute_53 = None
        mm_345: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_1207, permute_762);  view_1207 = permute_762 = None
        permute_763: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_761, [1, 0]);  permute_761 = None
        convert_element_type_1811: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_763, torch.float32);  permute_763 = None
        convert_element_type_1812: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_345, torch.float32);  mm_345 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1208: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1812, [10, 8472]);  convert_element_type_1812 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_308: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_340: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_157 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 37, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_308, 'DY': view_1208, 'DW': full_340, 'X': view_127, 'W': primals_79, 'Rstd': getitem_27}, tensors_to_clone = ['DX', 'DW']);  empty_308 = view_1208 = full_340 = view_127 = primals_79 = getitem_27 = None
        getitem_1214: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_157['DX']
        getitem_1215: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_157['DW'];  triton_kernel_wrapper_functional_proxy_157 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_1211: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1214, [10, 8472])
        slice_118: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1211, 1, 0, 6144);  view_1211 = None
        convert_element_type_1813: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_118, torch.bfloat16);  slice_118 = None
        view_1212: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1214, [10, 8472]);  getitem_1214 = None
        slice_119: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1212, 1, 6144, 8472);  view_1212 = None
        add_740: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_722, slice_119);  add_722 = slice_119 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_120: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1813, 1, 0, 2048)
        slice_121: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1813, 1, 2048, 6144);  convert_element_type_1813 = None
        view_1213: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_121, [10, 16, 256]);  slice_121 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_1214: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_120, [10, 32, 64]);  slice_120 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_1215: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_1214, [10, 32, 64]);  view_1214 = None
        permute_764: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_118, [0, 2, 1]);  view_118 = None
        bmm_51: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_764, view_1215);  permute_764 = None
        view_1216: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256])
        permute_766: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1216, [0, 2, 1]);  view_1216 = None
        expand_48: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_766, [10, 256, 64]);  permute_766 = None
        view_1217: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_48, [10, 256, 64]);  expand_48 = None
        permute_767: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.permute.default(view_1217, [0, 2, 1]);  view_1217 = None
        bmm_52: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_1215, permute_767);  view_1215 = permute_767 = None
        view_1218: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_51, [10, 256, 64]);  bmm_51 = None
        view_1219: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_52, [10, 32, 256]);  bmm_52 = None
        permute_768: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1218, [0, 2, 1]);  view_1218 = None
        add_741: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1191, permute_768);  view_1191 = permute_768 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_1220: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_1219, [10, 32, 256]);  view_1219 = None
        permute_769: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_112, [0, 2, 1]);  view_112 = None
        bmm_53: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_769, view_1220);  permute_769 = None
        view_1221: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_21, [10, 64, 256]);  getitem_21 = None
        expand_49: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_1221, [10, 64, 256]);  view_1221 = None
        view_1222: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_49, [10, 64, 256]);  expand_49 = None
        permute_771: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1222, [0, 2, 1]);  view_1222 = None
        bmm_54: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_1220, permute_771);  view_1220 = permute_771 = None
        view_1223: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_53, [10, 64, 256]);  bmm_53 = None
        add_742: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_741, view_1223);  add_741 = view_1223 = None
        view_1224: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_54, [10, 32, 64]);  bmm_54 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_1225: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_1224, [10, 2048]);  view_1224 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_772: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1225, [1, 0])
        mm_346: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_772, convert_element_type_203);  permute_772 = convert_element_type_203 = None
        permute_773: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_346, [1, 0]);  mm_346 = None
        permute_774: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_48, [1, 0]);  permute_48 = None
        mm_347: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_1225, permute_774);  view_1225 = permute_774 = None
        permute_775: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_773, [1, 0]);  permute_773 = None
        convert_element_type_1826: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_775, torch.float32);  permute_775 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1827: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_347, torch.float32);  mm_347 = None
        convert_element_type_1828: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_110, torch.float32);  view_110 = None
        neg_127: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1828)
        exp_127: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_127);  neg_127 = None
        add_743: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_127, 1);  exp_127 = None
        reciprocal_71: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_743);  add_743 = None
        mul_901: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_71, 1);  reciprocal_71 = None
        mul_902: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1827, mul_901);  convert_element_type_1827 = None
        sub_318: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_901);  mul_901 = None
        mul_903: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1828, sub_318);  convert_element_type_1828 = sub_318 = None
        add_744: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_903, 1);  mul_903 = None
        mul_904: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_902, add_744);  mul_902 = add_744 = None
        convert_element_type_1829: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_904, torch.bfloat16);  mul_904 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1226: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1829, [10, 512]);  convert_element_type_1829 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1830: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1226, torch.float32);  view_1226 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_97: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_24);  alias_24 = None
        mul_905: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1830, primals_77);  primals_77 = None
        mul_906: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_200, alias_97);  convert_element_type_200 = None
        mul_907: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_906, mul_905)
        sum_130: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_907, [1], True);  mul_907 = None
        div_124: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_906, 512)
        mul_908: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_124, sum_130);  div_124 = sum_130 = None
        sub_319: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_905, mul_908);  mul_905 = mul_908 = None
        mul_909: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_319, alias_97);  sub_319 = alias_97 = None
        mul_910: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1830, mul_906);  convert_element_type_1830 = mul_906 = None
        sum_131: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_910, [0]);  mul_910 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1831: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_909, torch.bfloat16);  mul_909 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1227: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1831, [10, 512]);  convert_element_type_1831 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_776: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_1227, [1, 0])
        mm_348: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_776, view_108);  permute_776 = view_108 = None
        permute_777: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_348, [1, 0]);  mm_348 = None
        permute_778: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_47, [1, 0]);  permute_47 = None
        mm_349: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1227, permute_778);  view_1227 = permute_778 = None
        permute_779: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_777, [1, 0]);  permute_777 = None
        convert_element_type_1836: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_779, torch.float32);  permute_779 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_1228: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_349, [10, 32, 256]);  mm_349 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_1229: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1212, [10, 64, 256]);  getitem_1212 = None
        slice_122: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1229, 1, 32, 64);  view_1229 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_47: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_122, view_1228, view_1213], 1);  slice_122 = view_1228 = view_1213 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_780: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_47, [0, 2, 1]);  cat_47 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_81: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_780, memory_format = torch.contiguous_format);  permute_780 = None
        view_1230: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_81, [2560, 80]);  clone_81 = None
        permute_781: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_1230, [1, 0])
        mm_350: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_781, view_106);  permute_781 = view_106 = None
        permute_782: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_350, [1, 0]);  mm_350 = None
        permute_783: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_44, [1, 0]);  permute_44 = None
        mm_351: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1230, permute_783);  view_1230 = permute_783 = None
        view_1231: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_351, [10, 256, 64]);  mm_351 = None
        permute_784: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_782, [1, 0]);  permute_782 = None
        convert_element_type_1841: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_784, torch.float32);  permute_784 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_785: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1231, [0, 2, 1]);  view_1231 = None
        add_745: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_742, permute_785);  add_742 = permute_785 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1232: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_745, [640, 256]);  add_745 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_309: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_343: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_158 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 35, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_309, 'DY': view_1232, 'DW': full_343, 'X': view_102, 'W': primals_74, 'Rstd': getitem_22}, tensors_to_clone = ['DX', 'DW']);  empty_309 = view_1232 = full_343 = view_102 = primals_74 = getitem_22 = None
        getitem_1216: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_158['DX']
        getitem_1217: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_158['DW'];  triton_kernel_wrapper_functional_proxy_158 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1234: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1216, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_1237: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1216, [10, 64, 256])
        slice_126: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1237, 1, 0, 32);  view_1237 = None
        view_1238: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_126, [10, 8192]);  slice_126 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_1239: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1216, [10, 64, 256])
        slice_127: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1239, 1, 0, 32);  view_1239 = None
        view_1240: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_127, [10, 8192]);  slice_127 = None
        permute_787: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_1240, [1, 0]);  view_1240 = None
        mm_352: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_787, convert_element_type_190);  permute_787 = convert_element_type_190 = None
        permute_788: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_352, [1, 0]);  mm_352 = None
        permute_789: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_42, [1, 0]);  permute_42 = None
        mm_353: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1238, permute_789);  view_1238 = permute_789 = None
        permute_790: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_788, [1, 0]);  permute_788 = None
        convert_element_type_1846: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_790, torch.float32);  permute_790 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1847: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_353, torch.float32);  mm_353 = None
        convert_element_type_1848: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_100, torch.float32);  view_100 = None
        neg_128: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1848)
        exp_128: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_128);  neg_128 = None
        add_746: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_128, 1);  exp_128 = None
        reciprocal_72: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_746);  add_746 = None
        mul_911: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_72, 1);  reciprocal_72 = None
        mul_912: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1847, mul_911);  convert_element_type_1847 = None
        sub_320: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_911);  mul_911 = None
        mul_913: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1848, sub_320);  convert_element_type_1848 = sub_320 = None
        add_747: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_913, 1);  mul_913 = None
        mul_914: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_912, add_747);  mul_912 = add_747 = None
        convert_element_type_1849: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_914, torch.bfloat16);  mul_914 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1241: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1849, [10, 2048]);  convert_element_type_1849 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1850: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1241, torch.float32);  view_1241 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_98: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_23);  alias_23 = None
        mul_915: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1850, primals_72);  primals_72 = None
        mul_916: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_187, alias_98);  convert_element_type_187 = None
        mul_917: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_916, mul_915)
        sum_132: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_917, [1], True);  mul_917 = None
        div_125: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_916, 2048)
        mul_918: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_125, sum_132);  div_125 = sum_132 = None
        sub_321: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_915, mul_918);  mul_915 = mul_918 = None
        mul_919: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_321, alias_98);  sub_321 = alias_98 = None
        mul_920: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1850, mul_916);  convert_element_type_1850 = mul_916 = None
        sum_133: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_920, [0]);  mul_920 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1851: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_919, torch.bfloat16);  mul_919 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1242: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1851, [10, 2048]);  convert_element_type_1851 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_791: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1242, [1, 0])
        mm_354: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_791, convert_element_type_183);  permute_791 = convert_element_type_183 = None
        permute_792: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_354, [1, 0]);  mm_354 = None
        permute_793: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_41, [1, 0]);  permute_41 = None
        mm_355: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1242, permute_793);  permute_793 = None
        permute_794: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_792, [1, 0]);  permute_792 = None
        convert_element_type_1856: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_794, torch.float32);  permute_794 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1857: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_355, torch.float32);  mm_355 = None
        convert_element_type_1858: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_98, torch.float32);  view_98 = None
        neg_129: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1858)
        exp_129: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_129);  neg_129 = None
        add_748: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_129, 1);  exp_129 = None
        reciprocal_73: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_748);  add_748 = None
        mul_921: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_73, 1);  reciprocal_73 = None
        mul_922: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1857, mul_921);  convert_element_type_1857 = None
        sub_322: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_921);  mul_921 = None
        mul_923: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1858, sub_322);  convert_element_type_1858 = sub_322 = None
        add_749: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_923, 1);  mul_923 = None
        mul_924: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_922, add_749);  mul_922 = add_749 = None
        convert_element_type_1859: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_924, torch.bfloat16);  mul_924 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1243: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1859, [10, 1024]);  convert_element_type_1859 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1860: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1243, torch.float32);  view_1243 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_99: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_22);  alias_22 = None
        mul_925: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1860, primals_70);  primals_70 = None
        mul_926: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_180, alias_99);  convert_element_type_180 = None
        mul_927: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_926, mul_925)
        sum_134: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_927, [1], True);  mul_927 = None
        div_126: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_926, 1024)
        mul_928: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_126, sum_134);  div_126 = sum_134 = None
        sub_323: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_925, mul_928);  mul_925 = mul_928 = None
        mul_929: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_323, alias_99);  sub_323 = alias_99 = None
        mul_930: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1860, mul_926);  convert_element_type_1860 = mul_926 = None
        sum_135: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_930, [0]);  mul_930 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1861: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_929, torch.bfloat16);  mul_929 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1244: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1861, [10, 1024]);  convert_element_type_1861 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_795: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1244, [1, 0])
        mm_356: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_795, convert_element_type_176);  permute_795 = convert_element_type_176 = None
        permute_796: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_356, [1, 0]);  mm_356 = None
        permute_797: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_40, [1, 0]);  permute_40 = None
        mm_357: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1244, permute_797);  view_1244 = permute_797 = None
        add_750: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1242, mm_357);  view_1242 = mm_357 = None
        permute_798: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_796, [1, 0]);  permute_796 = None
        convert_element_type_1866: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_798, torch.float32);  permute_798 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1867: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_750, torch.float32);  add_750 = None
        convert_element_type_1868: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_96, torch.float32);  view_96 = None
        neg_130: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1868)
        exp_130: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_130);  neg_130 = None
        add_751: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_130, 1);  exp_130 = None
        reciprocal_74: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_751);  add_751 = None
        mul_931: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_74, 1);  reciprocal_74 = None
        mul_932: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1867, mul_931);  convert_element_type_1867 = None
        sub_324: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_931);  mul_931 = None
        mul_933: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1868, sub_324);  convert_element_type_1868 = sub_324 = None
        add_752: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_933, 1);  mul_933 = None
        mul_934: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_932, add_752);  mul_932 = add_752 = None
        convert_element_type_1869: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_934, torch.bfloat16);  mul_934 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1245: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1869, [10, 2048]);  convert_element_type_1869 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1870: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1245, torch.float32);  view_1245 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_100: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_21);  alias_21 = None
        mul_935: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1870, primals_68);  primals_68 = None
        mul_936: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_173, alias_100);  convert_element_type_173 = None
        mul_937: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_936, mul_935)
        sum_136: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_937, [1], True);  mul_937 = None
        div_127: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_936, 2048)
        mul_938: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_127, sum_136);  div_127 = sum_136 = None
        sub_325: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_935, mul_938);  mul_935 = mul_938 = None
        mul_939: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_325, alias_100);  sub_325 = alias_100 = None
        mul_940: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1870, mul_936);  convert_element_type_1870 = mul_936 = None
        sum_137: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_940, [0]);  mul_940 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1871: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_939, torch.bfloat16);  mul_939 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1246: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1871, [10, 2048]);  convert_element_type_1871 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_799: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1246, [1, 0])
        mm_358: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_799, convert_element_type_169);  permute_799 = convert_element_type_169 = None
        permute_800: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_358, [1, 0]);  mm_358 = None
        permute_801: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_39, [1, 0]);  permute_39 = None
        mm_359: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1246, permute_801);  permute_801 = None
        permute_802: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_800, [1, 0]);  permute_800 = None
        convert_element_type_1876: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_802, torch.float32);  permute_802 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1877: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_359, torch.float32);  mm_359 = None
        convert_element_type_1878: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_94, torch.float32);  view_94 = None
        neg_131: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1878)
        exp_131: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_131);  neg_131 = None
        add_753: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_131, 1);  exp_131 = None
        reciprocal_75: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_753);  add_753 = None
        mul_941: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_75, 1);  reciprocal_75 = None
        mul_942: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1877, mul_941);  convert_element_type_1877 = None
        sub_326: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_941);  mul_941 = None
        mul_943: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1878, sub_326);  convert_element_type_1878 = sub_326 = None
        add_754: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_943, 1);  mul_943 = None
        mul_944: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_942, add_754);  mul_942 = add_754 = None
        convert_element_type_1879: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_944, torch.bfloat16);  mul_944 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1247: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1879, [10, 1024]);  convert_element_type_1879 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1880: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1247, torch.float32);  view_1247 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_101: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_20);  alias_20 = None
        mul_945: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1880, primals_66);  primals_66 = None
        mul_946: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_166, alias_101);  convert_element_type_166 = None
        mul_947: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_946, mul_945)
        sum_138: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_947, [1], True);  mul_947 = None
        div_128: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_946, 1024)
        mul_948: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_128, sum_138);  div_128 = sum_138 = None
        sub_327: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_945, mul_948);  mul_945 = mul_948 = None
        mul_949: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_327, alias_101);  sub_327 = alias_101 = None
        mul_950: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1880, mul_946);  convert_element_type_1880 = mul_946 = None
        sum_139: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_950, [0]);  mul_950 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1881: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_949, torch.bfloat16);  mul_949 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1248: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1881, [10, 1024]);  convert_element_type_1881 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_803: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1248, [1, 0])
        mm_360: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_803, convert_element_type_162);  permute_803 = convert_element_type_162 = None
        permute_804: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_360, [1, 0]);  mm_360 = None
        permute_805: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_38, [1, 0]);  permute_38 = None
        mm_361: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1248, permute_805);  view_1248 = permute_805 = None
        add_755: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1246, mm_361);  view_1246 = mm_361 = None
        permute_806: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_804, [1, 0]);  permute_804 = None
        convert_element_type_1886: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_806, torch.float32);  permute_806 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1887: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_755, torch.float32);  add_755 = None
        convert_element_type_1888: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_92, torch.float32);  view_92 = None
        neg_132: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1888)
        exp_132: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_132);  neg_132 = None
        add_756: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_132, 1);  exp_132 = None
        reciprocal_76: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_756);  add_756 = None
        mul_951: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_76, 1);  reciprocal_76 = None
        mul_952: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1887, mul_951);  convert_element_type_1887 = None
        sub_328: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_951);  mul_951 = None
        mul_953: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1888, sub_328);  convert_element_type_1888 = sub_328 = None
        add_757: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_953, 1);  mul_953 = None
        mul_954: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_952, add_757);  mul_952 = add_757 = None
        convert_element_type_1889: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_954, torch.bfloat16);  mul_954 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1249: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1889, [10, 2048]);  convert_element_type_1889 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1890: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1249, torch.float32);  view_1249 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_102: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_19);  alias_19 = None
        mul_955: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1890, primals_64);  primals_64 = None
        mul_956: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_159, alias_102);  convert_element_type_159 = None
        mul_957: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_956, mul_955)
        sum_140: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_957, [1], True);  mul_957 = None
        div_129: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_956, 2048)
        mul_958: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_129, sum_140);  div_129 = sum_140 = None
        sub_329: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_955, mul_958);  mul_955 = mul_958 = None
        mul_959: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_329, alias_102);  sub_329 = alias_102 = None
        mul_960: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1890, mul_956);  convert_element_type_1890 = mul_956 = None
        sum_141: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_960, [0]);  mul_960 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1891: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_959, torch.bfloat16);  mul_959 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1250: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1891, [10, 2048]);  convert_element_type_1891 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_807: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1250, [1, 0])
        mm_362: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(permute_807, convert_element_type_155);  permute_807 = convert_element_type_155 = None
        permute_808: "bf16[8472, 2048][1, 8472]cuda:0" = torch.ops.aten.permute.default(mm_362, [1, 0]);  mm_362 = None
        permute_809: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_37, [1, 0]);  permute_37 = None
        mm_363: "bf16[10, 8472][8472, 1]cuda:0" = torch.ops.aten.mm.default(view_1250, permute_809);  view_1250 = permute_809 = None
        permute_810: "bf16[2048, 8472][8472, 1]cuda:0" = torch.ops.aten.permute.default(permute_808, [1, 0]);  permute_808 = None
        convert_element_type_1896: "f32[2048, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_810, torch.float32);  permute_810 = None
        convert_element_type_1897: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_363, torch.float32);  mm_363 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1251: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1897, [10, 8472]);  convert_element_type_1897 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_310: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 8472], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_346: "f32[8472][1]cuda:0" = torch.ops.aten.full.default([8472], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_159 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 33, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_310, 'DY': view_1251, 'DW': full_346, 'X': view_88, 'W': primals_62, 'Rstd': getitem_20}, tensors_to_clone = ['DX', 'DW']);  empty_310 = view_1251 = full_346 = view_88 = primals_62 = getitem_20 = None
        getitem_1218: "f32[10, 8472][8472, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_159['DX']
        getitem_1219: "f32[8472][1]cuda:0" = triton_kernel_wrapper_functional_proxy_159['DW'];  triton_kernel_wrapper_functional_proxy_159 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_1254: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1218, [10, 8472])
        slice_130: "f32[10, 6144][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1254, 1, 0, 6144);  view_1254 = None
        convert_element_type_1898: "bf16[10, 6144][6144, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_130, torch.bfloat16);  slice_130 = None
        view_1255: "f32[10, 8472][8472, 1]cuda:0" = torch.ops.aten.view.default(getitem_1218, [10, 8472]);  getitem_1218 = None
        slice_131: "f32[10, 2328][8472, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1255, 1, 6144, 8472);  view_1255 = None
        add_758: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_740, slice_131);  add_740 = slice_131 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_132: "bf16[10, 2048][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1898, 1, 0, 2048)
        slice_133: "bf16[10, 4096][6144, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1898, 1, 2048, 6144);  convert_element_type_1898 = None
        view_1256: "bf16[10, 16, 256][6144, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_133, [10, 16, 256]);  slice_133 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_1257: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(slice_132, [10, 32, 64]);  slice_132 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_1258: "bf16[10, 32, 64][6144, 64, 1]cuda:0" = torch.ops.aten.view.default(view_1257, [10, 32, 64]);  view_1257 = None
        permute_811: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_79, [0, 2, 1]);  view_79 = None
        bmm_55: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.bmm.default(permute_811, view_1258);  permute_811 = None
        view_1259: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256])
        permute_813: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1259, [0, 2, 1]);  view_1259 = None
        expand_50: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.expand.default(permute_813, [10, 256, 64]);  permute_813 = None
        view_1260: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.view.default(expand_50, [10, 256, 64]);  expand_50 = None
        permute_814: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.permute.default(view_1260, [0, 2, 1]);  view_1260 = None
        bmm_56: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_1258, permute_814);  view_1258 = permute_814 = None
        view_1261: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_55, [10, 256, 64]);  bmm_55 = None
        view_1262: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_56, [10, 32, 256]);  bmm_56 = None
        permute_815: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1261, [0, 2, 1]);  view_1261 = None
        add_759: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1234, permute_815);  view_1234 = permute_815 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_1263: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_1262, [10, 32, 256]);  view_1262 = None
        permute_816: "bf16[10, 64, 32][2048, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_73, [0, 2, 1]);  view_73 = None
        bmm_57: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_816, view_1263);  permute_816 = None
        view_1264: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_14, [10, 64, 256]);  getitem_14 = None
        expand_51: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.expand.default(view_1264, [10, 64, 256]);  view_1264 = None
        view_1265: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(expand_51, [10, 64, 256]);  expand_51 = None
        permute_818: "bf16[10, 256, 64][16384, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_1265, [0, 2, 1]);  view_1265 = None
        bmm_58: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.bmm.default(view_1263, permute_818);  view_1263 = permute_818 = None
        view_1266: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_57, [10, 64, 256]);  bmm_57 = None
        add_760: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_759, view_1266);  add_759 = view_1266 = None
        view_1267: "bf16[10, 32, 64][2048, 64, 1]cuda:0" = torch.ops.aten.view.default(bmm_58, [10, 32, 64]);  bmm_58 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_1268: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(view_1267, [10, 2048]);  view_1267 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_819: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1268, [1, 0])
        mm_364: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_819, convert_element_type_147);  permute_819 = convert_element_type_147 = None
        permute_820: "bf16[512, 2048][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_364, [1, 0]);  mm_364 = None
        permute_821: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_32, [1, 0]);  permute_32 = None
        mm_365: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_1268, permute_821);  view_1268 = permute_821 = None
        permute_822: "bf16[2048, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_820, [1, 0]);  permute_820 = None
        convert_element_type_1911: "f32[2048, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_822, torch.float32);  permute_822 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1912: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_365, torch.float32);  mm_365 = None
        convert_element_type_1913: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_71, torch.float32);  view_71 = None
        neg_133: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1913)
        exp_133: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_133);  neg_133 = None
        add_761: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_133, 1);  exp_133 = None
        reciprocal_77: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_761);  add_761 = None
        mul_961: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_77, 1);  reciprocal_77 = None
        mul_962: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1912, mul_961);  convert_element_type_1912 = None
        sub_330: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_961);  mul_961 = None
        mul_963: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1913, sub_330);  convert_element_type_1913 = sub_330 = None
        add_762: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_963, 1);  mul_963 = None
        mul_964: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_962, add_762);  mul_962 = add_762 = None
        convert_element_type_1914: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_964, torch.bfloat16);  mul_964 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1269: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1914, [10, 512]);  convert_element_type_1914 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1915: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1269, torch.float32);  view_1269 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_103: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_18);  alias_18 = None
        mul_965: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1915, primals_60);  primals_60 = None
        mul_966: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_144, alias_103);  convert_element_type_144 = None
        mul_967: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_966, mul_965)
        sum_142: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_967, [1], True);  mul_967 = None
        div_130: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_966, 512)
        mul_968: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_130, sum_142);  div_130 = sum_142 = None
        sub_331: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_965, mul_968);  mul_965 = mul_968 = None
        mul_969: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_331, alias_103);  sub_331 = alias_103 = None
        mul_970: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1915, mul_966);  convert_element_type_1915 = mul_966 = None
        sum_143: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_970, [0]);  mul_970 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1916: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_969, torch.bfloat16);  mul_969 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1270: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1916, [10, 512]);  convert_element_type_1916 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_823: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_1270, [1, 0])
        mm_366: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_823, view_69);  permute_823 = view_69 = None
        permute_824: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_366, [1, 0]);  mm_366 = None
        permute_825: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_31, [1, 0]);  permute_31 = None
        mm_367: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1270, permute_825);  view_1270 = permute_825 = None
        permute_826: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_824, [1, 0]);  permute_824 = None
        convert_element_type_1921: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_826, torch.float32);  permute_826 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_1271: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_367, [10, 32, 256]);  mm_367 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_1272: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1216, [10, 64, 256]);  getitem_1216 = None
        slice_134: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1272, 1, 32, 64);  view_1272 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_48: "bf16[10, 80, 256][20480, 256, 1]cuda:0" = torch.ops.aten.cat.default([slice_134, view_1271, view_1256], 1);  slice_134 = view_1271 = view_1256 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_827: "bf16[10, 256, 80][20480, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_48, [0, 2, 1]);  cat_48 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_82: "bf16[10, 256, 80][20480, 80, 1]cuda:0" = torch.ops.aten.clone.default(permute_827, memory_format = torch.contiguous_format);  permute_827 = None
        view_1273: "bf16[2560, 80][80, 1]cuda:0" = torch.ops.aten.view.default(clone_82, [2560, 80]);  clone_82 = None
        permute_828: "bf16[80, 2560][1, 80]cuda:0" = torch.ops.aten.permute.default(view_1273, [1, 0])
        mm_368: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_828, view_67);  permute_828 = view_67 = None
        permute_829: "bf16[64, 80][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_368, [1, 0]);  mm_368 = None
        permute_830: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_28, [1, 0]);  permute_28 = None
        mm_369: "bf16[2560, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(view_1273, permute_830);  view_1273 = permute_830 = None
        view_1274: "bf16[10, 256, 64][16384, 64, 1]cuda:0" = torch.ops.aten.view.default(mm_369, [10, 256, 64]);  mm_369 = None
        permute_831: "bf16[80, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_829, [1, 0]);  permute_829 = None
        convert_element_type_1926: "f32[80, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_831, torch.float32);  permute_831 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_832: "bf16[10, 64, 256][16384, 1, 64]cuda:0" = torch.ops.aten.permute.default(view_1274, [0, 2, 1]);  view_1274 = None
        add_763: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(add_760, permute_832);  add_760 = permute_832 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1275: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_763, [640, 256]);  add_763 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_311: "bf16[640, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([640, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_349: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_160 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 31, grid = [(160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1), (160, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_311, 'DY': view_1275, 'DW': full_349, 'X': view_63, 'W': primals_57, 'Rstd': getitem_15}, tensors_to_clone = ['DX', 'DW']);  empty_311 = view_1275 = full_349 = view_63 = primals_57 = getitem_15 = None
        getitem_1220: "bf16[640, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_160['DX']
        getitem_1221: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_160['DW'];  triton_kernel_wrapper_functional_proxy_160 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1277: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1220, [10, 64, 256])
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:404 in forward, code: x = x.reshape(x.shape[0], -1, self.config.embedding_dim)
        view_1280: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1220, [10, 64, 256])
        slice_138: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1280, 1, 0, 32);  view_1280 = None
        view_1281: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_138, [10, 8192]);  slice_138 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        view_1282: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1220, [10, 64, 256])
        slice_139: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1282, 1, 0, 32);  view_1282 = None
        view_1283: "bf16[10, 8192][16384, 1]cuda:0" = torch.ops.aten.view.default(slice_139, [10, 8192]);  slice_139 = None
        permute_834: "bf16[8192, 10][1, 16384]cuda:0" = torch.ops.aten.permute.default(view_1283, [1, 0]);  view_1283 = None
        mm_370: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_834, convert_element_type_134);  permute_834 = convert_element_type_134 = None
        permute_835: "bf16[2048, 8192][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_370, [1, 0]);  mm_370 = None
        permute_836: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_26, [1, 0]);  permute_26 = None
        mm_371: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1281, permute_836);  view_1281 = permute_836 = None
        permute_837: "bf16[8192, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_835, [1, 0]);  permute_835 = None
        convert_element_type_1931: "f32[8192, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_837, torch.float32);  permute_837 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1932: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_371, torch.float32);  mm_371 = None
        convert_element_type_1933: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_61, torch.float32);  view_61 = None
        neg_134: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1933)
        exp_134: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_134);  neg_134 = None
        add_764: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_134, 1);  exp_134 = None
        reciprocal_78: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_764);  add_764 = None
        mul_971: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_78, 1);  reciprocal_78 = None
        mul_972: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1932, mul_971);  convert_element_type_1932 = None
        sub_332: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_971);  mul_971 = None
        mul_973: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1933, sub_332);  convert_element_type_1933 = sub_332 = None
        add_765: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_973, 1);  mul_973 = None
        mul_974: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_972, add_765);  mul_972 = add_765 = None
        convert_element_type_1934: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_974, torch.bfloat16);  mul_974 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1284: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1934, [10, 2048]);  convert_element_type_1934 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1935: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1284, torch.float32);  view_1284 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_104: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_17);  alias_17 = None
        mul_975: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1935, primals_55);  primals_55 = None
        mul_976: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_131, alias_104);  convert_element_type_131 = None
        mul_977: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_976, mul_975)
        sum_144: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_977, [1], True);  mul_977 = None
        div_131: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_976, 2048)
        mul_978: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_131, sum_144);  div_131 = sum_144 = None
        sub_333: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_975, mul_978);  mul_975 = mul_978 = None
        mul_979: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_333, alias_104);  sub_333 = alias_104 = None
        mul_980: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1935, mul_976);  convert_element_type_1935 = mul_976 = None
        sum_145: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_980, [0]);  mul_980 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1936: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_979, torch.bfloat16);  mul_979 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1285: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1936, [10, 2048]);  convert_element_type_1936 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_838: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1285, [1, 0])
        mm_372: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_838, convert_element_type_127);  permute_838 = convert_element_type_127 = None
        permute_839: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_372, [1, 0]);  mm_372 = None
        permute_840: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_25, [1, 0]);  permute_25 = None
        mm_373: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1285, permute_840);  permute_840 = None
        permute_841: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_839, [1, 0]);  permute_839 = None
        convert_element_type_1941: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_841, torch.float32);  permute_841 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1942: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_373, torch.float32);  mm_373 = None
        convert_element_type_1943: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_59, torch.float32);  view_59 = None
        neg_135: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1943)
        exp_135: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_135);  neg_135 = None
        add_766: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_135, 1);  exp_135 = None
        reciprocal_79: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_766);  add_766 = None
        mul_981: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_79, 1);  reciprocal_79 = None
        mul_982: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1942, mul_981);  convert_element_type_1942 = None
        sub_334: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_981);  mul_981 = None
        mul_983: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1943, sub_334);  convert_element_type_1943 = sub_334 = None
        add_767: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_983, 1);  mul_983 = None
        mul_984: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_982, add_767);  mul_982 = add_767 = None
        convert_element_type_1944: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_984, torch.bfloat16);  mul_984 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1286: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1944, [10, 1024]);  convert_element_type_1944 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1945: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1286, torch.float32);  view_1286 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_105: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_16);  alias_16 = None
        mul_985: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1945, primals_53);  primals_53 = None
        mul_986: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_124, alias_105);  convert_element_type_124 = None
        mul_987: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_986, mul_985)
        sum_146: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_987, [1], True);  mul_987 = None
        div_132: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_986, 1024)
        mul_988: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_132, sum_146);  div_132 = sum_146 = None
        sub_335: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_985, mul_988);  mul_985 = mul_988 = None
        mul_989: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_335, alias_105);  sub_335 = alias_105 = None
        mul_990: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1945, mul_986);  convert_element_type_1945 = mul_986 = None
        sum_147: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_990, [0]);  mul_990 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1946: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_989, torch.bfloat16);  mul_989 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1287: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1946, [10, 1024]);  convert_element_type_1946 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_842: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1287, [1, 0])
        mm_374: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_842, convert_element_type_120);  permute_842 = convert_element_type_120 = None
        permute_843: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_374, [1, 0]);  mm_374 = None
        permute_844: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_24, [1, 0]);  permute_24 = None
        mm_375: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1287, permute_844);  view_1287 = permute_844 = None
        add_768: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1285, mm_375);  view_1285 = mm_375 = None
        permute_845: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_843, [1, 0]);  permute_843 = None
        convert_element_type_1951: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_845, torch.float32);  permute_845 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_1952: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_768, torch.float32);  add_768 = None
        convert_element_type_1953: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_57, torch.float32);  view_57 = None
        neg_136: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1953)
        exp_136: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_136);  neg_136 = None
        add_769: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_136, 1);  exp_136 = None
        reciprocal_80: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_769);  add_769 = None
        mul_991: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_80, 1);  reciprocal_80 = None
        mul_992: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1952, mul_991);  convert_element_type_1952 = None
        sub_336: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_991);  mul_991 = None
        mul_993: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1953, sub_336);  convert_element_type_1953 = sub_336 = None
        add_770: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_993, 1);  mul_993 = None
        mul_994: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_992, add_770);  mul_992 = add_770 = None
        convert_element_type_1954: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_994, torch.bfloat16);  mul_994 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1288: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1954, [10, 2048]);  convert_element_type_1954 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1955: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1288, torch.float32);  view_1288 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_106: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_15);  alias_15 = None
        mul_995: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1955, primals_51);  primals_51 = None
        mul_996: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_117, alias_106);  convert_element_type_117 = None
        mul_997: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_996, mul_995)
        sum_148: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_997, [1], True);  mul_997 = None
        div_133: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_996, 2048)
        mul_998: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_133, sum_148);  div_133 = sum_148 = None
        sub_337: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_995, mul_998);  mul_995 = mul_998 = None
        mul_999: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_337, alias_106);  sub_337 = alias_106 = None
        mul_1000: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1955, mul_996);  convert_element_type_1955 = mul_996 = None
        sum_149: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1000, [0]);  mul_1000 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1956: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_999, torch.bfloat16);  mul_999 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1289: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1956, [10, 2048]);  convert_element_type_1956 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_846: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1289, [1, 0])
        mm_376: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_846, convert_element_type_113);  permute_846 = convert_element_type_113 = None
        permute_847: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_376, [1, 0]);  mm_376 = None
        permute_848: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_23, [1, 0]);  permute_23 = None
        mm_377: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(view_1289, permute_848);  permute_848 = None
        permute_849: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_847, [1, 0]);  permute_847 = None
        convert_element_type_1961: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_849, torch.float32);  permute_849 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_1962: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_377, torch.float32);  mm_377 = None
        convert_element_type_1963: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_55, torch.float32);  view_55 = None
        neg_137: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1963)
        exp_137: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.exp.default(neg_137);  neg_137 = None
        add_771: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_137, 1);  exp_137 = None
        reciprocal_81: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_771);  add_771 = None
        mul_1001: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_81, 1);  reciprocal_81 = None
        mul_1002: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1962, mul_1001);  convert_element_type_1962 = None
        sub_338: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1001);  mul_1001 = None
        mul_1003: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1963, sub_338);  convert_element_type_1963 = sub_338 = None
        add_772: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1003, 1);  mul_1003 = None
        mul_1004: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1002, add_772);  mul_1002 = add_772 = None
        convert_element_type_1964: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1004, torch.bfloat16);  mul_1004 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1290: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1964, [10, 1024]);  convert_element_type_1964 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1965: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1290, torch.float32);  view_1290 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_107: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_14);  alias_14 = None
        mul_1005: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1965, primals_49);  primals_49 = None
        mul_1006: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_110, alias_107);  convert_element_type_110 = None
        mul_1007: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1006, mul_1005)
        sum_150: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1007, [1], True);  mul_1007 = None
        div_134: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1006, 1024)
        mul_1008: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_134, sum_150);  div_134 = sum_150 = None
        sub_339: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1005, mul_1008);  mul_1005 = mul_1008 = None
        mul_1009: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_339, alias_107);  sub_339 = alias_107 = None
        mul_1010: "f32[10, 1024][1024, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1965, mul_1006);  convert_element_type_1965 = mul_1006 = None
        sum_151: "f32[1024][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1010, [0]);  mul_1010 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1966: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1009, torch.bfloat16);  mul_1009 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1291: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1966, [10, 1024]);  convert_element_type_1966 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_850: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1291, [1, 0])
        mm_378: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_850, convert_element_type_106);  permute_850 = convert_element_type_106 = None
        permute_851: "bf16[2048, 1024][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_378, [1, 0]);  mm_378 = None
        permute_852: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_22, [1, 0]);  permute_22 = None
        mm_379: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1291, permute_852);  view_1291 = permute_852 = None
        add_773: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1289, mm_379);  view_1289 = mm_379 = None
        permute_853: "bf16[1024, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_851, [1, 0]);  permute_851 = None
        convert_element_type_1971: "f32[1024, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_853, torch.float32);  permute_853 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1972: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_773, torch.float32);  add_773 = None
        convert_element_type_1973: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_53, torch.float32);  view_53 = None
        neg_138: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_1973)
        exp_138: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_138);  neg_138 = None
        add_774: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_138, 1);  exp_138 = None
        reciprocal_82: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_774);  add_774 = None
        mul_1011: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_82, 1);  reciprocal_82 = None
        mul_1012: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1972, mul_1011);  convert_element_type_1972 = None
        sub_340: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1011);  mul_1011 = None
        mul_1013: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1973, sub_340);  convert_element_type_1973 = sub_340 = None
        add_775: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1013, 1);  mul_1013 = None
        mul_1014: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1012, add_775);  mul_1012 = add_775 = None
        convert_element_type_1974: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1014, torch.bfloat16);  mul_1014 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1292: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1974, [10, 2048]);  convert_element_type_1974 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_1975: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1292, torch.float32);  view_1292 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_108: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_13);  alias_13 = None
        mul_1015: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1975, primals_47);  primals_47 = None
        mul_1016: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_103, alias_108);  convert_element_type_103 = None
        mul_1017: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1016, mul_1015)
        sum_152: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1017, [1], True);  mul_1017 = None
        div_135: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1016, 2048)
        mul_1018: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_135, sum_152);  div_135 = sum_152 = None
        sub_341: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1015, mul_1018);  mul_1015 = mul_1018 = None
        mul_1019: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_341, alias_108);  sub_341 = alias_108 = None
        mul_1020: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1975, mul_1016);  convert_element_type_1975 = mul_1016 = None
        sum_153: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1020, [0]);  mul_1020 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_1976: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1019, torch.bfloat16);  mul_1019 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1293: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1976, [10, 2048]);  convert_element_type_1976 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_854: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1293, [1, 0])
        mm_380: "bf16[2048, 14456][14456, 1]cuda:0" = torch.ops.aten.mm.default(permute_854, convert_element_type_99);  permute_854 = convert_element_type_99 = None
        permute_855: "bf16[14456, 2048][1, 14456]cuda:0" = torch.ops.aten.permute.default(mm_380, [1, 0]);  mm_380 = None
        permute_856: "bf16[2048, 14456][14456, 1]cuda:0" = torch.ops.aten.permute.default(permute_21, [1, 0]);  permute_21 = None
        mm_381: "bf16[10, 14456][14456, 1]cuda:0" = torch.ops.aten.mm.default(view_1293, permute_856);  view_1293 = permute_856 = None
        permute_857: "bf16[2048, 14456][14456, 1]cuda:0" = torch.ops.aten.permute.default(permute_855, [1, 0]);  permute_855 = None
        convert_element_type_1981: "f32[2048, 14456][14456, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_857, torch.float32);  permute_857 = None
        convert_element_type_1982: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_381, torch.float32);  mm_381 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1294: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_1982, [10, 14456]);  convert_element_type_1982 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_312: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 14456], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_352: "f32[14456][1]cuda:0" = torch.ops.aten.full.default([14456], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_161 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 29, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_312, 'DY': view_1294, 'DW': full_352, 'X': view_49, 'W': primals_45, 'Rstd': getitem_13}, tensors_to_clone = ['DX', 'DW']);  empty_312 = view_1294 = full_352 = view_49 = primals_45 = getitem_13 = None
        getitem_1222: "f32[10, 14456][14456, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_161['DX']
        getitem_1223: "f32[14456][1]cuda:0" = triton_kernel_wrapper_functional_proxy_161['DW'];  triton_kernel_wrapper_functional_proxy_161 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:398 in forward, code: x = torch.cat([x, dense_proj], dim=1)
        view_1297: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.view.default(getitem_1222, [10, 14456])
        slice_142: "f32[10, 12128][14456, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1297, 1, 0, 12128);  view_1297 = None
        convert_element_type_1983: "bf16[10, 12128][12128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_142, torch.bfloat16);  slice_142 = None
        view_1298: "f32[10, 14456][14456, 1]cuda:0" = torch.ops.aten.view.default(getitem_1222, [10, 14456]);  getitem_1222 = None
        slice_143: "f32[10, 2328][14456, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1298, 1, 12128, 14456);  view_1298 = None
        add_776: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.add.Tensor(add_758, slice_143);  add_758 = slice_143 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:395 in forward, code: x = torch.cat([x, fm_x.reshape(x.shape[0], -1)], dim=1)
        slice_144: "bf16[10, 8032][12128, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1983, 1, 0, 8032)
        slice_145: "bf16[10, 4096][12128, 1]cuda:0" = torch.ops.aten.slice.Tensor(convert_element_type_1983, 1, 8032, 12128);  convert_element_type_1983 = None
        view_1299: "bf16[10, 16, 256][12128, 256, 1]cuda:0" = torch.ops.aten.view.default(slice_145, [10, 16, 256]);  slice_145 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:393 in forward, code: x = x.reshape(x.shape[0], -1)
        view_1300: "bf16[10, 32, 251][12128, 251, 1]cuda:0" = torch.ops.aten.view.default(slice_144, [10, 32, 251]);  slice_144 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:813 in forward, code: res = vx @ x.transpose(1, 2)  # res: [b, v, n']
        view_1301: "bf16[10, 32, 251][12128, 251, 1]cuda:0" = torch.ops.aten.view.default(view_1300, [10, 32, 251]);  view_1300 = None
        permute_858: "bf16[10, 256, 32][8192, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_43, [0, 2, 1]);  view_43 = None
        bmm_59: "bf16[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.aten.bmm.default(permute_858, view_1301);  permute_858 = None
        permute_859: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.permute.default(view_44, [0, 2, 1]);  view_44 = None
        bmm_60: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.bmm.default(view_1301, permute_859);  view_1301 = permute_859 = None
        view_1302: "bf16[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.aten.view.default(bmm_59, [10, 256, 251]);  bmm_59 = None
        view_1303: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_60, [10, 32, 256]);  bmm_60 = None
        convert_element_type_1988: "f32[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1302, torch.float32);  view_1302 = None
        permute_860: "f32[10, 251, 256][64256, 1, 251]cuda:0" = torch.ops.aten.permute.default(convert_element_type_1988, [0, 2, 1]);  convert_element_type_1988 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:811 in forward, code: vx = v @ x  # [b, v, dim]
        view_1304: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(view_1303, [10, 32, 256]);  view_1303 = None
        permute_861: "bf16[10, 251, 32][8032, 1, 251]cuda:0" = torch.ops.aten.permute.default(view_39, [0, 2, 1]);  view_39 = None
        bmm_61: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.bmm.default(permute_861, view_1304);  permute_861 = None
        permute_862: "bf16[10, 256, 251][64256, 1, 256]cuda:0" = torch.ops.aten.permute.default(view_40, [0, 2, 1]);  view_40 = None
        bmm_62: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.bmm.default(view_1304, permute_862);  view_1304 = permute_862 = None
        view_1305: "bf16[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(bmm_61, [10, 251, 256]);  bmm_61 = None
        view_1306: "bf16[10, 32, 251][8032, 251, 1]cuda:0" = torch.ops.aten.view.default(bmm_62, [10, 32, 251]);  bmm_62 = None
        convert_element_type_1993: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1305, torch.float32);  view_1305 = None
        add_777: "f32[10, 251, 256][64256, 1, 251]cuda:0" = torch.ops.aten.add.Tensor(permute_860, convert_element_type_1993);  permute_860 = convert_element_type_1993 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:809 in forward, code: v = v.reshape(v.shape[0], -1, x.shape[1])  # [b, v, n']
        view_1307: "bf16[10, 8032][8032, 1]cuda:0" = torch.ops.aten.view.default(view_1306, [10, 8032]);  view_1306 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_863: "bf16[8032, 10][1, 8032]cuda:0" = torch.ops.aten.permute.default(view_1307, [1, 0])
        mm_382: "bf16[8032, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(permute_863, convert_element_type_89);  permute_863 = convert_element_type_89 = None
        permute_864: "bf16[512, 8032][1, 512]cuda:0" = torch.ops.aten.permute.default(mm_382, [1, 0]);  mm_382 = None
        permute_865: "bf16[8032, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_18, [1, 0]);  permute_18 = None
        mm_383: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.mm.default(view_1307, permute_865);  view_1307 = permute_865 = None
        permute_866: "bf16[8032, 512][512, 1]cuda:0" = torch.ops.aten.permute.default(permute_864, [1, 0]);  permute_864 = None
        convert_element_type_1998: "f32[8032, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_866, torch.float32);  permute_866 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_1999: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_383, torch.float32);  mm_383 = None
        convert_element_type_2000: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_37, torch.float32);  view_37 = None
        neg_139: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_2000)
        exp_139: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.exp.default(neg_139);  neg_139 = None
        add_778: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_139, 1);  exp_139 = None
        reciprocal_83: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_778);  add_778 = None
        mul_1021: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_83, 1);  reciprocal_83 = None
        mul_1022: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_1999, mul_1021);  convert_element_type_1999 = None
        sub_342: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1021);  mul_1021 = None
        mul_1023: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2000, sub_342);  convert_element_type_2000 = sub_342 = None
        add_779: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1023, 1);  mul_1023 = None
        mul_1024: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1022, add_779);  mul_1022 = add_779 = None
        convert_element_type_2001: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1024, torch.bfloat16);  mul_1024 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1308: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2001, [10, 512]);  convert_element_type_2001 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2002: "f32[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1308, torch.float32);  view_1308 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_109: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_12);  alias_12 = None
        mul_1025: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2002, primals_43);  primals_43 = None
        mul_1026: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_86, alias_109);  convert_element_type_86 = None
        mul_1027: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1026, mul_1025)
        sum_154: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1027, [1], True);  mul_1027 = None
        div_136: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1026, 512)
        mul_1028: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_136, sum_154);  div_136 = sum_154 = None
        sub_343: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1025, mul_1028);  mul_1025 = mul_1028 = None
        mul_1029: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_343, alias_109);  sub_343 = alias_109 = None
        mul_1030: "f32[10, 512][512, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2002, mul_1026);  convert_element_type_2002 = mul_1026 = None
        sum_155: "f32[512][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1030, [0]);  mul_1030 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2003: "bf16[10, 512][512, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1029, torch.bfloat16);  mul_1029 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1309: "bf16[10, 512][512, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2003, [10, 512]);  convert_element_type_2003 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_867: "bf16[512, 10][1, 512]cuda:0" = torch.ops.aten.permute.default(view_1309, [1, 0])
        mm_384: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(permute_867, view_35);  permute_867 = view_35 = None
        permute_868: "bf16[8192, 512][1, 8192]cuda:0" = torch.ops.aten.permute.default(mm_384, [1, 0]);  mm_384 = None
        permute_869: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_17, [1, 0]);  permute_17 = None
        mm_385: "bf16[10, 8192][8192, 1]cuda:0" = torch.ops.aten.mm.default(view_1309, permute_869);  view_1309 = permute_869 = None
        permute_870: "bf16[512, 8192][8192, 1]cuda:0" = torch.ops.aten.permute.default(permute_868, [1, 0]);  permute_868 = None
        convert_element_type_2008: "f32[512, 8192][8192, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_870, torch.float32);  permute_870 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn_modules.py:808 in forward, code: v = self.v_proj(x_proj.reshape(x_proj.shape[0], -1))
        view_1310: "bf16[10, 32, 256][8192, 256, 1]cuda:0" = torch.ops.aten.view.default(mm_385, [10, 32, 256]);  mm_385 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/nano_rl/rlnn.py:405 in forward, code: x = torch.cat([x, fuse_x], dim=1)
        view_1311: "bf16[10, 64, 256][16384, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1220, [10, 64, 256]);  getitem_1220 = None
        slice_146: "bf16[10, 32, 256][16384, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1311, 1, 32, 64);  view_1311 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:292 in forward, code: lce_outputs = torch.split(y, self.lce_splits, dim=1)
        cat_49: "bf16[10, 144, 256][36864, 256, 1]cuda:0" = torch.ops.aten.cat.default([view_1277, slice_146, view_1310, view_1299], 1);  view_1277 = slice_146 = view_1310 = view_1299 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:230 in forward, code: x = x.transpose(1, 2)  # [b, n', d]
        permute_871: "bf16[10, 256, 144][36864, 1, 256]cuda:0" = torch.ops.aten.permute.default(cat_49, [0, 2, 1]);  cat_49 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:229 in forward, code: x = self.linear(x)  # [b, d, n']
        clone_83: "bf16[10, 256, 144][36864, 144, 1]cuda:0" = torch.ops.aten.clone.default(permute_871, memory_format = torch.contiguous_format);  permute_871 = None
        view_1312: "bf16[2560, 144][144, 1]cuda:0" = torch.ops.aten.view.default(clone_83, [2560, 144]);  clone_83 = None
        permute_872: "bf16[144, 2560][1, 144]cuda:0" = torch.ops.aten.permute.default(view_1312, [1, 0])
        mm_386: "bf16[144, 251][251, 1]cuda:0" = torch.ops.aten.mm.default(permute_872, view_33);  permute_872 = view_33 = None
        permute_873: "bf16[251, 144][1, 251]cuda:0" = torch.ops.aten.permute.default(mm_386, [1, 0]);  mm_386 = None
        permute_874: "bf16[144, 251][251, 1]cuda:0" = torch.ops.aten.permute.default(permute_15, [1, 0]);  permute_15 = None
        mm_387: "bf16[2560, 251][251, 1]cuda:0" = torch.ops.aten.mm.default(view_1312, permute_874);  view_1312 = permute_874 = None
        view_1313: "bf16[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.aten.view.default(mm_387, [10, 256, 251]);  mm_387 = None
        permute_875: "bf16[144, 251][251, 1]cuda:0" = torch.ops.aten.permute.default(permute_873, [1, 0]);  permute_873 = None
        convert_element_type_2013: "f32[144, 251][251, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_875, torch.float32);  permute_875 = None
        convert_element_type_2014: "f32[10, 256, 251][64256, 251, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1313, torch.float32);  view_1313 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:228 in forward, code: x = x.transpose(1, 2)  # [b, d, n]
        permute_876: "f32[10, 251, 256][64256, 1, 251]cuda:0" = torch.ops.aten.permute.default(convert_element_type_2014, [0, 2, 1]);  convert_element_type_2014 = None
        add_780: "f32[10, 251, 256][64256, 1, 251]cuda:0" = torch.ops.aten.add.Tensor(add_777, permute_876);  add_777 = permute_876 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1314: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.view.default(add_776, [10, 2328]);  add_776 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_313: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.empty.memory_format([10, 2328], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_355: "f32[2328][1]cuda:0" = torch.ops.aten.full.default([2328], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_162 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 27, grid = [(2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1), (2, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_313, 'DY': view_1314, 'DW': full_355, 'X': view_29, 'W': primals_40, 'Rstd': getitem_7}, tensors_to_clone = ['DX', 'DW']);  empty_313 = view_1314 = full_355 = view_29 = primals_40 = getitem_7 = None
        getitem_1224: "f32[10, 2328][2328, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_162['DX']
        getitem_1225: "f32[2328][1]cuda:0" = triton_kernel_wrapper_functional_proxy_162['DW'];  triton_kernel_wrapper_functional_proxy_162 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        clone_84: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.clone.default(add_780, memory_format = torch.contiguous_format);  add_780 = None
        view_1316: "f32[2510, 256][256, 1]cuda:0" = torch.ops.aten.view.default(clone_84, [2510, 256]);  clone_84 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_314: "f32[2510, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([2510, 256], dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_358: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_163 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 25, grid = [(627, 1, 1), (627, 1, 1), (627, 1, 1), (627, 1, 1), (627, 1, 1), (627, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_314, 'DY': view_1316, 'DW': full_358, 'X': view_27, 'W': primals_39, 'Rstd': getitem_5}, tensors_to_clone = ['DX', 'DW']);  empty_314 = view_1316 = full_358 = view_27 = primals_39 = getitem_5 = None
        getitem_1226: "f32[2510, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_163['DX']
        getitem_1227: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_163['DW'];  triton_kernel_wrapper_functional_proxy_163 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1318: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_706, [514, 256]);  add_706 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_315: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_361: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_164 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 23, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_315, 'DY': view_1318, 'DW': full_361, 'X': view_25, 'W': primals_38, 'Rstd': getitem_3}, tensors_to_clone = ['DX', 'DW']);  empty_315 = view_1318 = full_361 = view_25 = primals_38 = getitem_3 = None
        getitem_1228: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_164['DX']
        getitem_1229: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_164['DW'];  triton_kernel_wrapper_functional_proxy_164 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1320: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(add_709, [514, 256]);  add_709 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1035 in backward, code: dx = torch.empty_like(x)
        empty_316: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.empty.memory_format([514, 256], dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1053 in backward, code: dweight.zero_()
        full_364: "f32[256][1]cuda:0" = torch.ops.aten.full.default([256], 0, dtype = torch.float32, layout = torch.strided, device = device(type='cuda', index=0), pin_memory = False)
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/triton/triton_layer_norm.py:1054 in backward, code: _weighted_rms_norm_bwd[(tile_num,)](
        triton_kernel_wrapper_functional_proxy_165 = torch.ops.higher_order.triton_kernel_wrapper_functional(kernel_idx = 5, constant_args_idx = 21, grid = [(128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1), (128, 1, 1)], tma_descriptor_metadata = {}, kwargs = {'DX': empty_316, 'DY': view_1320, 'DW': full_364, 'X': view_23, 'W': primals_37, 'Rstd': getitem_1}, tensors_to_clone = ['DX', 'DW']);  empty_316 = view_1320 = full_364 = view_23 = primals_37 = getitem_1 = None
        getitem_1230: "bf16[514, 256][256, 1]cuda:0" = triton_kernel_wrapper_functional_proxy_165['DX']
        getitem_1231: "f32[256][1]cuda:0" = triton_kernel_wrapper_functional_proxy_165['DW'];  triton_kernel_wrapper_functional_proxy_165 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:460 in forward, code: x = torch.cat(
        view_1323: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1226, [10, 251, 256])
        slice_151: "f32[10, 13, 256][64256, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1323, 1, 230, 243);  view_1323 = None
        convert_element_type_2015: "bf16[10, 13, 256][3328, 256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_151, torch.bfloat16);  slice_151 = None
        view_1324: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1226, [10, 251, 256])
        slice_152: "f32[10, 4, 256][64256, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1324, 1, 243, 247);  view_1324 = None
        convert_element_type_2016: "bf16[10, 4, 256][1024, 256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_152, torch.bfloat16);  slice_152 = None
        view_1325: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1226, [10, 251, 256])
        slice_153: "f32[10, 4, 256][64256, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1325, 1, 247, 251);  view_1325 = None
        convert_element_type_2017: "bf16[10, 4, 256][1024, 256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_153, torch.bfloat16);  slice_153 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:458 in forward, code: dense_proj = torch.cat([dense_projection, dense_features], dim=1)
        view_1327: "f32[10, 2328][2328, 1]cuda:0" = torch.ops.aten.view.default(getitem_1224, [10, 2328]);  getitem_1224 = None
        slice_156: "f32[10, 1024][2328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1327, 1, 0, 1024);  view_1327 = None
        convert_element_type_2018: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(slice_156, torch.bfloat16);  slice_156 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/minimal_viable_ai/models/ig_ranking/lsr/mb7_test_1/model_archs.py:166 in forward, code: dense_proj = dense_embedding.reshape(
        view_1328: "bf16[10, 4, 256][1024, 256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2018, [10, 4, 256]);  convert_element_type_2018 = None
        add_781: "bf16[10, 4, 256][1024, 256, 1]cuda:0" = torch.ops.aten.add.Tensor(convert_element_type_2017, view_1328);  convert_element_type_2017 = view_1328 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:544 in forward, code: output = self._mlp(gated_output).reshape(
        view_1329: "bf16[10, 1024][1024, 1]cuda:0" = torch.ops.aten.view.default(add_781, [10, 1024]);  add_781 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:179 in forward, code: output = self._output_proj(output)
        permute_877: "bf16[1024, 10][1, 1024]cuda:0" = torch.ops.aten.permute.default(view_1329, [1, 0])
        mm_388: "bf16[1024, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_877, convert_element_type_75);  permute_877 = convert_element_type_75 = None
        permute_878: "bf16[4096, 1024][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_388, [1, 0]);  mm_388 = None
        permute_879: "bf16[1024, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_12, [1, 0]);  permute_12 = None
        mm_389: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1329, permute_879);  view_1329 = permute_879 = None
        permute_880: "bf16[1024, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_878, [1, 0]);  permute_878 = None
        convert_element_type_2023: "f32[1024, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_880, torch.float32);  permute_880 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_2024: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_389, torch.float32);  mm_389 = None
        convert_element_type_2025: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_20, torch.float32);  view_20 = None
        neg_140: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_2025)
        exp_140: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.exp.default(neg_140);  neg_140 = None
        add_782: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_140, 1);  exp_140 = None
        reciprocal_84: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_782);  add_782 = None
        mul_1031: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_84, 1);  reciprocal_84 = None
        mul_1032: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2024, mul_1031);  convert_element_type_2024 = None
        sub_344: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1031);  mul_1031 = None
        mul_1033: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2025, sub_344);  convert_element_type_2025 = sub_344 = None
        add_783: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1033, 1);  mul_1033 = None
        mul_1034: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1032, add_783);  mul_1032 = add_783 = None
        convert_element_type_2026: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1034, torch.bfloat16);  mul_1034 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1330: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2026, [10, 4096]);  convert_element_type_2026 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2027: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1330, torch.float32);  view_1330 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_110: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_11);  alias_11 = None
        mul_1035: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2027, primals_33);  primals_33 = None
        mul_1036: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_72, alias_110);  convert_element_type_72 = None
        mul_1037: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1036, mul_1035)
        sum_156: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1037, [1], True);  mul_1037 = None
        div_137: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1036, 4096)
        mul_1038: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_137, sum_156);  div_137 = sum_156 = None
        sub_345: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1035, mul_1038);  mul_1035 = mul_1038 = None
        mul_1039: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_345, alias_110);  sub_345 = alias_110 = None
        mul_1040: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2027, mul_1036);  convert_element_type_2027 = mul_1036 = None
        sum_157: "f32[4096][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1040, [0]);  mul_1040 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2028: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1039, torch.bfloat16);  mul_1039 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1331: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2028, [10, 4096]);  convert_element_type_2028 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_881: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_1331, [1, 0])
        mm_390: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_881, convert_element_type_68);  permute_881 = convert_element_type_68 = None
        permute_882: "bf16[2048, 4096][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_390, [1, 0]);  mm_390 = None
        permute_883: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_11, [1, 0]);  permute_11 = None
        mm_391: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1331, permute_883);  view_1331 = permute_883 = None
        permute_884: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_882, [1, 0]);  permute_882 = None
        convert_element_type_2033: "f32[4096, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_884, torch.float32);  permute_884 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:173 in forward, code: residual_tensor = activation(residual_tensor)
        convert_element_type_2034: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_391, torch.float32);  mm_391 = None
        convert_element_type_2035: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_18, torch.float32);  view_18 = None
        neg_141: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_2035)
        exp_141: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_141);  neg_141 = None
        add_784: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_141, 1);  exp_141 = None
        reciprocal_85: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_784);  add_784 = None
        mul_1041: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_85, 1);  reciprocal_85 = None
        mul_1042: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2034, mul_1041);  convert_element_type_2034 = None
        sub_346: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1041);  mul_1041 = None
        mul_1043: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2035, sub_346);  convert_element_type_2035 = sub_346 = None
        add_785: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1043, 1);  mul_1043 = None
        mul_1044: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1042, add_785);  mul_1042 = add_785 = None
        convert_element_type_2036: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1044, torch.bfloat16);  mul_1044 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1332: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2036, [10, 2048]);  convert_element_type_2036 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2037: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1332, torch.float32);  view_1332 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_111: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_10);  alias_10 = None
        mul_1045: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2037, primals_31);  primals_31 = None
        mul_1046: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_65, alias_111);  convert_element_type_65 = None
        mul_1047: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1046, mul_1045)
        sum_158: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1047, [1], True);  mul_1047 = None
        div_138: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1046, 2048)
        mul_1048: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_138, sum_158);  div_138 = sum_158 = None
        sub_347: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1045, mul_1048);  mul_1045 = mul_1048 = None
        mul_1049: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_347, alias_111);  sub_347 = alias_111 = None
        mul_1050: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2037, mul_1046);  convert_element_type_2037 = mul_1046 = None
        sum_159: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1050, [0]);  mul_1050 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2038: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1049, torch.bfloat16);  mul_1049 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1333: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2038, [10, 2048]);  convert_element_type_2038 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_885: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1333, [1, 0])
        mm_392: "bf16[2048, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(permute_885, convert_element_type_61);  permute_885 = convert_element_type_61 = None
        permute_886: "bf16[4096, 2048][1, 4096]cuda:0" = torch.ops.aten.permute.default(mm_392, [1, 0]);  mm_392 = None
        permute_887: "bf16[2048, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_10, [1, 0]);  permute_10 = None
        mm_393: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mm.default(view_1333, permute_887);  permute_887 = None
        permute_888: "bf16[2048, 4096][4096, 1]cuda:0" = torch.ops.aten.permute.default(permute_886, [1, 0]);  permute_886 = None
        convert_element_type_2043: "f32[2048, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_888, torch.float32);  permute_888 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:176 in forward, code: output = activation(output)
        convert_element_type_2044: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_393, torch.float32);  mm_393 = None
        convert_element_type_2045: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_16, torch.float32);  view_16 = None
        neg_142: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_2045)
        exp_142: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.exp.default(neg_142);  neg_142 = None
        add_786: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_142, 1);  exp_142 = None
        reciprocal_86: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_786);  add_786 = None
        mul_1051: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_86, 1);  reciprocal_86 = None
        mul_1052: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2044, mul_1051);  convert_element_type_2044 = None
        sub_348: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1051);  mul_1051 = None
        mul_1053: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2045, sub_348);  convert_element_type_2045 = sub_348 = None
        add_787: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1053, 1);  mul_1053 = None
        mul_1054: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1052, add_787);  mul_1052 = add_787 = None
        convert_element_type_2046: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1054, torch.bfloat16);  mul_1054 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1334: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2046, [10, 4096]);  convert_element_type_2046 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2047: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1334, torch.float32);  view_1334 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_112: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_9);  alias_9 = None
        mul_1055: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2047, primals_29);  primals_29 = None
        mul_1056: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_58, alias_112);  convert_element_type_58 = None
        mul_1057: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1056, mul_1055)
        sum_160: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1057, [1], True);  mul_1057 = None
        div_139: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1056, 4096)
        mul_1058: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_139, sum_160);  div_139 = sum_160 = None
        sub_349: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1055, mul_1058);  mul_1055 = mul_1058 = None
        mul_1059: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_349, alias_112);  sub_349 = alias_112 = None
        mul_1060: "f32[10, 4096][4096, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2047, mul_1056);  convert_element_type_2047 = mul_1056 = None
        sum_161: "f32[4096][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1060, [0]);  mul_1060 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2048: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1059, torch.bfloat16);  mul_1059 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1335: "bf16[10, 4096][4096, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2048, [10, 4096]);  convert_element_type_2048 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_889: "bf16[4096, 10][1, 4096]cuda:0" = torch.ops.aten.permute.default(view_1335, [1, 0])
        mm_394: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(permute_889, convert_element_type_54);  permute_889 = convert_element_type_54 = None
        permute_890: "bf16[2048, 4096][1, 2048]cuda:0" = torch.ops.aten.permute.default(mm_394, [1, 0]);  mm_394 = None
        permute_891: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_9, [1, 0]);  permute_9 = None
        mm_395: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mm.default(view_1335, permute_891);  view_1335 = permute_891 = None
        add_788: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(view_1333, mm_395);  view_1333 = mm_395 = None
        permute_892: "bf16[4096, 2048][2048, 1]cuda:0" = torch.ops.aten.permute.default(permute_890, [1, 0]);  permute_890 = None
        convert_element_type_2053: "f32[4096, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_892, torch.float32);  permute_892 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:170 in forward, code: output = residual_tensor = activation(output)
        convert_element_type_2054: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(add_788, torch.float32);  add_788 = None
        convert_element_type_2055: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_14, torch.float32);  view_14 = None
        neg_143: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.neg.default(convert_element_type_2055)
        exp_143: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.exp.default(neg_143);  neg_143 = None
        add_789: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(exp_143, 1);  exp_143 = None
        reciprocal_87: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.reciprocal.default(add_789);  add_789 = None
        mul_1061: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(reciprocal_87, 1);  reciprocal_87 = None
        mul_1062: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2054, mul_1061);  convert_element_type_2054 = None
        sub_350: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, mul_1061);  mul_1061 = None
        mul_1063: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2055, sub_350);  convert_element_type_2055 = sub_350 = None
        add_790: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.add.Tensor(mul_1063, 1);  mul_1063 = None
        mul_1064: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1062, add_790);  mul_1062 = add_790 = None
        convert_element_type_2056: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1064, torch.bfloat16);  mul_1064 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1336: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2056, [10, 2048]);  convert_element_type_2056 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2057: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1336, torch.float32);  view_1336 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_113: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_8);  alias_8 = None
        mul_1065: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2057, primals_27);  primals_27 = None
        mul_1066: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_51, alias_113);  convert_element_type_51 = None
        mul_1067: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1066, mul_1065)
        sum_162: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1067, [1], True);  mul_1067 = None
        div_140: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1066, 2048)
        mul_1068: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_140, sum_162);  div_140 = sum_162 = None
        sub_351: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1065, mul_1068);  mul_1065 = mul_1068 = None
        mul_1069: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_351, alias_113);  sub_351 = alias_113 = None
        mul_1070: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2057, mul_1066);  convert_element_type_2057 = mul_1066 = None
        sum_163: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1070, [0]);  mul_1070 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2058: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1069, torch.bfloat16);  mul_1069 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1337: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2058, [10, 2048]);  convert_element_type_2058 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:168 in forward, code: output = mlp(output)
        permute_893: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1337, [1, 0])
        mm_396: "bf16[2048, 1304][1304, 1]cuda:0" = torch.ops.aten.mm.default(permute_893, convert_element_type_47);  permute_893 = convert_element_type_47 = None
        permute_894: "bf16[1304, 2048][1, 1304]cuda:0" = torch.ops.aten.permute.default(mm_396, [1, 0]);  mm_396 = None
        permute_895: "bf16[2048, 1304][1304, 1]cuda:0" = torch.ops.aten.permute.default(permute_8, [1, 0]);  permute_8 = None
        mm_397: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.aten.mm.default(view_1337, permute_895);  view_1337 = permute_895 = None
        permute_896: "bf16[2048, 1304][1304, 1]cuda:0" = torch.ops.aten.permute.default(permute_894, [1, 0]);  permute_894 = None
        convert_element_type_2063: "f32[2048, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_896, torch.float32);  permute_896 = None
        convert_element_type_2064: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mm_397, torch.float32);  mm_397 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:369 in forward, code: return input_tensor * self.excitation(input_tensor)
        mul_1071: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2064, constant_pad_nd);  convert_element_type_2064 = constant_pad_nd = None
        convert_element_type_2065: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1071, torch.bfloat16);  mul_1071 = None
        alias_114: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.aten.alias.default(alias_7);  alias_7 = None
        convert_element_type_2066: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(convert_element_type_2065, torch.float32);  convert_element_type_2065 = None
        convert_element_type_2067: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(alias_114, torch.float32);  alias_114 = None
        sub_352: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.sub.Tensor(1, convert_element_type_2067)
        mul_1072: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2067, sub_352);  convert_element_type_2067 = sub_352 = None
        mul_1073: "f32[10, 1304][1304, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2066, mul_1072);  convert_element_type_2066 = mul_1072 = None
        convert_element_type_2068: "bf16[10, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1073, torch.bfloat16);  mul_1073 = None
        permute_897: "bf16[1304, 88][88, 1]cuda:0" = torch.ops.aten.permute.default(permute_7, [1, 0]);  permute_7 = None
        mm_398: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.mm.default(convert_element_type_2068, permute_897);  permute_897 = None
        permute_898: "bf16[1304, 10][1, 1304]cuda:0" = torch.ops.aten.permute.default(convert_element_type_2068, [1, 0])
        mm_399: "bf16[1304, 88][88, 1]cuda:0" = torch.ops.aten.mm.default(permute_898, relu);  permute_898 = relu = None
        permute_899: "bf16[88, 1304][1, 88]cuda:0" = torch.ops.aten.permute.default(mm_399, [1, 0]);  mm_399 = None
        sum_164: "bf16[1, 1304][1304, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(convert_element_type_2068, [0], True);  convert_element_type_2068 = None
        view_1338: "bf16[1304][1]cuda:0" = torch.ops.aten.view.default(sum_164, [1304]);  sum_164 = None
        permute_900: "bf16[1304, 88][88, 1]cuda:0" = torch.ops.aten.permute.default(permute_899, [1, 0]);  permute_899 = None
        convert_element_type_2073: "f32[1304][1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1338, torch.float32);  view_1338 = None
        convert_element_type_2074: "f32[1304, 88][88, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_900, torch.float32);  permute_900 = None
        alias_115: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.alias.default(alias_6);  alias_6 = None
        le_12: "b8[10, 88][88, 1]cuda:0" = torch.ops.aten.le.Scalar(alias_115, 0);  alias_115 = None
        scalar_tensor_8: "bf16[][]cuda:0" = torch.ops.aten.scalar_tensor.default(0, dtype = torch.bfloat16, layout = torch.strided, device = device(type='cuda', index=0))
        where: "bf16[10, 88][88, 1]cuda:0" = torch.ops.aten.where.self(le_12, scalar_tensor_8, mm_398);  le_12 = scalar_tensor_8 = mm_398 = None
        permute_901: "bf16[88, 10][1, 88]cuda:0" = torch.ops.aten.permute.default(where, [1, 0])
        mm_400: "bf16[88, 1304][1304, 1]cuda:0" = torch.ops.aten.mm.default(permute_901, convert_element_type_36);  permute_901 = convert_element_type_36 = None
        permute_902: "bf16[1304, 88][1, 1304]cuda:0" = torch.ops.aten.permute.default(mm_400, [1, 0]);  mm_400 = None
        sum_165: "bf16[1, 88][88, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(where, [0], True);  where = None
        view_1339: "bf16[88][1]cuda:0" = torch.ops.aten.view.default(sum_165, [88]);  sum_165 = None
        permute_903: "bf16[88, 1304][1304, 1]cuda:0" = torch.ops.aten.permute.default(permute_902, [1, 0]);  permute_902 = None
        convert_element_type_2077: "f32[88][1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1339, torch.float32);  view_1339 = None
        convert_element_type_2078: "f32[88, 1304][1304, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_903, torch.float32);  permute_903 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:813 in forward, code: ).view(
        view_1340: "bf16[10, 3328][3328, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2015, [10, 3328]);  convert_element_type_2015 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:807 in forward, code: output = torch.cat(
        slice_157: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 0, 256)
        slice_158: "bf16[10, 2048][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 256, 2304)
        slice_159: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 2304, 2560)
        slice_160: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 2560, 2816)
        slice_161: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 2816, 3072)
        slice_162: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1340, 1, 3072, 3328);  view_1340 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1341: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_162, [10, 256]);  slice_162 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2079: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1341, torch.float32);  view_1341 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_116: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_5);  alias_5 = None
        mul_1074: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2079, primals_20);  primals_20 = None
        mul_1075: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_34, alias_116);  convert_element_type_34 = None
        mul_1076: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1075, mul_1074)
        sum_166: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1076, [1], True);  mul_1076 = None
        div_141: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1075, 256)
        mul_1077: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_141, sum_166);  div_141 = sum_166 = None
        sub_353: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1074, mul_1077);  mul_1074 = mul_1077 = None
        mul_1078: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_353, alias_116);  sub_353 = alias_116 = None
        mul_1079: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2079, mul_1075);  convert_element_type_2079 = mul_1075 = None
        sum_167: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1079, [0]);  mul_1079 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2080: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1078, torch.bfloat16);  mul_1078 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1342: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2080, [10, 256]);  convert_element_type_2080 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_904: "bf16[256, 10][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1342, [1, 0]);  view_1342 = None
        mm_401: "bf16[256, 128][128, 1]cuda:0" = torch.ops.aten.mm.default(permute_904, convert_element_type_30);  permute_904 = convert_element_type_30 = None
        permute_905: "bf16[128, 256][1, 128]cuda:0" = torch.ops.aten.permute.default(mm_401, [1, 0]);  mm_401 = None
        permute_906: "bf16[256, 128][128, 1]cuda:0" = torch.ops.aten.permute.default(permute_905, [1, 0]);  permute_905 = None
        convert_element_type_2083: "f32[256, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_906, torch.float32);  permute_906 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1343: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_161, [10, 256]);  slice_161 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2084: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1343, torch.float32);  view_1343 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_117: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_4);  alias_4 = None
        mul_1080: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2084, primals_17);  primals_17 = None
        mul_1081: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_28, alias_117);  convert_element_type_28 = None
        mul_1082: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1081, mul_1080)
        sum_168: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1082, [1], True);  mul_1082 = None
        div_142: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1081, 256)
        mul_1083: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_142, sum_168);  div_142 = sum_168 = None
        sub_354: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1080, mul_1083);  mul_1080 = mul_1083 = None
        mul_1084: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_354, alias_117);  sub_354 = alias_117 = None
        mul_1085: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2084, mul_1081);  convert_element_type_2084 = mul_1081 = None
        sum_169: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1085, [0]);  mul_1085 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2085: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1084, torch.bfloat16);  mul_1084 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1344: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2085, [10, 256]);  convert_element_type_2085 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_907: "bf16[256, 10][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1344, [1, 0]);  view_1344 = None
        mm_402: "bf16[256, 100][100, 1]cuda:0" = torch.ops.aten.mm.default(permute_907, convert_element_type_24);  permute_907 = convert_element_type_24 = None
        permute_908: "bf16[100, 256][1, 100]cuda:0" = torch.ops.aten.permute.default(mm_402, [1, 0]);  mm_402 = None
        permute_909: "bf16[256, 100][100, 1]cuda:0" = torch.ops.aten.permute.default(permute_908, [1, 0]);  permute_908 = None
        convert_element_type_2088: "f32[256, 100][100, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_909, torch.float32);  permute_909 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1345: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_160, [10, 256]);  slice_160 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2089: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1345, torch.float32);  view_1345 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_118: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_3);  alias_3 = None
        mul_1086: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2089, primals_14);  primals_14 = None
        mul_1087: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_22, alias_118);  convert_element_type_22 = None
        mul_1088: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1087, mul_1086)
        sum_170: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1088, [1], True);  mul_1088 = None
        div_143: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1087, 256)
        mul_1089: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_143, sum_170);  div_143 = sum_170 = None
        sub_355: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1086, mul_1089);  mul_1086 = mul_1089 = None
        mul_1090: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_355, alias_118);  sub_355 = alias_118 = None
        mul_1091: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2089, mul_1087);  convert_element_type_2089 = mul_1087 = None
        sum_171: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1091, [0]);  mul_1091 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2090: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1090, torch.bfloat16);  mul_1090 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1346: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2090, [10, 256]);  convert_element_type_2090 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_910: "bf16[256, 10][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1346, [1, 0]);  view_1346 = None
        mm_403: "bf16[256, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_910, convert_element_type_18);  permute_910 = convert_element_type_18 = None
        permute_911: "bf16[64, 256][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_403, [1, 0]);  mm_403 = None
        permute_912: "bf16[256, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_911, [1, 0]);  permute_911 = None
        convert_element_type_2093: "f32[256, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_912, torch.float32);  permute_912 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1347: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_159, [10, 256]);  slice_159 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2094: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1347, torch.float32);  view_1347 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_119: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_2);  alias_2 = None
        mul_1092: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2094, primals_11);  primals_11 = None
        mul_1093: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_16, alias_119);  convert_element_type_16 = None
        mul_1094: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1093, mul_1092)
        sum_172: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1094, [1], True);  mul_1094 = None
        div_144: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1093, 256)
        mul_1095: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_144, sum_172);  div_144 = sum_172 = None
        sub_356: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1092, mul_1095);  mul_1092 = mul_1095 = None
        mul_1096: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_356, alias_119);  sub_356 = alias_119 = None
        mul_1097: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2094, mul_1093);  convert_element_type_2094 = mul_1093 = None
        sum_173: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1097, [0]);  mul_1097 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2095: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1096, torch.bfloat16);  mul_1096 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1348: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2095, [10, 256]);  convert_element_type_2095 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_913: "bf16[256, 10][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1348, [1, 0]);  view_1348 = None
        mm_404: "bf16[256, 128][128, 1]cuda:0" = torch.ops.aten.mm.default(permute_913, convert_element_type_12);  permute_913 = convert_element_type_12 = None
        permute_914: "bf16[128, 256][1, 128]cuda:0" = torch.ops.aten.permute.default(mm_404, [1, 0]);  mm_404 = None
        permute_915: "bf16[256, 128][128, 1]cuda:0" = torch.ops.aten.permute.default(permute_914, [1, 0]);  permute_914 = None
        convert_element_type_2098: "f32[256, 128][128, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_915, torch.float32);  permute_915 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1349: "bf16[10, 2048][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_158, [10, 2048]);  slice_158 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2099: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1349, torch.float32);  view_1349 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_120: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias_1);  alias_1 = None
        mul_1098: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2099, primals_8);  primals_8 = None
        mul_1099: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_10, alias_120);  convert_element_type_10 = None
        mul_1100: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1099, mul_1098)
        sum_174: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1100, [1], True);  mul_1100 = None
        div_145: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1099, 2048)
        mul_1101: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_145, sum_174);  div_145 = sum_174 = None
        sub_357: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1098, mul_1101);  mul_1098 = mul_1101 = None
        mul_1102: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_357, alias_120);  sub_357 = alias_120 = None
        mul_1103: "f32[10, 2048][2048, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2099, mul_1099);  convert_element_type_2099 = mul_1099 = None
        sum_175: "f32[2048][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1103, [0]);  mul_1103 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2100: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1102, torch.bfloat16);  mul_1102 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1350: "bf16[10, 2048][2048, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2100, [10, 2048]);  convert_element_type_2100 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_916: "bf16[2048, 10][1, 2048]cuda:0" = torch.ops.aten.permute.default(view_1350, [1, 0]);  view_1350 = None
        mm_405: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.mm.default(permute_916, convert_element_type_6);  permute_916 = convert_element_type_6 = None
        permute_917: "bf16[1024, 2048][1, 1024]cuda:0" = torch.ops.aten.permute.default(mm_405, [1, 0]);  mm_405 = None
        permute_918: "bf16[2048, 1024][1024, 1]cuda:0" = torch.ops.aten.permute.default(permute_917, [1, 0]);  permute_917 = None
        convert_element_type_2103: "f32[2048, 1024][1024, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_918, torch.float32);  permute_918 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:166 in forward, code: ).view(shape)
        view_1351: "bf16[10, 256][3328, 1]cuda:0" = torch.ops.aten.view.default(slice_157, [10, 256]);  slice_157 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:58 in pytorch_rms_norm, code: return normalized.to(dtype)
        convert_element_type_2104: "f32[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(view_1351, torch.float32);  view_1351 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/torch/nn/functional.py:2954 in rms_norm, code: return torch.rms_norm(input, normalized_shape, weight, eps)
        alias_121: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.alias.default(alias);  alias = None
        mul_1104: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2104, primals_5);  primals_5 = None
        mul_1105: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_4, alias_121);  convert_element_type_4 = None
        mul_1106: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(mul_1105, mul_1104)
        sum_176: "f32[10, 1][1, 1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1106, [1], True);  mul_1106 = None
        div_146: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.div.Tensor(mul_1105, 256)
        mul_1107: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(div_146, sum_176);  div_146 = sum_176 = None
        sub_358: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.sub.Tensor(mul_1104, mul_1107);  mul_1104 = mul_1107 = None
        mul_1108: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(sub_358, alias_121);  sub_358 = alias_121 = None
        mul_1109: "f32[10, 256][256, 1]cuda:0" = torch.ops.aten.mul.Tensor(convert_element_type_2104, mul_1105);  convert_element_type_2104 = mul_1105 = None
        sum_177: "f32[256][1]cuda:0" = torch.ops.aten.sum.dim_IntList(mul_1109, [0]);  mul_1109 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/generative_recommenders/ops/pytorch/pt_layer_norm.py:49 in pytorch_rms_norm, code: x_float = x.to(torch.float32)
        convert_element_type_2105: "bf16[10, 256][256, 1]cuda:0" = torch.ops.prims.convert_element_type.default(mul_1108, torch.bfloat16);  mul_1108 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/ops/layer_norm.py:163 in forward, code: x = x.view(-1, shape[-1])
        view_1352: "bf16[10, 256][256, 1]cuda:0" = torch.ops.aten.view.default(convert_element_type_2105, [10, 256]);  convert_element_type_2105 = None
        
        # File: /mnt/xarfuse/uid-168871/0c2e5a73-seed-nspid4026531836_cgpid310632649-ns-4026531841/hammer/v2/modules/simple_mlps.py:809 in <listcomp>, code: self._embedding_proj[i](embedding_features[i])
        permute_919: "bf16[256, 10][1, 256]cuda:0" = torch.ops.aten.permute.default(view_1352, [1, 0]);  view_1352 = None
        mm_406: "bf16[256, 64][64, 1]cuda:0" = torch.ops.aten.mm.default(permute_919, convert_element_type);  permute_919 = convert_element_type = None
        permute_920: "bf16[64, 256][1, 64]cuda:0" = torch.ops.aten.permute.default(mm_406, [1, 0]);  mm_406 = None
        permute_921: "bf16[256, 64][64, 1]cuda:0" = torch.ops.aten.permute.default(permute_920, [1, 0]);  permute_920 = None
        convert_element_type_2108: "f32[256, 64][64, 1]cuda:0" = torch.ops.prims.convert_element_type.default(permute_921, torch.float32);  permute_921 = None
        
        # No stacktrace found for following nodes
        copy_: "i64[3][1]cuda:0" = torch.ops.aten.copy_.default(primals_113, getitem_120);  getitem_120 = copy_ = None
        copy__1: "i64[3][1]cuda:0" = torch.ops.aten.copy_.default(primals_122, getitem_129);  getitem_129 = copy__1 = None
        copy__2: "i64[3][1]cuda:0" = torch.ops.aten.copy_.default(primals_113, getitem_986);  primals_113 = getitem_986 = copy__2 = None
        copy__3: "i64[3][1]cuda:0" = torch.ops.aten.copy_.default(primals_122, getitem_978);  primals_122 = getitem_978 = copy__3 = None
        view_1353: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1228, [514, 256]);  getitem_1228 = None
        view_1354: "bf16[514, 256][256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1230, [514, 256]);  getitem_1230 = None
        view_1355: "f32[10, 251, 256][64256, 256, 1]cuda:0" = torch.ops.aten.view.default(getitem_1226, [10, 251, 256]);  getitem_1226 = None
        slice_163: "f32[10, 230, 256][64256, 256, 1]cuda:0" = torch.ops.aten.slice.Tensor(view_1355, 1, 0, 230);  view_1355 = None
        return pytree.tree_unflatten([
            select,  # PlainAOTOutput(idx=0)
            select_1,  # PlainAOTOutput(idx=1)
            select_2,  # PlainAOTOutput(idx=2)
            select_3,  # PlainAOTOutput(idx=3)
            select_4,  # PlainAOTOutput(idx=4)
            select_5,  # PlainAOTOutput(idx=5)
            select_6,  # PlainAOTOutput(idx=6)
            select_7,  # PlainAOTOutput(idx=7)
            select_8,  # PlainAOTOutput(idx=8)
            select_9,  # PlainAOTOutput(idx=9)
            select_10,  # PlainAOTOutput(idx=10)
            select_11,  # PlainAOTOutput(idx=11)
            select_12,  # PlainAOTOutput(idx=12)
            select_13,  # PlainAOTOutput(idx=13)
            select_14,  # PlainAOTOutput(idx=14)
            select_15,  # PlainAOTOutput(idx=15)
            select_16,  # PlainAOTOutput(idx=16)
            select_17,  # PlainAOTOutput(idx=17)
            addmm_17,  # PlainAOTOutput(idx=18)
            addmm_15,  # PlainAOTOutput(idx=19)
            getitem_266,  # PlainAOTOutput(idx=20)
            getitem_274,  # PlainAOTOutput(idx=21)
            _local_scalar_dense_13,  # PlainAOTOutput(idx=22)
            _local_scalar_dense_9,  # PlainAOTOutput(idx=23)
            bmm_20,  # IntermediateBaseAOTOutput(base_of=PlainAOTOutput(idx=0))
            view_1353,  # GradAOTOutput(grad_of=PlainAOTInput(idx=0))
            view_1354,  # GradAOTOutput(grad_of=PlainAOTInput(idx=1))
            None,  # None
            convert_element_type_2108,  # GradAOTOutput(grad_of=PlainAOTInput(idx=3))
            sum_177,  # GradAOTOutput(grad_of=PlainAOTInput(idx=4))
            convert_element_type_2103,  # GradAOTOutput(grad_of=PlainAOTInput(idx=5))
            None,  # None
            sum_175,  # GradAOTOutput(grad_of=PlainAOTInput(idx=7))
            convert_element_type_2098,  # GradAOTOutput(grad_of=PlainAOTInput(idx=8))
            None,  # None
            sum_173,  # GradAOTOutput(grad_of=PlainAOTInput(idx=10))
            convert_element_type_2093,  # GradAOTOutput(grad_of=PlainAOTInput(idx=11))
            None,  # None
            sum_171,  # GradAOTOutput(grad_of=PlainAOTInput(idx=13))
            convert_element_type_2088,  # GradAOTOutput(grad_of=PlainAOTInput(idx=14))
            None,  # None
            sum_169,  # GradAOTOutput(grad_of=PlainAOTInput(idx=16))
            convert_element_type_2083,  # GradAOTOutput(grad_of=PlainAOTInput(idx=17))
            None,  # None
            sum_167,  # GradAOTOutput(grad_of=PlainAOTInput(idx=19))
            None,  # None
            convert_element_type_2078,  # GradAOTOutput(grad_of=PlainAOTInput(idx=21))
            convert_element_type_2077,  # GradAOTOutput(grad_of=PlainAOTInput(idx=22))
            convert_element_type_2074,  # GradAOTOutput(grad_of=PlainAOTInput(idx=23))
            convert_element_type_2073,  # GradAOTOutput(grad_of=PlainAOTInput(idx=24))
            convert_element_type_2063,  # GradAOTOutput(grad_of=PlainAOTInput(idx=25))
            sum_163,  # GradAOTOutput(grad_of=PlainAOTInput(idx=26))
            convert_element_type_2053,  # GradAOTOutput(grad_of=PlainAOTInput(idx=27))
            sum_161,  # GradAOTOutput(grad_of=PlainAOTInput(idx=28))
            convert_element_type_2043,  # GradAOTOutput(grad_of=PlainAOTInput(idx=29))
            sum_159,  # GradAOTOutput(grad_of=PlainAOTInput(idx=30))
            convert_element_type_2033,  # GradAOTOutput(grad_of=PlainAOTInput(idx=31))
            sum_157,  # GradAOTOutput(grad_of=PlainAOTInput(idx=32))
            convert_element_type_2023,  # GradAOTOutput(grad_of=PlainAOTInput(idx=33))
            slice_163,  # GradAOTOutput(grad_of=PlainAOTInput(idx=34))
            convert_element_type_2016,  # GradAOTOutput(grad_of=PlainAOTInput(idx=35))
            getitem_1231,  # GradAOTOutput(grad_of=PlainAOTInput(idx=36))
            getitem_1229,  # GradAOTOutput(grad_of=PlainAOTInput(idx=37))
            getitem_1227,  # GradAOTOutput(grad_of=PlainAOTInput(idx=38))
            getitem_1225,  # GradAOTOutput(grad_of=PlainAOTInput(idx=39))
            convert_element_type_2013,  # GradAOTOutput(grad_of=PlainAOTInput(idx=40))
            convert_element_type_2008,  # GradAOTOutput(grad_of=PlainAOTInput(idx=41))
            sum_155,  # GradAOTOutput(grad_of=PlainAOTInput(idx=42))
            convert_element_type_1998,  # GradAOTOutput(grad_of=PlainAOTInput(idx=43))
            getitem_1223,  # GradAOTOutput(grad_of=PlainAOTInput(idx=44))
            convert_element_type_1981,  # GradAOTOutput(grad_of=PlainAOTInput(idx=45))
            sum_153,  # GradAOTOutput(grad_of=PlainAOTInput(idx=46))
            convert_element_type_1971,  # GradAOTOutput(grad_of=PlainAOTInput(idx=47))
            sum_151,  # GradAOTOutput(grad_of=PlainAOTInput(idx=48))
            convert_element_type_1961,  # GradAOTOutput(grad_of=PlainAOTInput(idx=49))
            sum_149,  # GradAOTOutput(grad_of=PlainAOTInput(idx=50))
            convert_element_type_1951,  # GradAOTOutput(grad_of=PlainAOTInput(idx=51))
            sum_147,  # GradAOTOutput(grad_of=PlainAOTInput(idx=52))
            convert_element_type_1941,  # GradAOTOutput(grad_of=PlainAOTInput(idx=53))
            sum_145,  # GradAOTOutput(grad_of=PlainAOTInput(idx=54))
            convert_element_type_1931,  # GradAOTOutput(grad_of=PlainAOTInput(idx=55))
            getitem_1221,  # GradAOTOutput(grad_of=PlainAOTInput(idx=56))
            convert_element_type_1926,  # GradAOTOutput(grad_of=PlainAOTInput(idx=57))
            convert_element_type_1921,  # GradAOTOutput(grad_of=PlainAOTInput(idx=58))
            sum_143,  # GradAOTOutput(grad_of=PlainAOTInput(idx=59))
            convert_element_type_1911,  # GradAOTOutput(grad_of=PlainAOTInput(idx=60))
            getitem_1219,  # GradAOTOutput(grad_of=PlainAOTInput(idx=61))
            convert_element_type_1896,  # GradAOTOutput(grad_of=PlainAOTInput(idx=62))
            sum_141,  # GradAOTOutput(grad_of=PlainAOTInput(idx=63))
            convert_element_type_1886,  # GradAOTOutput(grad_of=PlainAOTInput(idx=64))
            sum_139,  # GradAOTOutput(grad_of=PlainAOTInput(idx=65))
            convert_element_type_1876,  # GradAOTOutput(grad_of=PlainAOTInput(idx=66))
            sum_137,  # GradAOTOutput(grad_of=PlainAOTInput(idx=67))
            convert_element_type_1866,  # GradAOTOutput(grad_of=PlainAOTInput(idx=68))
            sum_135,  # GradAOTOutput(grad_of=PlainAOTInput(idx=69))
            convert_element_type_1856,  # GradAOTOutput(grad_of=PlainAOTInput(idx=70))
            sum_133,  # GradAOTOutput(grad_of=PlainAOTInput(idx=71))
            convert_element_type_1846,  # GradAOTOutput(grad_of=PlainAOTInput(idx=72))
            getitem_1217,  # GradAOTOutput(grad_of=PlainAOTInput(idx=73))
            convert_element_type_1841,  # GradAOTOutput(grad_of=PlainAOTInput(idx=74))
            convert_element_type_1836,  # GradAOTOutput(grad_of=PlainAOTInput(idx=75))
            sum_131,  # GradAOTOutput(grad_of=PlainAOTInput(idx=76))
            convert_element_type_1826,  # GradAOTOutput(grad_of=PlainAOTInput(idx=77))
            getitem_1215,  # GradAOTOutput(grad_of=PlainAOTInput(idx=78))
            convert_element_type_1811,  # GradAOTOutput(grad_of=PlainAOTInput(idx=79))
            sum_129,  # GradAOTOutput(grad_of=PlainAOTInput(idx=80))
            convert_element_type_1801,  # GradAOTOutput(grad_of=PlainAOTInput(idx=81))
            sum_127,  # GradAOTOutput(grad_of=PlainAOTInput(idx=82))
            convert_element_type_1791,  # GradAOTOutput(grad_of=PlainAOTInput(idx=83))
            sum_125,  # GradAOTOutput(grad_of=PlainAOTInput(idx=84))
            convert_element_type_1781,  # GradAOTOutput(grad_of=PlainAOTInput(idx=85))
            sum_123,  # GradAOTOutput(grad_of=PlainAOTInput(idx=86))
            convert_element_type_1771,  # GradAOTOutput(grad_of=PlainAOTInput(idx=87))
            sum_121,  # GradAOTOutput(grad_of=PlainAOTInput(idx=88))
            convert_element_type_1761,  # GradAOTOutput(grad_of=PlainAOTInput(idx=89))
            getitem_1213,  # GradAOTOutput(grad_of=PlainAOTInput(idx=90))
            convert_element_type_1756,  # GradAOTOutput(grad_of=PlainAOTInput(idx=91))
            convert_element_type_1751,  # GradAOTOutput(grad_of=PlainAOTInput(idx=92))
            sum_119,  # GradAOTOutput(grad_of=PlainAOTInput(idx=93))
            convert_element_type_1741,  # GradAOTOutput(grad_of=PlainAOTInput(idx=94))
            getitem_1211,  # GradAOTOutput(grad_of=PlainAOTInput(idx=95))
            convert_element_type_1726,  # GradAOTOutput(grad_of=PlainAOTInput(idx=96))
            sum_117,  # GradAOTOutput(grad_of=PlainAOTInput(idx=97))
            convert_element_type_1716,  # GradAOTOutput(grad_of=PlainAOTInput(idx=98))
            sum_115,  # GradAOTOutput(grad_of=PlainAOTInput(idx=99))
            convert_element_type_1706,  # GradAOTOutput(grad_of=PlainAOTInput(idx=100))
            sum_113,  # GradAOTOutput(grad_of=PlainAOTInput(idx=101))
            convert_element_type_1696,  # GradAOTOutput(grad_of=PlainAOTInput(idx=102))
            sum_111,  # GradAOTOutput(grad_of=PlainAOTInput(idx=103))
            convert_element_type_1686,  # GradAOTOutput(grad_of=PlainAOTInput(idx=104))
            sum_109,  # GradAOTOutput(grad_of=PlainAOTInput(idx=105))
            convert_element_type_1676,  # GradAOTOutput(grad_of=PlainAOTInput(idx=106))
            getitem_1209,  # GradAOTOutput(grad_of=PlainAOTInput(idx=107))
            convert_element_type_1671,  # GradAOTOutput(grad_of=PlainAOTInput(idx=108))
            convert_element_type_1670,  # GradAOTOutput(grad_of=PlainAOTInput(idx=109))
            convert_element_type_1669,  # GradAOTOutput(grad_of=PlainAOTInput(idx=110))
            convert_element_type_1668,  # GradAOTOutput(grad_of=PlainAOTInput(idx=111))
            None,  # None
            convert_element_type_1657,  # GradAOTOutput(grad_of=PlainAOTInput(idx=113))
            convert_element_type_1656,  # GradAOTOutput(grad_of=PlainAOTInput(idx=114))
            convert_element_type_1655,  # GradAOTOutput(grad_of=PlainAOTInput(idx=115))
            None,  # None
            convert_element_type_1650,  # GradAOTOutput(grad_of=PlainAOTInput(idx=117))
            convert_element_type_1649,  # GradAOTOutput(grad_of=PlainAOTInput(idx=118))
            convert_element_type_1648,  # GradAOTOutput(grad_of=PlainAOTInput(idx=119))
            convert_element_type_1647,  # GradAOTOutput(grad_of=PlainAOTInput(idx=120))
            None,  # None
            convert_element_type_1636,  # GradAOTOutput(grad_of=PlainAOTInput(idx=122))
            convert_element_type_1635,  # GradAOTOutput(grad_of=PlainAOTInput(idx=123))
            convert_element_type_1634,  # GradAOTOutput(grad_of=PlainAOTInput(idx=124))
            None,  # None
            convert_element_type_1629,  # GradAOTOutput(grad_of=PlainAOTInput(idx=126))
            None,  # None
            getitem_991,  # GradAOTOutput(grad_of=PlainAOTInput(idx=128))
            convert_element_type_1624,  # GradAOTOutput(grad_of=PlainAOTInput(idx=129))
            sum_104,  # GradAOTOutput(grad_of=PlainAOTInput(idx=130))
            sum_105,  # GradAOTOutput(grad_of=PlainAOTInput(idx=131))
            convert_element_type_1617,  # GradAOTOutput(grad_of=PlainAOTInput(idx=132))
            convert_element_type_1608,  # GradAOTOutput(grad_of=PlainAOTInput(idx=133))
            convert_element_type_1603,  # GradAOTOutput(grad_of=PlainAOTInput(idx=134))
            getitem_985,  # GradAOTOutput(grad_of=PlainAOTInput(idx=135))
            convert_element_type_1598,  # GradAOTOutput(grad_of=PlainAOTInput(idx=136))
            getitem_983,  # GradAOTOutput(grad_of=PlainAOTInput(idx=137))
            convert_element_type_1593,  # GradAOTOutput(grad_of=PlainAOTInput(idx=138))
            sum_100,  # GradAOTOutput(grad_of=PlainAOTInput(idx=139))
            sum_101,  # GradAOTOutput(grad_of=PlainAOTInput(idx=140))
            convert_element_type_1586,  # GradAOTOutput(grad_of=PlainAOTInput(idx=141))
            convert_element_type_1577,  # GradAOTOutput(grad_of=PlainAOTInput(idx=142))
            convert_element_type_1572,  # GradAOTOutput(grad_of=PlainAOTInput(idx=143))
            getitem_977,  # GradAOTOutput(grad_of=PlainAOTInput(idx=144))
            convert_element_type_1567,  # GradAOTOutput(grad_of=PlainAOTInput(idx=145))
            convert_element_type_1562,  # GradAOTOutput(grad_of=PlainAOTInput(idx=146))
            convert_element_type_1557,  # GradAOTOutput(grad_of=PlainAOTInput(idx=147))
            sum_97,  # GradAOTOutput(grad_of=PlainAOTInput(idx=148))
            convert_element_type_1547,  # GradAOTOutput(grad_of=PlainAOTInput(idx=149))
            getitem_975,  # GradAOTOutput(grad_of=PlainAOTInput(idx=150))
            convert_element_type_1532,  # GradAOTOutput(grad_of=PlainAOTInput(idx=151))
            sum_95,  # GradAOTOutput(grad_of=PlainAOTInput(idx=152))
            convert_element_type_1522,  # GradAOTOutput(grad_of=PlainAOTInput(idx=153))
            sum_93,  # GradAOTOutput(grad_of=PlainAOTInput(idx=154))
            convert_element_type_1512,  # GradAOTOutput(grad_of=PlainAOTInput(idx=155))
            sum_91,  # GradAOTOutput(grad_of=PlainAOTInput(idx=156))
            convert_element_type_1502,  # GradAOTOutput(grad_of=PlainAOTInput(idx=157))
            sum_89,  # GradAOTOutput(grad_of=PlainAOTInput(idx=158))
            convert_element_type_1492,  # GradAOTOutput(grad_of=PlainAOTInput(idx=159))
            sum_87,  # GradAOTOutput(grad_of=PlainAOTInput(idx=160))
            convert_element_type_1482,  # GradAOTOutput(grad_of=PlainAOTInput(idx=161))
            getitem_973,  # GradAOTOutput(grad_of=PlainAOTInput(idx=162))
            convert_element_type_1477,  # GradAOTOutput(grad_of=PlainAOTInput(idx=163))
            convert_element_type_1476,  # GradAOTOutput(grad_of=PlainAOTInput(idx=164))
            convert_element_type_1475,  # GradAOTOutput(grad_of=PlainAOTInput(idx=165))
            convert_element_type_1474,  # GradAOTOutput(grad_of=PlainAOTInput(idx=166))
            convert_element_type_1463,  # GradAOTOutput(grad_of=PlainAOTInput(idx=167))
            convert_element_type_1462,  # GradAOTOutput(grad_of=PlainAOTInput(idx=168))
            convert_element_type_1461,  # GradAOTOutput(grad_of=PlainAOTInput(idx=169))
            None,  # None
            convert_element_type_1456,  # GradAOTOutput(grad_of=PlainAOTInput(idx=171))
            convert_element_type_1455,  # GradAOTOutput(grad_of=PlainAOTInput(idx=172))
            convert_element_type_1454,  # GradAOTOutput(grad_of=PlainAOTInput(idx=173))
            convert_element_type_1453,  # GradAOTOutput(grad_of=PlainAOTInput(idx=174))
            convert_element_type_1442,  # GradAOTOutput(grad_of=PlainAOTInput(idx=175))
            convert_element_type_1441,  # GradAOTOutput(grad_of=PlainAOTInput(idx=176))
            convert_element_type_1440,  # GradAOTOutput(grad_of=PlainAOTInput(idx=177))
            None,  # None
            convert_element_type_1435,  # GradAOTOutput(grad_of=PlainAOTInput(idx=179))
            getitem_755,  # GradAOTOutput(grad_of=PlainAOTInput(idx=180))
            convert_element_type_1430,  # GradAOTOutput(grad_of=PlainAOTInput(idx=181))
            sum_82,  # GradAOTOutput(grad_of=PlainAOTInput(idx=182))
            sum_83,  # GradAOTOutput(grad_of=PlainAOTInput(idx=183))
            convert_element_type_1423,  # GradAOTOutput(grad_of=PlainAOTInput(idx=184))
            convert_element_type_1414,  # GradAOTOutput(grad_of=PlainAOTInput(idx=185))
            convert_element_type_1409,  # GradAOTOutput(grad_of=PlainAOTInput(idx=186))
            getitem_749,  # GradAOTOutput(grad_of=PlainAOTInput(idx=187))
            convert_element_type_1404,  # GradAOTOutput(grad_of=PlainAOTInput(idx=188))
            getitem_747,  # GradAOTOutput(grad_of=PlainAOTInput(idx=189))
            convert_element_type_1399,  # GradAOTOutput(grad_of=PlainAOTInput(idx=190))
            sum_78,  # GradAOTOutput(grad_of=PlainAOTInput(idx=191))
            sum_79,  # GradAOTOutput(grad_of=PlainAOTInput(idx=192))
            convert_element_type_1392,  # GradAOTOutput(grad_of=PlainAOTInput(idx=193))
            convert_element_type_1383,  # GradAOTOutput(grad_of=PlainAOTInput(idx=194))
            convert_element_type_1378,  # GradAOTOutput(grad_of=PlainAOTInput(idx=195))
            getitem_741,  # GradAOTOutput(grad_of=PlainAOTInput(idx=196))
            convert_element_type_1373,  # GradAOTOutput(grad_of=PlainAOTInput(idx=197))
            convert_element_type_1368,  # GradAOTOutput(grad_of=PlainAOTInput(idx=198))
            convert_element_type_1363,  # GradAOTOutput(grad_of=PlainAOTInput(idx=199))
            sum_75,  # GradAOTOutput(grad_of=PlainAOTInput(idx=200))
            convert_element_type_1353,  # GradAOTOutput(grad_of=PlainAOTInput(idx=201))
            getitem_739,  # GradAOTOutput(grad_of=PlainAOTInput(idx=202))
            convert_element_type_1338,  # GradAOTOutput(grad_of=PlainAOTInput(idx=203))
            sum_73,  # GradAOTOutput(grad_of=PlainAOTInput(idx=204))
            convert_element_type_1328,  # GradAOTOutput(grad_of=PlainAOTInput(idx=205))
            sum_71,  # GradAOTOutput(grad_of=PlainAOTInput(idx=206))
            convert_element_type_1318,  # GradAOTOutput(grad_of=PlainAOTInput(idx=207))
            sum_69,  # GradAOTOutput(grad_of=PlainAOTInput(idx=208))
            convert_element_type_1308,  # GradAOTOutput(grad_of=PlainAOTInput(idx=209))
            sum_67,  # GradAOTOutput(grad_of=PlainAOTInput(idx=210))
            convert_element_type_1298,  # GradAOTOutput(grad_of=PlainAOTInput(idx=211))
            sum_65,  # GradAOTOutput(grad_of=PlainAOTInput(idx=212))
            convert_element_type_1288,  # GradAOTOutput(grad_of=PlainAOTInput(idx=213))
            getitem_737,  # GradAOTOutput(grad_of=PlainAOTInput(idx=214))
            convert_element_type_1283,  # GradAOTOutput(grad_of=PlainAOTInput(idx=215))
            convert_element_type_1282,  # GradAOTOutput(grad_of=PlainAOTInput(idx=216))
            convert_element_type_1281,  # GradAOTOutput(grad_of=PlainAOTInput(idx=217))
            convert_element_type_1280,  # GradAOTOutput(grad_of=PlainAOTInput(idx=218))
            convert_element_type_1269,  # GradAOTOutput(grad_of=PlainAOTInput(idx=219))
            convert_element_type_1268,  # GradAOTOutput(grad_of=PlainAOTInput(idx=220))
            convert_element_type_1267,  # GradAOTOutput(grad_of=PlainAOTInput(idx=221))
            None,  # None
            convert_element_type_1262,  # GradAOTOutput(grad_of=PlainAOTInput(idx=223))
            convert_element_type_1261,  # GradAOTOutput(grad_of=PlainAOTInput(idx=224))
            convert_element_type_1260,  # GradAOTOutput(grad_of=PlainAOTInput(idx=225))
            convert_element_type_1259,  # GradAOTOutput(grad_of=PlainAOTInput(idx=226))
            convert_element_type_1248,  # GradAOTOutput(grad_of=PlainAOTInput(idx=227))
            convert_element_type_1247,  # GradAOTOutput(grad_of=PlainAOTInput(idx=228))
            convert_element_type_1246,  # GradAOTOutput(grad_of=PlainAOTInput(idx=229))
            None,  # None
            convert_element_type_1241,  # GradAOTOutput(grad_of=PlainAOTInput(idx=231))
            getitem_517,  # GradAOTOutput(grad_of=PlainAOTInput(idx=232))
            convert_element_type_1236,  # GradAOTOutput(grad_of=PlainAOTInput(idx=233))
            sum_60,  # GradAOTOutput(grad_of=PlainAOTInput(idx=234))
            sum_61,  # GradAOTOutput(grad_of=PlainAOTInput(idx=235))
            convert_element_type_1229,  # GradAOTOutput(grad_of=PlainAOTInput(idx=236))
            convert_element_type_1220,  # GradAOTOutput(grad_of=PlainAOTInput(idx=237))
            convert_element_type_1215,  # GradAOTOutput(grad_of=PlainAOTInput(idx=238))
            getitem_511,  # GradAOTOutput(grad_of=PlainAOTInput(idx=239))
            convert_element_type_1210,  # GradAOTOutput(grad_of=PlainAOTInput(idx=240))
            getitem_509,  # GradAOTOutput(grad_of=PlainAOTInput(idx=241))
            convert_element_type_1205,  # GradAOTOutput(grad_of=PlainAOTInput(idx=242))
            sum_56,  # GradAOTOutput(grad_of=PlainAOTInput(idx=243))
            sum_57,  # GradAOTOutput(grad_of=PlainAOTInput(idx=244))
            convert_element_type_1198,  # GradAOTOutput(grad_of=PlainAOTInput(idx=245))
            convert_element_type_1189,  # GradAOTOutput(grad_of=PlainAOTInput(idx=246))
            convert_element_type_1184,  # GradAOTOutput(grad_of=PlainAOTInput(idx=247))
            getitem_503,  # GradAOTOutput(grad_of=PlainAOTInput(idx=248))
            convert_element_type_1179,  # GradAOTOutput(grad_of=PlainAOTInput(idx=249))
            convert_element_type_1174,  # GradAOTOutput(grad_of=PlainAOTInput(idx=250))
            convert_element_type_1169,  # GradAOTOutput(grad_of=PlainAOTInput(idx=251))
            sum_53,  # GradAOTOutput(grad_of=PlainAOTInput(idx=252))
            convert_element_type_1159,  # GradAOTOutput(grad_of=PlainAOTInput(idx=253))
            getitem_501,  # GradAOTOutput(grad_of=PlainAOTInput(idx=254))
            convert_element_type_1144,  # GradAOTOutput(grad_of=PlainAOTInput(idx=255))
            sum_51,  # GradAOTOutput(grad_of=PlainAOTInput(idx=256))
            convert_element_type_1134,  # GradAOTOutput(grad_of=PlainAOTInput(idx=257))
            sum_49,  # GradAOTOutput(grad_of=PlainAOTInput(idx=258))
            convert_element_type_1124,  # GradAOTOutput(grad_of=PlainAOTInput(idx=259))
            sum_47,  # GradAOTOutput(grad_of=PlainAOTInput(idx=260))
            convert_element_type_1114,  # GradAOTOutput(grad_of=PlainAOTInput(idx=261))
            sum_45,  # GradAOTOutput(grad_of=PlainAOTInput(idx=262))
            convert_element_type_1104,  # GradAOTOutput(grad_of=PlainAOTInput(idx=263))
            sum_43,  # GradAOTOutput(grad_of=PlainAOTInput(idx=264))
            convert_element_type_1094,  # GradAOTOutput(grad_of=PlainAOTInput(idx=265))
            getitem_499,  # GradAOTOutput(grad_of=PlainAOTInput(idx=266))
            convert_element_type_1089,  # GradAOTOutput(grad_of=PlainAOTInput(idx=267))
            convert_element_type_1088,  # GradAOTOutput(grad_of=PlainAOTInput(idx=268))
            convert_element_type_1087,  # GradAOTOutput(grad_of=PlainAOTInput(idx=269))
            convert_element_type_1086,  # GradAOTOutput(grad_of=PlainAOTInput(idx=270))
            convert_element_type_1075,  # GradAOTOutput(grad_of=PlainAOTInput(idx=271))
            convert_element_type_1074,  # GradAOTOutput(grad_of=PlainAOTInput(idx=272))
            convert_element_type_1073,  # GradAOTOutput(grad_of=PlainAOTInput(idx=273))
            None,  # None
            convert_element_type_1068,  # GradAOTOutput(grad_of=PlainAOTInput(idx=275))
            convert_element_type_1067,  # GradAOTOutput(grad_of=PlainAOTInput(idx=276))
            convert_element_type_1066,  # GradAOTOutput(grad_of=PlainAOTInput(idx=277))
            convert_element_type_1065,  # GradAOTOutput(grad_of=PlainAOTInput(idx=278))
            convert_element_type_1054,  # GradAOTOutput(grad_of=PlainAOTInput(idx=279))
            convert_element_type_1053,  # GradAOTOutput(grad_of=PlainAOTInput(idx=280))
            convert_element_type_1052,  # GradAOTOutput(grad_of=PlainAOTInput(idx=281))
            None,  # None
            convert_element_type_1047,  # GradAOTOutput(grad_of=PlainAOTInput(idx=283))
            getitem_279,  # GradAOTOutput(grad_of=PlainAOTInput(idx=284))
            convert_element_type_1042,  # GradAOTOutput(grad_of=PlainAOTInput(idx=285))
            sum_38,  # GradAOTOutput(grad_of=PlainAOTInput(idx=286))
            sum_39,  # GradAOTOutput(grad_of=PlainAOTInput(idx=287))
            convert_element_type_1035,  # GradAOTOutput(grad_of=PlainAOTInput(idx=288))
            convert_element_type_1026,  # GradAOTOutput(grad_of=PlainAOTInput(idx=289))
            convert_element_type_1021,  # GradAOTOutput(grad_of=PlainAOTInput(idx=290))
            getitem_273,  # GradAOTOutput(grad_of=PlainAOTInput(idx=291))
            convert_element_type_1016,  # GradAOTOutput(grad_of=PlainAOTInput(idx=292))
            getitem_271,  # GradAOTOutput(grad_of=PlainAOTInput(idx=293))
            convert_element_type_1011,  # GradAOTOutput(grad_of=PlainAOTInput(idx=294))
            sum_34,  # GradAOTOutput(grad_of=PlainAOTInput(idx=295))
            sum_35,  # GradAOTOutput(grad_of=PlainAOTInput(idx=296))
            convert_element_type_1004,  # GradAOTOutput(grad_of=PlainAOTInput(idx=297))
            convert_element_type_995,  # GradAOTOutput(grad_of=PlainAOTInput(idx=298))
            convert_element_type_990,  # GradAOTOutput(grad_of=PlainAOTInput(idx=299))
            getitem_265,  # GradAOTOutput(grad_of=PlainAOTInput(idx=300))
            convert_element_type_985,  # GradAOTOutput(grad_of=PlainAOTInput(idx=301))
            convert_element_type_980,  # GradAOTOutput(grad_of=PlainAOTInput(idx=302))
            convert_element_type_975,  # GradAOTOutput(grad_of=PlainAOTInput(idx=303))
            sum_31,  # GradAOTOutput(grad_of=PlainAOTInput(idx=304))
            convert_element_type_965,  # GradAOTOutput(grad_of=PlainAOTInput(idx=305))
            getitem_263,  # GradAOTOutput(grad_of=PlainAOTInput(idx=306))
            convert_element_type_950,  # GradAOTOutput(grad_of=PlainAOTInput(idx=307))
            sum_29,  # GradAOTOutput(grad_of=PlainAOTInput(idx=308))
            convert_element_type_940,  # GradAOTOutput(grad_of=PlainAOTInput(idx=309))
            sum_27,  # GradAOTOutput(grad_of=PlainAOTInput(idx=310))
            convert_element_type_930,  # GradAOTOutput(grad_of=PlainAOTInput(idx=311))
            sum_25,  # GradAOTOutput(grad_of=PlainAOTInput(idx=312))
            convert_element_type_920,  # GradAOTOutput(grad_of=PlainAOTInput(idx=313))
            sum_23,  # GradAOTOutput(grad_of=PlainAOTInput(idx=314))
            convert_element_type_910,  # GradAOTOutput(grad_of=PlainAOTInput(idx=315))
            sum_21,  # GradAOTOutput(grad_of=PlainAOTInput(idx=316))
            view_563,  # GradAOTOutput(grad_of=PlainAOTInput(idx=317))
            squeeze_9,  # GradAOTOutput(grad_of=PlainAOTInput(idx=318))
            sum_15,  # GradAOTOutput(grad_of=PlainAOTInput(idx=319))
            sum_16,  # GradAOTOutput(grad_of=PlainAOTInput(idx=320))
            squeeze_7,  # GradAOTOutput(grad_of=PlainAOTInput(idx=321))
            sum_11,  # GradAOTOutput(grad_of=PlainAOTInput(idx=322))
            sum_12,  # GradAOTOutput(grad_of=PlainAOTInput(idx=323))
            squeeze_5,  # GradAOTOutput(grad_of=PlainAOTInput(idx=324))
            sum_7,  # GradAOTOutput(grad_of=PlainAOTInput(idx=325))
            sum_8,  # GradAOTOutput(grad_of=PlainAOTInput(idx=326))
            squeeze_3,  # GradAOTOutput(grad_of=PlainAOTInput(idx=327))
            sum_3,  # GradAOTOutput(grad_of=PlainAOTInput(idx=328))
            sum_4,  # GradAOTOutput(grad_of=PlainAOTInput(idx=329))
            squeeze_1,  # GradAOTOutput(grad_of=PlainAOTInput(idx=330))
        ], self._out_spec)
        