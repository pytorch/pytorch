I1117 21:33:19.273000 3014252 torch/_inductor/config.py:998] compile_threads set to 32
I1117 21:33:20.533000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.534000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
I1117 21:33:20.534000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
I1117 21:33:20.535000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
I1117 21:33:20.535000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
I1117 21:33:20.536000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.536000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
I1117 21:33:20.536000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
I1117 21:33:20.537000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
I1117 21:33:20.537000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
I1117 21:33:20.538000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
I1117 21:33:20.538000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
I1117 21:33:20.539000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
I1117 21:33:20.539000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
I1117 21:33:20.543000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.543000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.544000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
I1117 21:33:20.544000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
I1117 21:33:20.545000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
I1117 21:33:20.545000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.546000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.546000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 21:33:20.546000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 21:33:20.547000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 21:33:20.547000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 21:33:20.548000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 21:33:20.548000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 21:33:20.548000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
I1117 21:33:20.549000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
I1117 21:33:20.549000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
I1117 21:33:20.550000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
I1117 21:33:20.550000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
I1117 21:33:20.551000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
I1117 21:33:20.551000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
I1117 21:33:20.551000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
I1117 21:33:20.552000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
I1117 21:33:20.552000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
I1117 21:33:20.553000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
I1117 21:33:20.553000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
I1117 21:33:20.553000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
I1117 21:33:20.554000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
I1117 21:33:20.554000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
I1117 21:33:20.555000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
I1117 21:33:20.555000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
I1117 21:33:20.555000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
I1117 21:33:20.556000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
I1117 21:33:20.556000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
I1117 21:33:20.556000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
I1117 21:33:20.557000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
I1117 21:33:20.557000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
I1117 21:33:20.558000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
I1117 21:33:20.558000 3014252 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
I1117 21:33:21.237000 3014252 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
I1117 21:33:21.336000 3014252 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[2, 256, 128][32768, 128, 1]cuda:0", L_weight_: "f32[128][1]cuda:0"):
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_weight_ = L_weight_
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:110 in test_fn, code: return dynamic_range_op(x, weight)
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         dynamic_range_140328919271200_default: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_140328919271200.default(l_x_, l_weight_);  l_x_ = l_weight_ = None
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (dynamic_range_140328919271200_default,)
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 21:33:21.339000 3014252 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
V1117 21:33:21.956000 3014252 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fxstxvqjkyx2se3rg6ftveyi26f3t3kykiyerk75nc2t3hh4ilzv:
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [nmdhpeolv4qknqr5mc64zpia5mdhldjjvhzfpnosaqxe44im6mq] gm: <lambda>()
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1, arg1_1):
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0]     dynamic_range_140328919271200 = torch.ops.test_lib.dynamic_range_140328919271200.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0]     return (dynamic_range_140328919271200,)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0]     
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ffvbi42jvc443loukbs4l7f2ktyctumpl3p52q36kphvdbcz745] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2, 256, 128]), stride=(32768, 128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [hhafgaghh5icfiiv2s3gtezjixz3usyfz36wro23umj3t2dfcyw] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [gvxpmwqowbczb5p4mc74ktzcqr7zsr5phor6h6drv37eewafuix] torch_version: ï¿½ï¿½#KIï¿½!M;ï¿½ï¿½Gï¿½j'ï¿½ï¿½Yqï¿½lï¿½ï¿½dï¿½
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bfloat16_atomic_adds_enabled]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_epilogue_fusion_enabled]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_tma_only]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_hash_with_compile_cmd]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.cutlass_prescreening]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cuda.cutlass_enabled_ops]: all
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.use_binary_remote_cache]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.upload_to_binary_remote_cache]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.binary_remote_cache_force_write]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.enable_caching_codegen]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [6trwnwm4voevl4joplmkcssruwgd46kgqfejamut6kq662kstpd] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cuda.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
V1117 21:33:21.961000 3014252 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
V1117 21:33:21.962000 3014252 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fxstxvqjkyx2se3rg6ftveyi26f3t3kykiyerk75nc2t3hh4ilzv
I1117 21:33:21.963000 3014252 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fxstxvqjkyx2se3rg6ftveyi26f3t3kykiyerk75nc2t3hh4ilzv
V1117 21:33:21.963000 3014252 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
V1117 21:33:21.964000 3014252 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
I1117 21:33:21.964000 3014252 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[2, 256, 128][32768, 128, 1]cuda:0", arg1_1: "f32[128][1]cuda:0"):
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:110 in test_fn, code: return dynamic_range_op(x, weight)
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         dynamic_range_140328919271200: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_140328919271200.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (dynamic_range_140328919271200,)
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
V1117 21:33:21.982000 3014252 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
V1117 21:33:21.984000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:21.984000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:21.985000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %dynamic_range_140328919271200 : [num_users=1] = call_function[target=torch.ops.test_lib.dynamic_range_140328919271200.default](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 21:33:21.985000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function test_lib::dynamic_range_140328919271200 at 0x7f9ec224e3e0>
I1117 21:33:21.986000 3014252 torch/_inductor/kernel/custom_op.py:679] [0/0] === Range-based Autotuning for dynamic_range_autotuned ===
I1117 21:33:21.988000 3014252 torch/_inductor/kernel/custom_op.py:680] [0/0] Dispatch on: x[1], Ranges: [(1, 512), (513, 2048), (2049, inf)]
V1117 21:33:22.011000 3014252 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 21:33:22.012000 3014252 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 21:33:22.014000 3014252 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 21:33:22.015000 3014252 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 21:33:22.016000 3014252 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 21:33:22.017000 3014252 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py:3323: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  current_size = base.storage().size()
V1117 21:33:22.020000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.020000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.021000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 21:33:22.022000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.022000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 21:33:22.023000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.024000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 21:33:22.025000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.026000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 21:33:22.026000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.027000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 21:33:22.027000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.046000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.047000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, i2)
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, permu...,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.056000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.057000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0
I1117 21:33:22.064000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.065000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.066000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.067000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.067000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.080000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_op0 with estimated runtime 0.000214
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.088000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.093000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.094000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.095000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.102000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.103000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1 = args
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, (128, ), (1, ))
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, 65536, stream=stream0)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, )
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1])
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.105000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/jl/cjlzz4alwscfnx7qv7szsqgksflao55u5grqazovcehejghdoztt.py
V1117 21:33:22.112000 3014252 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.112000 3014252 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 21:33:22.113000 3014252 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 21:33:22.113000 3014252 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1117 21:33:22.206000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/jl/cjlzz4alwscfnx7qv7szsqgksflao55u5grqazovcehejghdoztt.py
I1117 21:33:22.207000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/jl/cjlzz4alwscfnx7qv7szsqgksflao55u5grqazovcehejghdoztt.py
V1117 21:33:22.225000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.226000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.226000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.227000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.227000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.237000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.238000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.238000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.239000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.240000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.240000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.242000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.242000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.243000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.244000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, i2)
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.247000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.249000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0
I1117 21:33:22.253000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.254000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.254000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.255000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.256000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.258000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_op0 with estimated runtime 0.000214
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.260000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.262000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.263000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.264000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.265000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.266000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1 = args
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, (128, ), (1, ))
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, 65536, stream=stream0)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, )
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1])
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.267000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.268000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/kf/ckfzyelihalw4o3d7kylhjgt36zp73grwyz3ucwmgkd2vfj2rczc.py
V1117 21:33:22.270000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/kf/ckfzyelihalw4o3d7kylhjgt36zp73grwyz3ucwmgkd2vfj2rczc.py
I1117 21:33:22.271000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/kf/ckfzyelihalw4o3d7kylhjgt36zp73grwyz3ucwmgkd2vfj2rczc.py
V1117 21:33:22.287000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.288000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.288000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.289000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.290000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.299000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.299000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.300000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 21:33:22.300000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7fa0d040d260>
V1117 21:33:22.302000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 21:33:22.302000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.304000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.304000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, i2)
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, view]),
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.307000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.308000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0
I1117 21:33:22.311000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.312000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.312000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.313000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.313000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.316000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_op0 with estimated runtime 0.000214
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.317000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.319000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.320000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.321000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.322000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.323000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1 = args
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, (128, ), (1, ))
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, 65536, stream=stream0)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, )
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1])
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.324000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.325000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/l3/cl3h37avub5jxhthfsbmxygrp6def57qfzpvi2zb7souagltktsb.py
V1117 21:33:22.327000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/l3/cl3h37avub5jxhthfsbmxygrp6def57qfzpvi2zb7souagltktsb.py
I1117 21:33:22.328000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/l3/cl3h37avub5jxhthfsbmxygrp6def57qfzpvi2zb7souagltktsb.py
V1117 21:33:22.344000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.345000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.345000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.346000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.346000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_1_512_medium_sequence_impl_1", "best_kernel_desc": "CustomOp medium_sequence_impl", "best_time": 0.005247999913990498}
V1117 21:33:22.364000 3014252 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.35s
V1117 21:33:22.365000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.366000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.367000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.367000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.368000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.368000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.370000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.370000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.371000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.372000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf0,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 21:33:22.372000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 21:33:22.373000 3014252 torch/_inductor/kernel/custom_op.py:609] [0/0] Inlining winning choice: dynamic_range_autotuned_range_1_512_medium_sequence_impl_1 (name=dynamic_range_autotuned_range_1_512)
V1117 21:33:22.374000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.374000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.375000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.376000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.377000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.378000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
I1117 21:33:22.379000 3014252 torch/_inductor/kernel/custom_op.py:724] [0/0] Range [1, 512]: Selected dynamic_range_autotuned_range_1_512_medium_sequence_impl_1
V1117 21:33:22.393000 3014252 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 21:33:22.394000 3014252 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 21:33:22.395000 3014252 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 21:33:22.397000 3014252 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 21:33:22.397000 3014252 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 21:33:22.398000 3014252 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1117 21:33:22.400000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.401000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.401000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 21:33:22.402000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.402000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 21:33:22.403000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.404000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 21:33:22.404000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.405000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 21:33:22.406000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.406000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 21:33:22.407000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.408000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.409000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, i2)
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, permu...,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.411000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.412000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0
I1117 21:33:22.416000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.416000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.417000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.418000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.418000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.420000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_op0 with estimated runtime 0.000214
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.422000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.424000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.425000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.426000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.427000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.428000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1 = args
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, (128, ), (1, ))
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, 65536, stream=stream0)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, )
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1])
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.429000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.430000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/fj/cfj7dkwkgqt62pycelq6rpzzdhlvogwpek4fkrglctlvf3zrcgjj.py
V1117 21:33:22.432000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/fj/cfj7dkwkgqt62pycelq6rpzzdhlvogwpek4fkrglctlvf3zrcgjj.py
I1117 21:33:22.433000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/fj/cfj7dkwkgqt62pycelq6rpzzdhlvogwpek4fkrglctlvf3zrcgjj.py
V1117 21:33:22.450000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.450000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.451000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.451000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.452000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.461000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.462000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.462000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.463000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.463000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.464000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.465000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.466000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.467000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.467000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, i2)
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.470000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0
I1117 21:33:22.474000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.475000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.475000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.476000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.476000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.479000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_op0 with estimated runtime 0.000214
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.481000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.482000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.483000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.484000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.486000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.487000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1 = args
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, (128, ), (1, ))
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, 65536, stream=stream0)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, )
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1])
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.488000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/5y/c5yiqlll3tsc3td3xhisotrmdkiynhmsrbgxfbqpvmw4iu5nkh4y.py
V1117 21:33:22.491000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/5y/c5yiqlll3tsc3td3xhisotrmdkiynhmsrbgxfbqpvmw4iu5nkh4y.py
I1117 21:33:22.491000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/5y/c5yiqlll3tsc3td3xhisotrmdkiynhmsrbgxfbqpvmw4iu5nkh4y.py
V1117 21:33:22.508000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.508000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.509000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.509000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.510000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.519000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.520000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.521000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 21:33:22.521000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7fa0d040d260>
V1117 21:33:22.522000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 21:33:22.523000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.524000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.525000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, i2)
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, view]),
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.527000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.528000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0
I1117 21:33:22.532000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.533000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.533000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.534000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.535000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.537000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_op0 with estimated runtime 0.000214
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.539000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.540000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.543000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.544000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.545000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.546000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1 = args
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, (128, ), (1, ))
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, 65536, stream=stream0)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, )
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1])
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.547000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.548000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/iz/cizrpmcvdg6q7kv55uzad2tmweye2aw32dl4xh6eknixuar3wlge.py
V1117 21:33:22.550000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/iz/cizrpmcvdg6q7kv55uzad2tmweye2aw32dl4xh6eknixuar3wlge.py
I1117 21:33:22.551000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/iz/cizrpmcvdg6q7kv55uzad2tmweye2aw32dl4xh6eknixuar3wlge.py
V1117 21:33:22.567000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.568000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.568000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.569000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.569000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4", "best_kernel_desc": "CustomOp medium_sequence_impl", "best_time": 0.005247999913990498}
V1117 21:33:22.587000 3014252 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.19s
V1117 21:33:22.589000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.589000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.590000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.590000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.591000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.592000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.593000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.593000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.594000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.595000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf2,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 21:33:22.596000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 21:33:22.596000 3014252 torch/_inductor/kernel/custom_op.py:609] [0/0] Inlining winning choice: dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4 (name=dynamic_range_autotuned_range_513_2048)
V1117 21:33:22.597000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.597000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.598000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.599000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.600000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.600000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
I1117 21:33:22.602000 3014252 torch/_inductor/kernel/custom_op.py:724] [0/0] Range [513, 2048]: Selected dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4
V1117 21:33:22.618000 3014252 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 21:33:22.618000 3014252 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 21:33:22.620000 3014252 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 21:33:22.622000 3014252 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 21:33:22.622000 3014252 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 21:33:22.623000 3014252 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1117 21:33:22.625000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.626000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.626000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 21:33:22.627000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.628000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 21:33:22.628000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.629000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 21:33:22.629000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7fa0d040f100>
V1117 21:33:22.630000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 21:33:22.631000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7fa0d040d440>
V1117 21:33:22.631000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 21:33:22.632000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.633000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.634000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, i2)
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, permu...,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.636000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.637000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0
I1117 21:33:22.641000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.641000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.642000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.643000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.643000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.645000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_op0 with estimated runtime 0.000214
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.648000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.649000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.650000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.651000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.652000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.653000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1 = args
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, (128, ), (1, ))
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, 65536, stream=stream0)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, )
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1])
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.654000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.655000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/e5/ce53bbyd4jn4itooyrdulb2egxmmmelkvzohfxgfactzbu3g4bjr.py
V1117 21:33:22.658000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/e5/ce53bbyd4jn4itooyrdulb2egxmmmelkvzohfxgfactzbu3g4bjr.py
I1117 21:33:22.658000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/e5/ce53bbyd4jn4itooyrdulb2egxmmmelkvzohfxgfactzbu3g4bjr.py
V1117 21:33:22.675000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.675000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.676000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.676000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.677000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.686000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.687000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.687000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.688000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.688000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.689000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.690000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.691000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.692000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.692000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, i2)
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.695000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.696000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0
I1117 21:33:22.699000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.700000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.700000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.701000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.702000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.704000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_op0 with estimated runtime 0.000214
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.706000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.707000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.709000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.710000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.711000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.712000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1 = args
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, (128, ), (1, ))
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, 65536, stream=stream0)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, )
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1])
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.713000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/sv/csvw3aktg5wgueiy2xshudkeuh2qrxoef5cxmu4bwkbqahszjw4z.py
V1117 21:33:22.716000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/sv/csvw3aktg5wgueiy2xshudkeuh2qrxoef5cxmu4bwkbqahszjw4z.py
I1117 21:33:22.716000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/sv/csvw3aktg5wgueiy2xshudkeuh2qrxoef5cxmu4bwkbqahszjw4z.py
V1117 21:33:22.733000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.733000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005280, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.734000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.734000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005280, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.735000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
V1117 21:33:22.744000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.745000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.746000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 21:33:22.746000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7fa0d040d260>
V1117 21:33:22.747000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 21:33:22.747000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.749000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 21:33:22.750000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, i2)
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, mul, view]),
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.752000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.753000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0
I1117 21:33:22.756000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.757000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.758000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.758000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.759000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.761000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_op0 with estimated runtime 0.000214
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.763000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.765000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.766000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.767000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.768000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.769000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1 = args
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, (128, ), (1, ))
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_140328919271200_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, 65536, stream=stream0)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, )
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1])
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.770000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/6p/c6pgmvzgqwkndukbwmlauajm6gvmraqq3ceenlgetncdibfwu6og.py
V1117 21:33:22.773000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/6p/c6pgmvzgqwkndukbwmlauajm6gvmraqq3ceenlgetncdibfwu6og.py
I1117 21:33:22.773000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/6p/c6pgmvzgqwkndukbwmlauajm6gvmraqq3ceenlgetncdibfwu6og.py
V1117 21:33:22.790000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.790000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.791000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.791000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.792000 3014252 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7", "best_kernel_desc": "CustomOp medium_sequence_impl", "best_time": 0.005247999913990498}
V1117 21:33:22.810000 3014252 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.19s
V1117 21:33:22.811000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.812000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.812000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.813000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.814000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.814000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.815000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.816000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.817000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.818000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf4,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 21:33:22.818000 3014252 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 21:33:22.819000 3014252 torch/_inductor/kernel/custom_op.py:609] [0/0] Inlining winning choice: dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7 (name=dynamic_range_autotuned_range_2049_inf)
V1117 21:33:22.820000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.820000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.821000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.821000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.823000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.823000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
I1117 21:33:22.825000 3014252 torch/_inductor/kernel/custom_op.py:724] [0/0] Range [2049, inf]: Selected dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7
V1117 21:33:22.826000 3014252 torch/_inductor/kernel/custom_op.py:393] [0/0] Matched choice 'dynamic_range_autotuned_range_1_512_medium_sequence_impl_1' to decomposition[1] 'medium_sequence_impl'
V1117 21:33:22.827000 3014252 torch/_inductor/kernel/custom_op.py:393] [0/0] Matched choice 'dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4' to decomposition[1] 'medium_sequence_impl'
V1117 21:33:22.828000 3014252 torch/_inductor/kernel/custom_op.py:393] [0/0] Matched choice 'dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7' to decomposition[1] 'medium_sequence_impl'
I1117 21:33:22.829000 3014252 torch/_inductor/kernel/custom_op.py:751] [0/0] â Completed autotuning for 3 ranges
I1117 21:33:22.830000 3014252 torch/_inductor/kernel/custom_op.py:772] [0/0] â Generated dispatch function saved to: /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py
I1117 21:33:22.832000 3014252 torch/_inductor/kernel/custom_op.py:777] [0/0] Creating runtime dispatch using make_fx tracing
V1117 21:33:22.833000 3014252 torch/_inductor/kernel/custom_op.py:822] [0/0] Tracing dispatch function with make_fx...
V1117 21:33:22.838000 3014252 torch/_inductor/kernel/custom_op.py:834] [0/0] GraphModule created with 6 nodes
I1117 21:33:22.840000 3014252 torch/_inductor/kernel/custom_op.py:838] [0/0] Analyzing range implementations for optimization...
I1117 21:33:22.841000 3014252 torch/_inductor/kernel/custom_op.py:855] [0/0] Found 1 unique implementations across 3 ranges
I1117 21:33:22.843000 3014252 torch/_inductor/kernel/custom_op.py:861] [0/0] All ranges use same implementation, generating single kernel
V1117 21:33:22.848000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 21:33:22.849000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 21:33:22.850000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 21:33:22.850000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7fa0d03fe520>
V1117 21:33:22.851000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 21:33:22.851000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7fa0d0476980>
V1117 21:33:22.853000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 21:33:22.853000 3014252 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7fa0d0456de0>
V1117 21:33:22.855000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 21:33:22.855000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 21:33:22.856000 3014252 torch/_inductor/graph.py:1602] [0/0] lowering return (dynamic_range_140328919271200,) 
V1117 21:33:22.857000 3014252 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   name=buf0,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.862000 3014252 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.863000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   name=buf2,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.865000 3014252 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.866000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   name=buf4,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.867000 3014252 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf5', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.868000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   name=buf6,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=dynamic_range_140328919271200,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200]),
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.871000 3014252 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 21:33:22.872000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output buf6
V1117 21:33:22.873000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf5
V1117 21:33:22.873000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op5
V1117 21:33:22.874000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf4
V1117 21:33:22.874000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op4
V1117 21:33:22.875000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf3
V1117 21:33:22.875000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op3
V1117 21:33:22.876000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf2
V1117 21:33:22.876000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op2
V1117 21:33:22.877000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf1
V1117 21:33:22.877000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op1
V1117 21:33:22.878000 3014252 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf0
V1117 21:33:22.878000 3014252 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op0
I1117 21:33:22.879000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.880000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.881000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.882000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.882000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.884000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node op6 with estimated runtime 0.000214
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_arg1_1, i2)
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140328919271200, clone, mul]),
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 21:33:22.886000 3014252 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 21:33:22.887000 3014252 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_buf0
I1117 21:33:22.893000 3014252 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 21:33:22.893000 3014252 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 21:33:22.894000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 21:33:22.895000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 21:33:22.895000 3014252 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 21:33:22.897000 3014252 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_op0 with estimated runtime 0.000214
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_arg0_1, %get_index), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_arg1_1, %get_index_1), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 21:33:22.901000 3014252 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 21:33:22.902000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.903000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.904000 3014252 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 21:33:22.906000 3014252 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140328919271200_0
V1117 21:33:22.907000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/f4/cf4uw2yr74dovidcvfc46id2heybq3ocfmoqby3p6qfpiidiohjt.py
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140328919271200_default => dynamic_range_140328919271200
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140328919271200_0 = async_compile.triton('triton_poi_fused_dynamic_range_140328919271200_0', '''
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140328919271200_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140328919271200_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned(args):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_arg0_1, dynamic_range_autotuned_autotuned_arg1_1 = args
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_arg1_1, (128, ), (1, ))
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_140328919271200_default], Original ATen: [test_lib.dynamic_range_140328919271200]
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_140328919271200_0.run(dynamic_range_autotuned_autotuned_arg0_1, dynamic_range_autotuned_autotuned_arg1_1, dynamic_range_autotuned_autotuned_buf0, 65536, stream=stream0)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_arg0_1
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_arg1_1
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_buf0, )
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         arg0_1, arg1_1 = args
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg1_1, (128, ), (1, ))
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_args = [arg0_1, arg1_1]
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf6,) = dynamic_range_autotuned_autotuned(dynamic_range_autotuned_autotuned_args)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg0_1
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg1_1
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (buf6, )
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 21:33:22.911000 3014252 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 21:33:22.912000 3014252 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/fp/cfpzlrdhkoihbz5tzqgs47shcw3zzago5oktllex7tozm5r6sm2e.py
V1117 21:33:22.915000 3014252 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/fp/cfpzlrdhkoihbz5tzqgs47shcw3zzago5oktllex7tozm5r6sm2e.py
I1117 21:33:22.916000 3014252 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/fp/cfpzlrdhkoihbz5tzqgs47shcw3zzago5oktllex7tozm5r6sm2e.py
I1117 21:33:22.919000 3014252 torch/_inductor/triton_bundler.py:197] [0/0] Saving 10 statically launchable CachingAutotuners
V1117 21:33:22.920000 3014252 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 21:33:22.920000 3014252 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 21:33:22.921000 3014252 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fxstxvqjkyx2se3rg6ftveyi26f3t3kykiyerk75nc2t3hh4ilzv
V1117 21:33:22.924000 3014252 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.969s
I1117 21:33:22.925000 3014252 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
I1117 21:33:22.925000 3014252 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
I1117 21:33:22.925000 3014252 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
I1117 21:33:22.926000 3014252 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
V1117 21:33:22.967000 3014252 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_dynamic_range_140328919271200_0, get:
V1117 21:33:22.968000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 21:33:22.968000 3014252 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.969000 3014252 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_dynamic_range_140328919271200_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 21:33:22.970000 3014252 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/f4/9744a167c9b11c8979fa8a51627481cd5dbb1da648158af968e966faffc50cb5.best_config
Running test on device: cuda

=== Verifying all implementations produce equivalent results ===
  â short_impl correct for seq_len=256
  â medium_impl correct for seq_len=256
  â long_impl correct for seq_len=256
  â short_impl correct for seq_len=1024
  â medium_impl correct for seq_len=1024
  â long_impl correct for seq_len=1024
  â short_impl correct for seq_len=4096
  â medium_impl correct for seq_len=4096
  â long_impl correct for seq_len=4096

=== Testing autotuning with compilation ===
  â Compiled version produces correct results
  â Dispatch function generated with torch.cond at /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py

â All tests passed!
I1117 21:33:24.991000 3014252 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1117 21:33:24.991000 3014252 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 10, exception: 0}
I1117 21:33:24.991000 3014252 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 10, exception: 0}
I1117 21:33:24.991000 3014252 torch/_inductor/remote_cache.py:432] 
