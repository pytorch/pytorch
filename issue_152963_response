‚óè Based on my investigation of the DTensor codebase, I've found several examples where sharding propagation rules branch on input tensor shapes, which relates to the issues described in issue_152963. Here are the key examples:

  Examples of Shape-Dependent Branching in Sharding Propagation Rules

  1. new_empty_strided factory function (torch/distributed/tensor/_ops/_tensor_ops.py:225)

  if tuple(input_shape) == tuple(output_shape) and input_spec.is_sharded():
  This branches on whether input and output shapes match. If they do, it allows preserving the input's sharding; otherwise, it replicates. This could cause recompilations with dynamic shapes since different shape values will take different branches.

  2. scatter operation (torch/distributed/tensor/_ops/_tensor_ops.py:616-620)

  if len(input_shape) == len(index_shape):
      for d in range(len(input_shape)):
          if d != dim and input_shape[d] == index_shape[d]:
              sharding: PlacementList = [Shard(d), Shard(d), Shard(d), Shard(d)]
  This checks if dimensions match between input and index tensors, then determines if sharding strategies can be applied. With unbacked SymInts (data-dependent shapes), these comparisons would cause graph breaks.

  3. gather operation (torch/distributed/tensor/_ops/_tensor_ops.py:648,662-666)

  if dim < len(index_shape) and index_shape[dim] == 1:
      # special handling for size-1 dimension
      ...

  if len(input_shape) == len(index_shape):
      for d in range(len(input_shape)):
          if d != dim:
              sharding: PlacementList = [Shard(d), Shard(d), Shard(d)]
  Similar to scatter, this checks index shape properties to determine valid sharding strategies.

  4. unbind operation (torch/distributed/tensor/_ops/_tensor_ops.py:1236)

  for _ in range(input_shape[unbind_dim])
  This creates output specs based on the size of the unbind dimension. With data-dependent shapes, input_shape[unbind_dim] would be an unbacked SymInt, making this loop problematic for compile.

  5. split operation (torch/distributed/tensor/_ops/_tensor_ops.py:1172-1174)

  output_size_list = (
      size_split(input_strategy.shape[dim], split_size_or_sections)
      if isinstance(split_size_or_sections, int)
      else split_size_or_sections
  )
  The size_split function performs arithmetic on tensor dimensions (N // i, N % i) which would be problematic with unbacked SymInts from data-dependent operations.

  6. Broadcasting in pointwise ops (torch/distributed/tensor/_ops/_common_rules.py:254)

  elif input.shape[i - start_dim] == 1:
      # mark singleton dim char as a special "1" in einop rule
      singleton_counter[i] += 1
  This checks for singleton dimensions (size 1) to handle broadcasting correctly. Shape comparisons like this will guard/specialize with dynamic shapes.

  7. NLL Loss channel dimension (torch/distributed/tensor/_ops/_math_ops.py:641)

  channel_dim = 1 if len(input_shape) >= 2 else 0
  Branches on tensor rank to determine the channel dimension, causing specialization on the number of dimensions.

  8. Flatten/reshape operations (torch/distributed/tensor/_ops/_view_ops.py:584-586)

  tensor_dim_size = global_input_shape[shard_placement.dim]
  mesh_dim_size = mesh_sizes[shard_mesh_dim]
  if tensor_dim_size % mesh_dim_size != 0:
      can_shard_dim = False
  This checks if a dimension is evenly divisible by the mesh size to determine if sharding is valid. With SymInts, this modulo check could be problematic.

  Key Issues Identified

  Problem 1: Specialization overhead - All these shape comparisons will cause the compiler to generate guards and potentially create multiple specialized compiled artifacts for different shapes.

  Problem 2: Unbacked SymInts - For data-dependent operations (like nonzero, masked_select, operations with variable-length outputs), the shapes become "unbacked SymInts" and any of these branching operations would cause graph breaks during compilation.

  Problem 3: Caching brittleness - The LRU cache used for sharding propagation doesn't properly handle SymInts, leading to cache misses and overhead in both eager and compile modes.

  The issue mentions a potential solution: defining "default paths" for sharding propagation rules when shapes are not known at compile time, avoiding the need to branch on unbacked SymInts.












Great! I've completed a thorough analysis of the utility functions in torch/distributed/tensor/_ops/utils.py and their usage throughout the codebase. Here's what I found:

  Key Utility Functions with Shape-Dependent Branching

  1. is_tensor_shardable(shape, spec) (utils.py:161-178)

  What it does:
  - Checks if a tensor with given shape can be sharded according to a DTensorSpec
  - Branches on: if dim_size < shards_map[i] (line 175)
  - Returns False if any dimension is smaller than the number of shards needed

  Branching logic:
  for i, dim_size in enumerate(shape):
      if shards_map[i] > 1 and dim_size < shards_map[i]:
          return False

  Where it's used:
  - expand_to_full_mesh_op_strategy (utils.py:368) - Core infrastructure that filters out invalid sharding strategies
  - Matrix operations (_matrix_ops.py:88, 143, 215-218) - Used in _mm_like_strategy, _addmm_like_strategy, _scaled_mm_like_strategy to filter strategies

  2. is_tensor_evenly_shardable(shape, spec) (utils.py:181-194)

  What it does:
  - Checks if tensor dimensions are evenly divisible by the number of shards
  - Branches on: if dim_size % shards_map[i] != 0 (line 191)
  - Returns False if any sharded dimension isn't evenly divisible

  Branching logic:
  for i, dim_size in enumerate(shape):
      if shards_map[i] > 1 and (dim_size % shards_map[i] != 0):
          return False

  Where it's used:
  - new_empty_strided (_tensor_ops.py:230) - Checks if non-replicate sharding is supported
  - nll_loss with MEAN reduction (_math_ops.py:700) - Validates that mean reduction won't be biased due to uneven sharding

  3. is_tensor_evenly_shardable_on_dim(shape, spec, dim) (utils.py:197-210)

  What it does:
  - Checks if a specific dimension is evenly divisible by shards
  - Branches on: return shape[dim] % num_shards == 0 (line 210)

  Where it's used:
  - Reduction operations with averaging (_math_ops.py:289) - Ensures avg reductions don't produce biased results on unevenly sharded tensors

  4. infer_broadcast_dims_map(common_shape, input_shape) (utils.py:223-234)

  What it does:
  - Maps dimensions after broadcasting
  - Branches on: if input_shape[idx] == common_shape[idx] (line 232)
  - Creates a mapping for how dimensions align after broadcast

  Branching logic:
  for idx in range(-1, -1 - input_ndim, -1):
      if input_shape[idx] == common_shape[idx]:
          broadcast_dims_map[common_ndim + idx] = input_ndim + idx

  Where it's used:
  - addmm/baddbmm operations (_matrix_ops.py:137) - Handles broadcasting of the bias/self tensor
  - Pointwise operations (_pointwise_ops.py:580) - Handles broadcasting across all pointwise ops

  5. expand_to_full_mesh_op_strategy (utils.py:291-389)

  Core infrastructure function - This is where many shape checks converge:

  Line 368-371:
  if not all(
      is_tensor_shardable(inp.shape, s)
      for inp, s in zip(input_args_strategy, input_specs)
  ):
      continue  # Skip this strategy

  This filters out strategies where tensors cannot be sharded according to the proposed spec. This branching affects MANY operations because expand_to_full_mesh_op_strategy is used by 15+ different operations.

  Broader Context and Impact

  Problem Scope

  These utility functions are fundamental infrastructure used throughout DTensor's sharding propagation system. The shape-dependent branching happens at multiple levels:

  1. Strategy Generation Phase: When generating possible sharding strategies for an operation
  2. Strategy Filtering Phase: When filtering out invalid/impossible strategies
  3. Cost Calculation Phase: When computing redistribution costs

  Operations Affected

  Based on my search, expand_to_full_mesh_op_strategy (which internally uses is_tensor_shardable) is called by:
  - Matrix operations: dot, mm, addmm, bmm, baddbmm, scaled_dot_product_attention, etc.
  - Tensor operations: scatter, gather, bucketize
  - Embedding operations: embedding, embedding_bag
  - Math operations: Reductions, normalization

  Dynamic Shapes Issues

  All these utilities have problems with dynamic shapes:

  Issue 1: Modulo operations with SymInts
  dim_size % shards_map[i] != 0  # Problematic with unbacked SymInts
  With data-dependent shapes (unbacked SymInts), this modulo check would cause a graph break during compilation.

  Issue 2: Comparison operations
  if dim_size < shards_map[i]  # Guards will be generated
  if input_shape[idx] == common_shape[idx]  # Specialization
  if prod(scale_self_strategy.shape) == 1  # Scalar detection branching
  All these comparisons cause the compiler to specialize and generate guards, potentially creating multiple compiled artifacts.

  Issue 3: Strategy filtering side effects
  When these checks fail, strategies are filtered out (via continue statements). With SymInts, the compiler might not be able to determine at compile time which strategies are valid, leading to either:
  - Conservative behavior (always replicate)
  - Graph breaks (if unbacked SymInts are involved)

  Caching Implications

  The issue mentions that "sharding prop uses an lru cache to reduce cpu overhead" and "this cache is problematic because it doesn't support symints." These utility functions are called during sharding propagation, so:
  - Every unique shape value causes a cache miss
  - With data-dependent operations producing different shapes on each iteration, cache becomes useless
  - This creates significant eager-mode overhead

  Potential Solutions

  As mentioned in the issue, possible approaches include:
  1. Default paths: Define "obvious" sharding rules that don't branch on shape (e.g., always replicate for data-dependent ops)
  2. Compiler-aware branching: Check if inside compiler and skip checks when shapes are unbacked SymInts
  3. Relaxed validation: Accept uneven sharding more broadly and handle it at runtime

  The challenge is that these utility functions enforce correctness constraints - they prevent invalid sharding strategies that would produce wrong results. Simply removing the checks could lead to correctness issues.
