diff --git a/torch/distributed/tensor/_api.py b/torch/distributed/tensor/_api.py
index 3cbc3c71488..2605295e6c5 100644
--- a/torch/distributed/tensor/_api.py
+++ b/torch/distributed/tensor/_api.py
@@ -363,6 +363,70 @@ class DTensor(torch.Tensor):
             placements=spec.placements,
         )
 
+    @classmethod
+    def __torch_function__(cls, func, types, args, kwargs=None):
+        out = super().__torch_function__(func, types, args, kwargs or {})
+
+        class DTensorGradPlacementHook:
+            """Hook to redistribute gradients to conjugate placements during backward pass."""
+
+            def __init__(self, grad_fn, forward_args):
+                self.placements = [
+                    (arg._spec.placements if isinstance(arg, DTensor) else None)
+                    for arg in forward_args
+                ]
+                self.grad_fn = grad_fn
+                self.grad_fn.register_hook(self._post)
+
+            def _post(self, grad_inputs, grad_outputs):
+                """Redistribute grad_inputs to conjugate placements."""
+                result = []
+                for target_placements, grad_input in zip(
+                    self._primal_to_conjugate(self.placements), grad_inputs
+                ):
+                    if isinstance(grad_input, DTensor) and target_placements is not None:
+                        redistributed = grad_input.redistribute(
+                            device_mesh=grad_input.device_mesh,
+                            placements=target_placements
+                        )
+                        result.append(redistributed)
+
+                        # Debug code
+                        import torch.distributed as dist
+                        if dist.get_rank() == 0 and target_placements != grad_input._spec.placements:
+                            print(f"DTensorGradPlacementHook::redistribute - grad_fn={self.grad_fn} {target_placements=} current_placements={grad_input._spec.placements}")
+
+                    else:
+                        result.append(grad_input)
+                return tuple(result)
+
+            @staticmethod
+            def _primal_to_conjugate(placements):
+                """Convert primal placements to conjugate placements for gradient flow."""
+                mapped = []
+                for p in placements:
+                    if p is None:
+                        mapped.append(None)
+                    elif isinstance(p, Shard):
+                        mapped.append(p)
+                    elif isinstance(p, Partial):
+                        mapped.append(Replicate())
+                    elif isinstance(p, Replicate):
+                        mapped.append(Partial("sum"))
+                    else:
+                        mapped.append(p)
+                return tuple(mapped)
+
+        def _register_grad_placment_hook(t):
+            if isinstance(t, torch.Tensor) and t.grad_fn is not None:
+                _ = DTensorGradPlacementHook(t.grad_fn, args)
+            return t
+
+        from torch.utils._pytree import tree_map
+        tree_map(_register_grad_placment_hook, out)
+        return out
+
+
     @classmethod
     def __torch_dispatch__(cls, func, types, args=(), kwargs=None):  # type: ignore[override]
         # We just need to have an implementation here; the __torch_dispatch__ machinery
