The state of DTensor + compile + dynamic shapes today is roughly:

(1) for generic "pt2-friendly" tensor subclasses, we support compiling them with dynamic shapes. This includes cases where both the outer subclass shape and it's inner tensor shape(s) vary independently.

(2) At the same time, dynamic shapes support imposes some extra requirements on tensor subclasses that DTensor does not fully meet today. One way to think of it is that any place where DTensor's code handles a tensor size, the code needs to be written in a way that is aware that this size might be a torch.SymInt object instead of a plain int.

There are a few cases where DTensor handling for SymInts needs some additional work:

(1) DTensorSpec. Today, this metadata holds the raw sizes/strides of the outer tensor shape. This is mainly a problem, because compile expects any subclass metadata to be able to be treated as a constant (SymInts are not supported). In particular, compile generates guards on the exact value of this metadata, and we will recompile if you have a graph input that is a tensor subclass with slightly different metadata.

There are a few ways we could fix this. One easier option might be the following: subclass authors can specify a custom method that allows them to customize how dynamo generates metadata guards. We could try having DTensor define its metadata guards to not include the outer_size/stride arguments on the DTensorSpec, and see if that is enough to avoid problems.

Hopefully that is enough. A more "pt2-friendly" option that would also be a more invasive DTensor change would be to remove outer_size/stride from DTensorSpec entirely, and instead require any code that needs to access this outer_size/stride to take in the actual DTensor itself as an argument, to read the sizes/strides from.

(2) sharding propagation caching. In eager mode, sharding prop uses an lru cache to reduce cpu overhead. Under compile, this cache is problematic because it doesn't support symints. We have some logic in DTensor today to check for symints and skip the cache if there are any, but this logic is brittle. Instead, we should probably have all DTensor caching logic branch on whether or not we are currently inside of the compiler

(3) sharding propagation rules themselves. Some of these rules are written to branch on input tensor shapes. This has two potential problems for compile. The first problem is that any branching will cause compile to specialize and emit guards, forcing a recompile if the condition fails. This can be desirable in some cases but not others, depending on whether we would prefer to generate one generic compiled artifact for some compiled DTensor code, or if we want specialized artifacts for different shpes.

The second problem is around data dependent shapes. If there is any data-dependent code in the model, then this will manifest inside of compile as tensor's whose shape are "unbacked symints", meaning they have no backing "hint" that compile can take advantage of. When unbacked symints show up inside of the compiler, any code that branches on these shapes will result in a graph break. One way to deal with this is to audit places where we branch on shape, and agree on a "default path" that we can take for some of our sharding prop rules, if the shape is not actually known at compile time.

There are pro's and con's to this approach. In the longer run, we may want to generate specialized compiled artifacts for different value ranges of these shapes.

There is a related issue here that @tianyu-l ran into around dynamic shapes and sharding prop in eager mode: when dealing with data-dependent operations, the existing sharding prop caching will cache miss on every new shape, incurring significant eager overhead. It might be worth thinking about how to design the compiler-related improvements in the context of this problem. For example: one option might be to hardcode some "obvious" sharding prop rules for TP, that don't require specializing on shape, and use them when we know our shapes are data dependent. I have a very rough prototype of this here: #150582
