


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.cuda.memory &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/cuda/memory.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/cuda/memory.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../cuda.html">torch.cuda</a> &gt;</li>
        
      <li>torch.cuda.memory</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.cuda.memory</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">collections</span>
<span class="kn">import</span> <span class="nn">contextlib</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">is_initialized</span><span class="p">,</span> <span class="n">_get_device_index</span><span class="p">,</span> <span class="n">_lazy_init</span>
<span class="kn">from</span> <span class="nn">torch.types</span> <span class="kn">import</span> <span class="n">Device</span>

<span class="k">def</span> <span class="nf">_host_allocator</span><span class="p">():</span>
    <span class="n">_lazy_init</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaHostAllocator</span><span class="p">()</span>


<span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
<span class="k">def</span> <span class="nf">_free_mutex</span><span class="p">():</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_lock_mutex</span><span class="p">()</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">yield</span>
    <span class="k">finally</span><span class="p">:</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_unlock_mutex</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">caching_allocator_alloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs a memory allocation using the CUDA memory allocator.</span>

<span class="sd">    Memory is allocated for a given device and a stream, this</span>
<span class="sd">    function is intended to be used for interoperability with other</span>
<span class="sd">    frameworks. Allocated memory is released through</span>
<span class="sd">    :func:`~torch.cuda.caching_allocator_delete`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        size (int): number of bytes to be allocated.</span>
<span class="sd">        device (torch.device or int, optional): selected device. If it is</span>
<span class="sd">            ``None`` the default CUDA device is used.</span>
<span class="sd">        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then</span>
<span class="sd">            the default stream for the selected device is used.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">device</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_device</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stream</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">current_stream</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">streams</span><span class="o">.</span><span class="n">Stream</span><span class="p">):</span>
        <span class="n">stream</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">cuda_stream</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">stream</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;Invalid type for stream argument, must be &#39;</span>
                        <span class="s1">&#39;`torch.cuda.Stream` or `int` representing a pointer &#39;</span>
                        <span class="s1">&#39;to a exisiting stream&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaCachingAllocator_raw_alloc</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">stream</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">caching_allocator_delete</span><span class="p">(</span><span class="n">mem_ptr</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deletes memory allocated using the CUDA memory allocator.</span>

<span class="sd">    Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.</span>
<span class="sd">    is freed here. The associated device and stream are tracked inside</span>
<span class="sd">    the allocator.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        mem_ptr (int): memory address to be freed by the allocator.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_cudaCachingAllocator_raw_delete</span><span class="p">(</span><span class="n">mem_ptr</span><span class="p">)</span>


<div class="viewcode-block" id="empty_cache"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.empty_cache">[docs]</a><span class="k">def</span> <span class="nf">empty_cache</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Releases all unoccupied cached memory currently held by the caching</span>
<span class="sd">    allocator so that those can be used in other GPU application and visible in</span>
<span class="sd">    `nvidia-smi`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        :func:`~torch.cuda.empty_cache` doesn&#39;t increase the amount of GPU</span>
<span class="sd">        memory available for PyTorch. However, it may help reduce fragmentation</span>
<span class="sd">        of GPU memory in certain cases. See :ref:`cuda-memory-management` for</span>
<span class="sd">        more details about GPU memory management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">is_initialized</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_emptyCache</span><span class="p">()</span></div>


<div class="viewcode-block" id="memory_stats"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_stats">[docs]</a><span class="k">def</span> <span class="nf">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a dictionary of CUDA memory allocator statistics for a</span>
<span class="sd">    given device.</span>

<span class="sd">    The return value of this function is a dictionary of statistics, each of</span>
<span class="sd">    which is a non-negative integer.</span>

<span class="sd">    Core statistics:</span>

<span class="sd">    - ``&quot;allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of allocation requests received by the memory allocator.</span>
<span class="sd">    - ``&quot;allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of allocated memory.</span>
<span class="sd">    - ``&quot;segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of reserved segments from ``cudaMalloc()``.</span>
<span class="sd">    - ``&quot;reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of reserved memory.</span>
<span class="sd">    - ``&quot;active.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of active memory blocks.</span>
<span class="sd">    - ``&quot;active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of active memory.</span>
<span class="sd">    - ``&quot;inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      number of inactive, non-releasable memory blocks.</span>
<span class="sd">    - ``&quot;inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}&quot;``:</span>
<span class="sd">      amount of inactive, non-releasable memory.</span>

<span class="sd">    For these core statistics, values are broken down as follows.</span>

<span class="sd">    Pool type:</span>

<span class="sd">    - ``all``: combined statistics across all memory pools.</span>
<span class="sd">    - ``large_pool``: statistics for the large allocation pool</span>
<span class="sd">      (as of October 2019, for size &gt;= 1MB allocations).</span>
<span class="sd">    - ``small_pool``: statistics for the small allocation pool</span>
<span class="sd">      (as of October 2019, for size &lt; 1MB allocations).</span>

<span class="sd">    Metric type:</span>

<span class="sd">    - ``current``: current value of this metric.</span>
<span class="sd">    - ``peak``: maximum value of this metric.</span>
<span class="sd">    - ``allocated``: historical total increase in this metric.</span>
<span class="sd">    - ``freed``: historical total decrease in this metric.</span>

<span class="sd">    In addition to the core statistics, we also provide some simple event</span>
<span class="sd">    counters:</span>

<span class="sd">    - ``&quot;num_alloc_retries&quot;``: number of failed ``cudaMalloc`` calls that</span>
<span class="sd">      result in a cache flush and retry.</span>
<span class="sd">    - ``&quot;num_ooms&quot;``: number of out-of-memory errors thrown.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistics for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">_recurse_add_to_result</span><span class="p">(</span><span class="n">prefix</span><span class="p">,</span> <span class="n">obj</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">obj</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">prefix</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">prefix</span> <span class="o">+=</span> <span class="s2">&quot;.&quot;</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">obj</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">_recurse_add_to_result</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">result</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">prefix</span><span class="p">,</span> <span class="n">obj</span><span class="p">))</span>

    <span class="n">stats</span> <span class="o">=</span> <span class="n">memory_stats_as_nested_dict</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">_recurse_add_to_result</span><span class="p">(</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">stats</span><span class="p">)</span>
    <span class="n">result</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">collections</span><span class="o">.</span><span class="n">OrderedDict</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">memory_stats_as_nested_dict</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary.&quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_memoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_accumulated_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the &quot;accumulated&quot; (historical) stats tracked by the CUDA memory allocator.</span>

<span class="sd">    See :func:`~torch.cuda.memory_stats` for details. Accumulated stats correspond to</span>
<span class="sd">    the `&quot;allocated&quot;` and `&quot;freed&quot;` keys in each individual stat dict, as well as</span>
<span class="sd">    `&quot;num_alloc_retries&quot;` and `&quot;num_ooms&quot;`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_resetAccumulatedMemoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the &quot;peak&quot; stats tracked by the CUDA memory allocator.</span>

<span class="sd">    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the</span>
<span class="sd">    `&quot;peak&quot;` key in each individual stat dict.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_resetPeakMemoryStats</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>


<div class="viewcode-block" id="reset_max_memory_allocated"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.reset_max_memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">reset_max_memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the starting point in tracking maximum GPU memory occupied by</span>
<span class="sd">    tensors for a given device.</span>

<span class="sd">    See :func:`~torch.cuda.max_memory_allocated` for details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets</span>
<span class="sd">        /all/ peak memory stats.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, &quot;</span>
        <span class="s2">&quot;which resets /all/ peak memory stats.&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="reset_max_memory_cached"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.reset_max_memory_cached">[docs]</a><span class="k">def</span> <span class="nf">reset_max_memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets the starting point in tracking maximum GPU memory managed by the</span>
<span class="sd">    caching allocator for a given device.</span>

<span class="sd">    See :func:`~torch.cuda.max_memory_cached` for details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets</span>
<span class="sd">        /all/ peak memory stats.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.reset_max_memory_cached now calls torch.cuda.reset_peak_memory_stats, &quot;</span>
        <span class="s2">&quot;which resets /all/ peak memory stats.&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">reset_peak_memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_allocated"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current GPU memory occupied by tensors in bytes for a given</span>
<span class="sd">    device.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        This is likely less than the amount shown in `nvidia-smi` since some</span>
<span class="sd">        unused memory can be held by the caching allocator and some context</span>
<span class="sd">        needs to be created on GPU. See :ref:`cuda-memory-management` for more</span>
<span class="sd">        details about GPU memory management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="s2">&quot;allocated_bytes.all.current&quot;</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_memory_allocated"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.max_memory_allocated">[docs]</a><span class="k">def</span> <span class="nf">max_memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum GPU memory occupied by tensors in bytes for a given</span>
<span class="sd">    device.</span>

<span class="sd">    By default, this returns the peak allocated memory since the beginning of</span>
<span class="sd">    this program. :func:`~torch.cuda.reset_peak_stats` can be used to</span>
<span class="sd">    reset the starting point in tracking this metric. For example, these two</span>
<span class="sd">    functions can measure the peak allocated memory usage of each iteration in a</span>
<span class="sd">    training loop.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="s2">&quot;allocated_bytes.all.peak&quot;</span><span class="p">]</span></div>


<div class="viewcode-block" id="memory_reserved"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_reserved">[docs]</a><span class="k">def</span> <span class="nf">memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the current GPU memory managed by the caching allocator in bytes</span>
<span class="sd">    for a given device.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="s2">&quot;reserved_bytes.all.current&quot;</span><span class="p">]</span></div>


<div class="viewcode-block" id="max_memory_reserved"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.max_memory_reserved">[docs]</a><span class="k">def</span> <span class="nf">max_memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the maximum GPU memory managed by the caching allocator in bytes</span>
<span class="sd">    for a given device.</span>

<span class="sd">    By default, this returns the peak cached memory since the beginning of this</span>
<span class="sd">    program. :func:`~torch.cuda.reset_peak_stats` can be used to reset</span>
<span class="sd">    the starting point in tracking this metric. For example, these two functions</span>
<span class="sd">    can measure the peak cached memory amount of each iteration in a training</span>
<span class="sd">    loop.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            statistic for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)[</span><span class="s2">&quot;reserved_bytes.all.peak&quot;</span><span class="p">]</span></div>


<div class="viewcode-block" id="memory_cached"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_cached">[docs]</a><span class="k">def</span> <span class="nf">memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deprecated; see :func:`~torch.cuda.memory_reserved`.&quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.memory_cached has been renamed to torch.cuda.memory_reserved&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_memory_cached"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.max_memory_cached">[docs]</a><span class="k">def</span> <span class="nf">max_memory_cached</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Deprecated; see :func:`~torch.cuda.max_memory_reserved`.&quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
        <span class="s2">&quot;torch.cuda.max_memory_cached has been renamed to torch.cuda.max_memory_reserved&quot;</span><span class="p">,</span>
        <span class="ne">FutureWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">max_memory_reserved</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span></div>


<div class="viewcode-block" id="memory_snapshot"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_snapshot">[docs]</a><span class="k">def</span> <span class="nf">memory_snapshot</span><span class="p">():</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a snapshot of the CUDA memory allocator state across all devices.</span>

<span class="sd">    Interpreting the output of this function requires familiarity with the</span>
<span class="sd">    memory allocator internals.</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_cuda_memorySnapshot</span><span class="p">()</span></div>


<div class="viewcode-block" id="memory_summary"><a class="viewcode-back" href="../../../cuda.html#torch.cuda.memory_summary">[docs]</a><span class="k">def</span> <span class="nf">memory_summary</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">abbreviated</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a human-readable printout of the current memory allocator</span>
<span class="sd">    statistics for a given device.</span>

<span class="sd">    This can be useful to display periodically during training, or when</span>
<span class="sd">    handling out-of-memory exceptions.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            printout for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>
<span class="sd">        abbreviated (bool, optional): whether to return an abbreviated summary</span>
<span class="sd">            (default: False).</span>

<span class="sd">    .. note::</span>
<span class="sd">        See :ref:`cuda-memory-management` for more details about GPU memory</span>
<span class="sd">        management.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">stats</span> <span class="o">=</span> <span class="n">memory_stats</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_size</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">pref_sz</span><span class="p">):</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;B &quot;</span><span class="p">,</span> <span class="s2">&quot;KB&quot;</span><span class="p">,</span> <span class="s2">&quot;MB&quot;</span><span class="p">,</span> <span class="s2">&quot;GB&quot;</span><span class="p">,</span> <span class="s2">&quot;TB&quot;</span><span class="p">,</span> <span class="s2">&quot;PB&quot;</span><span class="p">]</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">new_prefix</span> <span class="ow">in</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="n">pref_sz</span> <span class="o">&lt;</span> <span class="mi">768</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">new_prefix</span>
            <span class="n">sz</span> <span class="o">//=</span> <span class="mi">1024</span>
            <span class="n">pref_sz</span> <span class="o">/=</span> <span class="mi">1024</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{:7d}</span><span class="s2"> </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_format_count</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">pref_cnt</span><span class="p">):</span>
        <span class="n">prefixes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot; &quot;</span><span class="p">,</span> <span class="s2">&quot;K&quot;</span><span class="p">,</span> <span class="s2">&quot;M&quot;</span><span class="p">]</span>
        <span class="n">prefix</span> <span class="o">=</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">new_prefix</span> <span class="ow">in</span> <span class="n">prefixes</span><span class="p">[</span><span class="mi">1</span><span class="p">:]:</span>
            <span class="k">if</span> <span class="n">pref_cnt</span> <span class="o">&lt;</span> <span class="mi">750</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">new_prefix</span>
            <span class="n">cnt</span> <span class="o">//=</span> <span class="mi">1000</span>
            <span class="n">pref_cnt</span> <span class="o">/=</span> <span class="mi">1000</span>
        <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{:7d}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">cnt</span><span class="p">,</span> <span class="n">prefix</span><span class="p">)</span>

    <span class="n">metrics_to_display</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">(</span><span class="s2">&quot;allocated_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Allocated memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;active_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Active memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;reserved_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU reserved memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;inactive_split_bytes&quot;</span><span class="p">,</span> <span class="s2">&quot;Non-releasable memory&quot;</span><span class="p">,</span> <span class="n">_format_size</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;allocation&quot;</span><span class="p">,</span> <span class="s2">&quot;Allocations&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;active&quot;</span><span class="p">,</span> <span class="s2">&quot;Active allocs&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;segment&quot;</span><span class="p">,</span> <span class="s2">&quot;GPU reserved segments&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
        <span class="p">(</span><span class="s2">&quot;inactive_split&quot;</span><span class="p">,</span> <span class="s2">&quot;Non-releasable allocs&quot;</span><span class="p">,</span> <span class="n">_format_count</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">{_:16}</span><span class="s2"> PyTorch CUDA memory summary, device ID </span><span class="si">{device:&lt;17d}</span><span class="s2"> &quot;</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;  </span><span class="si">{_:9}</span><span class="s2"> CUDA OOMs: </span><span class="si">{num_ooms:&lt;12d}</span><span class="s2"> | </span><span class="si">{_:6}</span><span class="s2"> cudaMalloc retries: </span><span class="si">{num_alloc_retries:&lt;8d}</span><span class="s2">  &quot;</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  &quot;</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">metric_key</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">,</span> <span class="n">formatter</span> <span class="ow">in</span> <span class="n">metrics_to_display</span><span class="p">:</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;-&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>
        <span class="n">submetrics</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;all&quot;</span><span class="p">,</span> <span class="n">metric_name</span><span class="p">)]</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">abbreviated</span><span class="p">:</span>
            <span class="n">submetrics</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;large_pool&quot;</span><span class="p">,</span> <span class="s2">&quot;      from large pool&quot;</span><span class="p">))</span>
            <span class="n">submetrics</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="s2">&quot;small_pool&quot;</span><span class="p">,</span> <span class="s2">&quot;      from small pool&quot;</span><span class="p">))</span>

        <span class="n">current_prefval</span><span class="p">,</span> <span class="n">peak_prefval</span><span class="p">,</span> <span class="n">allocated_prefval</span><span class="p">,</span> <span class="n">freed_prefval</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

        <span class="k">for</span> <span class="n">submetric_key</span><span class="p">,</span> <span class="n">submetric_name</span> <span class="ow">in</span> <span class="n">submetrics</span><span class="p">:</span>
            <span class="n">prefix</span> <span class="o">=</span> <span class="n">metric_key</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span> <span class="o">+</span> <span class="n">submetric_key</span> <span class="o">+</span> <span class="s2">&quot;.&quot;</span>

            <span class="n">current</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;current&quot;</span><span class="p">]</span>
            <span class="n">peak</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;peak&quot;</span><span class="p">]</span>
            <span class="n">allocated</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;allocated&quot;</span><span class="p">]</span>
            <span class="n">freed</span> <span class="o">=</span> <span class="n">stats</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s2">&quot;freed&quot;</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">current_prefval</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">current_prefval</span> <span class="o">=</span> <span class="n">current</span>
                <span class="n">peak_prefval</span> <span class="o">=</span> <span class="n">peak</span>
                <span class="n">allocated_prefval</span> <span class="o">=</span> <span class="n">allocated</span>
                <span class="n">freed_prefval</span> <span class="o">=</span> <span class="n">freed</span>

            <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot; </span><span class="si">{:&lt;21}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> | </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">submetric_name</span><span class="p">,</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">current</span><span class="p">,</span> <span class="n">current_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">peak</span><span class="p">,</span> <span class="n">peak_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">allocated</span><span class="p">,</span> <span class="n">allocated_prefval</span><span class="p">),</span>
                <span class="n">formatter</span><span class="p">(</span><span class="n">freed</span><span class="p">,</span> <span class="n">freed_prefval</span><span class="p">)),</span>
            <span class="p">)</span>

    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;=&quot;</span> <span class="o">*</span> <span class="mi">75</span><span class="p">)</span>

    <span class="n">fmt_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;_&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="n">device</span><span class="p">}</span>
    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">fmt_dict</span><span class="p">[</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">)]</span> <span class="o">=</span> <span class="n">v</span>
    <span class="k">return</span> <span class="s2">&quot;|&quot;</span> <span class="o">+</span> <span class="s2">&quot;|</span><span class="se">\n</span><span class="s2">|&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="n">fmt_dict</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;|</span><span class="se">\n</span><span class="s2">&quot;</span></div>


<span class="k">def</span> <span class="nf">list_gpu_processes</span><span class="p">(</span><span class="n">device</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Device</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns a human-readable printout of the running processes</span>
<span class="sd">    and their GPU memory use for a given device.</span>

<span class="sd">    This can be useful to display periodically during training, or when</span>
<span class="sd">    handling out-of-memory exceptions.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        device (torch.device or int, optional): selected device. Returns</span>
<span class="sd">            printout for the current device, given by :func:`~torch.cuda.current_device`,</span>
<span class="sd">            if :attr:`device` is ``None`` (default).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="kn">import</span> <span class="nn">pynvml</span>  <span class="c1"># type: ignore</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">return</span><span class="p">(</span><span class="s2">&quot;pynvml module not found, please install pynvml&quot;</span><span class="p">)</span>
    <span class="kn">from</span> <span class="nn">pynvml</span> <span class="kn">import</span> <span class="n">NVMLError_DriverNotLoaded</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlInit</span><span class="p">()</span>
    <span class="k">except</span> <span class="n">NVMLError_DriverNotLoaded</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">(</span><span class="s2">&quot;cuda driver can&#39;t be loaded, is cuda enabled?&quot;</span><span class="p">)</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">_get_device_index</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">handle</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetHandleByIndex</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">procs</span> <span class="o">=</span> <span class="n">pynvml</span><span class="o">.</span><span class="n">nvmlDeviceGetComputeRunningProcesses</span><span class="p">(</span><span class="n">handle</span><span class="p">)</span>
    <span class="n">lines</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GPU:</span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">procs</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;no processes are running&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">procs</span><span class="p">:</span>
        <span class="n">mem</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">usedGpuMemory</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">)</span>
        <span class="n">lines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;process </span><span class="si">{</span><span class="n">p</span><span class="o">.</span><span class="n">pid</span><span class="si">:</span><span class="s2">&gt;10d</span><span class="si">}</span><span class="s2"> uses </span><span class="si">{</span><span class="n">mem</span><span class="si">:</span><span class="s2">&gt;12.3f</span><span class="si">}</span><span class="s2"> MB GPU memory&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">lines</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>