


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.tensor &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/tensor.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/tensor.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../index.html">Module code</a> &gt;</li>
        
          <li><a href="../torch.html">torch</a> &gt;</li>
        
      <li>torch.tensor</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.tensor</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch._C</span> <span class="k">as</span> <span class="nn">_C</span>
<span class="kn">from</span> <span class="nn">torch._namedtensor_internals</span> <span class="kn">import</span> <span class="n">update_names</span><span class="p">,</span> <span class="n">check_serializing_named_tensor</span><span class="p">,</span> <span class="n">resolve_ellipsis</span>
<span class="kn">from</span> <span class="nn">torch._namedtensor_internals</span> <span class="kn">import</span> <span class="n">unzip_namedshape</span><span class="p">,</span> <span class="n">single_ellipsis_index</span><span class="p">,</span> <span class="n">is_ellipsis</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">OrderedDict</span>
<span class="kn">import</span> <span class="nn">torch.utils.hooks</span> <span class="k">as</span> <span class="nn">hooks</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_add_docstr</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">from</span> <span class="nn">numbers</span> <span class="kn">import</span> <span class="n">Number</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span>


<span class="k">def</span> <span class="nf">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="c1"># functools.wraps doesn&#39;t work well with methods in python 2</span>
    <span class="n">method_assignments</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;__name__&#39;</span><span class="p">,</span> <span class="s1">&#39;__doc__&#39;</span><span class="p">)</span>
    <span class="n">assigned</span> <span class="o">=</span> <span class="n">functools</span><span class="o">.</span><span class="n">WRAPPER_ASSIGNMENTS</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">assigned</span><span class="o">=</span><span class="n">assigned</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">args</span><span class="p">)</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">wrapped</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>
    <span class="k">return</span> <span class="n">wrapped</span>


<span class="c1"># NB: If you subclass Tensor, and want to share the subclassed class</span>
<span class="c1"># across processes, you must also update torch/multiprocessing/reductions.py</span>
<span class="c1"># to define a ForkingPickler serialization mode for the class.</span>
<span class="c1">#</span>
<span class="c1"># NB: If you add a new method to Tensor, you must update</span>
<span class="c1"># torch/__init__.py.in to add a type annotation for your method;</span>
<span class="c1"># otherwise, it will not show up in autocomplete.</span>
<span class="k">class</span> <span class="nc">Tensor</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__deepcopy__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">):</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">memo</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;Only Tensors created explicitly by the user &quot;</span>
                               <span class="s2">&quot;(graph leaves) support the deepcopy protocol at the moment&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">in</span> <span class="n">memo</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xla&#39;</span><span class="p">:</span>
                <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_storage</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">__deepcopy__</span><span class="p">(</span><span class="n">memo</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
                    <span class="c1"># quantizer_params can be different type based on torch attribute</span>
                    <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">()</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">):</span>
                        <span class="n">quantizer_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">(),</span> \
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span> \
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span> \
                            <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">()</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported qscheme </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2"> in deepcopy&quot;</span><span class="p">)</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">(</span>
                        <span class="n">new_storage</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                        <span class="n">quantizer_params</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">new_tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">new</span><span class="p">()</span>
                    <span class="n">new_tensor</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="n">new_storage</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
                    <span class="n">new_tensor</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">memo</span><span class="p">[</span><span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)]</span> <span class="o">=</span> <span class="n">new_tensor</span>
            <span class="k">return</span> <span class="n">new_tensor</span>

    <span class="k">def</span> <span class="nf">__reduce_ex__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__reduce_ex__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">proto</span><span class="p">)</span>
        <span class="n">check_serializing_named_tensor</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">hooks</span><span class="o">.</span><span class="n">warn_if_has_hooks</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">backward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="c1"># Note: Numpy array is chosen to be the rebuild component for XLA Tensor.</span>
        <span class="c1"># We considered a few options:</span>
        <span class="c1"># 1. CPU tensor can&#39;t be used here.</span>
        <span class="c1">#    Otherwise in torch.load CPU storage is reconstructed with randomly</span>
        <span class="c1">#    initialized data, moved onto XLA device, and then storage is updated</span>
        <span class="c1">#    to the serialized content. This works perfectly for CPU/CUDA but not XLA.</span>
        <span class="c1">#    XLA tensor is disconnected with storage so it doesn&#39;t get the update.</span>
        <span class="c1"># 2. Python list is not a good fit due to performance reason.</span>
        <span class="c1">#    `tolist()` converts every single element in the tensor into python objects</span>
        <span class="c1">#    and serialize them one by one.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">type</span> <span class="o">==</span> <span class="s1">&#39;xla&#39;</span><span class="p">:</span>
            <span class="n">arg_xla</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
                       <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span>
                       <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_xla_tensor</span><span class="p">,</span> <span class="n">arg_xla</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
            <span class="c1"># quantizer_params can be different type based on torch attribute</span>
            <span class="n">quantizer_params</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">qscheme</span><span class="p">,</span> <span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">:</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">q_scale</span><span class="p">(),</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">q_zero_point</span><span class="p">())</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine_float_qparams</span><span class="p">):</span>
                <span class="c1"># convert scales and zero points to tuple to avoid recursive calls</span>
                <span class="c1"># when/if we get multi-axis quantized tensors in the future, the shape</span>
                <span class="c1"># is recoverable from the main tensor shape</span>
                <span class="n">quantizer_params</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">per_channel_affine</span><span class="p">,</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_scales</span><span class="p">(),</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_zero_points</span><span class="p">(),</span>
                                    <span class="bp">self</span><span class="o">.</span><span class="n">q_per_channel_axis</span><span class="p">())</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Serialization is not supported for tensors of type </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">qscheme</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="n">args_qtensor</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">(),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                            <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                            <span class="n">quantizer_params</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                            <span class="n">backward_hooks</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_qtensor</span><span class="p">,</span> <span class="n">args_qtensor</span><span class="p">)</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layout</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">sparse_coo</span><span class="p">:</span>
                <span class="n">args_sparse</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                               <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_indices</span><span class="p">(),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">_values</span><span class="p">(),</span>
                                <span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s1">&#39;sparse tensor __reduce_ex__ for layout `</span><span class="si">%s</span><span class="s1">`&#39;</span> <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layout</span><span class="p">))</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_sparse_tensor</span><span class="p">,</span> <span class="n">args_sparse</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">storage_offset</span><span class="p">(),</span>
                    <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">(),</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span>
                    <span class="n">backward_hooks</span><span class="p">)</span>  <span class="c1"># previously was self._backward_hooks</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_utils</span><span class="o">.</span><span class="n">_rebuild_tensor_v2</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="c1"># Warning: this method is NOT called when you torch.load() a tensor;</span>
        <span class="c1"># that is managed by _rebuild_tensor_v2</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;__setstate__ can be only called on leaf Tensors&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Tensor</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_</span><span class="p">(</span><span class="o">*</span><span class="n">state</span><span class="p">)</span>
            <span class="k">return</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="c1"># legacy serialization of Variable</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">state</span> <span class="o">=</span> <span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="c1"># The setting of _backward_hooks is expected to be a no-op.</span>
        <span class="c1"># See Note [Don&#39;t serialize hooks]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">state</span>

    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__repr__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="c1"># All strings are unicode in Python 3.</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_tensor_str</span><span class="o">.</span><span class="n">_str</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.backward"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.backward">[docs]</a>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes the gradient of current tensor w.r.t. graph leaves.</span>

<span class="sd">        The graph is differentiated using the chain rule. If the tensor is</span>
<span class="sd">        non-scalar (i.e. its data has more than one element) and requires</span>
<span class="sd">        gradient, the function additionally requires specifying ``gradient``.</span>
<span class="sd">        It should be a tensor of matching type and location, that contains</span>
<span class="sd">        the gradient of the differentiated function w.r.t. ``self``.</span>

<span class="sd">        This function accumulates gradients in the leaves - you might need to zero</span>
<span class="sd">        ``.grad`` attributes or set them to ``None`` before calling it.</span>
<span class="sd">        See :ref:`Default gradient layouts&lt;default-grad-layouts&gt;`</span>
<span class="sd">        for details on the memory layout of accumulated gradients.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            gradient (Tensor or None): Gradient w.r.t. the</span>
<span class="sd">                tensor. If it is a tensor, it will be automatically converted</span>
<span class="sd">                to a Tensor that does not require grad unless ``create_graph`` is True.</span>
<span class="sd">                None values can be specified for scalar Tensors or ones that</span>
<span class="sd">                don&#39;t require grad. If a None value would be acceptable then</span>
<span class="sd">                this argument is optional.</span>
<span class="sd">            retain_graph (bool, optional): If ``False``, the graph used to compute</span>
<span class="sd">                the grads will be freed. Note that in nearly all cases setting</span>
<span class="sd">                this option to True is not needed and often can be worked around</span>
<span class="sd">                in a much more efficient way. Defaults to the value of</span>
<span class="sd">                ``create_graph``.</span>
<span class="sd">            create_graph (bool, optional): If ``True``, graph of the derivative will</span>
<span class="sd">                be constructed, allowing to compute higher order derivative</span>
<span class="sd">                products. Defaults to ``False``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">backward</span><span class="p">,</span>
                <span class="n">relevant_args</span><span class="p">,</span>
                <span class="bp">self</span><span class="p">,</span>
                <span class="n">gradient</span><span class="o">=</span><span class="n">gradient</span><span class="p">,</span>
                <span class="n">retain_graph</span><span class="o">=</span><span class="n">retain_graph</span><span class="p">,</span>
                <span class="n">create_graph</span><span class="o">=</span><span class="n">create_graph</span><span class="p">)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">gradient</span><span class="p">,</span> <span class="n">retain_graph</span><span class="p">,</span> <span class="n">create_graph</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.register_hook"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.register_hook">[docs]</a>    <span class="k">def</span> <span class="nf">register_hook</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Registers a backward hook.</span>

<span class="sd">        The hook will be called every time a gradient with respect to the</span>
<span class="sd">        Tensor is computed. The hook should have the following signature::</span>

<span class="sd">            hook(grad) -&gt; Tensor or None</span>


<span class="sd">        The hook should not modify its argument, but it can optionally return</span>
<span class="sd">        a new gradient which will be used in place of :attr:`grad`.</span>

<span class="sd">        This function returns a handle with a method ``handle.remove()``</span>
<span class="sd">        that removes the hook from the module.</span>

<span class="sd">        Example::</span>

<span class="sd">            &gt;&gt;&gt; v = torch.tensor([0., 0., 0.], requires_grad=True)</span>
<span class="sd">            &gt;&gt;&gt; h = v.register_hook(lambda grad: grad * 2)  # double the gradient</span>
<span class="sd">            &gt;&gt;&gt; v.backward(torch.tensor([1., 2., 3.]))</span>
<span class="sd">            &gt;&gt;&gt; v.grad</span>

<span class="sd">             2</span>
<span class="sd">             4</span>
<span class="sd">             6</span>
<span class="sd">            [torch.FloatTensor of size (3,)]</span>

<span class="sd">            &gt;&gt;&gt; h.remove()  # removes the hook</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">register_hook</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">hook</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;cannot register a hook on a tensor that &quot;</span>
                               <span class="s2">&quot;doesn&#39;t require gradient&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">grad_fn</span><span class="o">.</span><span class="n">_register_hook_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">handle</span> <span class="o">=</span> <span class="n">hooks</span><span class="o">.</span><span class="n">RemovableHandle</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_backward_hooks</span><span class="p">[</span><span class="n">handle</span><span class="o">.</span><span class="n">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">hook</span>
        <span class="k">return</span> <span class="n">handle</span></div>

    <span class="k">def</span> <span class="nf">reinforce</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reward</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">trim</span><span class="p">(</span><span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)])</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="n">trim</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;&quot;&quot;reinforce() was removed.</span>
<span class="s2">            Use torch.distributions instead.</span>
<span class="s2">            See https://pytorch.org/docs/master/distributions.html</span>

<span class="s2">            Instead of:</span>

<span class="s2">            probs = policy_network(state)</span>
<span class="s2">            action = probs.multinomial()</span>
<span class="s2">            next_state, reward = env.step(action)</span>
<span class="s2">            action.reinforce(reward)</span>
<span class="s2">            action.backward()</span>

<span class="s2">            Use:</span>

<span class="s2">            probs = policy_network(state)</span>
<span class="s2">            # NOTE: categorical is equivalent to what used to be called multinomial</span>
<span class="s2">            m = torch.distributions.Categorical(probs)</span>
<span class="s2">            action = m.sample()</span>
<span class="s2">            next_state, reward = env.step(action)</span>
<span class="s2">            loss = -m.log_prob(action) * reward</span>
<span class="s2">            loss.backward()</span>
<span class="s2">        &quot;&quot;&quot;</span><span class="p">))</span>

    <span class="n">detach</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">detach</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Returns a new Tensor, detached from the current graph.</span>

<span class="s2">    The result will never require gradient.</span>

<span class="s2">    .. note::</span>

<span class="s2">      Returned Tensor shares the same storage with the original one.</span>
<span class="s2">      In-place modifications on either of them will be seen, and may trigger</span>
<span class="s2">      errors in correctness checks.</span>
<span class="s2">      IMPORTANT NOTE: Previously, in-place size / stride / storage changes</span>
<span class="s2">      (such as `resize_` / `resize_as_` / `set_` / `transpose_`) to the returned tensor</span>
<span class="s2">      also update the original tensor. Now, these in-place changes will not update the</span>
<span class="s2">      original tensor anymore, and will instead trigger an error.</span>
<span class="s2">      For sparse tensors:</span>
<span class="s2">      In-place indices / values changes (such as `zero_` / `copy_` / `add_`) to the</span>
<span class="s2">      returned tensor will not update the original tensor anymore, and will instead</span>
<span class="s2">      trigger an error.</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

    <span class="n">detach_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">detach_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    Detaches the Tensor from the graph that created it, making it a leaf.</span>
<span class="s2">    Views cannot be detached in-place.</span>
<span class="s2">    &quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.retain_grad"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.retain_grad">[docs]</a>    <span class="k">def</span> <span class="nf">retain_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Enables .grad attribute for non-leaf Tensors.&quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">retain_grad</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;can&#39;t retain_grad on Tensor that has requires_grad=False&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span><span class="p">:</span>  <span class="c1"># no-op for leaves</span>
            <span class="k">return</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;retains_grad&#39;</span><span class="p">):</span>
            <span class="k">return</span>
        <span class="n">weak_self</span> <span class="o">=</span> <span class="n">weakref</span><span class="o">.</span><span class="n">ref</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="k">def</span> <span class="nf">retain_grad_hook</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
            <span class="n">var</span> <span class="o">=</span> <span class="n">weak_self</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">var</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">return</span>
            <span class="k">if</span> <span class="n">var</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="n">var</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">var</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">contiguous_format</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">var</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">var</span><span class="o">.</span><span class="n">_grad</span> <span class="o">+</span> <span class="n">grad</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">register_hook</span><span class="p">(</span><span class="n">retain_grad_hook</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">retains_grad</span> <span class="o">=</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="Tensor.is_shared"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.is_shared">[docs]</a>    <span class="k">def</span> <span class="nf">is_shared</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Checks if tensor is in shared memory.</span>

<span class="sd">        This is always ``True`` for CUDA tensors.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">is_shared</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">is_shared</span><span class="p">()</span></div>

<div class="viewcode-block" id="Tensor.share_memory_"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.share_memory_">[docs]</a>    <span class="k">def</span> <span class="nf">share_memory_</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Moves the underlying storage to shared memory.</span>

<span class="sd">        This is a no-op if the underlying storage is already in shared memory</span>
<span class="sd">        and for CUDA tensors. Tensors in shared memory cannot be resized.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">share_memory_</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span></div>

    <span class="k">def</span> <span class="fm">__reversed__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reverses the tensor along dimension 0.&quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__reversed__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<div class="viewcode-block" id="Tensor.norm"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.norm">[docs]</a>    <span class="k">def</span> <span class="nf">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s2">&quot;fro&quot;</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.norm`&quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">norm</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.lu"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.lu">[docs]</a>    <span class="k">def</span> <span class="nf">lu</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.lu`&quot;&quot;&quot;</span>
        <span class="c1"># If get_infos is True, then we don&#39;t need to check for errors and vice versa</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">lu</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">get_infos</span><span class="o">=</span><span class="n">get_infos</span><span class="p">)</span>
        <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_lu_with_info</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pivot</span><span class="o">=</span><span class="n">pivot</span><span class="p">,</span> <span class="n">check_errors</span><span class="o">=</span><span class="p">(</span><span class="ow">not</span> <span class="n">get_infos</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">get_infos</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span><span class="p">,</span> <span class="n">infos</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LU</span><span class="p">,</span> <span class="n">pivots</span></div>

<div class="viewcode-block" id="Tensor.stft"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.stft">[docs]</a>    <span class="k">def</span> <span class="nf">stft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">window</span><span class="p">:</span> <span class="s1">&#39;Optional[Tensor]&#39;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
             <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">pad_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;reflect&#39;</span><span class="p">,</span> <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
             <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">return_complex</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.stft`</span>

<span class="sd">        .. warning::</span>
<span class="sd">          This function changed signature at version 0.4.1. Calling with</span>
<span class="sd">          the previous signature may cause error or return incorrect result.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">stft</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span>
                <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span> <span class="n">pad_mode</span><span class="o">=</span><span class="n">pad_mode</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span>
                <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">stft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span>
                          <span class="n">pad_mode</span><span class="p">,</span> <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.istft"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.istft">[docs]</a>    <span class="k">def</span> <span class="nf">istft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">win_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">window</span><span class="p">:</span> <span class="s1">&#39;Optional[Tensor]&#39;</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">center</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">normalized</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
              <span class="n">onesided</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
              <span class="n">return_complex</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.istft`&quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">istft</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="o">=</span><span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="o">=</span><span class="n">win_length</span><span class="p">,</span>
                <span class="n">window</span><span class="o">=</span><span class="n">window</span><span class="p">,</span> <span class="n">center</span><span class="o">=</span><span class="n">center</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="o">=</span><span class="n">onesided</span><span class="p">,</span> <span class="n">length</span><span class="o">=</span><span class="n">length</span><span class="p">,</span>
                <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">istft</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_fft</span><span class="p">,</span> <span class="n">hop_length</span><span class="p">,</span> <span class="n">win_length</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">center</span><span class="p">,</span>
                           <span class="n">normalized</span><span class="p">,</span> <span class="n">onesided</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">return_complex</span><span class="o">=</span><span class="n">return_complex</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">resize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">sizes</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>
        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">resize_as</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">resize_as</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="p">)</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;non-inplace resize_as is deprecated&quot;</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.autograd._functions</span> <span class="kn">import</span> <span class="n">Resize</span>
        <span class="k">return</span> <span class="n">Resize</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

<div class="viewcode-block" id="Tensor.split"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.split">[docs]</a>    <span class="k">def</span> <span class="nf">split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;See :func:`torch.split`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">split</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">split_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">split_size</span><span class="p">)</span>
                <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">split_with_sizes</span><span class="p">(</span><span class="n">split_size</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unique"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.unique">[docs]</a>    <span class="k">def</span> <span class="nf">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Returns the unique elements of the input tensor.</span>

<span class="sd">        See :func:`torch.unique`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">sorted</span><span class="o">=</span><span class="nb">sorted</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span></div>

<div class="viewcode-block" id="Tensor.unique_consecutive"><a class="viewcode-back" href="../../tensors.html#torch.Tensor.unique_consecutive">[docs]</a>    <span class="k">def</span> <span class="nf">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Eliminates all but the first element from every consecutive group of equivalent elements.</span>

<span class="sd">        See :func:`torch.unique_consecutive`</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">Tensor</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span>
                <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique_consecutive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">return_inverse</span><span class="o">=</span><span class="n">return_inverse</span><span class="p">,</span> <span class="n">return_counts</span><span class="o">=</span><span class="n">return_counts</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="fm">__rsub__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__rsub__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">_C</span><span class="o">.</span><span class="n">_VariableFunctions</span><span class="o">.</span><span class="n">rsub</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__rdiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__rdiv__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_floating_point</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">is_complex</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">other</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">get_default_dtype</span><span class="p">())</span><span class="o">.</span><span class="n">reciprocal</span><span class="p">()</span> <span class="o">*</span> <span class="n">other</span>

    <span class="fm">__rtruediv__</span> <span class="o">=</span> <span class="n">__rdiv__</span>
    <span class="fm">__itruediv__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">__idiv__</span>

    <span class="fm">__pow__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">pow</span>

    <span class="k">def</span> <span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__format__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="n">format_spec</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">object</span><span class="o">.</span><span class="fm">__format__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">format_spec</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__ipow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>  <span class="c1"># type: ignore[misc]</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="nb">type</span><span class="p">(</span><span class="n">other</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__ipow__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">NotImplemented</span>

    <span class="nd">@_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rpow__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">result_type</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span>

    <span class="nd">@_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__floordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">)</span>

    <span class="nd">@_wrap_type_error_to_not_implemented</span>
    <span class="k">def</span> <span class="fm">__rfloordiv__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">other</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">floor_divide</span><span class="p">(</span><span class="n">other</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

    <span class="fm">__neg__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">neg</span>

    <span class="fm">__eq__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">eq</span><span class="p">)</span>
    <span class="fm">__ne__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">ne</span><span class="p">)</span>
    <span class="fm">__lt__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">lt</span><span class="p">)</span>
    <span class="fm">__le__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">le</span><span class="p">)</span>
    <span class="fm">__gt__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">gt</span><span class="p">)</span>
    <span class="fm">__ge__</span> <span class="o">=</span> <span class="n">_wrap_type_error_to_not_implemented</span><span class="p">(</span><span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">ge</span><span class="p">)</span>
    <span class="fm">__abs__</span> <span class="o">=</span> <span class="n">_C</span><span class="o">.</span><span class="n">_TensorBase</span><span class="o">.</span><span class="n">abs</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__len__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;len() of a 0-d tensor&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__iter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># NB: we use &#39;imap&#39; and not &#39;map&#39; here, so that in Python 2 we get a</span>
        <span class="c1"># generator and don&#39;t eagerly perform all the indexes.  This could</span>
        <span class="c1"># save us work, and also helps keep trace ordering deterministic</span>
        <span class="c1"># (e.g., if you zip(*hiddens), the eager map will force all the</span>
        <span class="c1"># indexes of hiddens[0] before hiddens[1], while the generator</span>
        <span class="c1"># map will interleave them.)</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__iter__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s1">&#39;iteration over a 0-d tensor&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Iterating over a tensor might cause the trace to be incorrect. &#39;</span>
                          <span class="s1">&#39;Passing a tensor of different shape won</span><span class="se">\&#39;</span><span class="s1">t change the number of &#39;</span>
                          <span class="s1">&#39;iterations executed (and might lead to errors or silently give &#39;</span>
                          <span class="s1">&#39;incorrect results).&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">RuntimeWarning</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">unbind</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

    <span class="k">def</span> <span class="fm">__hash__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__hash__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">id</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__dir__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__dir__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_quantized</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Only a small subset of methods are supported for quantized tensors.&#39;</span><span class="p">)</span>
        <span class="n">tensor_methods</span> <span class="o">=</span> <span class="nb">dir</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="p">)</span>
        <span class="n">tensor_methods</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s1">&#39;volatile&#39;</span><span class="p">)</span>  <span class="c1"># deprecated</span>
        <span class="n">attrs</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="n">tensor_methods</span> <span class="o">+</span> <span class="n">attrs</span>

        <span class="c1"># property only available dense, cuda tensors</span>
        <span class="k">if</span> <span class="p">(</span><span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="n">keys</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="s2">&quot;__cuda_array_interface__&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>

    <span class="c1"># Numpy array interface, to support `numpy.asarray(tensor) -&gt; ndarray`</span>
    <span class="n">__array_priority__</span> <span class="o">=</span> <span class="mi">1000</span>    <span class="c1"># prefer Tensor ops over numpy ones</span>

    <span class="k">def</span> <span class="nf">__array__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__array__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="c1"># Wrap Numpy array again in a suitable tensor when done, to support e.g.</span>
    <span class="c1"># `numpy.sin(tensor) -&gt; tensor` or `numpy.greater(tensor, 0) -&gt; ByteTensor`</span>
    <span class="k">def</span> <span class="nf">__array_wrap__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__array_wrap__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">array</span><span class="o">=</span><span class="n">array</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">array</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">bool</span><span class="p">:</span>
            <span class="c1"># Workaround, torch has no built-in bool tensor</span>
            <span class="n">array</span> <span class="o">=</span> <span class="n">array</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">array</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__contains__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Check if `element` is present in tensor</span>

<span class="sd">        Arguments:</span>
<span class="sd">            element (Tensor or scalar): element to be checked</span>
<span class="sd">                for presence in current tensor&quot;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="fm">__contains__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">element</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">element</span><span class="p">,</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Number</span><span class="p">)):</span>
            <span class="c1"># type hint doesn&#39;t understand the __contains__ result array</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">element</span> <span class="o">==</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>  <span class="c1"># type: ignore[union-attr]</span>

        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
            <span class="s2">&quot;Tensor.__contains__ only supports Tensor or scalar, but you passed in a </span><span class="si">%s</span><span class="s2">.&quot;</span> <span class="o">%</span>
            <span class="nb">type</span><span class="p">(</span><span class="n">element</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">__cuda_array_interface__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Array view description for cuda tensors.</span>

<span class="sd">        See:</span>
<span class="sd">        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">__cuda_array_interface__</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="c1"># raise AttributeError for unsupported tensors, so that</span>
        <span class="c1"># hasattr(cpu_tensor, &quot;__cuda_array_interface__&quot;) is False.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on non-CUDA tensor type: </span><span class="si">%s</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;If CUDA data is required use tensor.cuda() to copy tensor to device memory.&quot;</span> <span class="o">%</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on sparse type: </span><span class="si">%s</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;Use Tensor.to_dense() to convert to a dense tensor first.&quot;</span> <span class="o">%</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">type</span><span class="p">()</span>
            <span class="p">)</span>

        <span class="c1"># RuntimeError, matching tensor.__array__() behavior.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Can&#39;t get __cuda_array_interface__ on Variable that requires grad. &quot;</span>
                <span class="s2">&quot;If gradients aren&#39;t required, use var.detach() to get Variable that doesn&#39;t require grad.&quot;</span>
            <span class="p">)</span>

        <span class="c1"># CUDA devices are little-endian and tensors are stored in native byte</span>
        <span class="c1"># order. 1-byte entries are endian-agnostic.</span>
        <span class="n">typestr</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex64</span><span class="p">:</span> <span class="s2">&quot;&lt;c8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">complex128</span><span class="p">:</span> <span class="s2">&quot;&lt;c16&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span> <span class="s2">&quot;&lt;f2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">:</span> <span class="s2">&quot;&lt;f4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">float64</span><span class="p">:</span> <span class="s2">&quot;&lt;f8&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span> <span class="s2">&quot;|u1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">:</span> <span class="s2">&quot;|i1&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int16</span><span class="p">:</span> <span class="s2">&quot;&lt;i2&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int32</span><span class="p">:</span> <span class="s2">&quot;&lt;i4&quot;</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span> <span class="s2">&quot;&lt;i8&quot;</span><span class="p">,</span>
        <span class="p">}[</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">]</span>

        <span class="n">itemsize</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span><span class="o">.</span><span class="n">element_size</span><span class="p">()</span>

        <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="c1"># __cuda_array_interface__ v2 requires the strides to be omitted</span>
            <span class="c1"># (either not set or set to None) for C-contiguous arrays.</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">strides</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">itemsize</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">())</span>
        <span class="n">data_ptr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">data_ptr</span><span class="p">()</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">data</span> <span class="o">=</span> <span class="p">(</span><span class="n">data_ptr</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>  <span class="c1"># read-only is false</span>

        <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span><span class="n">typestr</span><span class="o">=</span><span class="n">typestr</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="n">strides</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">refine_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Refines the dimension names of :attr:`self` according to :attr:`names`.</span>

<span class="sd">        Refining is a special case of renaming that &quot;lifts&quot; unnamed dimensions.</span>
<span class="sd">        A ``None`` dim can be refined to have any name; a named dim can only be</span>
<span class="sd">        refined to have the same name.</span>

<span class="sd">        Because named tensors can coexist with unnamed tensors, refining names</span>
<span class="sd">        gives a nice way to write named-tensor-aware code that works with both</span>
<span class="sd">        named and unnamed tensors.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded greedily; it is expanded in-place to fill</span>
<span class="sd">        :attr:`names` to the same length as ``self.dim()`` using names from the</span>
<span class="sd">        corresponding indices of ``self.names``.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Arguments:</span>
<span class="sd">            names (iterable of str): The desired names of the output tensor. May</span>
<span class="sd">                contain up to one Ellipsis.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.randn(32, 3, 128, 128)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs = imgs.refine_names(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>
<span class="sd">            &gt;&gt;&gt; named_imgs.names</span>
<span class="sd">            (&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 3, 5, 7, 11)</span>
<span class="sd">            &gt;&gt;&gt; tensor = tensor.refine_names(&#39;A&#39;, ..., &#39;B&#39;, &#39;C&#39;)</span>
<span class="sd">            &gt;&gt;&gt; tensor.names</span>
<span class="sd">            (&#39;A&#39;, None, None, &#39;B&#39;, &#39;C&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">refine_names</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">names</span> <span class="o">=</span> <span class="n">resolve_ellipsis</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">names</span><span class="p">,</span> <span class="s1">&#39;refine_names&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">refine_names</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">align_to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Permutes the dimensions of the :attr:`self` tensor to match the order</span>
<span class="sd">        specified in :attr:`names`, adding size-one dims for any new names.</span>

<span class="sd">        All of the dims of :attr:`self` must be named in order to use this method.</span>
<span class="sd">        The resulting tensor is a view on the original tensor.</span>

<span class="sd">        All dimension names of :attr:`self` must be present in :attr:`names`.</span>
<span class="sd">        :attr:`names` may contain additional names that are not in ``self.names``;</span>
<span class="sd">        the output tensor has a size-one dimension for each of those new names.</span>

<span class="sd">        :attr:`names` may contain up to one Ellipsis (``...``).</span>
<span class="sd">        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`</span>
<span class="sd">        that are not mentioned in :attr:`names`, in the order that they appear</span>
<span class="sd">        in :attr:`self`.</span>

<span class="sd">        Python 2 does not support Ellipsis but one may use a string literal</span>
<span class="sd">        instead (``&#39;...&#39;``).</span>

<span class="sd">        Arguments:</span>
<span class="sd">            names (iterable of str): The desired dimension ordering of the</span>
<span class="sd">                output tensor. May contain up to one Ellipsis that is expanded</span>
<span class="sd">                to all unmentioned dim names of :attr:`self`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; tensor = torch.randn(2, 2, 2, 2, 2, 2)</span>
<span class="sd">            &gt;&gt;&gt; named_tensor = tensor.refine_names(&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;)</span>

<span class="sd">            # Move the F and E dims to the front while keeping the rest in order</span>
<span class="sd">            &gt;&gt;&gt; named_tensor.align_to(&#39;F&#39;, &#39;E&#39;, ...)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">align_to</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">)</span>
        <span class="n">ellipsis_idx</span> <span class="o">=</span> <span class="n">single_ellipsis_index</span><span class="p">(</span><span class="n">names</span><span class="p">,</span> <span class="s1">&#39;align_to&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ellipsis_idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">align_to</span><span class="p">(</span>
            <span class="p">[</span><span class="n">name</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">names</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">is_ellipsis</span><span class="p">(</span><span class="n">name</span><span class="p">)],</span>
            <span class="n">ellipsis_idx</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">unflatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Expands the dimension :attr:`dim` of the :attr:`self` tensor over multiple dimensions</span>
<span class="sd">        of sizes given by :attr:`sizes`.</span>

<span class="sd">        * :attr:`sizes` is the new shape of the unflattened dimension and it can be a `Tuple[int]` as well</span>
<span class="sd">          as `torch.Size` if :attr:`self` is a `Tensor`, or `namedshape` (Tuple[(name: str, size: int)])</span>
<span class="sd">          if :attr:`self` is a `NamedTensor`. The total number of elements in sizes must match the number</span>
<span class="sd">          of elements in the original dim being unflattened.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            dim (Union[int, str]): Dimension to unflatten</span>
<span class="sd">            sizes (Union[Tuple[int] or torch.Size, Tuple[Tuple[str, int]]]): New shape of the unflattened dimension</span>

<span class="sd">        Examples:</span>
<span class="sd">            &gt;&gt;&gt; torch.randn(3, 4, 1).unflatten(1, (2, 2)).shape</span>
<span class="sd">            torch.Size([3, 2, 2, 1])</span>
<span class="sd">            &gt;&gt;&gt; torch.randn(2, 4, names=(&#39;A&#39;, &#39;B&#39;)).unflatten(&#39;B&#39;, ((&#39;B1&#39;, 2), (&#39;B2&#39;, 2)))</span>
<span class="sd">            tensor([[[-1.1772,  0.0180],</span>
<span class="sd">                    [ 0.2412,  0.1431]],</span>

<span class="sd">                    [[-1.1819, -0.8899],</span>
<span class="sd">                    [ 1.5813,  0.2274]]], names=(&#39;A&#39;, &#39;B1&#39;, &#39;B2&#39;))</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">unflatten</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">sizes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;unflatten: sizes must be non-empty&quot;</span><span class="p">)</span>

        <span class="n">names</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="n">OrderedDict</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))):</span>
            <span class="n">names</span><span class="p">,</span> <span class="n">sizes</span> <span class="o">=</span> <span class="n">unzip_namedshape</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">unflatten</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">sizes</span><span class="p">,</span> <span class="n">names</span><span class="p">)</span>


    <span class="k">def</span> <span class="nf">rename_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;In-place version of :meth:`~Tensor.rename`.&quot;&quot;&quot;</span>

        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">rename_</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">)</span>

        <span class="c1"># Note [rename_ / rename API]</span>
        <span class="c1"># The Python API for these is different from the C++ API. In Python:</span>
        <span class="c1"># 1) tensor.rename(*names) takes a vararglist of names</span>
        <span class="c1"># 2) tensor.rename(**rename_map) takes a map of names to rename.</span>
        <span class="c1"># C++ is static, making it difficult to implement similar behavior.</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">rename</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Renames dimension names of :attr:`self`.</span>

<span class="sd">        There are two main usages:</span>

<span class="sd">        ``self.rename(**rename_map)`` returns a view on tensor that has dims</span>
<span class="sd">        renamed as specified in the mapping :attr:`rename_map`.</span>

<span class="sd">        ``self.rename(*names)`` returns a view on tensor, renaming all</span>
<span class="sd">        dimensions positionally using :attr:`names`.</span>
<span class="sd">        Use ``self.rename(None)`` to drop names on a tensor.</span>

<span class="sd">        One cannot specify both positional args :attr:`names` and keyword args</span>
<span class="sd">        :attr:`rename_map`.</span>

<span class="sd">        Examples::</span>

<span class="sd">            &gt;&gt;&gt; imgs = torch.rand(2, 3, 5, 7, names=(&#39;N&#39;, &#39;C&#39;, &#39;H&#39;, &#39;W&#39;))</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(N=&#39;batch&#39;, C=&#39;channels&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channels&#39;, &#39;H&#39;, &#39;W&#39;)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(None)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (None,)</span>

<span class="sd">            &gt;&gt;&gt; renamed_imgs = imgs.rename(&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>
<span class="sd">            &gt;&gt;&gt; renamed_imgs.names</span>
<span class="sd">            (&#39;batch&#39;, &#39;channel&#39;, &#39;height&#39;, &#39;width&#39;)</span>

<span class="sd">        .. warning::</span>
<span class="sd">            The named tensor API is experimental and subject to change.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">rename</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">names</span><span class="p">,</span> <span class="o">**</span><span class="n">rename_map</span><span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">return</span> <span class="n">update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">rename_map</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_update_names</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">_update_names</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">names</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span>

        <span class="c1"># See Note [rename_ / rename API]</span>
        <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">rename_</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">Tensor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This attribute is ``None`` by default and becomes a Tensor the first time a call to</span>
<span class="sd">        :func:`backward` computes gradients for ``self``.</span>
<span class="sd">        The attribute will then contain the gradients computed and future calls to</span>
<span class="sd">        :func:`backward` will accumulate (add) gradients into it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="fm">__get__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">requires_grad</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;retains_grad&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_leaf</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad &quot;</span>
                          <span class="s2">&quot;attribute won&#39;t be populated during autograd.backward(). If you indeed want the gradient &quot;</span>
                          <span class="s2">&quot;for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the &quot;</span>
                          <span class="s2">&quot;non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See &quot;</span>
                          <span class="s2">&quot;github.com/pytorch/pytorch/pull/30531 for more informations.&quot;</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span>

    <span class="nd">@grad</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_grad</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="fm">__set__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">new_grad</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span> <span class="o">=</span> <span class="n">new_grad</span>

    <span class="nd">@grad</span><span class="o">.</span><span class="n">deleter</span>
    <span class="k">def</span> <span class="nf">grad</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">relevant_args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">,)</span>
        <span class="kn">from</span> <span class="nn">torch.overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">relevant_args</span><span class="p">):</span>
            <span class="c1"># TODO mypy doesn&#39;t support @property, see: https://github.com/python/mypy/issues/6185</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">Tensor</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="fm">__delete__</span><span class="p">,</span> <span class="n">relevant_args</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>  <span class="c1"># type: ignore[attr-defined]</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">_grad</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__torch_function__</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">types</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(),</span> <span class="n">kwargs</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        This __torch_function__ implementation wraps subclasses such that</span>
<span class="sd">        methods called on subclasses return a subclass instance instead of</span>
<span class="sd">        a ``torch.Tensor`` instance.</span>

<span class="sd">        One corollary to this is that you need coverage for torch.Tensor</span>
<span class="sd">        methods if implementing __torch_function__ for subclasses.</span>

<span class="sd">        We recommend always calling ``super().__torch_function__`` as the base</span>
<span class="sd">        case when doing the above.</span>

<span class="sd">        While not mandatory, we recommend making `__torch_function__` a classmethod.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">issubclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">types</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">NotImplemented</span>

        <span class="k">with</span> <span class="n">_C</span><span class="o">.</span><span class="n">DisableTorchFunction</span><span class="p">():</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span>

    <span class="vm">__module__</span> <span class="o">=</span> <span class="s1">&#39;torch&#39;</span>


<span class="k">def</span> <span class="nf">_convert</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="bp">cls</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">cls</span> <span class="ow">is</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">ret</span><span class="o">.</span><span class="n">as_subclass</span><span class="p">(</span><span class="bp">cls</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ret</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">_convert</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span> <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">ret</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">ret</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
         <script src="../../_static/jquery.js"></script>
         <script src="../../_static/underscore.js"></script>
         <script src="../../_static/doctools.js"></script>
         <script src="../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>