


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.quantized.modules.conv &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/quantized/modules/conv.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 

  
  <script src="../../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/quantized/modules/conv.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../../torch.html">torch</a> &gt;</li>
        
          <li><a href="../modules.html">torch.nn.quantized.modules</a> &gt;</li>
        
      <li>torch.nn.quantized.modules.conv</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.quantized.modules.conv</h1><div class="highlight"><pre>
<span></span><span class="c1"># coding=utf-8</span>
<span class="sa">r</span><span class="sd">&quot;&quot;&quot;Quantized convolution modules.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">List</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic</span> <span class="k">as</span> <span class="nn">nni</span>
<span class="kn">import</span> <span class="nn">torch.nn.intrinsic.qat</span> <span class="k">as</span> <span class="nn">nniqat</span>

<span class="kn">from</span> <span class="nn">torch._ops</span> <span class="kn">import</span> <span class="n">ops</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.utils</span> <span class="kn">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span>
<span class="kn">from</span> <span class="nn">torch.nn.quantized.modules.utils</span> <span class="kn">import</span> <span class="n">_pair_from_first</span>
<span class="kn">from</span> <span class="nn">torch.nn.quantized.modules.utils</span> <span class="kn">import</span> <span class="n">_quantize_weight</span>
<span class="kn">from</span> <span class="nn">torch.nn.utils</span> <span class="kn">import</span> <span class="n">fuse_conv_bn_weights</span>

<span class="k">class</span> <span class="nc">_ConvNd</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
                 <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span>
                 <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                <span class="s2">&quot;Currently only zero-padding is supported by quantized conv&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">in_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;in_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">out_channels</span> <span class="o">%</span> <span class="n">groups</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;out_channels must be divisible by groups&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">transposed</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">output_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="c1"># Initialize as NCHW. set_weight will internally transpose to NHWC.</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">:</span>
            <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">weight_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">]</span>
        <span class="n">qweight</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_empty_affine_quantized</span><span class="p">(</span>
            <span class="n">weight_shape</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
            <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">zero_point</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">)</span>
        <span class="n">bias_float</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="kc">None</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">set_weight_bias</span><span class="p">(</span><span class="n">qweight</span><span class="p">,</span> <span class="n">bias_float</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mf">1.0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">def</span> <span class="nf">extra_repr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="s1">&#39;</span><span class="si">{in_channels}</span><span class="s1">, </span><span class="si">{out_channels}</span><span class="s1">, kernel_size=</span><span class="si">{kernel_size}</span><span class="s1">&#39;</span>
             <span class="s1">&#39;, stride=</span><span class="si">{stride}</span><span class="s1">, scale=</span><span class="si">{scale}</span><span class="s1">, zero_point=</span><span class="si">{zero_point}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, padding=</span><span class="si">{padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, dilation=</span><span class="si">{dilation}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">):</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, output_padding=</span><span class="si">{output_padding}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, groups=</span><span class="si">{groups}</span><span class="s1">&#39;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">()</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">s</span> <span class="o">+=</span> <span class="s1">&#39;, bias=False&#39;</span>
        <span class="k">return</span> <span class="n">s</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="vm">__dict__</span><span class="p">)</span>

    <span class="c1"># ===== Serialization methods =====</span>
    <span class="c1"># The special consideration here is that we have to unpack the weights into</span>
    <span class="c1"># their regular QTensor form for serialization. Packed weights should not</span>
    <span class="c1"># live outside the process in which they were created, rather they should be</span>
    <span class="c1"># derived from the QTensor weight.</span>
    <span class="c1">#   self</span>
    <span class="c1">#   |--- weight : Tensor</span>
    <span class="c1">#   |--- bias : Tensor</span>
    <span class="c1">#</span>
    <span class="c1"># TODO: maybe change to this when https://github.com/pytorch/pytorch/pull/32958 is landed</span>
    <span class="c1">#   self</span>
    <span class="c1">#   |--- _packed_params : Conv2dPackedParamsBase or Conv3dPackedParamsBase</span>
    <span class="k">def</span> <span class="nf">_save_to_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_save_to_state_dict</span><span class="p">(</span><span class="n">destination</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">keep_vars</span><span class="p">)</span>
        <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;weight&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">w</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;bias&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;scale&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">)</span>
        <span class="n">destination</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;zero_point&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">__getstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">,</span>
            <span class="n">w</span><span class="p">,</span>
            <span class="n">b</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">training</span>
        <span class="p">)</span>

    <span class="c1"># ===== Deserialization methods =====</span>
    <span class="c1"># Counterpart to the serialization methods, we must pack the serialized</span>
    <span class="c1"># QTensor weight into its packed format for use by the FBGEMM ops.</span>
    <span class="k">def</span> <span class="nf">_load_from_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="n">strict</span><span class="p">,</span>
                              <span class="n">missing_keys</span><span class="p">,</span> <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weight_bias</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;weight&#39;</span><span class="p">],</span> <span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;bias&#39;</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;weight&#39;</span><span class="p">)</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;bias&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;scale&#39;</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;scale&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">state_dict</span><span class="p">[</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;zero_point&#39;</span><span class="p">])</span>
        <span class="n">state_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">prefix</span> <span class="o">+</span> <span class="s1">&#39;zero_point&#39;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">_load_from_state_dict</span><span class="p">(</span>
            <span class="n">state_dict</span><span class="p">,</span> <span class="n">prefix</span><span class="p">,</span> <span class="n">local_metadata</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="n">missing_keys</span><span class="p">,</span>
            <span class="n">unexpected_keys</span><span class="p">,</span> <span class="n">error_msgs</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">export</span>
    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transposed</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">7</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">8</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">9</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_weight_bias</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="mi">11</span><span class="p">])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">12</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">13</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="mi">14</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">get_qconv</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod</span><span class="p">,</span> <span class="n">activation_post_process</span><span class="p">,</span> <span class="n">weight_post_process</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a qconv object and returns it.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">weight_post_process</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weight_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
        <span class="n">weight_post_process</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">act_scale</span><span class="p">,</span> <span class="n">act_zp</span> <span class="o">=</span> <span class="n">activation_post_process</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">weight_post_process</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> \
            <span class="s1">&#39;Weight observer must have a dtype of qint8&#39;</span>
        <span class="n">qweight</span> <span class="o">=</span> <span class="n">_quantize_weight</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">weight_post_process</span><span class="p">)</span>
        <span class="n">qconv</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">set_weight_bias</span><span class="p">(</span><span class="n">qweight</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">act_scale</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">act_zp</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">qconv</span>


<div class="viewcode-block" id="Conv1d"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv1d">[docs]</a><span class="k">class</span> <span class="nc">Conv1d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D convolution over a quantized input signal composed of</span>
<span class="sd">    several quantized input planes.</span>

<span class="sd">    For details on input arguments, parameters, and implementation see</span>
<span class="sd">    :class:`~torch.nn.Conv1d`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `zeros` is supported for the :attr:`padding_mode` argument.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `torch.quint8` is supported for the input data type.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor):     packed tensor derived from the learnable weight</span>
<span class="sd">                             parameter.</span>
<span class="sd">        scale (Tensor):      scalar for the output scale</span>
<span class="sd">        zero_point (Tensor): scalar for the output zero point</span>

<span class="sd">    See :class:`~torch.nn.Conv1d` for other attributes.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv1d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 100)</span>
<span class="sd">        &gt;&gt;&gt; # quantize input to quint8</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,</span>
<span class="sd">                                                dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; output = m(q_input)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_FLOAT_MODULE</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair_from_first</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair_from_first</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair_from_first</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair_from_first</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_single</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;QuantizedConv1d&#39;</span>

    <span class="k">def</span> <span class="nf">set_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv1d_prepack</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv1d_unpack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Temporarily using len(shape) instead of ndim due to JIT issue</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/23890</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape must be `(N, C, L)`!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>

<div class="viewcode-block" id="Conv1d.from_float"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv1d.from_float">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a quantized module from a float module or qparams_dict.</span>

<span class="sd">        Args:</span>
<span class="sd">            mod (Module): a float module, either produced by torch.quantization</span>
<span class="sd">              utilities or provided by the user</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="p">,</span> \
            <span class="s1">&#39; nnq.&#39;</span> <span class="o">+</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;.from_float only works for &#39;</span> <span class="o">+</span> \
            <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">),</span> \
            <span class="s1">&#39;Input float module must have qconfig defined.&#39;</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="n">nni</span><span class="o">.</span><span class="n">ConvReLU1d</span><span class="p">:</span>
            <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activation_post_process</span>
            <span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_qconv</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">activation_post_process</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv2d"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv2d">[docs]</a><span class="k">class</span> <span class="nc">Conv2d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D convolution over a quantized input signal composed of</span>
<span class="sd">    several quantized input planes.</span>

<span class="sd">    For details on input arguments, parameters, and implementation see</span>
<span class="sd">    :class:`~torch.nn.Conv2d`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `zeros` is supported for the :attr:`padding_mode` argument.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `torch.quint8` is supported for the input data type.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor):     packed tensor derived from the learnable weight</span>
<span class="sd">                             parameter.</span>
<span class="sd">        scale (Tensor):      scalar for the output scale</span>
<span class="sd">        zero_point (Tensor): scalar for the output zero point</span>

<span class="sd">    See :class:`~torch.nn.Conv2d` for other attributes.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; # quantize input to quint8</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; output = m(q_input)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_FLOAT_MODULE</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;QuantizedConv2d&#39;</span>

    <span class="k">def</span> <span class="nf">set_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv2d_prepack</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Temporarily using len(shape) instead of ndim due to JIT issue</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/23890</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape must be `(N, C, H, W)`!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>

<div class="viewcode-block" id="Conv2d.from_float"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv2d.from_float">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a quantized module from a float module or qparams_dict.</span>

<span class="sd">        Args:</span>
<span class="sd">            mod (Module): a float module, either produced by torch.quantization</span>
<span class="sd">              utilities or provided by the user</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;weight_fake_quant&#39;</span><span class="p">):</span>
            <span class="c1"># assert type(mod) == cls.__QAT_MODULE, &#39; nnq.&#39; + cls.__name__ + \</span>
            <span class="c1"># &#39;.from_float only works for &#39; + cls.__QAT_MODULE.__name__</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="n">nniqat</span><span class="o">.</span><span class="n">ConvBn2d</span><span class="p">:</span>
                <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">fuse_conv_bn_weights</span><span class="p">(</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">running_mean</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">running_var</span><span class="p">,</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bn</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;activation_post_process&#39;</span><span class="p">),</span> \
                <span class="s1">&#39;Input QAT module must have observer attached&#39;</span>
            <span class="n">weight_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">weight_fake_quant</span>
            <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="p">,</span> \
                <span class="s1">&#39; nnq.&#39;</span> <span class="o">+</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;.from_float only works for &#39;</span> <span class="o">+</span> \
                <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="o">.</span><span class="vm">__name__</span>
            <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">),</span> \
                <span class="s1">&#39;Input float module must have qconfig defined.&#39;</span>
            <span class="c1"># workaround for sequential, ConvReLU2d should probably</span>
            <span class="c1"># inherit from Conv2d instead</span>
            <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="n">nni</span><span class="o">.</span><span class="n">ConvReLU2d</span><span class="p">:</span>
                <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activation_post_process</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span>
            <span class="n">weight_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_qconv</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">activation_post_process</span><span class="p">,</span> <span class="n">weight_post_process</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Conv3d"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv3d">[docs]</a><span class="k">class</span> <span class="nc">Conv3d</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D convolution over a quantized input signal composed of</span>
<span class="sd">    several quantized input planes.</span>

<span class="sd">    For details on input arguments, parameters, and implementation see</span>
<span class="sd">    :class:`~torch.nn.Conv3d`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `zeros` is supported for the :attr:`padding_mode` argument.</span>

<span class="sd">    .. note::</span>
<span class="sd">        Only `torch.quint8` is supported for the input data type.</span>


<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor):     packed tensor derived from the learnable weight</span>
<span class="sd">                             parameter.</span>
<span class="sd">        scale (Tensor):      scalar for the output scale</span>
<span class="sd">        zero_point (Tensor): scalar for the output zero point</span>

<span class="sd">    See :class:`~torch.nn.Conv3d` for other attributes.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding and dilation</span>
<span class="sd">        &gt;&gt;&gt; m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 56, 56, 56)</span>
<span class="sd">        &gt;&gt;&gt; # quantize input to quint8</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; output = m(q_input)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_FLOAT_MODULE</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv3d</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Conv3d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">False</span><span class="p">,</span> <span class="n">_triple</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;QuantizedConv3d&#39;</span>

    <span class="k">def</span> <span class="nf">set_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv3d_prepack</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Temporarily using len(shape) instead of ndim due to JIT issue</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/23890</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape must be `(N, C, D, H, W)`!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv3d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>

<div class="viewcode-block" id="Conv3d.from_float"><a class="viewcode-back" href="../../../../../torch.nn.quantized.html#torch.nn.quantized.Conv3d.from_float">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a quantized module from a float module or qparams_dict.</span>

<span class="sd">        Args:</span>
<span class="sd">            mod (Module): a float module, either produced by torch.quantization</span>
<span class="sd">              utilities or provided by the user</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="p">,</span> \
            <span class="s1">&#39; nnq.&#39;</span> <span class="o">+</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;.from_float only works for &#39;</span> <span class="o">+</span> \
            <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">),</span> \
            <span class="s1">&#39;Input float module must have qconfig defined.&#39;</span>
        <span class="c1"># Workaround for sequential, ConvReLU3d should probably inherit from</span>
        <span class="c1"># Conv3d instead</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="n">nni</span><span class="o">.</span><span class="n">ConvReLU3d</span><span class="p">:</span>
            <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activation_post_process</span>
            <span class="n">mod</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">activation_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="o">.</span><span class="n">get_qconv</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">activation_post_process</span><span class="p">)</span></div></div>

<span class="c1"># === Transposed Convolutions ===</span>

<span class="k">class</span> <span class="nc">_ConvTransposeNd</span><span class="p">(</span><span class="n">_ConvNd</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
                 <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
                 <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Only &quot;zeros&quot; padding mode is supported for </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">transposed</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_input_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
        <span class="c1"># type: (List[int], List[int], List[int]) -&gt; List[int]</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
        <span class="k">for</span> <span class="n">kdx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
            <span class="n">pad</span> <span class="o">=</span> <span class="p">(</span><span class="n">dilation</span><span class="p">[</span><span class="n">kdx</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">kernel_size</span><span class="p">[</span><span class="n">kdx</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="n">kdx</span><span class="p">])</span>
            <span class="n">res</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_float</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
        <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a quantized module from a float module or qparams_dict.</span>
<span class="sd">        Args:</span>
<span class="sd">            mod (Module): a float module, either produced by torch.quantization</span>
<span class="sd">              utilities or provided by the user</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span> <span class="o">==</span> <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="p">,</span> \
            <span class="s1">&#39; nnq.&#39;</span> <span class="o">+</span> <span class="bp">cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="o">+</span> <span class="s1">&#39;.from_float only works for &#39;</span> <span class="o">+</span> \
            <span class="bp">cls</span><span class="o">.</span><span class="n">_FLOAT_MODULE</span><span class="o">.</span><span class="vm">__name__</span>
        <span class="k">assert</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="s1">&#39;qconfig&#39;</span><span class="p">),</span> \
            <span class="s1">&#39;Input float module must have qconfig defined.&#39;</span>
        <span class="n">weight_post_process</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">qconfig</span><span class="o">.</span><span class="n">weight</span><span class="p">()</span>
        <span class="n">weight_post_process</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">act_scale</span><span class="p">,</span> <span class="n">act_zp</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">activation_post_process</span><span class="o">.</span><span class="n">calculate_qparams</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">weight_post_process</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> \
            <span class="s1">&#39;Weight observer must have a dtype of qint8&#39;</span>
        <span class="n">qweight</span> <span class="o">=</span> <span class="n">_quantize_weight</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">weight_post_process</span><span class="p">)</span>
        <span class="n">qconv</span> <span class="o">=</span> <span class="bp">cls</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">,</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                    <span class="n">mod</span><span class="o">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">padding_mode</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">set_weight_bias</span><span class="p">(</span><span class="n">qweight</span><span class="p">,</span> <span class="n">mod</span><span class="o">.</span><span class="n">bias</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">act_scale</span><span class="p">)</span>
        <span class="n">qconv</span><span class="o">.</span><span class="n">zero_point</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">act_zp</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">qconv</span>


<span class="k">class</span> <span class="nc">ConvTranspose1d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>
<span class="sd">    For details on input arguments, parameters, and implementation see</span>
<span class="sd">    :class:`~torch.nn.ConvTranspose1d`.</span>

<span class="sd">    .. note:: Currently only the QNNPACK engine is implemented.</span>
<span class="sd">        Please, set the `torch.backends.quantized.engine = &#39;qnnpack&#39;`</span>

<span class="sd">    For special notes, please, see :class:`~torch.nn.quantized.Conv1d`</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor):     packed tensor derived from the learnable weight</span>
<span class="sd">                             parameter.</span>
<span class="sd">        scale (Tensor):      scalar for the output scale</span>
<span class="sd">        zero_point (Tensor): scalar for the output zero point</span>
<span class="sd">    See :class:`~torch.nn.ConvTranspose2d` for other attributes.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; torch.backends.quantized.engine = &#39;qnnpack&#39;</span>
<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nnq.ConvTranspose1d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nnq.ConvTranspose1d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50)</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; output = m(q_input)</span>
<span class="sd">        &gt;&gt;&gt; # exact output size can be also specified as an argument</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 16, 12)</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; downsample = nnq.Conv1d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; upsample = nnq.ConvTranspose1d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; h = downsample(q_input)</span>
<span class="sd">        &gt;&gt;&gt; h.size()</span>
<span class="sd">        torch.Size([1, 16, 6])</span>
<span class="sd">        &gt;&gt;&gt; output = upsample(h, output_size=input.size())</span>
<span class="sd">        &gt;&gt;&gt; output.size()</span>
<span class="sd">        torch.Size([1, 16, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_FLOAT_MODULE</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose1d</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose1d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;QuantizedConvTranpose1d&#39;</span>

    <span class="k">def</span> <span class="nf">set_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv_transpose1d_prepack</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv_transpose1d_unpack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">w</span>

    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Temporarily using len(shape) instead of ndim due to JIT issue</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/23890</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape must be `(N, C, L)`!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">ConvTranspose2d</span><span class="p">(</span><span class="n">_ConvTransposeNd</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D transposed convolution operator over an input image</span>
<span class="sd">    composed of several input planes.</span>
<span class="sd">    For details on input arguments, parameters, and implementation see</span>
<span class="sd">    :class:`~torch.nn.ConvTranspose2d`.</span>

<span class="sd">    .. note:: Currently only the QNNPACK engine is implemented.</span>
<span class="sd">        Please, set the `torch.backends.quantized.engine = &#39;qnnpack&#39;`</span>

<span class="sd">    For special notes, please, see :class:`~torch.nn.quantized.Conv2d`</span>

<span class="sd">    Attributes:</span>
<span class="sd">        weight (Tensor):     packed tensor derived from the learnable weight</span>
<span class="sd">                             parameter.</span>
<span class="sd">        scale (Tensor):      scalar for the output scale</span>
<span class="sd">        zero_point (Tensor): scalar for the output zero point</span>
<span class="sd">    See :class:`~torch.nn.ConvTranspose2d` for other attributes.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; torch.backends.quantized.engine = &#39;qnnpack&#39;</span>
<span class="sd">        &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="sd">        &gt;&gt;&gt; m = nnq.ConvTranspose2d(16, 33, 3, stride=2)</span>
<span class="sd">        &gt;&gt;&gt; # non-square kernels and unequal stride and with padding</span>
<span class="sd">        &gt;&gt;&gt; m = nnq.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 100)</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; output = m(q_input)</span>
<span class="sd">        &gt;&gt;&gt; # exact output size can be also specified as an argument</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(1, 16, 12, 12)</span>
<span class="sd">        &gt;&gt;&gt; q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)</span>
<span class="sd">        &gt;&gt;&gt; downsample = nnq.Conv2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; upsample = nnq.ConvTranspose2d(16, 16, 3, stride=2, padding=1)</span>
<span class="sd">        &gt;&gt;&gt; h = downsample(q_input)</span>
<span class="sd">        &gt;&gt;&gt; h.size()</span>
<span class="sd">        torch.Size([1, 16, 6, 6])</span>
<span class="sd">        &gt;&gt;&gt; output = upsample(h, output_size=input.size())</span>
<span class="sd">        &gt;&gt;&gt; output.size()</span>
<span class="sd">        torch.Size([1, 16, 12, 12])</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">_FLOAT_MODULE</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">):</span>
        <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="n">dilation</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">)</span>
        <span class="n">output_padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_padding</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">ConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span>
            <span class="kc">True</span><span class="p">,</span> <span class="n">output_padding</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">padding_mode</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_name</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39;QuantizedConvTranpose2d&#39;</span>

    <span class="k">def</span> <span class="nf">set_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
        <span class="c1"># type: (torch.Tensor, Optional[torch.Tensor]) -&gt; None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv_transpose2d_prepack</span><span class="p">(</span>
            <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_weight_bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv2d_unpack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span>

    <span class="k">def</span> <span class="nf">weight</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">w</span>

    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_weight_bias</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">b</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">):</span>
        <span class="c1"># Temporarily using len(shape) instead of ndim due to JIT issue</span>
        <span class="c1"># https://github.com/pytorch/pytorch/issues/23890</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input shape must be `(N, C, H, W)`!&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span><span class="o">.</span><span class="n">quantized</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
            <span class="nb">input</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_packed_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">zero_point</span><span class="p">)</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../../" src="../../../../../_static/documentation_options.js"></script>
         <script src="../../../../../_static/jquery.js"></script>
         <script src="../../../../../_static/underscore.js"></script>
         <script src="../../../../../_static/doctools.js"></script>
         <script src="../../../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>