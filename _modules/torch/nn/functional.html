


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.functional &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 

  
  <script src="../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/functional.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.functional</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.functional</h1><div class="highlight"><pre>
<span></span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Functional interface&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">math</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch._C</span> <span class="kn">import</span> <span class="n">_infer_size</span><span class="p">,</span> <span class="n">_add_docstr</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">_reduction</span> <span class="k">as</span> <span class="n">_Reduction</span>
<span class="kn">from</span> <span class="nn">.modules</span> <span class="kn">import</span> <span class="n">utils</span>
<span class="kn">from</span> <span class="nn">.modules.utils</span> <span class="kn">import</span> <span class="n">_single</span><span class="p">,</span> <span class="n">_pair</span><span class="p">,</span> <span class="n">_triple</span><span class="p">,</span> <span class="n">_list_with_default</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">grad</span>  <span class="c1"># noqa: F401</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">_VF</span>
<span class="kn">from</span> <span class="nn">.._jit_internal</span> <span class="kn">import</span> <span class="n">boolean_dispatch</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">_overload</span><span class="p">,</span> <span class="n">Tuple</span>
<span class="kn">from</span> <span class="nn">..overrides</span> <span class="kn">import</span> <span class="n">has_torch_function</span><span class="p">,</span> <span class="n">handle_torch_function</span>


<span class="n">Tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>

<span class="n">conv1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D convolution over an input signal composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.Conv1d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or</span>
<span class="s2">      a one-element tuple `(sW,)`. Default: 1</span>
<span class="s2">    padding: implicit paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a one-element tuple `(padW,)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a one-element tuple `(dW,)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; F.conv1d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.Conv2d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>


<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iH , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kH , kW)`</span>
<span class="s2">    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: ``None``</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; filters = torch.randn(8,4,3,3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1,4,5,5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv2d(inputs, filters, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D convolution over an input image composed of several input</span>
<span class="s2">planes.</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.Conv3d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iT , iH , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kT , kH , kW)`</span>
<span class="s2">    bias: optional bias tensor of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: 1</span>
<span class="s2">    padding: implicit paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`. Default: 0</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by</span>
<span class="s2">      the number of groups. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; filters = torch.randn(33, 16, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; F.conv3d(inputs, filters)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_transpose1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose1d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 1D transposed convolution operator over an input signal</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose1d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sW,)``. Default: 1</span>
<span class="s2">    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padW,)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dW,)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 5)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose1d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">conv_transpose2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose2d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 2D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;.</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose2d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iH , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kH , kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple ``(out_padH, out_padW)``.</span>
<span class="s2">      Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple ``(dH, dW)``. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # With square kernels and equal stride</span>
<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(1, 4, 5, 5)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(4, 8, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose2d(inputs, weights, padding=1)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_transpose3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_transpose3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">conv_transpose3d(input, weight, bias=None, stride=1, padding=0, output_padding=0, groups=1, dilation=1) -&gt; Tensor</span>

<span class="s2">Applies a 3D transposed convolution operator over an input image</span>
<span class="s2">composed of several input planes, sometimes also called &quot;deconvolution&quot;</span>

<span class="s2">This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="s2">See :class:`~torch.nn.ConvTranspose3d` for details and output shape.</span>

<span class="s2">Note:</span>
<span class="s2">    In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="s2">    may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="s2">    undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="s2">    a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="s2">    True``.</span>
<span class="s2">    Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iT , iH , iW)`</span>
<span class="s2">    weight: filters of shape :math:`(\text{in\_channels} , \frac{\text{out\_channels}}{\text</span><span class="si">{groups}</span><span class="s2">} , kT , kH , kW)`</span>
<span class="s2">    bias: optional bias of shape :math:`(\text{out\_channels})`. Default: None</span>
<span class="s2">    stride: the stride of the convolving kernel. Can be a single number or a</span>
<span class="s2">      tuple ``(sT, sH, sW)``. Default: 1</span>
<span class="s2">    padding: ``dilation * (kernel_size - 1) - padding`` zero-padding will be added to both</span>
<span class="s2">      sides of each dimension in the input. Can be a single number or a tuple</span>
<span class="s2">      ``(padT, padH, padW)``. Default: 0</span>
<span class="s2">    output_padding: additional size added to one side of each dimension in the</span>
<span class="s2">      output shape. Can be a single number or a tuple</span>
<span class="s2">      ``(out_padT, out_padH, out_padW)``. Default: 0</span>
<span class="s2">    groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the</span>
<span class="s2">      number of groups. Default: 1</span>
<span class="s2">    dilation: the spacing between kernel elements. Can be a single number or</span>
<span class="s2">      a tuple `(dT, dH, dW)`. Default: 1</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; inputs = torch.randn(20, 16, 50, 10, 20)</span>
<span class="s2">    &gt;&gt;&gt; weights = torch.randn(16, 33, 3, 3, 3)</span>
<span class="s2">    &gt;&gt;&gt; F.conv_transpose3d(inputs, weights)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>  <span class="c1"># noqa: E501</span>

<span class="n">conv_tbc</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">conv_tbc</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Applies a 1-dimensional sequence convolution over an input sequence.</span>
<span class="s2">Input and output dimensions are (Time, Batch, Channels) - hence TBC.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text{sequence length} \times batch \times \text{in\_channels})`</span>
<span class="s2">    weight: filter of shape (:math:`\text{kernel width} \times \text{in\_channels} \times \text{out\_channels}`)</span>
<span class="s2">    bias: bias of shape (:math:`\text{out\_channels}`)</span>
<span class="s2">    pad: number of timesteps to pad. Default: 0</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="c1"># Pooling</span>
<span class="n">avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool1d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True) -&gt; Tensor</span>

<span class="s2">Applies a 1D average pooling over an input signal composed of several</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iW)`</span>
<span class="s2">    kernel_size: the size of the window. Can be a single number or a</span>
<span class="s2">      tuple `(kW,)`</span>
<span class="s2">    stride: the stride of the window. Can be a single number or a tuple</span>
<span class="s2">      `(sW,)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padW,)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` to compute the</span>
<span class="s2">        output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; # pool of square window of size=3, stride=2</span>
<span class="s2">    &gt;&gt;&gt; input = torch.tensor([[[1, 2, 3, 4, 5, 6, 7]]], dtype=torch.float32)</span>
<span class="s2">    &gt;&gt;&gt; F.avg_pool1d(input, kernel_size=3, stride=2)</span>
<span class="s2">    tensor([[[ 2.,  4.,  6.]]])</span>

<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">avg_pool2d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool2d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool2d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -&gt; Tensor</span>

<span class="s2">Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size</span>
<span class="s2">:math:`sH \times sW` steps. The number of output features is equal to the number of</span>
<span class="s2">input planes.</span>

<span class="s2">See :class:`~torch.nn.AvgPool2d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iH , iW)`</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple `(kH, kW)`</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padH, padW)`. Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape. Default: ``False``</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation. Default: ``True``</span>
<span class="s2">    divisor_override: if specified, it will be used as divisor, otherwise</span>
<span class="s2">         size of the pooling region will be used. Default: None</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">avg_pool3d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">avg_pool3d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">avg_pool3d(input, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None) -&gt; Tensor</span>

<span class="s2">Applies 3D average-pooling operation in :math:`kT \times kH \times kW` regions by step</span>
<span class="s2">size :math:`sT \times sH \times sW` steps. The number of output features is equal to</span>
<span class="s2">:math:`\lfloor\frac{\text{input planes}}</span><span class="si">{sT}</span><span class="s2">\rfloor`.</span>

<span class="s2">See :class:`~torch.nn.AvgPool3d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor :math:`(\text</span><span class="si">{minibatch}</span><span class="s2"> , \text{in\_channels} , iT \times iH , iW)`</span>
<span class="s2">    kernel_size: size of the pooling region. Can be a single number or a</span>
<span class="s2">      tuple `(kT, kH, kW)`</span>
<span class="s2">    stride: stride of the pooling operation. Can be a single number or a</span>
<span class="s2">      tuple `(sT, sH, sW)`. Default: :attr:`kernel_size`</span>
<span class="s2">    padding: implicit zero paddings on both sides of the input. Can be a</span>
<span class="s2">      single number or a tuple `(padT, padH, padW)`, Default: 0</span>
<span class="s2">    ceil_mode: when True, will use `ceil` instead of `floor` in the formula</span>
<span class="s2">        to compute the output shape</span>
<span class="s2">    count_include_pad: when True, will include the zero-padding in the</span>
<span class="s2">        averaging calculation</span>
<span class="s2">    divisor_override: if specified, it will be used as divisor, otherwise</span>
<span class="s2">        size of the pooling region will be used. Default: None</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fractional_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                       <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], Optional[BroadcastingList2[float]], bool, Optional[Tensor]) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies 2D fractional max pooling over an input signal composed of several input planes.</span>

<span class="sd">    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham</span>

<span class="sd">    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic</span>
<span class="sd">    step size determined by the target output size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window to take a max over.</span>
<span class="sd">                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k`)</span>
<span class="sd">                     or a tuple `(kH, kW)`</span>
<span class="sd">        output_size: the target output size of the image of the form :math:`oH \times oW`.</span>
<span class="sd">                     Can be a tuple `(oH, oW)` or a single number :math:`oH` for a square image :math:`oH \times oH`</span>
<span class="sd">        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.</span>
<span class="sd">                      This has to be a number or tuple in the range (0, 1)</span>
<span class="sd">        return_indices: if ``True``, will return the indices along with the outputs.</span>
<span class="sd">                        Useful to pass to :func:`~torch.nn.functional.max_unpool2d`.</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32)</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window of size=3, and target output size 13x12</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_size=(13, 12))</span>
<span class="sd">        &gt;&gt;&gt; # pool of square window and target output size being half of input image size</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool2d(input, 3, output_ratio=(0.5, 0.5))</span>

<span class="sd">    .. _Fractional MaxPooling:</span>
<span class="sd">        http://arxiv.org/abs/1412.6071</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">fractional_max_pool2d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="n">output_ratio</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">,</span> <span class="n">_random_samples</span><span class="o">=</span><span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;fractional_max_pool2d requires specifying either &quot;</span>
                         <span class="s2">&quot;an output_size or an output_ratio&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">_output_ratio</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">])]</span>

    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fractional_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], Optional[BroadcastingList2[float]], bool, Optional[Tensor]) -&gt; Tensor  # noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">fractional_max_pool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="n">output_ratio</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">,</span> <span class="n">_random_samples</span><span class="o">=</span><span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fractional_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                              <span class="n">output_ratio</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span>
                                              <span class="n">_random_samples</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">fractional_max_pool2d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">fractional_max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_fractional_max_pool2d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;fractional_max_pool2d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">fractional_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                       <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                       <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], Optional[BroadcastingList3[float]], bool, Optional[Tensor]) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies 3D fractional max pooling over an input signal composed of several input planes.</span>

<span class="sd">    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham</span>

<span class="sd">    The max-pooling operation is applied in :math:`kT \times kH \times kW` regions by a stochastic</span>
<span class="sd">    step size determined by the target output size.</span>
<span class="sd">    The number of output features is equal to the number of input planes.</span>

<span class="sd">    Args:</span>
<span class="sd">        kernel_size: the size of the window to take a max over.</span>
<span class="sd">                     Can be a single number :math:`k` (for a square kernel of :math:`k \times k \times k`)</span>
<span class="sd">                     or a tuple `(kT, kH, kW)`</span>
<span class="sd">        output_size: the target output size of the form :math:`oT \times oH \times oW`.</span>
<span class="sd">                     Can be a tuple `(oT, oH, oW)` or a single number :math:`oH` for a cubic output</span>
<span class="sd">                      :math:`oH \times oH \times oH`</span>
<span class="sd">        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.</span>
<span class="sd">                      This has to be a number or tuple in the range (0, 1)</span>
<span class="sd">        return_indices: if ``True``, will return the indices along with the outputs.</span>
<span class="sd">                        Useful to pass to :func:`~torch.nn.functional.max_unpool3d`.</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(20, 16, 50, 32, 16)</span>
<span class="sd">        &gt;&gt;&gt; # pool of cubic window of size=3, and target output size 13x12x11</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool3d(input, 3, output_size=(13, 12, 11))</span>
<span class="sd">        &gt;&gt;&gt; # pool of cubic window and target output size being half of input size</span>
<span class="sd">        &gt;&gt;&gt; F.fractional_max_pool3d(input, 3, output_ratio=(0.5, 0.5, 0.5))</span>

<span class="sd">    .. _Fractional MaxPooling:</span>
<span class="sd">        http://arxiv.org/abs/1412.6071</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">fractional_max_pool3d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="n">output_ratio</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">,</span> <span class="n">_random_samples</span><span class="o">=</span><span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;fractional_max_pool3d requires specifying either &quot;</span>
                         <span class="s2">&quot;an output_size or an output_ratio&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">output_ratio</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="n">_output_ratio</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">output_ratio</span><span class="p">)</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                       <span class="nb">int</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="n">_output_ratio</span><span class="p">[</span><span class="mi">2</span><span class="p">])]</span>

    <span class="k">if</span> <span class="n">_random_samples</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_random_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">fractional_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">_random_samples</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_fractional_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                           <span class="n">output_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                           <span class="n">_random_samples</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], Optional[BroadcastingList3[float]], bool, Optional[Tensor]) -&gt; Tensor  # noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">fractional_max_pool3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span> <span class="n">output_ratio</span><span class="o">=</span><span class="n">output_ratio</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">,</span> <span class="n">_random_samples</span><span class="o">=</span><span class="n">_random_samples</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fractional_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                                              <span class="n">output_ratio</span><span class="p">,</span> <span class="n">return_indices</span><span class="p">,</span>
                                              <span class="n">_random_samples</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">fractional_max_pool3d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">fractional_max_pool3d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_fractional_max_pool3d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;fractional_max_pool3d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], BroadcastingList1[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool1d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool1d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], BroadcastingList1[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool1d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool1d</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>

<span class="n">max_pool1d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool1d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool1d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;max_pool1d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], BroadcastingList2[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool2d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], BroadcastingList2[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>

<span class="n">max_pool2d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool2d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;max_pool2d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                            <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], BroadcastingList3[int], bool, bool) -&gt; Tuple[Tensor, Tensor]  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D max pooling over an input signal composed of several input</span>
<span class="sd">    planes.</span>

<span class="sd">    See :class:`~torch.nn.MaxPool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool3d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_pool3d_with_indices</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], BroadcastingList3[int], bool, bool) -&gt; Tensor  # noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_pool3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stride</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">max_pool3d</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>

<span class="n">max_pool3d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">max_pool3d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_max_pool3d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;max_pool3d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], List[int], List[int], Optional[List[int]]) -&gt; List[int]</span>
    <span class="n">input_size</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">default_size</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="p">[])</span>
    <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
        <span class="n">default_size</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_size</span><span class="p">[</span><span class="n">d</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span>
                            <span class="n">kernel_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">padding</span><span class="p">[</span><span class="n">d</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">default_size</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;output_size should be a sequence containing &quot;</span>
                             <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> or </span><span class="si">{}</span><span class="s2"> elements, but it has a length of &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span>
                             <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                     <span class="nb">len</span><span class="p">(</span><span class="n">output_size</span><span class="p">)))</span>
        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)):</span>
            <span class="n">min_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">-</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
            <span class="n">max_size</span> <span class="o">=</span> <span class="n">default_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">+</span> <span class="n">stride</span><span class="p">[</span><span class="n">d</span><span class="p">]</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">min_size</span> <span class="o">&lt;</span> <span class="n">output_size</span><span class="p">[</span><span class="n">d</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">max_size</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s1">&#39;invalid output_size &quot;</span><span class="si">{}</span><span class="s1">&quot; (dim </span><span class="si">{}</span><span class="s1"> must be between </span><span class="si">{}</span><span class="s1"> and </span><span class="si">{}</span><span class="s1">)&#39;</span>
                    <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">min_size</span><span class="p">,</span> <span class="n">max_size</span><span class="p">))</span>

        <span class="n">ret</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="max_unpool1d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.max_unpool1d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList1[int], Optional[BroadcastingList1[int]], BroadcastingList1[int], Optional[BroadcastingList1[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool1d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_unpool1d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_single</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">indices</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span>
                                     <span class="n">output_size</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool2d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.max_unpool2d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList2[int], Optional[BroadcastingList2[int]], BroadcastingList2[int], Optional[BroadcastingList2[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool2d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_unpool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="max_unpool3d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.max_unpool3d">[docs]</a><span class="k">def</span> <span class="nf">max_unpool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, BroadcastingList3[int], Optional[BroadcastingList3[int]], BroadcastingList3[int], Optional[BroadcastingList3[int]]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes a partial inverse of :class:`MaxPool3d`.</span>

<span class="sd">    See :class:`~torch.nn.MaxUnpool3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">max_unpool3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">)</span>
    <span class="n">kernel_size</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">_stride</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="n">padding</span> <span class="o">=</span> <span class="n">_triple</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_unpool_output_size</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span>
                                      <span class="n">output_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">max_unpool3d</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">indices</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">_stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool2d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.lp_pool2d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, Optional[BroadcastingList2[int]], bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool2d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">lp_pool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="n">kw</span><span class="p">,</span> <span class="n">kh</span> <span class="o">=</span> <span class="n">utils</span><span class="o">.</span><span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kw</span> <span class="o">*</span> <span class="n">kh</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<div class="viewcode-block" id="lp_pool1d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.lp_pool1d">[docs]</a><span class="k">def</span> <span class="nf">lp_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, Optional[BroadcastingList1[int]], bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D power-average pooling over an input signal composed of</span>
<span class="sd">    several input planes. If the sum of all inputs to the power of `p` is</span>
<span class="sd">    zero, the gradient is set to zero as well.</span>

<span class="sd">    See :class:`~torch.nn.LPPool1d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">lp_pool1d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">stride</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">norm_type</span><span class="p">),</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">ceil_mode</span><span class="o">=</span><span class="n">ceil_mode</span><span class="p">)</span>

    <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">*</span> <span class="n">relu</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">out</span><span class="p">)))</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">1.</span> <span class="o">/</span> <span class="n">norm_type</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">adaptive_max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 1D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool1d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool1d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_adaptive_max_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList1[int], bool) -&gt; Tensor</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool1d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool1d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool1d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool1d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool1d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;adaptive_max_pool1d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adaptive_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 2D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool2d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_adaptive_max_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], bool) -&gt; Tensor</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool2d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool2d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool2d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool2d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;adaptive_max_pool2d&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">adaptive_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], bool) -&gt; Tuple[Tensor, Tensor]</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a 3D adaptive max pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveMaxPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">        return_indices: whether to return pooling indices. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool3d_with_indices</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_adaptive_max_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">return_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int], bool) -&gt; Tensor</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_max_pool3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span>
                <span class="n">return_indices</span><span class="o">=</span><span class="n">return_indices</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adaptive_max_pool3d_with_indices</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">adaptive_max_pool3d</span> <span class="o">=</span> <span class="n">boolean_dispatch</span><span class="p">(</span>
    <span class="n">arg_name</span><span class="o">=</span><span class="s1">&#39;return_indices&#39;</span><span class="p">,</span>
    <span class="n">arg_index</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">default</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">if_true</span><span class="o">=</span><span class="n">adaptive_max_pool3d_with_indices</span><span class="p">,</span>
    <span class="n">if_false</span><span class="o">=</span><span class="n">_adaptive_max_pool3d</span><span class="p">,</span>
    <span class="n">module_name</span><span class="o">=</span><span class="vm">__name__</span><span class="p">,</span>
    <span class="n">func_name</span><span class="o">=</span><span class="s1">&#39;adaptive_max_pool3d&#39;</span><span class="p">)</span>


<span class="n">adaptive_avg_pool1d</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">adaptive_avg_pool1d</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">adaptive_avg_pool1d(input, output_size) -&gt; Tensor</span>

<span class="s2">Applies a 1D adaptive average pooling over an input signal composed of</span>
<span class="s2">several input planes.</span>

<span class="s2">See :class:`~torch.nn.AdaptiveAvgPool1d` for details and output shape.</span>

<span class="s2">Args:</span>
<span class="s2">    output_size: the target output size (single integer)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="adaptive_avg_pool2d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.adaptive_avg_pool2d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 2D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool2d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            double-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_avg_pool2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">_output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">)</span></div>


<div class="viewcode-block" id="adaptive_avg_pool3d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.adaptive_avg_pool3d">[docs]</a><span class="k">def</span> <span class="nf">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList3[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a 3D adaptive average pooling over an input signal composed of</span>
<span class="sd">    several input planes.</span>

<span class="sd">    See :class:`~torch.nn.AdaptiveAvgPool3d` for details and output shape.</span>

<span class="sd">    Args:</span>
<span class="sd">        output_size: the target output size (single integer or</span>
<span class="sd">            triple-integer tuple)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">adaptive_avg_pool3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="n">_output_size</span> <span class="o">=</span> <span class="n">_list_with_default</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_output_size</span><span class="p">)</span></div>


<span class="c1"># Activation functions</span>
<div class="viewcode-block" id="dropout"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.dropout">[docs]</a><span class="k">def</span> <span class="nf">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    During training, randomly zeroes some of the elements of the input</span>
<span class="sd">    tensor with probability :attr:`p` using samples from a Bernoulli</span>
<span class="sd">    distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of an element to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Default: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">dropout</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="alpha_dropout"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.alpha_dropout">[docs]</a><span class="k">def</span> <span class="nf">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies alpha dropout to the input.</span>

<span class="sd">    See :class:`~torch.nn.AlphaDropout` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">alpha_dropout</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">alpha_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="dropout2d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.dropout2d">[docs]</a><span class="k">def</span> <span class="nf">dropout2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly zero out entire channels (a channel is a 2D feature map,</span>
<span class="sd">    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the</span>
<span class="sd">    batched input is a 2D tensor :math:`\text{input}[i, j]`) of the input tensor).</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call with</span>
<span class="sd">    probability :attr:`p` using samples from a Bernoulli distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout2d` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of a channel to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Default: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">dropout2d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="dropout3d"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.dropout3d">[docs]</a><span class="k">def</span> <span class="nf">dropout3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly zero out entire channels (a channel is a 3D feature map,</span>
<span class="sd">    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the</span>
<span class="sd">    batched input is a 3D tensor :math:`\text{input}[i, j]`) of the input tensor).</span>
<span class="sd">    Each channel will be zeroed out independently on every forward call with</span>
<span class="sd">    probability :attr:`p` using samples from a Bernoulli distribution.</span>

<span class="sd">    See :class:`~torch.nn.Dropout3d` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: probability of a channel to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Default: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># This is 100% the same code as dropout2d. We duplicate this code so that</span>
    <span class="c1"># stack traces are not confusing.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">dropout3d</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<div class="viewcode-block" id="feature_alpha_dropout"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.feature_alpha_dropout">[docs]</a><span class="k">def</span> <span class="nf">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Randomly masks out entire channels (a channel is a feature map,</span>
<span class="sd">    e.g. the :math:`j`-th channel of the :math:`i`-th sample in the batch input</span>
<span class="sd">    is a tensor :math:`\text{input}[i, j]`) of the input tensor). Instead of</span>
<span class="sd">    setting activations to zero, as in regular Dropout, the activations are set</span>
<span class="sd">    to the negative saturation value of the SELU activation function.</span>

<span class="sd">    Each element will be masked independently on every forward call with</span>
<span class="sd">    probability :attr:`p` using samples from a Bernoulli distribution.</span>
<span class="sd">    The elements to be masked are randomized on every forward call, and scaled</span>
<span class="sd">    and shifted to maintain zero mean and unit variance.</span>

<span class="sd">    See :class:`~torch.nn.FeatureAlphaDropout` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        p: dropout probability of a channel to be zeroed. Default: 0.5</span>
<span class="sd">        training: apply dropout if is ``True``. Default: ``True``</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">feature_alpha_dropout</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
                <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&lt;</span> <span class="mf">0.</span> <span class="ow">or</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mf">1.</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;dropout probability has to be between 0 and 1, &quot;</span>
                         <span class="s2">&quot;but got </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">p</span><span class="p">))</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">feature_alpha_dropout_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">inplace</span>
            <span class="k">else</span> <span class="n">_VF</span><span class="o">.</span><span class="n">feature_alpha_dropout</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">training</span><span class="p">))</span></div>


<span class="k">def</span> <span class="nf">_threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Thresholds each element of the input Tensor.</span>

<span class="sd">    See :class:`~torch.nn.Threshold` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">_threshold</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_VF</span><span class="o">.</span><span class="n">threshold_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">_VF</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">threshold</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span>

<span class="c1"># We define this function as _threshold because it takes an argument</span>
<span class="c1"># named threshold, which clobbers the recursive reference to the</span>
<span class="c1"># function needed for __torch_function__ support</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="n">_threshold</span>

<span class="n">threshold_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">_VF</span><span class="o">.</span><span class="n">threshold_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">threshold_(input, threshold, value) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~threshold`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.relu">[docs]</a><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the rectified linear unit function element-wise. See</span>
<span class="sd">    :class:`~torch.nn.ReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">relu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="glu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.glu">[docs]</a><span class="k">def</span> <span class="nf">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    glu(input, dim=-1) -&gt; Tensor</span>

<span class="sd">    The gated linear unit. Computes:</span>

<span class="sd">    .. math ::</span>
<span class="sd">        \text{GLU}(a, b) = a \otimes \sigma(b)</span>

<span class="sd">    where `input` is split in half along `dim` to form `a` and `b`, :math:`\sigma`</span>
<span class="sd">    is the sigmoid function and :math:`\otimes` is the element-wise product between matrices.</span>

<span class="sd">    See `Language Modeling with Gated Convolutional Networks &lt;https://arxiv.org/abs/1612.08083&gt;`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input tensor</span>
<span class="sd">        dim (int): dimension on which to split the input. Default: -1</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">glu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;glu does not support scalars because halving size must be even&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">glu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardtanh"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.hardtanh">[docs]</a><span class="k">def</span> <span class="nf">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">min_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="n">max_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardtanh(input, min_val=-1., max_val=1., inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the HardTanh function element-wise. See :class:`~torch.nn.Hardtanh` for more</span>
<span class="sd">    details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">hardtanh</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="o">=</span><span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="o">=</span><span class="n">max_val</span><span class="p">,</span>
                <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">min_val</span><span class="p">,</span> <span class="n">max_val</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">hardtanh_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardtanh_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">hardtanh_(input, min_val=-1., max_val=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~hardtanh`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="relu6"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.relu6">[docs]</a><span class="k">def</span> <span class="nf">relu6</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;relu6(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{ReLU6}(x) = \min(\max(0,x), 6)`.</span>

<span class="sd">    See :class:`~torch.nn.ReLU6` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">relu6</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">hardtanh</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="n">inplace</span><span class="p">)</span></div>


<div class="viewcode-block" id="elu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.elu">[docs]</a><span class="k">def</span> <span class="nf">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies element-wise,</span>
<span class="sd">    :math:`\text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))`.</span>

<span class="sd">    See :class:`~torch.nn.ELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">elu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                                         <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">elu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">elu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">elu_(input, alpha=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~elu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="selu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.selu">[docs]</a><span class="k">def</span> <span class="nf">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;selu(input, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))`,</span>
<span class="sd">    with :math:`\alpha=1.6732632423543772848170429916717` and</span>
<span class="sd">    :math:`scale=1.0507009873554804934193349852946`.</span>

<span class="sd">    See :class:`~torch.nn.SELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">selu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">selu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">selu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">selu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">selu_(input) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~selu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="celu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.celu">[docs]</a><span class="k">def</span> <span class="nf">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;celu(input, alpha=1., inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))`.</span>

<span class="sd">    See :class:`~torch.nn.CELU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">celu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span>
                                         <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">celu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">celu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>

<span class="n">celu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">celu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">celu_(input, alpha=1.) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~celu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="leaky_relu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.leaky_relu">[docs]</a><span class="k">def</span> <span class="nf">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    leaky_relu(input, negative_slope=0.01, inplace=False) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)`</span>

<span class="sd">    See :class:`~torch.nn.LeakyReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">leaky_relu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="o">=</span><span class="n">negative_slope</span><span class="p">,</span>
                <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">negative_slope</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">leaky_relu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">leaky_relu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">leaky_relu_(input, negative_slope=0.01) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~leaky_relu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="prelu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.prelu">[docs]</a><span class="k">def</span> <span class="nf">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;prelu(input, weight) -&gt; Tensor</span>

<span class="sd">    Applies element-wise the function</span>
<span class="sd">    :math:`\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)` where weight is a</span>
<span class="sd">    learnable parameter.</span>

<span class="sd">    See :class:`~torch.nn.PReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">prelu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">prelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span></div>


<div class="viewcode-block" id="rrelu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.rrelu">[docs]</a><span class="k">def</span> <span class="nf">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">8</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="mf">1.</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;rrelu(input, lower=1./8, upper=1./3, training=False, inplace=False) -&gt; Tensor</span>

<span class="sd">    Randomized leaky ReLU.</span>

<span class="sd">    See :class:`~torch.nn.RReLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">rrelu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="o">=</span><span class="n">upper</span><span class="p">,</span>
                <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rrelu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">training</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">result</span></div>


<span class="n">rrelu_</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">rrelu_</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">rrelu_(input, lower=1./8, upper=1./3, training=False) -&gt; Tensor</span>

<span class="s2">In-place version of :func:`~rrelu`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">logsigmoid</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">log_sigmoid</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">logsigmoid(input) -&gt; Tensor</span>

<span class="s2">Applies element-wise :math:`\text</span><span class="si">{LogSigmoid}</span><span class="s2">(x_i) = \log \left(\frac</span><span class="si">{1}</span><span class="s2">{1 + \exp(-x_i)}\right)`</span>

<span class="s2">See :class:`~torch.nn.LogSigmoid` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="gelu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.gelu">[docs]</a><span class="k">def</span> <span class="nf">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;gelu(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise the function</span>
<span class="sd">    :math:`\text{GELU}(x) = x * \Phi(x)`</span>

<span class="sd">    where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.</span>

<span class="sd">    See `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">gelu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="hardshrink"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.hardshrink">[docs]</a><span class="k">def</span> <span class="nf">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    hardshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="sd">    Applies the hard shrinkage function element-wise</span>

<span class="sd">    See :class:`~torch.nn.Hardshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">hardshrink</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="n">lambd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hardshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">lambd</span><span class="p">)</span></div>


<div class="viewcode-block" id="tanhshrink"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.tanhshrink">[docs]</a><span class="k">def</span> <span class="nf">tanhshrink</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanhshrink(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, :math:`\text{Tanhshrink}(x) = x - \text{Tanh}(x)`</span>

<span class="sd">    See :class:`~torch.nn.Tanhshrink` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">tanhshrink</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">-</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="softsign"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.softsign">[docs]</a><span class="k">def</span> <span class="nf">softsign</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;softsign(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise, the function :math:`\text{SoftSign}(x) = \frac{x}{1 + |x|}`</span>

<span class="sd">    See :class:`~torch.nn.Softsign` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">softsign</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span></div>


<span class="n">softplus</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softplus</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softplus(input, beta=1, threshold=20) -&gt; Tensor</span>

<span class="s2">Applies element-wise, the function :math:`\text</span><span class="si">{Softplus}</span><span class="s2">(x) = \frac</span><span class="si">{1}</span><span class="s2">{\beta} * \log(1 + \exp(\beta * x))`.</span>

<span class="s2">For numerical stability the implementation reverts to the linear function</span>
<span class="s2">when :math:`input \times \beta &gt; threshold`.</span>

<span class="s2">See :class:`~torch.nn.Softplus` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_softmax_dim</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">ndim</span><span class="p">,</span> <span class="n">stacklevel</span><span class="p">):</span>
    <span class="c1"># type: (str, int, int) -&gt; int</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Implicit dimension choice for </span><span class="si">{}</span><span class="s2"> has been deprecated. &quot;</span>
                  <span class="s2">&quot;Change the call to include dim=X as an argument.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">),</span> <span class="n">stacklevel</span><span class="o">=</span><span class="n">stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span> <span class="ow">or</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">ret</span>


<div class="viewcode-block" id="softmin"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.softmin">[docs]</a><span class="k">def</span> <span class="nf">softmin</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmin function.</span>

<span class="sd">    Note that :math:`\text{Softmin}(x) = \text{Softmax}(-x)`. See softmax definition for mathematical formula.</span>

<span class="sd">    See :class:`~torch.nn.Softmin` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmin will be computed (so every slice</span>
<span class="sd">            along dim will sum to 1).</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">          If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">          is performed. This is useful for preventing data type overflows. Default: None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">softmin</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="n">_stacklevel</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmin&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="softmax"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.softmax">[docs]</a><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax function.</span>

<span class="sd">    Softmax is defined as:</span>

<span class="sd">    :math:`\text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}`</span>

<span class="sd">    It is applied to all slices along dim, and will re-scale them so that the elements</span>
<span class="sd">    lie in the range `[0, 1]` and sum to 1.</span>

<span class="sd">    See :class:`~torch.nn.Softmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which softmax will be computed.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">          If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">          is performed. This is useful for preventing data type overflows. Default: None.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This function doesn&#39;t work directly with NLLLoss,</span>
<span class="sd">        which expects the Log to be computed between the Softmax and itself.</span>
<span class="sd">        Use log_softmax instead (it&#39;s faster and has better numerical properties).</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">softmax</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="n">_stacklevel</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="gumbel_softmax"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.gumbel_softmax">[docs]</a><span class="k">def</span> <span class="nf">gumbel_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, bool, float, int) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Samples from the Gumbel-Softmax distribution (`Link 1`_  `Link 2`_) and optionally discretizes.</span>

<span class="sd">    Args:</span>
<span class="sd">      logits: `[..., num_features]` unnormalized log probabilities</span>
<span class="sd">      tau: non-negative scalar temperature</span>
<span class="sd">      hard: if ``True``, the returned samples will be discretized as one-hot vectors,</span>
<span class="sd">            but will be differentiated as if it is the soft sample in autograd</span>
<span class="sd">      dim (int): A dimension along which softmax will be computed. Default: -1.</span>

<span class="sd">    Returns:</span>
<span class="sd">      Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.</span>
<span class="sd">      If ``hard=True``, the returned samples will be one-hot, otherwise they will</span>
<span class="sd">      be probability distributions that sum to 1 across `dim`.</span>

<span class="sd">    .. note::</span>
<span class="sd">      This function is here for legacy reasons, may be removed from nn.Functional in the future.</span>

<span class="sd">    .. note::</span>
<span class="sd">      The main trick for `hard` is to do  `y_hard - y_soft.detach() + y_soft`</span>

<span class="sd">      It achieves two things:</span>
<span class="sd">      - makes the output value exactly one-hot</span>
<span class="sd">      (since we add then subtract y_soft value)</span>
<span class="sd">      - makes the gradient equal to y_soft gradient</span>
<span class="sd">      (since we strip all other gradients)</span>

<span class="sd">    Examples::</span>
<span class="sd">        &gt;&gt;&gt; logits = torch.randn(20, 32)</span>
<span class="sd">        &gt;&gt;&gt; # Sample soft categorical using reparametrization trick:</span>
<span class="sd">        &gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=False)</span>
<span class="sd">        &gt;&gt;&gt; # Sample hard categorical using &quot;Straight-through&quot; trick:</span>
<span class="sd">        &gt;&gt;&gt; F.gumbel_softmax(logits, tau=1, hard=True)</span>

<span class="sd">    .. _Link 1:</span>
<span class="sd">        https://arxiv.org/abs/1611.00712</span>
<span class="sd">    .. _Link 2:</span>
<span class="sd">        https://arxiv.org/abs/1611.01144</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">logits</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">gumbel_softmax</span><span class="p">,</span> <span class="p">(</span><span class="n">logits</span><span class="p">,),</span> <span class="n">logits</span><span class="p">,</span> <span class="n">tau</span><span class="o">=</span><span class="n">tau</span><span class="p">,</span> <span class="n">hard</span><span class="o">=</span><span class="n">hard</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">eps</span> <span class="o">!=</span> <span class="mf">1e-10</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;`eps` parameter is deprecated and has no effect.&quot;</span><span class="p">)</span>

    <span class="n">gumbels</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span><span class="o">.</span><span class="n">exponential_</span><span class="p">()</span><span class="o">.</span><span class="n">log</span><span class="p">()</span>  <span class="c1"># ~Gumbel(0,1)</span>
    <span class="n">gumbels</span> <span class="o">=</span> <span class="p">(</span><span class="n">logits</span> <span class="o">+</span> <span class="n">gumbels</span><span class="p">)</span> <span class="o">/</span> <span class="n">tau</span>  <span class="c1"># ~Gumbel(logits,tau)</span>
    <span class="n">y_soft</span> <span class="o">=</span> <span class="n">gumbels</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">hard</span><span class="p">:</span>
        <span class="c1"># Straight through.</span>
        <span class="n">index</span> <span class="o">=</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">y_hard</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">legacy_contiguous_format</span><span class="p">)</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">)</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_hard</span> <span class="o">-</span> <span class="n">y_soft</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span> <span class="o">+</span> <span class="n">y_soft</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Reparametrization trick.</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">y_soft</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="log_softmax"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.log_softmax">[docs]</a><span class="k">def</span> <span class="nf">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[int], int, Optional[int]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies a softmax followed by a logarithm.</span>

<span class="sd">    While mathematically equivalent to log(softmax(x)), doing these two</span>
<span class="sd">    operations separately is slower, and numerically unstable. This function</span>
<span class="sd">    uses an alternative formulation to compute the output and gradient correctly.</span>

<span class="sd">    See :class:`~torch.nn.LogSoftmax` for more details.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        dim (int): A dimension along which log_softmax will be computed.</span>
<span class="sd">        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.</span>
<span class="sd">          If specified, the input tensor is casted to :attr:`dtype` before the operation</span>
<span class="sd">          is performed. This is useful for preventing data type overflows. Default: None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">log_softmax</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">_stacklevel</span><span class="o">=</span><span class="n">_stacklevel</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="n">_get_softmax_dim</span><span class="p">(</span><span class="s1">&#39;log_softmax&#39;</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">_stacklevel</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="n">softshrink</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">softshrink</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">softshrink(input, lambd=0.5) -&gt; Tensor</span>

<span class="s2">Applies the soft shrinkage function elementwise</span>

<span class="s2">See :class:`~torch.nn.Softshrink` for more details.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="tanh"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.tanh">[docs]</a><span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;tanh(input) -&gt; Tensor</span>

<span class="sd">    Applies element-wise,</span>
<span class="sd">    :math:`\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Tanh` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.tanh is deprecated. Use torch.tanh instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">tanh</span><span class="p">()</span></div>


<div class="viewcode-block" id="sigmoid"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.sigmoid">[docs]</a><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;sigmoid(input) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function :math:`\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}`</span>

<span class="sd">    See :class:`~torch.nn.Sigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">()</span></div>


<div class="viewcode-block" id="hardsigmoid"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.hardsigmoid">[docs]</a><span class="k">def</span> <span class="nf">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;hardsigmoid(input) -&gt; Tensor</span>

<span class="sd">    Applies the element-wise function</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Hardsigmoid}(x) = \begin{cases}</span>
<span class="sd">            0 &amp; \text{if~} x \le -3, \\</span>
<span class="sd">            1 &amp; \text{if~} x \ge +3, \\</span>
<span class="sd">            x / 6 + 1 / 2 &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        inplace: If set to ``True``, will do this operation in-place. Default: ``False``</span>

<span class="sd">    See :class:`~torch.nn.Hardsigmoid` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">hardsigmoid</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardsigmoid_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<div class="viewcode-block" id="linear"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.linear">[docs]</a><span class="k">def</span> <span class="nf">linear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.</span>

<span class="sd">    This operator supports :ref:`TensorFloat32&lt;tf32_on_ampere&gt;`.</span>

<span class="sd">    Shape:</span>

<span class="sd">        - Input: :math:`(N, *, in\_features)` N is the batch size, `*` means any number of</span>
<span class="sd">          additional dimensions</span>
<span class="sd">        - Weight: :math:`(out\_features, in\_features)`</span>
<span class="sd">        - Bias: :math:`(out\_features)`</span>
<span class="sd">        - Output: :math:`(N, *, out\_features)`</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">linear</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># fused op is marginally faster</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">output</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="k">if</span> <span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">bias</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">output</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="bilinear"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.bilinear">[docs]</a><span class="k">def</span> <span class="nf">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Applies a bilinear transformation to the incoming data:</span>
<span class="sd">    :math:`y = x_1^T A x_2 + b`</span>

<span class="sd">    Shape:</span>

<span class="sd">        - input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\text{in1\_features}`</span>
<span class="sd">          and :math:`*` means any number of additional dimensions.</span>
<span class="sd">          All but the last dimension of the inputs should be the same.</span>
<span class="sd">        - input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\text{in2\_features}`</span>
<span class="sd">        - weight: :math:`(\text{out\_features}, \text{in1\_features},</span>
<span class="sd">          \text{in2\_features})`</span>
<span class="sd">        - bias: :math:`(\text{out\_features})`</span>
<span class="sd">        - output: :math:`(N, *, H_{out})` where :math:`H_{out}=\text{out\_features}`</span>
<span class="sd">          and all but the last dimension are the same shape as the input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">bilinear</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span></div>

<div class="viewcode-block" id="silu"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.silu">[docs]</a><span class="k">def</span> <span class="nf">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the silu function, element-wise.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{silu}(x) = x * \sigma(x), \text{where } \sigma(x) \text{ is the logistic sigmoid.}</span>

<span class="sd">    .. note::</span>
<span class="sd">        See `Gaussian Error Linear Units (GELUs) &lt;https://arxiv.org/abs/1606.08415&gt;`_</span>
<span class="sd">        where the SiLU (Sigmoid Linear Unit) was originally coined, and see</span>
<span class="sd">        `Sigmoid-Weighted Linear Units for Neural Network Function Approximation</span>
<span class="sd">        in Reinforcement Learning &lt;https://arxiv.org/abs/1702.03118&gt;`_ and `Swish:</span>
<span class="sd">        a Self-Gated Activation Function &lt;https://arxiv.org/abs/1710.05941v1&gt;`_</span>
<span class="sd">        where the SiLU was experimented with later.</span>

<span class="sd">    See :class:`~torch.nn.SiLU` for more details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">silu</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">silu_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>

<div class="viewcode-block" id="hardswish"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.hardswish">[docs]</a><span class="k">def</span> <span class="nf">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">inplace</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies the hardswish function, element-wise, as described in the paper:</span>

<span class="sd">    `Searching for MobileNetV3`_.</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{Hardswish}(x) = \begin{cases}</span>
<span class="sd">            0 &amp; \text{if~} x \le -3, \\</span>
<span class="sd">            x &amp; \text{if~} x \ge +3, \\</span>
<span class="sd">            x \cdot (x + 3) /6 &amp; \text{otherwise}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    See :class:`~torch.nn.Hardswish` for more details.</span>

<span class="sd">    .. _`Searching for MobileNetV3`:</span>
<span class="sd">        https://arxiv.org/abs/1905.02244</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span><span class="n">hardswish</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="n">inplace</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">inplace</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardswish_</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">hardswish</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, float) -&gt; Tensor</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>


<div class="viewcode-block" id="embedding"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.embedding">[docs]</a><span class="k">def</span> <span class="nf">embedding</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span>
              <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[int], Optional[float], float, bool, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;A simple lookup table that looks up embeddings in a fixed dictionary and size.</span>

<span class="sd">    This module is often used to retrieve word embeddings using indices.</span>
<span class="sd">    The input to the module is a list of indices, and the embedding matrix,</span>
<span class="sd">    and the output is the corresponding word embeddings.</span>

<span class="sd">    See :class:`torch.nn.Embedding` for more details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,</span>
<span class="sd">            and number of columns equal to the embedding size</span>
<span class="sd">        padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`</span>
<span class="sd">                                         (initialized to zeros) whenever it encounters the index.</span>
<span class="sd">        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`</span>
<span class="sd">                                    is renormalized to have norm :attr:`max_norm`.</span>
<span class="sd">                                    Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: LongTensor of arbitrary shape containing the indices to extract</span>
<span class="sd">        - Weight: Embedding matrix of floating point type with shape `(V, embedding_dim)`,</span>
<span class="sd">                            where V = maximum index + 1 and embedding_dim = the embedding size</span>
<span class="sd">        - Output: `(*, embedding_dim)`, where `*` is the input shape</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[1,2,4,5],[4,3,2,9]])</span>
<span class="sd">        &gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix)</span>
<span class="sd">        tensor([[[ 0.8490,  0.9625,  0.6753],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.4161,  0.2419,  0.7383]],</span>

<span class="sd">                [[ 0.6246,  0.9751,  0.3618],</span>
<span class="sd">                 [ 0.0237,  0.7794,  0.0528],</span>
<span class="sd">                 [ 0.9666,  0.7761,  0.6108],</span>
<span class="sd">                 [ 0.3385,  0.8612,  0.1867]]])</span>

<span class="sd">        &gt;&gt;&gt; # example with padding_idx</span>
<span class="sd">        &gt;&gt;&gt; weights = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; weights[0, :].zero_()</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = weights</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([[0,2,0,5]])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)</span>
<span class="sd">        tensor([[[ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.5609,  0.5384,  0.8720],</span>
<span class="sd">                 [ 0.0000,  0.0000,  0.0000],</span>
<span class="sd">                 [ 0.6262,  0.2438,  0.7471]]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">padding_idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">padding_idx</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
        <span class="k">elif</span> <span class="n">padding_idx</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">padding_idx</span> <span class="o">&gt;=</span> <span class="o">-</span><span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="s1">&#39;Padding_idx must be within num_embeddings&#39;</span>
            <span class="n">padding_idx</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding_idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">padding_idx</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># `embedding_renorm_` will call .contiguous() on input anyways, so we</span>
        <span class="c1"># call it here and take advantage of the improved locality in the</span>
        <span class="c1"># `embedding` call below too.</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># XXX: equivalent to</span>
        <span class="c1"># with torch.no_grad():</span>
        <span class="c1">#   torch.nembedding_renorm_</span>
        <span class="c1"># remove once script supports set_grad_enabled</span>
        <span class="n">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">padding_idx</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">sparse</span><span class="p">)</span></div>


<div class="viewcode-block" id="embedding_bag"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.embedding_bag">[docs]</a><span class="k">def</span> <span class="nf">embedding_bag</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">norm_type</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                  <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                  <span class="n">per_sample_weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_last_offset</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[float], float, bool, str, bool, Optional[Tensor], bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Computes sums, means or maxes of `bags` of embeddings, without instantiating the</span>
<span class="sd">    intermediate embeddings.</span>

<span class="sd">    See :class:`torch.nn.EmbeddingBag` for more details.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (LongTensor): Tensor containing bags of indices into the embedding matrix</span>
<span class="sd">        weight (Tensor): The embedding matrix with number of rows equal to the maximum possible index + 1,</span>
<span class="sd">            and number of columns equal to the embedding size</span>
<span class="sd">        offsets (LongTensor, optional): Only used when :attr:`input` is 1D. :attr:`offsets` determines</span>
<span class="sd">                             the starting index position of each bag (sequence) in :attr:`input`.</span>
<span class="sd">        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`</span>
<span class="sd">                                    is renormalized to have norm :attr:`max_norm`.</span>
<span class="sd">                                    Note: this will modify :attr:`weight` in-place.</span>
<span class="sd">        norm_type (float, optional): The ``p`` in the ``p``-norm to compute for the :attr:`max_norm` option.</span>
<span class="sd">                                     Default ``2``.</span>
<span class="sd">        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of</span>
<span class="sd">                                                the words in the mini-batch. Default ``False``.</span>
<span class="sd">                                                Note: this option is not supported when ``mode=&quot;max&quot;``.</span>
<span class="sd">        mode (string, optional): ``&quot;sum&quot;``, ``&quot;mean&quot;`` or ``&quot;max&quot;``. Specifies the way to reduce the bag.</span>
<span class="sd">                                 Default: ``&quot;mean&quot;``</span>
<span class="sd">        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` will be a sparse tensor. See Notes under</span>
<span class="sd">                                 :class:`torch.nn.Embedding` for more details regarding sparse gradients.</span>
<span class="sd">                                 Note: this option is not supported when ``mode=&quot;max&quot;``.</span>
<span class="sd">        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None</span>
<span class="sd">            to indicate all weights should be taken to be 1. If specified, :attr:`per_sample_weights`</span>
<span class="sd">            must have exactly the same shape as input and is treated as having the same</span>
<span class="sd">            :attr:`offsets`, if those are not None.</span>

<span class="sd">        include_last_offset (bool, optional): if ``True``, the size of offsets is equal to the number of bags + 1.</span>
<span class="sd">        The last element is the size of the input, or the ending index position of the last bag (sequence).</span>


<span class="sd">    Shape:</span>

<span class="sd">        - :attr:`input` (LongTensor) and :attr:`offsets` (LongTensor, optional)</span>

<span class="sd">          - If :attr:`input` is 2D of shape `(B, N)`,</span>

<span class="sd">            it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and</span>
<span class="sd">            this will return ``B`` values aggregated in a way depending on the :attr:`mode`.</span>
<span class="sd">            :attr:`offsets` is ignored and required to be ``None`` in this case.</span>

<span class="sd">          - If :attr:`input` is 1D of shape `(N)`,</span>

<span class="sd">            it will be treated as a concatenation of multiple bags (sequences).</span>
<span class="sd">            :attr:`offsets` is required to be a 1D tensor containing the</span>
<span class="sd">            starting index positions of each bag in :attr:`input`. Therefore,</span>
<span class="sd">            for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as</span>
<span class="sd">            having ``B`` bags. Empty bags (i.e., having 0-length) will have</span>
<span class="sd">            returned vectors filled by zeros.</span>

<span class="sd">        - :attr:`weight` (Tensor): the learnable weights of the module of</span>
<span class="sd">          shape `(num_embeddings, embedding_dim)`</span>

<span class="sd">        - :attr:`per_sample_weights` (Tensor, optional). Has the same shape as</span>
<span class="sd">          :attr:`input`.</span>

<span class="sd">        - :attr:`output`: aggregated embedding values of shape `(B, embedding_dim)`</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # an Embedding module containing 10 tensors of size 3</span>
<span class="sd">        &gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)</span>
<span class="sd">        &gt;&gt;&gt; # a batch of 2 samples of 4 indices each</span>
<span class="sd">        &gt;&gt;&gt; input = torch.tensor([1,2,4,5,4,3,2,9])</span>
<span class="sd">        &gt;&gt;&gt; offsets = torch.tensor([0,4])</span>
<span class="sd">        &gt;&gt;&gt; F.embedding_bag(embedding_matrix, input, offsets)</span>
<span class="sd">        tensor([[ 0.3397,  0.3552,  0.5545],</span>
<span class="sd">                [ 0.5893,  0.4386,  0.5882]])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">embedding_bag</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">offsets</span><span class="o">=</span><span class="n">offsets</span><span class="p">,</span> <span class="n">max_norm</span><span class="o">=</span><span class="n">max_norm</span><span class="p">,</span>
                <span class="n">norm_type</span><span class="o">=</span><span class="n">norm_type</span><span class="p">,</span> <span class="n">scale_grad_by_freq</span><span class="o">=</span><span class="n">scale_grad_by_freq</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
                <span class="n">sparse</span><span class="o">=</span><span class="n">sparse</span><span class="p">,</span> <span class="n">per_sample_weights</span><span class="o">=</span><span class="n">per_sample_weights</span><span class="p">,</span>
                <span class="n">include_last_offset</span><span class="o">=</span><span class="n">include_last_offset</span><span class="p">)</span>
    <span class="c1"># Check for backward compatibility.</span>
    <span class="c1"># Used to be embedding_bag(weight, input, ...)</span>
    <span class="c1"># Now is     embedding_bag(input, weight, ...)</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Argument order of nn.functional.embedding_bag was changed. &quot;</span>
                      <span class="s2">&quot;Usage `embedding_bag(weight, input, ...)` is deprecated, &quot;</span>
                      <span class="s2">&quot;and should now be `embedding_bag(input, weight, ...)`.&quot;</span><span class="p">)</span>
        <span class="n">weight</span><span class="p">,</span> <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span>

    <span class="k">if</span> <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="n">per_sample_weights</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;embedding_bag: If per_sample_weights (</span><span class="si">{}</span><span class="s2">) is not None, &quot;</span>
                         <span class="s2">&quot;then it must have the same shape as the input (</span><span class="si">{}</span><span class="s2">)&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">per_sample_weights</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">type_str</span> <span class="o">=</span> <span class="s2">&quot;&lt;unknown&gt;&quot;</span>
            <span class="c1"># TODO: Remove this once script supports type() calls</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
                <span class="n">type_str</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">offsets</span><span class="p">))</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;if input is 2D, then offsets has to be None&quot;</span>
                             <span class="s2">&quot;, as input is treated is a mini-batch of&quot;</span>
                             <span class="s2">&quot; fixed length sequences. However, found &quot;</span>
                             <span class="s2">&quot;offsets of type </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">type_str</span><span class="p">))</span>
        <span class="n">offsets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
                               <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">per_sample_weights</span> <span class="o">=</span> <span class="n">per_sample_weights</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">offsets</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor but got None&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">offsets</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;offsets has to be a 1D Tensor&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input has to be 1D or 2D Tensor,&quot;</span>
                         <span class="s2">&quot; but got Tensor of dimension </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;max&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">2</span>

        <span class="k">if</span> <span class="n">scale_grad_by_freq</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support scaling the gradient by the frequency&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">sparse</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max mode does not support sparse weights&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;mode has to be one of sum, mean or max&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">max_norm</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># XXX: equivalent to</span>
        <span class="c1"># with torch.no_grad():</span>
        <span class="c1">#   torch.nembedding_renorm_</span>
        <span class="c1"># remove once script supports set_grad_enabled</span>
        <span class="n">_no_grad_embedding_renorm_</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">norm_type</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">per_sample_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;embedding_bag: per_sample_weights was not None. &quot;</span>
                                  <span class="s2">&quot;per_sample_weights is only supported for mode=&#39;sum&#39; &quot;</span>
                                  <span class="s2">&quot;(got mode=&#39;</span><span class="si">{}</span><span class="s2">&#39;). Please open a feature request on GitHub.&quot;</span>
                                  <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>

    <span class="n">ret</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">embedding_bag</span><span class="p">(</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="nb">input</span><span class="p">,</span>
        <span class="n">offsets</span><span class="p">,</span>
        <span class="n">scale_grad_by_freq</span><span class="p">,</span>
        <span class="n">mode_enum</span><span class="p">,</span>
        <span class="n">sparse</span><span class="p">,</span>
        <span class="n">per_sample_weights</span><span class="p">,</span>
        <span class="n">include_last_offset</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<span class="k">def</span> <span class="nf">_verify_batch_size</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="c1"># type: (List[int]) -&gt; None</span>
    <span class="c1"># XXX: JIT script does not support the reduce from functools, and mul op is a</span>
    <span class="c1"># builtin, which cannot be used as a value to a func yet, so rewrite this size</span>
    <span class="c1"># check to a simple equivalent for loop</span>
    <span class="c1">#</span>
    <span class="c1"># TODO: make use of reduce like below when JIT is ready with the missing features:</span>
    <span class="c1"># from operator import mul</span>
    <span class="c1"># from functools import reduce</span>
    <span class="c1">#</span>
    <span class="c1">#   if reduce(mul, size[2:], size[0]) == 1</span>
    <span class="n">size_prods</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">):</span>
        <span class="n">size_prods</span> <span class="o">*=</span> <span class="n">size</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">size_prods</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected more than 1 value per channel when training, got input size </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>


<div class="viewcode-block" id="batch_norm"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.batch_norm">[docs]</a><span class="k">def</span> <span class="nf">batch_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor], bool, float, float) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Batch Normalization for each channel across a batch of data.</span>

<span class="sd">    See :class:`~torch.nn.BatchNorm1d`, :class:`~torch.nn.BatchNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.BatchNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">batch_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="n">_verify_batch_size</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">batch_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span>
        <span class="n">training</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="instance_norm"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.instance_norm">[docs]</a><span class="k">def</span> <span class="nf">instance_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">running_var</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                  <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">use_input_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Tensor], bool, float, float) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Instance Normalization for each channel in each data sample in a</span>
<span class="sd">    batch.</span>

<span class="sd">    See :class:`~torch.nn.InstanceNorm1d`, :class:`~torch.nn.InstanceNorm2d`,</span>
<span class="sd">    :class:`~torch.nn.InstanceNorm3d` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">instance_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">running_mean</span><span class="o">=</span><span class="n">running_mean</span><span class="p">,</span>
                <span class="n">running_var</span><span class="o">=</span><span class="n">running_var</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
                <span class="n">use_input_stats</span><span class="o">=</span><span class="n">use_input_stats</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">_verify_batch_size</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">instance_norm</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">running_mean</span><span class="p">,</span> <span class="n">running_var</span><span class="p">,</span>
        <span class="n">use_input_stats</span><span class="p">,</span> <span class="n">momentum</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span>
    <span class="p">)</span></div>


<div class="viewcode-block" id="layer_norm"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.layer_norm">[docs]</a><span class="k">def</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], Optional[Tensor], Optional[Tensor], float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Layer Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.LayerNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">layer_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">normalized_shape</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, int, Optional[Tensor], Optional[Tensor], float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies Group Normalization for last certain number of dimensions.</span>

<span class="sd">    See :class:`~torch.nn.GroupNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">group_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">)</span>
    <span class="n">_verify_batch_size</span><span class="p">([</span>
        <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">]</span>
        <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">group_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">num_groups</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">backends</span><span class="o">.</span><span class="n">cudnn</span><span class="o">.</span><span class="n">enabled</span><span class="p">)</span>


<div class="viewcode-block" id="local_response_norm"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.local_response_norm">[docs]</a><span class="k">def</span> <span class="nf">local_response_norm</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.75</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mf">1.</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, int, float, float, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Applies local response normalization over an input signal composed of</span>
<span class="sd">    several input planes, where channels occupy the second dimension.</span>
<span class="sd">    Applies normalization across channels.</span>

<span class="sd">    See :class:`~torch.nn.LocalResponseNorm` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">local_response_norm</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 3D or higher dimensionality </span><span class="se">\</span>
<span class="s1">                         input (got </span><span class="si">{}</span><span class="s1"> dimensions)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>
    <span class="n">div</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool2d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">sizes</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">avg_pool3d</span><span class="p">(</span><span class="n">div</span><span class="p">,</span> <span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">sizes</span><span class="p">)</span>
    <span class="n">div</span> <span class="o">=</span> <span class="n">div</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">div</span></div>


<span class="c1"># loss</span>

<div class="viewcode-block" id="ctc_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.ctc_loss">[docs]</a><span class="k">def</span> <span class="nf">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Tensor, int, str, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Connectionist Temporal Classification loss.</span>

<span class="sd">    See :class:`~torch.nn.CTCLoss` for details.</span>

<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_probs: :math:`(T, N, C)` where `C = number of characters in alphabet including blank`,</span>
<span class="sd">            `T = input length`, and `N = batch size`.</span>
<span class="sd">            The logarithmized probabilities of the outputs</span>
<span class="sd">            (e.g. obtained with :func:`torch.nn.functional.log_softmax`).</span>
<span class="sd">        targets: :math:`(N, S)` or `(sum(target_lengths))`.</span>
<span class="sd">            Targets cannot be blank. In the second form, the targets are assumed to be concatenated.</span>
<span class="sd">        input_lengths: :math:`(N)`.</span>
<span class="sd">            Lengths of the inputs (must each be :math:`\leq T`)</span>
<span class="sd">        target_lengths: :math:`(N)`.</span>
<span class="sd">            Lengths of the targets</span>
<span class="sd">        blank (int, optional):</span>
<span class="sd">            Blank label. Default :math:`0`.</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the output losses will be divided by the target lengths and</span>
<span class="sd">            then the mean over the batch is taken, ``&#39;sum&#39;``: the output will be</span>
<span class="sd">            summed. Default: ``&#39;mean&#39;``</span>
<span class="sd">        zero_infinity (bool, optional):</span>
<span class="sd">            Whether to zero infinite losses and the associated gradients.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">            Infinite losses mainly occur when the inputs are too short</span>
<span class="sd">            to be aligned to the targets.</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(10,30,(16,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="n">blank</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span>
                          <span class="n">zero_infinity</span><span class="p">)</span></div>


<div class="viewcode-block" id="nll_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.nll_loss">[docs]</a><span class="k">def</span> <span class="nf">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
             <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], int, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.NLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \geq 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Example::</span>

<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = F.nll_loss(F.log_softmax(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">nll_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span>
                <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected 2 or more dimensions (got </span><span class="si">{}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">))</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected input batch_size (</span><span class="si">{}</span><span class="s1">) to match target batch_size (</span><span class="si">{}</span><span class="s1">).&#39;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span>
    <span class="k">if</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dim</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">ignore_index</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># dim == 3 or dim &gt; 4</span>
        <span class="n">n</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">c</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span><span class="n">n</span><span class="p">,)</span> <span class="o">+</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">!=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected target size </span><span class="si">{}</span><span class="s1">, got </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">out_size</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>
        <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="c1"># support empty batches, see #15870</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">nll_loss2d</span><span class="p">(</span>
                <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">)</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">out</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="poisson_nll_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.poisson_nll_loss">[docs]</a><span class="k">def</span> <span class="nf">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, bool, bool, Optional[bool], float, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Poisson negative log likelihood loss.</span>

<span class="sd">    See :class:`~torch.nn.PoissonNLLLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: expectation of underlying Poisson distribution.</span>
<span class="sd">        target: random sample :math:`target \sim \text{Poisson}(input)`.</span>
<span class="sd">        log_input: if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target} * \text{input}`, if ``False`` then loss is</span>
<span class="sd">            :math:`\text{input} - \text{target} * \log(\text{input}+\text{eps})`. Default: ``True``</span>
<span class="sd">        full: whether to compute full loss, i. e. to add the Stirling</span>
<span class="sd">            approximation term. Default: ``False``</span>
<span class="sd">            :math:`\text{target} * \log(\text{target}) - \text{target} + 0.5 * \log(2 * \pi * \text{target})`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input`=``False``. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">poisson_nll_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="n">log_input</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="n">full</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;none&#39;</span> <span class="ow">and</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;mean&#39;</span> <span class="ow">and</span> <span class="n">reduction</span> <span class="o">!=</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">reduction</span> <span class="o">+</span> <span class="s2">&quot; is not valid&quot;</span><span class="p">)</span>

    <span class="n">ret</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">poisson_nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="p">,</span> <span class="n">full</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="kl_div"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.kl_div">[docs]</a><span class="k">def</span> <span class="nf">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The `Kullback-Leibler divergence Loss</span>
<span class="sd">    &lt;https://en.wikipedia.org/wiki/Kullback-Leibler_divergence&gt;`__</span>

<span class="sd">    See :class:`~torch.nn.KLDivLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;batchmean&#39;`` | ``&#39;sum&#39;`` | ``&#39;mean&#39;``.</span>
<span class="sd">            ``&#39;none&#39;``: no reduction will be applied</span>
<span class="sd">            ``&#39;batchmean&#39;``: the sum of the output will be divided by the batchsize</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed</span>
<span class="sd">            ``&#39;mean&#39;``: the output will be divided by the number of elements in the output</span>
<span class="sd">            Default: ``&#39;mean&#39;``</span>
<span class="sd">        log_target (bool): A flag indicating whether ``target`` is passed in the log space.</span>
<span class="sd">            It is recommended to pass certain distributions (like ``softmax``)</span>
<span class="sd">            in the log space to avoid numerical issues caused by explicit ``log``.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    .. note::</span>
<span class="sd">        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,</span>
<span class="sd">        and in the meantime, specifying either of those two args will override :attr:`reduction`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        :attr:``reduction`` = ``&#39;mean&#39;`` doesn&#39;t return the true kl divergence value, please use</span>
<span class="sd">        :attr:``reduction`` = ``&#39;batchmean&#39;`` which aligns with KL math definition.</span>
<span class="sd">        In the next major release, ``&#39;mean&#39;`` will be changed to be the same as &#39;batchmean&#39;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">kl_div</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span>
                <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="n">log_target</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;reduction: &#39;mean&#39; divides the total loss by both the batch size and the support size.&quot;</span>
                          <span class="s2">&quot;&#39;batchmean&#39; divides only by the batch size, and aligns with the KL div math definition.&quot;</span>
                          <span class="s2">&quot;&#39;mean&#39; will be changed to behave the same as &#39;batchmean&#39; in the next major release.&quot;</span><span class="p">)</span>

        <span class="c1"># special case for batchmean</span>
        <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span><span class="p">:</span>
            <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="n">reduced</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="n">log_target</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;batchmean&#39;</span> <span class="ow">and</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">reduced</span> <span class="o">=</span> <span class="n">reduced</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">reduced</span></div>


<div class="viewcode-block" id="cross_entropy"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                  <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], int, Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines `log_softmax` and `nll_loss` in a single</span>
<span class="sd">    function.</span>

<span class="sd">    See :class:`~torch.nn.CrossEntropyLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor) : :math:`(N, C)` where `C = number of classes` or :math:`(N, C, H, W)`</span>
<span class="sd">            in case of 2D Loss, or :math:`(N, C, d_1, d_2, ..., d_K)` where :math:`K \geq 1`</span>
<span class="sd">            in the case of K-dimensional loss.</span>
<span class="sd">        target (Tensor) : :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`,</span>
<span class="sd">            or :math:`(N, d_1, d_2, ..., d_K)` where :math:`K \geq 1` for</span>
<span class="sd">            K-dimensional loss.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets. Default: -100</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(5, (3,), dtype=torch.int64)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.cross_entropy(input, target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">cross_entropy</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">nll_loss</span><span class="p">(</span><span class="n">log_softmax</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.binary_cross_entropy">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output.</span>

<span class="sd">    See :class:`~torch.nn.BCELoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">                if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; input = torch.randn((3, 2), requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.rand((3, 2), requires_grad=False)</span>
<span class="sd">        &gt;&gt;&gt; loss = F.binary_cross_entropy(F.sigmoid(input), target)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">binary_cross_entropy</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">!=</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">) is deprecated. &quot;</span>
                         <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">new_size</span> <span class="o">=</span> <span class="n">_infer_size</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">weight</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">weight</span> <span class="o">=</span> <span class="n">weight</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">new_size</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span>
        <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="binary_cross_entropy_with_logits"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.binary_cross_entropy_with_logits">[docs]</a><span class="k">def</span> <span class="nf">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                     <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that measures Binary Cross Entropy between target and output</span>
<span class="sd">    logits.</span>

<span class="sd">    See :class:`~torch.nn.BCEWithLogitsLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor of arbitrary shape</span>
<span class="sd">        target: Tensor of the same shape as input</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight</span>
<span class="sd">            if provided it&#39;s repeated to match input tensor shape</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples.</span>
<span class="sd">                Must be a vector with length equal to the number of classes.</span>

<span class="sd">    Examples::</span>

<span class="sd">         &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">         &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">         &gt;&gt;&gt; loss = F.binary_cross_entropy_with_logits(input, target)</span>
<span class="sd">         &gt;&gt;&gt; loss.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">binary_cross_entropy_with_logits</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
                <span class="n">pos_weight</span><span class="o">=</span><span class="n">pos_weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Target size (</span><span class="si">{}</span><span class="s2">) must be the same as input size (</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="smooth_l1_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.smooth_l1_loss">[docs]</a><span class="k">def</span> <span class="nf">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below beta and an L1 term otherwise.</span>

<span class="sd">    See :class:`~torch.nn.SmoothL1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">smooth_l1_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span>
                <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                      <span class="s2">&quot;This will likely lead to incorrect results due to broadcasting. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                      <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>

    <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">),</span> <span class="n">beta</span><span class="p">)</span></div>


<div class="viewcode-block" id="l1_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.l1_loss">[docs]</a><span class="k">def</span> <span class="nf">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;l1_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    Function that takes the mean element-wise absolute value difference.</span>

<span class="sd">    See :class:`~torch.nn.L1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">l1_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                      <span class="s2">&quot;This will likely lead to incorrect results due to broadcasting. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                      <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>


    <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span></div>


<div class="viewcode-block" id="mse_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.mse_loss">[docs]</a><span class="k">def</span> <span class="nf">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;mse_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    Measures the element-wise mean squared error.</span>

<span class="sd">    See :class:`~torch.nn.MSELoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">mse_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()</span> <span class="o">==</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()):</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Using a target size (</span><span class="si">{}</span><span class="s2">) that is different to the input size (</span><span class="si">{}</span><span class="s2">). &quot;</span>
                      <span class="s2">&quot;This will likely lead to incorrect results due to broadcasting. &quot;</span>
                      <span class="s2">&quot;Please ensure they have the same size.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span>
                      <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>

    <span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">expanded_input</span><span class="p">,</span> <span class="n">expanded_target</span><span class="p">,</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">))</span></div>


<div class="viewcode-block" id="margin_ranking_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.margin_ranking_loss">[docs]</a><span class="k">def</span> <span class="nf">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;margin_ranking_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MarginRankingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">margin_ranking_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input1</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">input2</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">((</span><span class="s2">&quot;margin_ranking_loss does not support scalars, got sizes: &quot;</span>
                            <span class="s2">&quot;input1: </span><span class="si">{}</span><span class="s2">, input2: </span><span class="si">{}</span><span class="s2">, target: </span><span class="si">{}</span><span class="s2"> &quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">input1</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">input2</span><span class="o">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">())))</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="hinge_embedding_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.hinge_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                         <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;hinge_embedding_loss(input, target, margin=1.0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.HingeEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">hinge_embedding_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_margin_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.multilabel_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">multilabel_margin_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span>
                <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="soft_margin_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;soft_margin_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.SoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">soft_margin_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span>
                <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multilabel_soft_margin_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.multilabel_soft_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multilabel_soft_margin_loss(input, target, weight=None, size_average=None) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiLabelSoftMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">multilabel_soft_margin_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="n">logsigmoid</span><span class="p">(</span><span class="o">-</span><span class="nb">input</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">weight</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># only return N loss values</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;none&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;sum&#39;</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">ret</span> <span class="o">=</span> <span class="nb">input</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="n">reduction</span> <span class="o">+</span> <span class="s2">&quot; is not valid&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ret</span></div>


<div class="viewcode-block" id="cosine_embedding_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.cosine_embedding_loss">[docs]</a><span class="k">def</span> <span class="nf">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                          <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;cosine_embedding_loss(input1, input2, target, margin=0, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.CosineEmbeddingLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>  <span class="c1"># noqa</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">cosine_embedding_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span>
                <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="multi_margin_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.multi_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                      <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, int, float, Optional[Tensor], Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;multi_margin_loss(input, target, p=1, margin=1, weight=None, size_average=None,</span>
<span class="sd">                          reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">    See :class:`~torch.nn.MultiMarginLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">multi_margin_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span>
                <span class="n">weight</span><span class="o">=</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only p == 1 and p == 2 supported&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight must be one-dimensional&#39;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<span class="n">pixel_shuffle</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pixel_shuffle</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)` to a</span>
<span class="s2">tensor of shape :math:`(*, C, H \times r, W \times r)`.</span>

<span class="s2">See :class:`~torch.nn.PixelShuffle` for details.</span>

<span class="s2">Args:</span>
<span class="s2">    input (Tensor): the input tensor</span>
<span class="s2">    upscale_factor (int): factor to increase spatial resolution by</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; input = torch.randn(1, 9, 4, 4)</span>
<span class="s2">    &gt;&gt;&gt; output = torch.nn.functional.pixel_shuffle(input, 3)</span>
<span class="s2">    &gt;&gt;&gt; print(output.size())</span>
<span class="s2">    torch.Size([1, 1, 12, 12])</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="n">channel_shuffle</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">channel_shuffle</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Divide the channels in a tensor of shape :math:`(*, C , H, W)`</span>
<span class="s2">into g groups and rearrange them as :math:`(*, C \frac g, g, H, W)`,</span>
<span class="s2">while keeping the original tensor shape.</span>

<span class="s2">See :class:`~torch.nn.ChannelShuffle` for details.</span>

<span class="s2">Args:</span>
<span class="s2">    input (Tensor): the input tensor</span>
<span class="s2">    groups (int): number of groups to divide channels in and rearrange.</span>

<span class="s2">Examples::</span>

<span class="s2">    &gt;&gt;&gt; input = torch.randn(1, 4, 2, 2)</span>
<span class="s2">    &gt;&gt;&gt; print(input)</span>
<span class="s2">    [[[[1, 2],</span>
<span class="s2">       [3, 4]],</span>
<span class="s2">      [[5, 6],</span>
<span class="s2">       [7, 8]],</span>
<span class="s2">      [[9, 10],</span>
<span class="s2">       [11, 12]],</span>
<span class="s2">      [[13, 14],</span>
<span class="s2">       [15, 16]],</span>
<span class="s2">     ]]</span>
<span class="s2">    &gt;&gt;&gt; output = torch.nn.functional.channel_shuffle(input, 2)</span>
<span class="s2">    &gt;&gt;&gt; print(output)</span>
<span class="s2">    [[[[1, 2],</span>
<span class="s2">       [3, 4]],</span>
<span class="s2">      [[9, 10],</span>
<span class="s2">       [11, 12]],</span>
<span class="s2">      [[5, 6],</span>
<span class="s2">       [7, 8]],</span>
<span class="s2">      [[13, 14],</span>
<span class="s2">       [15, 16]],</span>
<span class="s2">     ]]</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[float], str, Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>


<div class="viewcode-block" id="upsample"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.upsample">[docs]</a><span class="k">def</span> <span class="nf">upsample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(...)``.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    The algorithm used for upsampling is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric upsampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for upsampling are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only)</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.</span>
<span class="sd">        mode (string): algorithm used for upsampling:</span>
<span class="sd">            ``&#39;nearest&#39;`` | ``&#39;linear&#39;`` | ``&#39;bilinear&#39;`` | ``&#39;bicubic&#39;`` |</span>
<span class="sd">            ``&#39;trilinear&#39;``. Default: ``&#39;nearest&#39;``</span>
<span class="sd">        align_corners (bool, optional): Geometrically, we consider the pixels of the</span>
<span class="sd">            input and output as squares rather than points.</span>
<span class="sd">            If set to ``True``, the input and output tensors are aligned by the</span>
<span class="sd">            center points of their corner pixels, preserving the values at the corner pixels.</span>
<span class="sd">            If set to ``False``, the input and output tensors are aligned by the corner</span>
<span class="sd">            points of their corner pixels, and the interpolation uses edge value padding</span>
<span class="sd">            for out-of-boundary values, making this operation *independent* of input size</span>
<span class="sd">            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`</span>
<span class="sd">            is ``&#39;linear&#39;``, ``&#39;bilinear&#39;``, ``&#39;bicubic&#39;`` or ``&#39;trilinear&#39;``.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    .. note::</span>
<span class="sd">        With ``mode=&#39;bicubic&#39;``, it&#39;s possible to cause overshoot, in other words it can produce</span>
<span class="sd">        negative values or values greater than 255 for images.</span>
<span class="sd">        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot</span>
<span class="sd">        when displaying the image.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span></div>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[List[float]], str, Optional[bool], Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[List[float]], str, Optional[bool], Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[float], str, Optional[bool], Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[float], str, Optional[bool], Optional[bool]) -&gt; Tensor</span>
    <span class="k">pass</span>

<div class="viewcode-block" id="interpolate"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.interpolate">[docs]</a><span class="k">def</span> <span class="nf">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[List[float]], str, Optional[bool], Optional[bool]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Down/up samples the input to either the given :attr:`size` or the given</span>
<span class="sd">    :attr:`scale_factor`</span>

<span class="sd">    The algorithm used for interpolation is determined by :attr:`mode`.</span>

<span class="sd">    Currently temporal, spatial and volumetric sampling are supported, i.e.</span>
<span class="sd">    expected inputs are 3-D, 4-D or 5-D in shape.</span>

<span class="sd">    The input dimensions are interpreted in the form:</span>
<span class="sd">    `mini-batch x channels x [optional depth] x [optional height] x width`.</span>

<span class="sd">    The modes available for resizing are: `nearest`, `linear` (3D-only),</span>
<span class="sd">    `bilinear`, `bicubic` (4D-only), `trilinear` (5D-only), `area`</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): the input tensor</span>
<span class="sd">        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):</span>
<span class="sd">            output spatial size.</span>
<span class="sd">        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.</span>
<span class="sd">        mode (str): algorithm used for upsampling:</span>
<span class="sd">            ``&#39;nearest&#39;`` | ``&#39;linear&#39;`` | ``&#39;bilinear&#39;`` | ``&#39;bicubic&#39;`` |</span>
<span class="sd">            ``&#39;trilinear&#39;`` | ``&#39;area&#39;``. Default: ``&#39;nearest&#39;``</span>
<span class="sd">        align_corners (bool, optional): Geometrically, we consider the pixels of the</span>
<span class="sd">            input and output as squares rather than points.</span>
<span class="sd">            If set to ``True``, the input and output tensors are aligned by the</span>
<span class="sd">            center points of their corner pixels, preserving the values at the corner pixels.</span>
<span class="sd">            If set to ``False``, the input and output tensors are aligned by the corner</span>
<span class="sd">            points of their corner pixels, and the interpolation uses edge value padding</span>
<span class="sd">            for out-of-boundary values, making this operation *independent* of input size</span>
<span class="sd">            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`</span>
<span class="sd">            is ``&#39;linear&#39;``, ``&#39;bilinear&#39;``, ``&#39;bicubic&#39;`` or ``&#39;trilinear&#39;``.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">        recompute_scale_factor (bool, optional): recompute the scale_factor for use in the</span>
<span class="sd">            interpolation calculation.  When `scale_factor` is passed as a parameter, it is used</span>
<span class="sd">            to compute the `output_size`.  If `recompute_scale_factor` is ```False`` or not specified,</span>
<span class="sd">            the passed-in `scale_factor` will be used in the interpolation computation.</span>
<span class="sd">            Otherwise, a new `scale_factor` will be computed based on the output and input sizes for</span>
<span class="sd">            use in the interpolation computation (i.e. the computation will be identical to if the computed</span>
<span class="sd">            `output_size` were passed-in explicitly).  Note that when `scale_factor` is floating-point,</span>
<span class="sd">            the recomputed scale_factor may differ from the one passed in due to rounding and precision</span>
<span class="sd">            issues.</span>

<span class="sd">    .. note::</span>
<span class="sd">        With ``mode=&#39;bicubic&#39;``, it&#39;s possible to cause overshoot, in other words it can produce</span>
<span class="sd">        negative values or values greater than 255 for images.</span>
<span class="sd">        Explicitly call ``result.clamp(min=0, max=255)`` if you want to reduce the overshoot</span>
<span class="sd">        when displaying the image.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        With ``align_corners = True``, the linearly interpolating modes</span>
<span class="sd">        (`linear`, `bilinear`, and `trilinear`) don&#39;t proportionally align the</span>
<span class="sd">        output and input pixels, and thus the output values can depend on the</span>
<span class="sd">        input size. This was the default behavior for these modes up to version</span>
<span class="sd">        0.3.1. Since then, the default behavior is ``align_corners = False``.</span>
<span class="sd">        See :class:`~torch.nn.Upsample` for concrete examples on how this</span>
<span class="sd">        affects the outputs.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When scale_factor is specified, if recompute_scale_factor=True,</span>
<span class="sd">        scale_factor is used to compute the output_size which will then</span>
<span class="sd">        be used to infer new scales for the interpolation.</span>
<span class="sd">        The default behavior for recompute_scale_factor changed to False</span>
<span class="sd">        in 1.6.0, and scale_factor is used in the interpolation</span>
<span class="sd">        calculation.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">interpolate</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="n">scale_factor</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">,</span>
                <span class="n">recompute_scale_factor</span><span class="o">=</span><span class="n">recompute_scale_factor</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span> <span class="s1">&#39;area&#39;</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;align_corners option can only be set with the &quot;</span>
                             <span class="s2">&quot;interpolating modes: linear | bilinear | bicubic | trilinear&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Default upsampling behavior when mode=</span><span class="si">{}</span><span class="s2"> is changed &quot;</span>
                          <span class="s2">&quot;to align_corners=False since 0.4.0. Please specify &quot;</span>
                          <span class="s2">&quot;align_corners=True if the old behavior is desired. &quot;</span>
                          <span class="s2">&quot;See the documentation of nn.Upsample for details.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
            <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="n">dim</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">-</span> <span class="mi">2</span>  <span class="c1"># Number of spatial dimensions.</span>

    <span class="c1"># Process size and scale_factor.  Validate that exactly one is set.</span>
    <span class="c1"># Validate its length if it is a list, or expand it if it is a scalar.</span>
    <span class="c1"># After this block, exactly one of output_size and scale_factors will</span>
    <span class="c1"># be non-None, and it will be a list (or tuple).</span>
    <span class="k">if</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;only one of size or scale_factor should be defined&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">scale_factors</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;size shape must match input shape. &#39;</span>
                                 <span class="s1">&#39;Input is </span><span class="si">{}</span><span class="s1">D, size is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)))</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="n">size</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">size</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">elif</span> <span class="n">scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">size</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="n">output_size</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)</span> <span class="o">!=</span> <span class="n">dim</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;scale_factor shape must match input shape. &#39;</span>
                                 <span class="s1">&#39;Input is </span><span class="si">{}</span><span class="s1">D, scale_factor is </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scale_factor</span><span class="p">)))</span>
            <span class="n">scale_factors</span> <span class="o">=</span> <span class="n">scale_factor</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">scale_factors</span> <span class="o">=</span> <span class="p">[</span><span class="n">scale_factor</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;either size or scale_factor should be defined&#39;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">recompute_scale_factor</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># only warn when the scales have floating values since</span>
        <span class="c1"># the result for ints is the same with/without recompute_scale_factor</span>
        <span class="k">if</span> <span class="n">scale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">scale</span> <span class="ow">in</span> <span class="n">scale_factors</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="n">scale</span><span class="p">)</span> <span class="o">!=</span> <span class="n">scale</span><span class="p">:</span>
                    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;The default behavior for interpolate/upsample with float scale_factor changed &quot;</span>
                                  <span class="s2">&quot;in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, &quot;</span>
                                  <span class="s2">&quot;instead of relying on the computed output size. &quot;</span>
                                  <span class="s2">&quot;If you wish to restore the old behavior, please set recompute_scale_factor=True. &quot;</span>
                                  <span class="s2">&quot;See the documentation of nn.Upsample for details. &quot;</span><span class="p">)</span>
                    <span class="k">break</span>
    <span class="k">elif</span> <span class="n">recompute_scale_factor</span> <span class="ow">and</span> <span class="n">size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;recompute_scale_factor is not meaningful with an explicit size.&quot;</span><span class="p">)</span>

    <span class="c1"># &quot;area&quot; mode always requires an explicit size rather than scale factor.</span>
    <span class="c1"># Re-use the recompute_scale_factor code path.</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;area&quot;</span> <span class="ow">and</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">recompute_scale_factor</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">if</span> <span class="n">recompute_scale_factor</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">recompute_scale_factor</span><span class="p">:</span>
        <span class="c1"># We compute output_size here, then un-set scale_factors.</span>
        <span class="c1"># The C++ code will recompute it based on the (integer) output size.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">()</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_get_tracing_state</span><span class="p">():</span>
            <span class="c1"># make scale_factor a tensor in tracing so constant doesn&#39;t get baked in</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">[(</span><span class="n">torch</span><span class="o">.</span><span class="n">floor</span><span class="p">((</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">scale_factors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                           <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">scale_factors</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">output_size</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span> <span class="n">scale_factors</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
        <span class="n">scale_factors</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_nearest3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;area&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">adaptive_avg_pool3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_linear1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bilinear2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_trilinear3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bicubic&#39;</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">upsample_bicubic2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">,</span> <span class="n">scale_factors</span><span class="p">)</span>

    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 3D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;trilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 4D input, but trilinear mode needs 5D input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;linear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but linear mode needs 3D input&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Got 5D input, but bilinear mode needs 4D input&quot;</span><span class="p">)</span>

    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D, 4D and 5D input Tensors supported&quot;</span>
                              <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">D) for the modes: nearest | linear | bilinear | bicubic | trilinear&quot;</span>
                              <span class="s2">&quot; (got </span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="n">mode</span><span class="p">))</span></div>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[float]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[float]) -&gt; Tensor</span>
    <span class="k">pass</span>

<div class="viewcode-block" id="upsample_nearest"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.upsample_nearest">[docs]</a><span class="k">def</span> <span class="nf">upsample_nearest</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using nearest neighbours&#39; pixel values.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with ``nn.functional.interpolate(..., mode=&#39;nearest&#39;)``.</span>

<span class="sd">    Currently spatial and volumetric upsampling are supported (i.e. expected</span>
<span class="sd">    inputs are 4 or 5 dimensional).</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatia</span>
<span class="sd">            size.</span>
<span class="sd">        scale_factor (int): multiplier for spatial size. Has to be an integer.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_nearest is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">)</span></div>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[float]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[float]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[int], Optional[List[float]]) -&gt; Tensor</span>
    <span class="k">pass</span>

<span class="nd">@_overload</span>  <span class="c1"># noqa: F811</span>
<span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="c1"># type: (Tensor, Optional[List[int]], Optional[List[float]]) -&gt; Tensor</span>
    <span class="k">pass</span>

<div class="viewcode-block" id="upsample_bilinear"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.upsample_bilinear">[docs]</a><span class="k">def</span> <span class="nf">upsample_bilinear</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">scale_factor</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>  <span class="c1"># noqa: F811</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Upsamples the input, using bilinear upsampling.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This function is deprecated in favor of :func:`torch.nn.functional.interpolate`.</span>
<span class="sd">        This is equivalent with</span>
<span class="sd">        ``nn.functional.interpolate(..., mode=&#39;bilinear&#39;, align_corners=True)``.</span>

<span class="sd">    Expected inputs are spatial (4 dimensional). Use `upsample_trilinear` fo</span>
<span class="sd">    volumetric (5 dimensional) inputs.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input</span>
<span class="sd">        size (int or Tuple[int, int]): output spatial size.</span>
<span class="sd">        scale_factor (int or Tuple[int, int]): multiplier for spatial size</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># DeprecationWarning is ignored by default</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;nn.functional.upsample_bilinear is deprecated. Use nn.functional.interpolate instead.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">interpolate</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">scale_factor</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


<span class="n">GRID_SAMPLE_INTERPOLATION_MODES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;nearest&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">GRID_SAMPLE_PADDING_MODES</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;zeros&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="s1">&#39;border&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="s1">&#39;reflection&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="p">}</span>


<div class="viewcode-block" id="grid_sample"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.grid_sample">[docs]</a><span class="k">def</span> <span class="nf">grid_sample</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;bilinear&#39;</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, str, str, Optional[bool]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Given an :attr:`input` and a flow-field :attr:`grid`, computes the</span>
<span class="sd">    ``output`` using :attr:`input` values and pixel locations from :attr:`grid`.</span>

<span class="sd">    Currently, only spatial (4-D) and volumetric (5-D) :attr:`input` are</span>
<span class="sd">    supported.</span>

<span class="sd">    In the spatial (4-D) case, for :attr:`input` with shape</span>
<span class="sd">    :math:`(N, C, H_\text{in}, W_\text{in})` and :attr:`grid` with shape</span>
<span class="sd">    :math:`(N, H_\text{out}, W_\text{out}, 2)`, the output will have shape</span>
<span class="sd">    :math:`(N, C, H_\text{out}, W_\text{out})`.</span>

<span class="sd">    For each output location ``output[n, :, h, w]``, the size-2 vector</span>
<span class="sd">    ``grid[n, h, w]`` specifies :attr:`input` pixel locations ``x`` and ``y``,</span>
<span class="sd">    which are used to interpolate the output value ``output[n, :, h, w]``.</span>
<span class="sd">    In the case of 5D inputs, ``grid[n, d, h, w]`` specifies the</span>
<span class="sd">    ``x``, ``y``, ``z`` pixel locations for interpolating</span>
<span class="sd">    ``output[n, :, d, h, w]``. :attr:`mode` argument specifies ``nearest`` or</span>
<span class="sd">    ``bilinear`` interpolation method to sample the input pixels.</span>

<span class="sd">    :attr:`grid` specifies the sampling pixel locations normalized by the</span>
<span class="sd">    :attr:`input` spatial dimensions. Therefore, it should have most values in</span>
<span class="sd">    the range of ``[-1, 1]``. For example, values ``x = -1, y = -1`` is the</span>
<span class="sd">    left-top pixel of :attr:`input`, and values  ``x = 1, y = 1`` is the</span>
<span class="sd">    right-bottom pixel of :attr:`input`.</span>

<span class="sd">    If :attr:`grid` has values outside the range of ``[-1, 1]``, the corresponding</span>
<span class="sd">    outputs are handled as defined by :attr:`padding_mode`. Options are</span>

<span class="sd">        * ``padding_mode=&quot;zeros&quot;``: use ``0`` for out-of-bound grid locations,</span>
<span class="sd">        * ``padding_mode=&quot;border&quot;``: use border values for out-of-bound grid locations,</span>
<span class="sd">        * ``padding_mode=&quot;reflection&quot;``: use values at locations reflected by</span>
<span class="sd">          the border for out-of-bound grid locations. For location far away</span>
<span class="sd">          from the border, it will keep being reflected until becoming in bound,</span>
<span class="sd">          e.g., (normalized) pixel location ``x = -3.5`` reflects by border ``-1``</span>
<span class="sd">          and becomes ``x&#39; = 1.5``, then reflects by border ``1`` and becomes</span>
<span class="sd">          ``x&#39;&#39; = -0.5``.</span>

<span class="sd">    Note:</span>
<span class="sd">        This function is often used in conjunction with :func:`affine_grid`</span>
<span class="sd">        to build `Spatial Transformer Networks`_ .</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    Note:</span>
<span class="sd">        NaN values in :attr:`grid` would be interpreted as ``-1``.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): input of shape :math:`(N, C, H_\text{in}, W_\text{in})` (4-D case)</span>
<span class="sd">                        or :math:`(N, C, D_\text{in}, H_\text{in}, W_\text{in})` (5-D case)</span>
<span class="sd">        grid (Tensor): flow-field of shape :math:`(N, H_\text{out}, W_\text{out}, 2)` (4-D case)</span>
<span class="sd">                       or :math:`(N, D_\text{out}, H_\text{out}, W_\text{out}, 3)` (5-D case)</span>
<span class="sd">        mode (str): interpolation mode to calculate output values</span>
<span class="sd">            ``&#39;bilinear&#39;`` | ``&#39;nearest&#39;``. Default: ``&#39;bilinear&#39;``</span>
<span class="sd">            Note: When ``mode=&#39;bilinear&#39;`` and the input is 5-D, the interpolation mode</span>
<span class="sd">            used internally will actually be trilinear. However, when the input is 4-D,</span>
<span class="sd">            the interpolation mode will legitimately be bilinear.</span>
<span class="sd">        padding_mode (str): padding mode for outside grid values</span>
<span class="sd">            ``&#39;zeros&#39;`` | ``&#39;border&#39;`` | ``&#39;reflection&#39;``. Default: ``&#39;zeros&#39;``</span>
<span class="sd">        align_corners (bool, optional): Geometrically, we consider the pixels of the</span>
<span class="sd">            input  as squares rather than points.</span>
<span class="sd">            If set to ``True``, the extrema (``-1`` and ``1``) are considered as referring</span>
<span class="sd">            to the center points of the input&#39;s corner pixels. If set to ``False``, they</span>
<span class="sd">            are instead considered as referring to the corner points of the input&#39;s corner</span>
<span class="sd">            pixels, making the sampling more resolution agnostic.</span>
<span class="sd">            This option parallels the ``align_corners`` option in</span>
<span class="sd">            :func:`interpolate`, and so whichever option is used here</span>
<span class="sd">            should also be used there to resize the input image before grid sampling.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor</span>

<span class="sd">    .. _`Spatial Transformer Networks`:</span>
<span class="sd">        https://arxiv.org/abs/1506.02025</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When ``align_corners = True``, the grid positions depend on the pixel</span>
<span class="sd">        size relative to the input image size, and so the locations sampled by</span>
<span class="sd">        :func:`grid_sample` will differ for the same input given at different</span>
<span class="sd">        resolutions (that is, after being upsampled or downsampled).</span>
<span class="sd">        The default behavior up to version 1.2.0 was ``align_corners = True``.</span>
<span class="sd">        Since then, the default behavior has been changed to ``align_corners = False``,</span>
<span class="sd">        in order to bring it in line with the default for :func:`interpolate`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">grid_sample</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
                <span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;bilinear&#39;</span> <span class="ow">and</span> <span class="n">mode</span> <span class="o">!=</span> <span class="s1">&#39;nearest&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nn.functional.grid_sample(): expected mode to be &quot;</span>
                         <span class="s2">&quot;&#39;bilinear&#39; or &#39;nearest&#39;, but got: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;zeros&#39;</span> <span class="ow">and</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;border&#39;</span> <span class="ow">and</span> <span class="n">padding_mode</span> <span class="o">!=</span> <span class="s1">&#39;reflection&#39;</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;nn.functional.grid_sample(): expected padding_mode &quot;</span>
                         <span class="s2">&quot;to be &#39;zeros&#39;, &#39;border&#39;, or &#39;reflection&#39;, &quot;</span>
                         <span class="s2">&quot;but got: &#39;</span><span class="si">{}</span><span class="s2">&#39;&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">padding_mode</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;bilinear&#39;</span><span class="p">:</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># mode == &#39;nearest&#39;</span>
        <span class="n">mode_enum</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;zeros&#39;</span><span class="p">:</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s1">&#39;border&#39;</span><span class="p">:</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># padding_mode == &#39;reflection&#39;</span>
        <span class="n">padding_mode_enum</span> <span class="o">=</span> <span class="mi">2</span>

    <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Default grid_sample and affine_grid behavior has changed &quot;</span>
                      <span class="s2">&quot;to align_corners=False since 1.3.0. Please specify &quot;</span>
                      <span class="s2">&quot;align_corners=True if the old behavior is desired. &quot;</span>
                      <span class="s2">&quot;See the documentation of grid_sample for details.&quot;</span><span class="p">)</span>
        <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">grid_sampler</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="n">mode_enum</span><span class="p">,</span> <span class="n">padding_mode_enum</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span></div>


<div class="viewcode-block" id="affine_grid"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.affine_grid">[docs]</a><span class="k">def</span> <span class="nf">affine_grid</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], Optional[bool]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Generates a 2D or 3D flow field (sampling grid), given a batch of</span>
<span class="sd">    affine matrices :attr:`theta`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This function is often used in conjunction with :func:`grid_sample`</span>
<span class="sd">        to build `Spatial Transformer Networks`_ .</span>

<span class="sd">    Args:</span>
<span class="sd">        theta (Tensor): input batch of affine matrices with shape</span>
<span class="sd">            (:math:`N \times 2 \times 3`) for 2D or</span>
<span class="sd">            (:math:`N \times 3 \times 4`) for 3D</span>
<span class="sd">        size (torch.Size): the target output image size.</span>
<span class="sd">            (:math:`N \times C \times H \times W` for 2D or</span>
<span class="sd">            :math:`N \times C \times D \times H \times W` for 3D)</span>
<span class="sd">            Example: torch.Size((32, 3, 24, 24))</span>
<span class="sd">        align_corners (bool, optional): if ``True``, consider ``-1`` and ``1``</span>
<span class="sd">            to refer to the centers of the corner pixels rather than the image corners.</span>
<span class="sd">            Refer to :func:`grid_sample` for a more complete description.</span>
<span class="sd">            A grid generated by :func:`affine_grid` should be passed to :func:`grid_sample`</span>
<span class="sd">            with the same setting for this option.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    Returns:</span>
<span class="sd">        output (Tensor): output Tensor of size (:math:`N \times H \times W \times 2`)</span>

<span class="sd">    .. _`Spatial Transformer Networks`:</span>
<span class="sd">        https://arxiv.org/abs/1506.02025</span>

<span class="sd">    .. warning::</span>
<span class="sd">        When ``align_corners = True``, the grid positions depend on the pixel</span>
<span class="sd">        size relative to the input image size, and so the locations sampled by</span>
<span class="sd">        :func:`grid_sample` will differ for the same input given at different</span>
<span class="sd">        resolutions (that is, after being upsampled or downsampled).</span>
<span class="sd">        The default behavior up to version 1.2.0 was ``align_corners = True``.</span>
<span class="sd">        Since then, the default behavior has been changed to ``align_corners = False``,</span>
<span class="sd">        in order to bring it in line with the default for :func:`interpolate`.</span>
<span class="sd">    .. warning::</span>
<span class="sd">        When ``align_corners = True``, 2D affine transforms on 1D data and</span>
<span class="sd">        3D affine transforms on 2D data (that is, when one of the spatial</span>
<span class="sd">        dimensions has unit size) are ill-defined, and not an intended use case.</span>
<span class="sd">        This is not a problem when ``align_corners = False``.</span>
<span class="sd">        Up to version 1.2.0, all grid points along a unit dimension were</span>
<span class="sd">        considered arbitrarily to be at ``-1``.</span>
<span class="sd">        From version 1.3.0, under ``align_corners = True`` all grid points</span>
<span class="sd">        along a unit dimension are considered to be at ```0``</span>
<span class="sd">        (the center of the input image).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="n">theta</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">affine_grid</span><span class="p">,</span> <span class="p">(</span><span class="n">theta</span><span class="p">,),</span> <span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="o">=</span><span class="n">align_corners</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Default grid_sample and affine_grid behavior has changed &quot;</span>
                      <span class="s2">&quot;to align_corners=False since 1.3.0. Please specify &quot;</span>
                      <span class="s2">&quot;align_corners=True if the old behavior is desired. &quot;</span>
                      <span class="s2">&quot;See the documentation of grid_sample for details.&quot;</span><span class="p">)</span>
        <span class="n">align_corners</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="c1"># enforce floating point dtype on theta</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">theta</span><span class="o">.</span><span class="n">is_floating_point</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected theta to have floating point type, but got </span><span class="si">{}</span><span class="s2">&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">theta</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span>
    <span class="c1"># check that shapes and sizes match</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">2</span> <span class="ow">or</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected a batch of 2D affine matrices of shape Nx2x3 &quot;</span>
                             <span class="s2">&quot;for size </span><span class="si">{}</span><span class="s2">. Got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">spatial_size</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>  <span class="c1"># spatial dimension sizes</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">theta</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">3</span> <span class="ow">or</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected a batch of 3D affine matrices of shape Nx3x4 &quot;</span>
                             <span class="s2">&quot;for size </span><span class="si">{}</span><span class="s2">. Got </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">,</span> <span class="n">theta</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="n">spatial_size</span> <span class="o">=</span> <span class="n">size</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:]</span>  <span class="c1"># spatial dimension sizes</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;affine_grid only supports 4D and 5D sizes, &quot;</span>
                                  <span class="s2">&quot;for 2D and 3D affine transforms, respectively. &quot;</span>
                                  <span class="s2">&quot;Got size </span><span class="si">{}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>
    <span class="c1"># check for empty span</span>
    <span class="k">if</span> <span class="n">align_corners</span> <span class="ow">and</span> <span class="nb">min</span><span class="p">(</span><span class="n">spatial_size</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Since version 1.3.0, affine_grid behavior has changed &quot;</span>
                      <span class="s2">&quot;for unit-size grids when align_corners=True. &quot;</span>
                      <span class="s2">&quot;This is not an intended use case of affine_grid. &quot;</span>
                      <span class="s2">&quot;See the documentation of affine_grid for details.&quot;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="nb">min</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected non-zero, positive output size. Got </span><span class="si">{}</span><span class="s2">&quot;</span>
                         <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">size</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">affine_grid_generator</span><span class="p">(</span><span class="n">theta</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">align_corners</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">_pad</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;constant&#39;</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int], str, float) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pads tensor.</span>

<span class="sd">    Padding size:</span>
<span class="sd">        The padding size by which to pad some dimensions of :attr:`input`</span>
<span class="sd">        are described starting from the last dimension and moving forward.</span>
<span class="sd">        :math:`\left\lfloor\frac{\text{len(pad)}}{2}\right\rfloor` dimensions</span>
<span class="sd">        of ``input`` will be padded.</span>
<span class="sd">        For example, to pad only the last dimension of the input tensor, then</span>
<span class="sd">        :attr:`pad` has the form</span>
<span class="sd">        :math:`(\text{padding\_left}, \text{padding\_right})`;</span>
<span class="sd">        to pad the last 2 dimensions of the input tensor, then use</span>
<span class="sd">        :math:`(\text{padding\_left}, \text{padding\_right},`</span>
<span class="sd">        :math:`\text{padding\_top}, \text{padding\_bottom})`;</span>
<span class="sd">        to pad the last 3 dimensions, use</span>
<span class="sd">        :math:`(\text{padding\_left}, \text{padding\_right},`</span>
<span class="sd">        :math:`\text{padding\_top}, \text{padding\_bottom}`</span>
<span class="sd">        :math:`\text{padding\_front}, \text{padding\_back})`.</span>

<span class="sd">    Padding mode:</span>
<span class="sd">        See :class:`torch.nn.ConstantPad2d`, :class:`torch.nn.ReflectionPad2d`, and</span>
<span class="sd">        :class:`torch.nn.ReplicationPad2d` for concrete examples on how each of the</span>
<span class="sd">        padding modes works. Constant padding is implemented for arbitrary dimensions.</span>
<span class="sd">        Replicate padding is implemented for padding the last 3 dimensions of 5D input</span>
<span class="sd">        tensor, or the last 2 dimensions of 4D input tensor, or the last dimension of</span>
<span class="sd">        3D input tensor. Reflect padding is only implemented for padding the last 2</span>
<span class="sd">        dimensions of 4D input tensor, or the last dimension of 3D input tensor.</span>

<span class="sd">    Note:</span>
<span class="sd">        When using the CUDA backend, this operation may induce nondeterministic</span>
<span class="sd">        behaviour in its backward pass that is not easily switched off.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>

<span class="sd">    Args:</span>
<span class="sd">        input (Tensor): N-dimensional tensor</span>
<span class="sd">        pad (tuple): m-elements tuple, where</span>
<span class="sd">            :math:`\frac{m}{2} \leq` input dimensions and :math:`m` is even.</span>
<span class="sd">        mode: ``&#39;constant&#39;``, ``&#39;reflect&#39;``, ``&#39;replicate&#39;`` or ``&#39;circular&#39;``.</span>
<span class="sd">            Default: ``&#39;constant&#39;``</span>
<span class="sd">        value: fill value for ``&#39;constant&#39;`` padding. Default: ``0``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p1d = (1, 1) # pad last dim by 1 on each side</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p1d, &quot;constant&quot;, 0)  # effectively zero padding</span>
<span class="sd">        &gt;&gt;&gt; print(out.size())</span>
<span class="sd">        torch.Size([3, 3, 4, 4])</span>
<span class="sd">        &gt;&gt;&gt; p2d = (1, 1, 2, 2) # pad last dim by (1, 1) and 2nd to last by (2, 2)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p2d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.size())</span>
<span class="sd">        torch.Size([3, 3, 8, 4])</span>
<span class="sd">        &gt;&gt;&gt; t4d = torch.empty(3, 3, 4, 2)</span>
<span class="sd">        &gt;&gt;&gt; p3d = (0, 1, 2, 1, 3, 3) # pad by (0, 1), (2, 1), and (3, 3)</span>
<span class="sd">        &gt;&gt;&gt; out = F.pad(t4d, p3d, &quot;constant&quot;, 0)</span>
<span class="sd">        &gt;&gt;&gt; print(out.size())</span>
<span class="sd">        torch.Size([3, 9, 7, 3])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">_pad</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">value</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding length must be divisible by 2&#39;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="o">&lt;=</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">(),</span> <span class="s1">&#39;Padding length too large&#39;</span>
    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;constant&#39;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_VF</span><span class="o">.</span><span class="n">constant_pad_nd</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">value</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Padding mode &quot;</span><span class="si">{}</span><span class="s1">&quot;&quot; doesn</span><span class="se">\&#39;</span><span class="s1">t take in value argument&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;3D tensors expect 2 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad1d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;circular&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_pad_circular</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">,</span> <span class="s1">&#39;4D tensors expect 4 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">reflection_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad2d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;circular&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_pad_circular</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">elif</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">pad</span><span class="p">)</span> <span class="o">==</span> <span class="mi">6</span><span class="p">,</span> <span class="s1">&#39;5D tensors expect 6 values for padding&#39;</span>
            <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;reflect&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;replicate&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">replication_pad3d</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s1">&#39;circular&#39;</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">_pad_circular</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">pad</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Only 3D, 4D, 5D padding with non-constant padding are supported for now&quot;</span><span class="p">)</span>

<span class="c1"># We define this function as _pad because it takes an argument</span>
<span class="c1"># named pad, which clobbers the recursive reference to the pad</span>
<span class="c1"># function needed for __torch_function__ support</span>
<span class="n">pad</span> <span class="o">=</span> <span class="n">_pad</span>

<span class="c1"># distance</span>


<div class="viewcode-block" id="pairwise_distance"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.pairwise_distance">[docs]</a><span class="k">def</span> <span class="nf">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, float, float, bool) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`torch.nn.PairwiseDistance` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">pairwise_distance</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span> <span class="n">keepdim</span><span class="p">)</span></div>


<span class="n">pdist</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">pdist</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">pdist(input, p=2) -&gt; Tensor</span>

<span class="s2">Computes the p-norm distance between every pair of row vectors in the input.</span>
<span class="s2">This is identical to the upper triangular portion, excluding the diagonal, of</span>
<span class="s2">`torch.norm(input[:, None] - input, dim=2, p=p)`. This function will be faster</span>
<span class="s2">if the rows are contiguous.</span>

<span class="s2">If input has shape :math:`N \times M` then the output will have shape</span>
<span class="s2">:math:`\frac</span><span class="si">{1}{2}</span><span class="s2"> N (N - 1)`.</span>

<span class="s2">This function is equivalent to `scipy.spatial.distance.pdist(input,</span>
<span class="s2">&#39;minkowski&#39;, p=p)` if :math:`p \in (0, \infty)`. When :math:`p = 0` it is</span>
<span class="s2">equivalent to `scipy.spatial.distance.pdist(input, &#39;hamming&#39;) * M`.</span>
<span class="s2">When :math:`p = \infty`, the closest scipy function is</span>
<span class="s2">`scipy.spatial.distance.pdist(xn, lambda x, y: np.abs(x - y).max())`.</span>

<span class="s2">Args:</span>
<span class="s2">    input: input tensor of shape :math:`N \times M`.</span>
<span class="s2">    p: p value for the p-norm distance to calculate between each vector pair</span>
<span class="s2">        :math:`\in [0, \infty]`.</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">cosine_similarity</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">cosine_similarity(x1, x2, dim=1, eps=1e-8) -&gt; Tensor</span>

<span class="s2">Returns cosine similarity between x1 and x2, computed along dim.</span>

<span class="s2">.. math ::</span>
<span class="s2">    \text</span><span class="si">{similarity}</span><span class="s2"> = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}</span>

<span class="s2">Args:</span>
<span class="s2">    x1 (Tensor): First input.</span>
<span class="s2">    x2 (Tensor): Second input (of size matching x1).</span>
<span class="s2">    dim (int, optional): Dimension of vectors. Default: 1</span>
<span class="s2">    eps (float, optional): Small value to avoid division by zero.</span>
<span class="s2">        Default: 1e-8</span>

<span class="s2">Shape:</span>
<span class="s2">    - Input: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`.</span>
<span class="s2">    - Output: :math:`(\ast_1, \ast_2)` where 1 is at position `dim`.</span>

<span class="s2">Example::</span>

<span class="s2">    &gt;&gt;&gt; input1 = torch.randn(100, 128)</span>
<span class="s2">    &gt;&gt;&gt; input2 = torch.randn(100, 128)</span>
<span class="s2">    &gt;&gt;&gt; output = F.cosine_similarity(input1, input2)</span>
<span class="s2">    &gt;&gt;&gt; print(output)</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<span class="n">one_hot</span> <span class="o">=</span> <span class="n">_add_docstr</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">one_hot(tensor, num_classes=-1) -&gt; LongTensor</span>

<span class="s2">Takes LongTensor with index values of shape ``(*)`` and returns a tensor</span>
<span class="s2">of shape ``(*, num_classes)`` that have zeros everywhere except where the</span>
<span class="s2">index of last dimension matches the corresponding value of the input tensor,</span>
<span class="s2">in which case it will be 1.</span>

<span class="s2">See also `One-hot on Wikipedia`_ .</span>

<span class="s2">.. _One-hot on Wikipedia:</span>
<span class="s2">    https://en.wikipedia.org/wiki/One-hot</span>

<span class="s2">Arguments:</span>
<span class="s2">    tensor (LongTensor): class values of any shape.</span>
<span class="s2">    num_classes (int):  Total number of classes. If set to -1, the number</span>
<span class="s2">        of classes will be inferred as one greater than the largest class</span>
<span class="s2">        value in the input tensor.</span>

<span class="s2">Returns:</span>
<span class="s2">    LongTensor that has one more dimension with 1 values at the</span>
<span class="s2">    index of last dimension indicated by the input, and 0 everywhere</span>
<span class="s2">    else.</span>

<span class="s2">Examples:</span>
<span class="s2">    &gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3)</span>
<span class="s2">    tensor([[1, 0, 0],</span>
<span class="s2">            [0, 1, 0],</span>
<span class="s2">            [0, 0, 1],</span>
<span class="s2">            [1, 0, 0],</span>
<span class="s2">            [0, 1, 0]])</span>
<span class="s2">    &gt;&gt;&gt; F.one_hot(torch.arange(0, 5) % 3, num_classes=5)</span>
<span class="s2">    tensor([[1, 0, 0, 0, 0],</span>
<span class="s2">            [0, 1, 0, 0, 0],</span>
<span class="s2">            [0, 0, 1, 0, 0],</span>
<span class="s2">            [1, 0, 0, 0, 0],</span>
<span class="s2">            [0, 1, 0, 0, 0]])</span>
<span class="s2">    &gt;&gt;&gt; F.one_hot(torch.arange(0, 6).view(3,2) % 3)</span>
<span class="s2">    tensor([[[1, 0, 0],</span>
<span class="s2">             [0, 1, 0]],</span>
<span class="s2">            [[0, 0, 1],</span>
<span class="s2">             [1, 0, 0]],</span>
<span class="s2">            [[0, 1, 0],</span>
<span class="s2">             [0, 0, 1]]])</span>
<span class="s2">&quot;&quot;&quot;</span><span class="p">)</span>


<div class="viewcode-block" id="triplet_margin_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.triplet_margin_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                        <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, float, float, float, bool, Optional[bool], Optional[bool], str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`~torch.nn.TripletMarginLoss` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">triplet_margin_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span>
                <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="n">swap</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="n">reduce</span><span class="p">,</span>
                <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_enum</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="p">,</span> <span class="n">eps</span><span class="p">,</span>
                                     <span class="n">swap</span><span class="p">,</span> <span class="n">reduction_enum</span><span class="p">)</span></div>


<div class="viewcode-block" id="triplet_margin_with_distance_loss"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.triplet_margin_with_distance_loss">[docs]</a><span class="k">def</span> <span class="nf">triplet_margin_with_distance_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">distance_function</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                      <span class="n">margin</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, Tensor, Tensor, Optional[Callable[[Tensor, Tensor], Tensor]], float, bool, str) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    See :class:`~torch.nn.TripletMarginWithDistanceLoss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;F.triplet_margin_with_distance_loss does not support JIT scripting: &quot;</span>
                                  <span class="s2">&quot;functions requiring Callables cannot be scripted.&quot;</span><span class="p">)</span>

    <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
            <span class="n">triplet_margin_with_distance_loss</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span>
            <span class="n">distance_function</span><span class="o">=</span><span class="n">distance_function</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="n">margin</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>

    <span class="n">distance_function</span> <span class="o">=</span> <span class="n">distance_function</span> <span class="k">if</span> <span class="n">distance_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">pairwise_distance</span>

    <span class="n">positive_dist</span> <span class="o">=</span> <span class="n">distance_function</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">)</span>
    <span class="n">negative_dist</span> <span class="o">=</span> <span class="n">distance_function</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">swap</span><span class="p">:</span>
        <span class="n">swap_dist</span> <span class="o">=</span> <span class="n">distance_function</span><span class="p">(</span><span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">)</span>
        <span class="n">negative_dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">negative_dist</span><span class="p">,</span> <span class="n">swap_dist</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">positive_dist</span> <span class="o">-</span> <span class="n">negative_dist</span> <span class="o">+</span> <span class="n">margin</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>

    <span class="n">reduction_enum</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">get_enum</span><span class="p">(</span><span class="n">reduction</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="k">elif</span> <span class="n">reduction_enum</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">output</span></div>


<div class="viewcode-block" id="normalize"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.normalize">[docs]</a><span class="k">def</span> <span class="nf">normalize</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, float, int, float, Optional[Tensor]) -&gt; Tensor</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Performs :math:`L_p` normalization of inputs over specified dimension.</span>

<span class="sd">    For a tensor :attr:`input` of sizes :math:`(n_0, ..., n_{dim}, ..., n_k)`, each</span>
<span class="sd">    :math:`n_{dim}` -element vector :math:`v` along dimension :attr:`dim` is transformed as</span>

<span class="sd">    .. math::</span>
<span class="sd">        v = \frac{v}{\max(\lVert v \rVert_p, \epsilon)}.</span>

<span class="sd">    With the default arguments it uses the Euclidean norm over vectors along dimension :math:`1` for normalization.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: input tensor of any shape</span>
<span class="sd">        p (float): the exponent value in the norm formulation. Default: 2</span>
<span class="sd">        dim (int): the dimension to reduce. Default: 1</span>
<span class="sd">        eps (float): small value to avoid division by zero. Default: 1e-12</span>
<span class="sd">        out (Tensor, optional): the output tensor. If :attr:`out` is used, this</span>
<span class="sd">                                operation won&#39;t be differentiable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">normalize</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_min</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">input</span> <span class="o">/</span> <span class="n">denom</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">denom</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_min_</span><span class="p">(</span><span class="n">eps</span><span class="p">)</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">out</span><span class="p">)</span></div>


<span class="k">def</span> <span class="nf">assert_int_or_pair</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="n">arg_name</span><span class="p">,</span> <span class="n">message</span><span class="p">):</span>
    <span class="c1"># type: (List[int], str, str) -&gt; None</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">arg</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="n">message</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">arg_name</span><span class="p">)</span>


<div class="viewcode-block" id="unfold"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.unfold">[docs]</a><span class="k">def</span> <span class="nf">unfold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], BroadcastingList2[int], BroadcastingList2[int], BroadcastingList2[int]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Extracts sliding local blocks from an batched input tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 4-D input tensors (batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    .. warning::</span>

<span class="sd">        More than one element of the unfolded tensor may refer to a single</span>
<span class="sd">        memory location. As a result, in-place operations (especially ones that</span>
<span class="sd">        are vectorized) may result in incorrect behavior. If you need to write</span>
<span class="sd">        to the tensor, please clone it first.</span>


<span class="sd">    See :class:`torch.nn.Unfold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">unfold</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 4D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">im2col</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                                   <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 4D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>


<div class="viewcode-block" id="fold"><a class="viewcode-back" href="../../../nn.functional.html#torch.nn.functional.fold">[docs]</a><span class="k">def</span> <span class="nf">fold</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, BroadcastingList2[int], BroadcastingList2[int], BroadcastingList2[int], BroadcastingList2[int], BroadcastingList2[int]) -&gt; Tensor  # noqa</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Combines an array of sliding local blocks into a large containing</span>
<span class="sd">    tensor.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Currently, only 3-D output tensors (unfolded batched image-like tensors) are</span>
<span class="sd">        supported.</span>

<span class="sd">    See :class:`torch.nn.Fold` for details</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">((</span><span class="nb">input</span><span class="p">,)):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">fold</span><span class="p">,</span> <span class="p">(</span><span class="nb">input</span><span class="p">,),</span> <span class="nb">input</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">{}</span><span class="s1"> must be int or 2-tuple for 3D input&#39;</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">,</span> <span class="s1">&#39;output_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="s1">&#39;kernel_size&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">,</span> <span class="s1">&#39;dilation&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="s1">&#39;padding&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>
        <span class="n">assert_int_or_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">,</span> <span class="s1">&#39;stride&#39;</span><span class="p">,</span> <span class="n">msg</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">_C</span><span class="o">.</span><span class="n">_nn</span><span class="o">.</span><span class="n">col2im</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">_pair</span><span class="p">(</span><span class="n">output_size</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">),</span>
                                   <span class="n">_pair</span><span class="p">(</span><span class="n">dilation</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">padding</span><span class="p">),</span> <span class="n">_pair</span><span class="p">(</span><span class="n">stride</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;Input Error: Only 3D input Tensors are supported (got </span><span class="si">{}</span><span class="s2">D)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span></div>


<span class="k">def</span> <span class="nf">_pad_circular</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">padding</span><span class="p">):</span>
    <span class="c1"># type: (Tensor, List[int]) -&gt; Tensor</span>
    <span class="sd">&quot;&quot;&quot;Circularly pads tensor.</span>

<span class="sd">    Tensor values at the beginning are used to pad the end, and values at the</span>
<span class="sd">    end are used to pad the beginning. For example, consider a single dimension</span>
<span class="sd">    with values [0, 1, 2, 3]. With circular padding of (1, 1) it would be</span>
<span class="sd">    padded to [3, 0, 1, 2, 3, 0], and with padding (1, 2) it would be padded to</span>
<span class="sd">    [3, 0, 1, 2, 3, 0, 1]. If negative padding is applied then the ends of the</span>
<span class="sd">    tensor get removed. With circular padding of (-1, -1) the previous example</span>
<span class="sd">    would become [1, 2]. Circular padding of (-1, 1) would produce</span>
<span class="sd">    [1, 2, 3, 1].</span>

<span class="sd">    The first and second dimensions of the tensor are not padded.</span>

<span class="sd">    Args:</span>
<span class="sd">        input: Tensor with shape :math:`(N, C, D[, H, W])`.</span>
<span class="sd">        padding: Tuple containing the number of elements to pad each side of</span>
<span class="sd">            the tensor. The length of padding must be twice the number of</span>
<span class="sd">            paddable dimensions. For example, the length of padding should be 4</span>
<span class="sd">            for a tensor of shape :math:`(N, C, H, W)`, and the length should</span>
<span class="sd">            be 6 for a tensor of shape :math:`(N, C, D, H, W)`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; x = torch.tensor([[[[0, 1, 2], [3, 4, 5]]]])  # Create tensor</span>
<span class="sd">        &gt;&gt;&gt; # Example 1</span>
<span class="sd">        &gt;&gt;&gt; padding = (1, 1, 1, 1)</span>
<span class="sd">        &gt;&gt;&gt; y = F.pad(x, padding, mode=&#39;circular&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(y)</span>
<span class="sd">        tensor([[[[5, 3, 4, 5, 3],</span>
<span class="sd">                  [2, 0, 1, 2, 0],</span>
<span class="sd">                  [5, 3, 4, 5, 3],</span>
<span class="sd">                  [2, 0, 1, 2, 0]]]])</span>
<span class="sd">        &gt;&gt;&gt; print(y.shape)</span>
<span class="sd">        torch.Size([1, 1, 4, 5])</span>
<span class="sd">        &gt;&gt;&gt; # Example 2</span>
<span class="sd">        &gt;&gt;&gt; padding = (1, 1, 2, 2)</span>
<span class="sd">        &gt;&gt;&gt; z = F.pad(x, padding, mode=&#39;circular&#39;)</span>
<span class="sd">        &gt;&gt;&gt; print(z)</span>
<span class="sd">        tensor([[[[2, 0, 1, 2, 0],</span>
<span class="sd">                  [5, 3, 4, 5, 3],</span>
<span class="sd">                  [2, 0, 1, 2, 0],</span>
<span class="sd">                  [5, 3, 4, 5, 3],</span>
<span class="sd">                  [2, 0, 1, 2, 0],</span>
<span class="sd">                  [5, 3, 4, 5, 3]]]])</span>
<span class="sd">        &gt;&gt;&gt; print(z.shape)</span>
<span class="sd">        torch.Size([1, 1, 6, 5])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">in_shape</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">paddable_shape</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
    <span class="n">ndim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">paddable_shape</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paddable_shape</span><span class="p">):</span>
        <span class="c1"># Only supports wrapping around once</span>
        <span class="k">assert</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">&lt;=</span> <span class="n">size</span><span class="p">,</span> \
            <span class="s2">&quot;Padding value causes wrapping around more than once.&quot;</span>
        <span class="k">assert</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)]</span> <span class="o">&lt;=</span> <span class="n">size</span><span class="p">,</span> \
            <span class="s2">&quot;Padding value causes wrapping around more than once.&quot;</span>
        <span class="c1"># Negative padding should not result in negative sizes</span>
        <span class="k">assert</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)]</span> <span class="o">+</span> <span class="n">size</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">,</span> \
            <span class="s2">&quot;Negative padding value is resulting in an empty dimension.&quot;</span>

    <span class="c1"># Get shape of padded tensor</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[:</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">size</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">paddable_shape</span><span class="p">):</span>
        <span class="n">out_shape</span> <span class="o">+=</span> <span class="p">(</span><span class="n">size</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">idx</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)],)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">out_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">layout</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">layout</span><span class="p">,</span>
                      <span class="n">device</span><span class="o">=</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="c1"># Put original array in padded array</span>
    <span class="k">if</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">out_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_d1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_d1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">out_d0</span><span class="p">:</span><span class="n">out_d1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">input</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">in_d0</span><span class="p">:</span><span class="n">in_d1</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">out_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_d1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out_h0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_h1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_d1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_h0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_h1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">out_d0</span><span class="p">:</span><span class="n">out_d1</span><span class="p">,</span> <span class="n">out_h0</span><span class="p">:</span><span class="n">out_h1</span><span class="p">]</span> <span class="o">=</span> \
            <span class="nb">input</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">in_d0</span><span class="p">:</span><span class="n">in_d1</span><span class="p">,</span> <span class="n">in_h0</span><span class="p">:</span><span class="n">in_h1</span><span class="p">]</span>
    <span class="k">elif</span> <span class="n">ndim</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="n">out_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_d1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out_h0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_h1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out_w0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">out_w1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_d0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_d1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_h0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_h1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">in_w0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">in_w1</span> <span class="o">=</span> <span class="n">in_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="o">-</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">out</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">out_d0</span><span class="p">:</span><span class="n">out_d1</span><span class="p">,</span> <span class="n">out_h0</span><span class="p">:</span><span class="n">out_h1</span><span class="p">,</span> <span class="n">out_w0</span><span class="p">:</span><span class="n">out_w1</span><span class="p">]</span> <span class="o">=</span> \
            <span class="nb">input</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">in_d0</span><span class="p">:</span><span class="n">in_d1</span><span class="p">,</span> <span class="n">in_h0</span><span class="p">:</span><span class="n">in_h1</span><span class="p">,</span> <span class="n">in_w0</span><span class="p">:</span><span class="n">in_w1</span><span class="p">]</span>

    <span class="c1"># The following steps first pad the beginning of the tensor (left side),</span>
    <span class="c1"># and then pad the end of the tensor (right side).</span>
    <span class="c1"># Note: Corners will be written more than once when ndim &gt; 1.</span>

    <span class="c1"># Only in cases where padding values are &gt; 0 are when additional copying</span>
    <span class="c1"># is required.</span>

    <span class="c1"># Pad first dimension (depth)</span>
    <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">i0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">i1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">o0</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">o1</span> <span class="o">=</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">i0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">i1</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">o0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">o1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
        <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>

    <span class="c1"># Pad second dimension (height)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">i0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">i1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">o0</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">o1</span> <span class="o">=</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">]</span>
            <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">i0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">i1</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">o0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">o1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
            <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>

    <span class="c1"># Pad third dimension (width)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">i0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">i1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">o0</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">o1</span> <span class="o">=</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">]</span>
            <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">i0</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
            <span class="n">i1</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">6</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span> <span class="o">+</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span>
            <span class="n">o0</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span> <span class="o">-</span> <span class="n">padding</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">]</span>
            <span class="n">o1</span> <span class="o">=</span> <span class="n">out_shape</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
            <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">o0</span><span class="p">:</span><span class="n">o1</span><span class="p">]</span> <span class="o">=</span> \
                <span class="n">out</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">i0</span><span class="p">:</span><span class="n">i1</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">multi_head_attention_forward</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">key</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">value</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">embed_dim_to_check</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                 <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                                 <span class="n">in_proj_weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">in_proj_bias</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">bias_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">bias_v</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">],</span>
                                 <span class="n">add_zero_attn</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
                                 <span class="n">dropout_p</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
                                 <span class="n">out_proj_weight</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">out_proj_bias</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span>
                                 <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                 <span class="n">key_padding_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">need_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                 <span class="n">attn_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">use_separate_proj_weight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                                 <span class="n">q_proj_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">k_proj_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">v_proj_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">static_k</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                                 <span class="n">static_v</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
                                 <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Args:</span>
<span class="sd">        query, key, value: map a query and a set of key-value pairs to an output.</span>
<span class="sd">            See &quot;Attention Is All You Need&quot; for more details.</span>
<span class="sd">        embed_dim_to_check: total dimension of the model.</span>
<span class="sd">        num_heads: parallel attention heads.</span>
<span class="sd">        in_proj_weight, in_proj_bias: input projection weight and bias.</span>
<span class="sd">        bias_k, bias_v: bias of the key and value sequences to be added at dim=0.</span>
<span class="sd">        add_zero_attn: add a new batch of zeros to the key and</span>
<span class="sd">                       value sequences at dim=1.</span>
<span class="sd">        dropout_p: probability of an element to be zeroed.</span>
<span class="sd">        out_proj_weight, out_proj_bias: the output projection weight and bias.</span>
<span class="sd">        training: apply dropout if is ``True``.</span>
<span class="sd">        key_padding_mask: if provided, specified padding elements in the key will</span>
<span class="sd">            be ignored by the attention. This is an binary mask. When the value is True,</span>
<span class="sd">            the corresponding value on the attention layer will be filled with -inf.</span>
<span class="sd">        need_weights: output attn_output_weights.</span>
<span class="sd">        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all</span>
<span class="sd">            the batches while a 3D mask allows to specify a different mask for the entries of each batch.</span>
<span class="sd">        use_separate_proj_weight: the function accept the proj. weights for query, key,</span>
<span class="sd">            and value in different forms. If false, in_proj_weight will be used, which is</span>
<span class="sd">            a combination of q_proj_weight, k_proj_weight, v_proj_weight.</span>
<span class="sd">        q_proj_weight, k_proj_weight, v_proj_weight, in_proj_bias: input projection weight and bias.</span>
<span class="sd">        static_k, static_v: static key and value used for attention operators.</span>


<span class="sd">    Shape:</span>
<span class="sd">        Inputs:</span>
<span class="sd">        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is</span>
<span class="sd">          the embedding dimension.</span>
<span class="sd">        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is</span>
<span class="sd">          the embedding dimension.</span>
<span class="sd">        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is</span>
<span class="sd">          the embedding dimension.</span>
<span class="sd">        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.</span>
<span class="sd">          If a ByteTensor is provided, the non-zero positions will be ignored while the zero positions</span>
<span class="sd">          will be unchanged. If a BoolTensor is provided, the positions with the</span>
<span class="sd">          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.</span>
<span class="sd">        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.</span>
<span class="sd">          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,</span>
<span class="sd">          S is the source sequence length. attn_mask ensures that position i is allowed to attend the unmasked</span>
<span class="sd">          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend</span>
<span class="sd">          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``</span>
<span class="sd">          are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor</span>
<span class="sd">          is provided, it will be added to the attention weight.</span>
<span class="sd">        - static_k: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,</span>
<span class="sd">          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.</span>
<span class="sd">        - static_v: :math:`(N*num_heads, S, E/num_heads)`, where S is the source sequence length,</span>
<span class="sd">          N is the batch size, E is the embedding dimension. E/num_heads is the head dimension.</span>

<span class="sd">        Outputs:</span>
<span class="sd">        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,</span>
<span class="sd">          E is the embedding dimension.</span>
<span class="sd">        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,</span>
<span class="sd">          L is the target sequence length, S is the source sequence length.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">is_scripting</span><span class="p">():</span>
        <span class="n">tens_ops</span> <span class="o">=</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">in_proj_weight</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">,</span> <span class="n">bias_k</span><span class="p">,</span> <span class="n">bias_v</span><span class="p">,</span>
                    <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">any</span><span class="p">([</span><span class="nb">type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">tens_ops</span><span class="p">])</span> <span class="ow">and</span> <span class="n">has_torch_function</span><span class="p">(</span><span class="n">tens_ops</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">handle_torch_function</span><span class="p">(</span>
                <span class="n">multi_head_attention_forward</span><span class="p">,</span> <span class="n">tens_ops</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span>
                <span class="n">embed_dim_to_check</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">in_proj_weight</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">,</span>
                <span class="n">bias_k</span><span class="p">,</span> <span class="n">bias_v</span><span class="p">,</span> <span class="n">add_zero_attn</span><span class="p">,</span> <span class="n">dropout_p</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span>
                <span class="n">out_proj_bias</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span> <span class="n">key_padding_mask</span><span class="o">=</span><span class="n">key_padding_mask</span><span class="p">,</span>
                <span class="n">need_weights</span><span class="o">=</span><span class="n">need_weights</span><span class="p">,</span> <span class="n">attn_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">,</span>
                <span class="n">use_separate_proj_weight</span><span class="o">=</span><span class="n">use_separate_proj_weight</span><span class="p">,</span>
                <span class="n">q_proj_weight</span><span class="o">=</span><span class="n">q_proj_weight</span><span class="p">,</span> <span class="n">k_proj_weight</span><span class="o">=</span><span class="n">k_proj_weight</span><span class="p">,</span>
                <span class="n">v_proj_weight</span><span class="o">=</span><span class="n">v_proj_weight</span><span class="p">,</span> <span class="n">static_k</span><span class="o">=</span><span class="n">static_k</span><span class="p">,</span> <span class="n">static_v</span><span class="o">=</span><span class="n">static_v</span><span class="p">)</span>
    <span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="k">assert</span> <span class="n">embed_dim</span> <span class="o">==</span> <span class="n">embed_dim_to_check</span>
    <span class="c1"># allow MHA to have different sizes for the feature dimension</span>
    <span class="k">assert</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">//</span> <span class="n">num_heads</span>
    <span class="k">assert</span> <span class="n">head_dim</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="s2">&quot;embed_dim must be divisible by num_heads&quot;</span>
    <span class="n">scaling</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">head_dim</span><span class="p">)</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_separate_proj_weight</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span> <span class="ow">and</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
            <span class="c1"># self-attention</span>
            <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">in_proj_weight</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
            <span class="c1"># encoder-decoder attention</span>
            <span class="c1"># This is inline in_proj function with in_proj_weight and in_proj_bias</span>
            <span class="n">_b</span> <span class="o">=</span> <span class="n">in_proj_bias</span>
            <span class="n">_start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">_end</span> <span class="o">=</span> <span class="n">embed_dim</span>
            <span class="n">_w</span> <span class="o">=</span> <span class="n">in_proj_weight</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">if</span> <span class="n">_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_b</span> <span class="o">=</span> <span class="n">_b</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">]</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">_w</span><span class="p">,</span> <span class="n">_b</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">key</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">value</span> <span class="ow">is</span> <span class="kc">None</span>
                <span class="n">k</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">v</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="k">else</span><span class="p">:</span>

                <span class="c1"># This is inline in_proj function with in_proj_weight and in_proj_bias</span>
                <span class="n">_b</span> <span class="o">=</span> <span class="n">in_proj_bias</span>
                <span class="n">_start</span> <span class="o">=</span> <span class="n">embed_dim</span>
                <span class="n">_end</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">_w</span> <span class="o">=</span> <span class="n">in_proj_weight</span><span class="p">[</span><span class="n">_start</span><span class="p">:,</span> <span class="p">:]</span>
                <span class="k">if</span> <span class="n">_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">_b</span> <span class="o">=</span> <span class="n">_b</span><span class="p">[</span><span class="n">_start</span><span class="p">:]</span>
                <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">_w</span><span class="p">,</span> <span class="n">_b</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># This is inline in_proj function with in_proj_weight and in_proj_bias</span>
            <span class="n">_b</span> <span class="o">=</span> <span class="n">in_proj_bias</span>
            <span class="n">_start</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">_end</span> <span class="o">=</span> <span class="n">embed_dim</span>
            <span class="n">_w</span> <span class="o">=</span> <span class="n">in_proj_weight</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">if</span> <span class="n">_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_b</span> <span class="o">=</span> <span class="n">_b</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">]</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">_w</span><span class="p">,</span> <span class="n">_b</span><span class="p">)</span>

            <span class="c1"># This is inline in_proj function with in_proj_weight and in_proj_bias</span>
            <span class="n">_b</span> <span class="o">=</span> <span class="n">in_proj_bias</span>
            <span class="n">_start</span> <span class="o">=</span> <span class="n">embed_dim</span>
            <span class="n">_end</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="n">_w</span> <span class="o">=</span> <span class="n">in_proj_weight</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">,</span> <span class="p">:]</span>
            <span class="k">if</span> <span class="n">_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_b</span> <span class="o">=</span> <span class="n">_b</span><span class="p">[</span><span class="n">_start</span><span class="p">:</span><span class="n">_end</span><span class="p">]</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">_w</span><span class="p">,</span> <span class="n">_b</span><span class="p">)</span>

            <span class="c1"># This is inline in_proj function with in_proj_weight and in_proj_bias</span>
            <span class="n">_b</span> <span class="o">=</span> <span class="n">in_proj_bias</span>
            <span class="n">_start</span> <span class="o">=</span> <span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">2</span>
            <span class="n">_end</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">_w</span> <span class="o">=</span> <span class="n">in_proj_weight</span><span class="p">[</span><span class="n">_start</span><span class="p">:,</span> <span class="p">:]</span>
            <span class="k">if</span> <span class="n">_b</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">_b</span> <span class="o">=</span> <span class="n">_b</span><span class="p">[</span><span class="n">_start</span><span class="p">:]</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">_w</span><span class="p">,</span> <span class="n">_b</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">q_proj_weight_non_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">q_proj_weight</span><span class="p">)</span>
        <span class="n">len1</span><span class="p">,</span> <span class="n">len2</span> <span class="o">=</span> <span class="n">q_proj_weight_non_opt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">len1</span> <span class="o">==</span> <span class="n">embed_dim</span> <span class="ow">and</span> <span class="n">len2</span> <span class="o">==</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">k_proj_weight_non_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">k_proj_weight</span><span class="p">)</span>
        <span class="n">len1</span><span class="p">,</span> <span class="n">len2</span> <span class="o">=</span> <span class="n">k_proj_weight_non_opt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">len1</span> <span class="o">==</span> <span class="n">embed_dim</span> <span class="ow">and</span> <span class="n">len2</span> <span class="o">==</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">v_proj_weight_non_opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">_unwrap_optional</span><span class="p">(</span><span class="n">v_proj_weight</span><span class="p">)</span>
        <span class="n">len1</span><span class="p">,</span> <span class="n">len2</span> <span class="o">=</span> <span class="n">v_proj_weight_non_opt</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
        <span class="k">assert</span> <span class="n">len1</span> <span class="o">==</span> <span class="n">embed_dim</span> <span class="ow">and</span> <span class="n">len2</span> <span class="o">==</span> <span class="n">value</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">in_proj_bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">q_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">embed_dim</span><span class="p">])</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">k_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">[</span><span class="n">embed_dim</span><span class="p">:(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)])</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">v_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">[(</span><span class="n">embed_dim</span> <span class="o">*</span> <span class="mi">2</span><span class="p">):])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">q</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">q_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">)</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">k_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">)</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">v_proj_weight_non_opt</span><span class="p">,</span> <span class="n">in_proj_bias</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">scaling</span>

    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span> <span class="ow">or</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float64</span> <span class="ow">or</span> \
            <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> <span class="ow">or</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span> <span class="ow">or</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">,</span> \
            <span class="s1">&#39;Only float, byte, and bool types are supported for attn_mask, not </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Byte tensor for attn_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span><span class="p">)</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">!=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The size of the 2D attn_mask is not correct.&#39;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">list</span><span class="p">(</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">!=</span> <span class="p">[</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">key</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)]:</span>
                <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;The size of the 3D attn_mask is not correct.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;attn_mask&#39;s dimension </span><span class="si">{}</span><span class="s2"> is not supported&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">attn_mask</span><span class="o">.</span><span class="n">dim</span><span class="p">()))</span>
        <span class="c1"># attn_mask&#39;s dim is 3 now.</span>

    <span class="c1"># convert ByteTensor key_padding_mask to bool</span>
    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">uint8</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Byte tensor for key_padding_mask in nn.MultiheadAttention is deprecated. Use bool tensor instead.&quot;</span><span class="p">)</span>
        <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">bias_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">bias_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">static_k</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">static_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">bias_k</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
            <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">v</span><span class="p">,</span> <span class="n">bias_v</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="mi">1</span><span class="p">)])</span>
            <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">static_k</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;bias cannot be added to static key.&quot;</span>
            <span class="k">assert</span> <span class="n">static_v</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;bias cannot be added to static value.&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">bias_k</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="k">assert</span> <span class="n">bias_v</span> <span class="ow">is</span> <span class="kc">None</span>

    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">static_k</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">static_k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="n">static_k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="n">head_dim</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">static_k</span>

    <span class="k">if</span> <span class="n">static_v</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">static_v</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span>
        <span class="k">assert</span> <span class="n">static_v</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="n">head_dim</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">static_v</span>

    <span class="n">src_len</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">bsz</span>
        <span class="k">assert</span> <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">src_len</span>

    <span class="k">if</span> <span class="n">add_zero_attn</span><span class="p">:</span>
        <span class="n">src_len</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">k</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">k</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">k</span><span class="o">.</span><span class="n">device</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">key_padding_mask</span> <span class="o">=</span> <span class="n">pad</span><span class="p">(</span><span class="n">key_padding_mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
    <span class="k">assert</span> <span class="nb">list</span><span class="p">(</span><span class="n">attn_output_weights</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">==</span> <span class="p">[</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">]</span>

    <span class="k">if</span> <span class="n">attn_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bool</span><span class="p">:</span>
            <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attn_output_weights</span> <span class="o">+=</span> <span class="n">attn_mask</span>


    <span class="k">if</span> <span class="n">key_padding_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">masked_fill</span><span class="p">(</span>
            <span class="n">key_padding_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span>
            <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;-inf&#39;</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>

    <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span>
        <span class="n">attn_output_weights</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">attn_output_weights</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout_p</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">attn_output_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">list</span><span class="p">(</span><span class="n">attn_output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="o">==</span> <span class="p">[</span><span class="n">bsz</span> <span class="o">*</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">]</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">attn_output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">tgt_len</span><span class="p">,</span> <span class="n">bsz</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    <span class="n">attn_output</span> <span class="o">=</span> <span class="n">linear</span><span class="p">(</span><span class="n">attn_output</span><span class="p">,</span> <span class="n">out_proj_weight</span><span class="p">,</span> <span class="n">out_proj_bias</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">need_weights</span><span class="p">:</span>
        <span class="c1"># average attention weights over heads</span>
        <span class="n">attn_output_weights</span> <span class="o">=</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">bsz</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">,</span> <span class="n">src_len</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="n">attn_output_weights</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">num_heads</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">attn_output</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
         <script src="../../../_static/jquery.js"></script>
         <script src="../../../_static/underscore.js"></script>
         <script src="../../../_static/doctools.js"></script>
         <script src="../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>