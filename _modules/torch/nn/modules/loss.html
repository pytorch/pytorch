


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>torch.nn.modules.loss &mdash; PyTorch master documentation</title>
  

  
  
  
  
    <link rel="canonical" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/css/jit.css" type="text/css" />
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../../_static/katex-math.css" type="text/css" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 

  
  <script src="../../../../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../../../../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorchâ€™s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>


<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  master (1.7.0a0+03e4e94 )
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
<div>
  <a style="color:#F05732" href="https://pytorch.org/docs/stable/_modules/torch/nn/modules/loss.html">
    You are viewing unstable developer preview docs.
    Click here to view docs for latest stable release.
  </a>
</div>

            
            
              
            
            
              <p class="caption"><span class="caption-text">Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/amp_examples.html">Automatic Mixed Precision examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/autograd.html">Autograd mechanics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/broadcasting.html">Broadcasting semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cpu_threading_torchscript_inference.html">CPU threading and TorchScript inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/cuda.html">CUDA semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/extending.html">Extending PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/faq.html">Frequently Asked Questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/large_scale_deployments.html">Features for large-scale deployments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/multiprocessing.html">Multiprocessing best practices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/randomness.html">Reproducibility</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/serialization.html">Serialization semantics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../notes/windows.html">Windows FAQ</a></li>
</ul>
<p class="caption"><span class="caption-text">Language Bindings</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_index.html">C++</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/javadoc/">Javadoc</a></li>
</ul>
<p class="caption"><span class="caption-text">Python API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../torch.html">torch</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.html">torch.nn</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.functional.html">torch.nn.functional</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensors.html">torch.Tensor</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_attributes.html">Tensor Attributes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensor_view.html">Tensor Views</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../autograd.html">torch.autograd</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cuda.html">torch.cuda</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../amp.html">torch.cuda.amp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../backends.html">torch.backends</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributed.html">torch.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../distributions.html">torch.distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../fft.html">torch.fft</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../futures.html">torch.futures</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../hub.html">torch.hub</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../jit.html">torch.jit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../linalg.html">torch.linalg</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../nn.init.html">torch.nn.init</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx.html">torch.onnx</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../optim.html">torch.optim</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../complex_numbers.html">Complex Numbers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../quantization.html">Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../rpc.html">Distributed RPC Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../random.html">torch.random</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../sparse.html">torch.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../storage.html">torch.Storage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../bottleneck.html">torch.utils.bottleneck</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../checkpoint.html">torch.utils.checkpoint</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../cpp_extension.html">torch.utils.cpp_extension</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../data.html">torch.utils.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../dlpack.html">torch.utils.dlpack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../mobile_optimizer.html">torch.utils.mobile_optimizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../model_zoo.html">torch.utils.model_zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../tensorboard.html">torch.utils.tensorboard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../type_info.html">Type Info</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../named_tensor.html">Named Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../name_inference.html">Named Tensors operator coverage</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../__config__.html">torch.__config__</a></li>
</ul>
<p class="caption"><span class="caption-text">Libraries</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/audio">torchaudio</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/text">torchtext</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/vision">torchvision</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/elastic/">TorchElastic</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pytorch.org/serve">TorchServe</a></li>
<li class="toctree-l1"><a class="reference external" href="http://pytorch.org/xla/">PyTorch on XLA Devices</a></li>
</ul>
<p class="caption"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/contribution_guide.html">PyTorch Contribution Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/governance.html">PyTorch Governance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../community/persons_of_interest.html">PyTorch Governance | Persons of Interest</a></li>
</ul>

            
          

        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../../../../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="../../../index.html">Module code</a> &gt;</li>
        
          <li><a href="../../../torch.html">torch</a> &gt;</li>
        
      <li>torch.nn.modules.loss</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <h1>Source code for torch.nn.modules.loss</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">.distance</span> <span class="kn">import</span> <span class="n">PairwiseDistance</span>
<span class="kn">from</span> <span class="nn">.module</span> <span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">..</span> <span class="kn">import</span> <span class="n">_reduction</span> <span class="k">as</span> <span class="n">_Reduction</span>

<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">Tensor</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>


<span class="k">class</span> <span class="nc">_Loss</span><span class="p">(</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">_Reduction</span><span class="o">.</span><span class="n">legacy_get_string</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>


<span class="k">class</span> <span class="nc">_WeightedLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean absolute error (MAE) between each element in</span>
<span class="sd">    the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left| x_n - y_n \right|,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`n` elements each.</span>

<span class="sd">    The sum operation still operates over all the elements, and divides by :math:`n`.</span>

<span class="sd">    The division by :math:`n` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(N, *)`, same shape as the input</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.L1Loss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss. It is useful to train a classification</span>
<span class="sd">    problem with `C` classes.</span>

<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning</span>
<span class="sd">    weight to each of the classes. This is particularly useful when you have an</span>
<span class="sd">    unbalanced training set.</span>

<span class="sd">    The `input` given through a forward call is expected to contain</span>
<span class="sd">    log-probabilities of each class. `input` has to be a Tensor of size either</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case (described later).</span>

<span class="sd">    Obtaining log-probabilities in a neural network is easily achieved by</span>
<span class="sd">    adding a  `LogSoftmax`  layer in the last layer of your network.</span>
<span class="sd">    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra</span>
<span class="sd">    layer.</span>

<span class="sd">    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`</span>
<span class="sd">    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts</span>
<span class="sd">    this class index (this index may not necessarily be in the class range).</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_{y_n} x_{n,y_n}, \quad</span>
<span class="sd">        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</span>

<span class="sd">    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and</span>
<span class="sd">    :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;</span>
<span class="sd">            \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">            \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Can also be used for higher dimension inputs, such as 2D images, by providing</span>
<span class="sd">    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,</span>
<span class="sd">    where :math:`K` is the number of dimensions, and a target of appropriate shape</span>
<span class="sd">    (see below). In the case of images, it computes NLL loss per-pixel.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When</span>
<span class="sd">            :attr:`size_average` is ``True``, the loss is averaged over</span>
<span class="sd">            non-ignored targets.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `C = number of classes`, or</span>
<span class="sd">          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of</span>
<span class="sd">          K-dimensional loss.</span>
<span class="sd">        - Output: scalar.</span>
<span class="sd">          If :attr:`reduction` is ``&#39;none&#39;``, then the same size as the target: :math:`(N)`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case</span>
<span class="sd">          of K-dimensional loss.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; # input is of size N x C = 3 x 5</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.tensor([1, 0, 4])</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # 2D loss example (used, for example, with image inputs)</span>
<span class="sd">        &gt;&gt;&gt; N, C = 5, 4</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.NLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; # input is of size N x C x height x width</span>
<span class="sd">        &gt;&gt;&gt; data = torch.randn(N, 16, 10, 10)</span>
<span class="sd">        &gt;&gt;&gt; conv = nn.Conv2d(16, C, (3, 3))</span>
<span class="sd">        &gt;&gt;&gt; m = nn.LogSoftmax(dim=1)</span>
<span class="sd">        &gt;&gt;&gt; # each element in target has to have 0 &lt;= value &lt; C</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(conv(data)), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">NLLLoss2d</span><span class="p">(</span><span class="n">NLLLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;NLLLoss2d has been deprecated. &quot;</span>
                      <span class="s2">&quot;Please use NLLLoss instead as a drop-in replacement and see &quot;</span>
                      <span class="s2">&quot;https://pytorch.org/docs/master/nn.html#torch.nn.NLLLoss for more details.&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NLLLoss2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">PoissonNLLLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Negative log likelihood loss with Poisson distribution of target.</span>

<span class="sd">    The loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{target} \sim \mathrm{Poisson}(\text{input})</span>

<span class="sd">        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})</span>
<span class="sd">                                    + \log(\text{target!})</span>

<span class="sd">    The last term can be omitted or approximated with Stirling formula. The</span>
<span class="sd">    approximation is used for target values more than 1. For targets less or</span>
<span class="sd">    equal to 1 zeros are added to the loss.</span>

<span class="sd">    Args:</span>
<span class="sd">        log_input (bool, optional): if ``True`` the loss is computed as</span>
<span class="sd">            :math:`\exp(\text{input}) - \text{target}*\text{input}`, if ``False`` the loss is</span>
<span class="sd">            :math:`\text{input} - \text{target}*\log(\text{input}+\text{eps})`.</span>
<span class="sd">        full (bool, optional): whether to compute full loss, i. e. to add the</span>
<span class="sd">            Stirling approximation term</span>

<span class="sd">            .. math::</span>
<span class="sd">                \text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when</span>
<span class="sd">            :attr:`log_input = False`. Default: 1e-8</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.PoissonNLLLoss()</span>
<span class="sd">        &gt;&gt;&gt; log_input = torch.randn(5, 2, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(5, 2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(log_input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar by default. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N, *)`,</span>
<span class="sd">          the same shape as the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;log_input&#39;</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">full</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PoissonNLLLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_input</span> <span class="o">=</span> <span class="n">log_input</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">full</span> <span class="o">=</span> <span class="n">full</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">poisson_nll_loss</span><span class="p">(</span><span class="n">log_input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">log_input</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_input</span><span class="p">,</span> <span class="n">full</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">full</span><span class="p">,</span>
                                  <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">KLDivLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Kullback-Leibler divergence loss measure</span>

<span class="sd">    `Kullback-Leibler divergence`_ is a useful distance measure for continuous</span>
<span class="sd">    distributions and is often useful when performing direct regression over</span>
<span class="sd">    the space of (discretely sampled) continuous output distributions.</span>

<span class="sd">    As with :class:`~torch.nn.NLLLoss`, the `input` given is expected to contain</span>
<span class="sd">    *log-probabilities* and is not restricted to a 2D Tensor.</span>
<span class="sd">    The targets are interpreted as *probabilities* by default, but could be considered</span>
<span class="sd">    as *log-probabilities* with :attr:`log_target` set to ``True``.</span>

<span class="sd">    This criterion expects a `target` `Tensor` of the same size as the</span>
<span class="sd">    `input` `Tensor`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad</span>
<span class="sd">        l_n = y_n \cdot \left( \log y_n - x_n \right)</span>

<span class="sd">    where the index :math:`N` spans all dimensions of ``input`` and :math:`L` has the same</span>
<span class="sd">    shape as ``input``. If :attr:`reduction` is not ``&#39;none&#39;`` (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;} \\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    In default :attr:`reduction` mode ``&#39;mean&#39;``, the losses are averaged for each minibatch over observations</span>
<span class="sd">    **as well as** over dimensions. ``&#39;batchmean&#39;`` mode gives the correct KL divergence where losses</span>
<span class="sd">    are averaged over batch dimension only. ``&#39;mean&#39;`` mode&#39;s behavior will be changed to the same as</span>
<span class="sd">    ``&#39;batchmean&#39;`` in the next major release.</span>

<span class="sd">    .. _`kullback-leibler divergence`: https://en.wikipedia.org/wiki/Kullback-Leibler_divergence</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;batchmean&#39;`` | ``&#39;sum&#39;`` | ``&#39;mean&#39;``.</span>
<span class="sd">            ``&#39;none&#39;``: no reduction will be applied.</span>
<span class="sd">            ``&#39;batchmean&#39;``: the sum of the output will be divided by batchsize.</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed.</span>
<span class="sd">            ``&#39;mean&#39;``: the output will be divided by the number of elements in the output.</span>
<span class="sd">            Default: ``&#39;mean&#39;``</span>
<span class="sd">        log_target (bool, optional): Specifies whether `target` is passed in the log space.</span>
<span class="sd">            Default: ``False``</span>

<span class="sd">    .. note::</span>
<span class="sd">        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,</span>
<span class="sd">        and in the meantime, specifying either of those two args will override :attr:`reduction`.</span>

<span class="sd">    .. note::</span>
<span class="sd">        :attr:`reduction` = ``&#39;mean&#39;`` doesn&#39;t return the true kl divergence value, please use</span>
<span class="sd">        :attr:`reduction` = ``&#39;batchmean&#39;`` which aligns with KL math definition.</span>
<span class="sd">        In the next major release, ``&#39;mean&#39;`` will be changed to be the same as ``&#39;batchmean&#39;``.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar by default. If :attr:``reduction`` is ``&#39;none&#39;``, then :math:`(N, *)`,</span>
<span class="sd">          the same shape as the input</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">log_target</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">KLDivLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_target</span> <span class="o">=</span> <span class="n">log_target</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">log_target</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_target</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the mean squared error (squared L2 norm) between</span>
<span class="sd">    each element in the input :math:`x` and target :math:`y`.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = \left( x_n - y_n \right)^2,</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total</span>
<span class="sd">    of :math:`n` elements each.</span>

<span class="sd">    The mean operation still operates over all the elements, and divides by :math:`n`.</span>

<span class="sd">    The division by :math:`n` can be avoided if one sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MSELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3, 5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MSELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BCELoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the Binary Cross Entropy</span>
<span class="sd">    between the target and the output:</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets :math:`y` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be</span>
<span class="sd">    mathematically undefined in the above loss equation. PyTorch chooses to set</span>
<span class="sd">    :math:`\log (0) = -\infty`, since :math:`\lim_{x\to 0} \log (x) = -\infty`.</span>
<span class="sd">    However, an infinite term in the loss equation is not desirable for several reasons.</span>

<span class="sd">    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be</span>
<span class="sd">    multiplying 0 with infinity. Secondly, if we have an infinite loss value, then</span>
<span class="sd">    we would also have an infinite term in our gradient, since</span>
<span class="sd">    :math:`\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty`.</span>
<span class="sd">    This would make BCELoss&#39;s backward method nonlinear with respect to :math:`x_n`,</span>
<span class="sd">    and using it for things like linear regression would not be straight-forward.</span>

<span class="sd">    Our solution is that BCELoss clamps its log function outputs to be greater than</span>
<span class="sd">    or equal to -100. This way, we can always have a finite loss value and a linear</span>
<span class="sd">    backward method.</span>


<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N, *)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; m = nn.Sigmoid()</span>
<span class="sd">        &gt;&gt;&gt; loss = nn.BCELoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(m(input), target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BCEWithLogitsLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This loss combines a `Sigmoid` layer and the `BCELoss` in one single</span>
<span class="sd">    class. This version is more numerically stable than using a plain `Sigmoid`</span>
<span class="sd">    followed by a `BCELoss` as, by combining the operations into one layer,</span>
<span class="sd">    we take advantage of the log-sum-exp trick for numerical stability.</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)</span>
<span class="sd">        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],</span>

<span class="sd">    where :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    This is used for measuring the error of a reconstruction in for example</span>
<span class="sd">    an auto-encoder. Note that the targets `t[i]` should be numbers</span>
<span class="sd">    between 0 and 1.</span>

<span class="sd">    It&#39;s possible to trade off recall and precision by adding weights to positive examples.</span>
<span class="sd">    In the case of multi-label classification the loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad</span>
<span class="sd">        l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})</span>
<span class="sd">        + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],</span>

<span class="sd">    where :math:`c` is the class number (:math:`c &gt; 1` for multi-label binary classification,</span>
<span class="sd">    :math:`c = 1` for single-label binary classification),</span>
<span class="sd">    :math:`n` is the number of the sample in the batch and</span>
<span class="sd">    :math:`p_c` is the weight of the positive answer for the class :math:`c`.</span>

<span class="sd">    :math:`p_c &gt; 1` increases the recall, :math:`p_c &lt; 1` increases the precision.</span>

<span class="sd">    For example, if a dataset contains 100 positive and 300 negative examples of a single class,</span>
<span class="sd">    then `pos_weight` for the class should be equal to :math:`\frac{300}{100}=3`.</span>
<span class="sd">    The loss would act as if the dataset contains :math:`3\times 100=300` positive examples.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10</span>
<span class="sd">        &gt;&gt;&gt; output = torch.full([10, 64], 1.5)  # A prediction (logit)</span>
<span class="sd">        &gt;&gt;&gt; pos_weight = torch.ones([64])  # All weights are equal to 1</span>
<span class="sd">        &gt;&gt;&gt; criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)</span>
<span class="sd">        &gt;&gt;&gt; criterion(output, target)  # -log(sigmoid(1.5))</span>
<span class="sd">        tensor(0.2014)</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to the loss</span>
<span class="sd">            of each batch element. If given, has to be a Tensor of size `nbatch`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        pos_weight (Tensor, optional): a weight of positive examples.</span>
<span class="sd">                Must be a vector with length equal to the number of classes.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N, *)`, same</span>
<span class="sd">          shape as input.</span>

<span class="sd">     Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.BCEWithLogitsLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3).random_(2)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">pos_weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCEWithLogitsLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;weight&#39;</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;pos_weight&#39;</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">binary_cross_entropy_with_logits</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span>
                                                  <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                                                  <span class="n">pos_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pos_weight</span><span class="p">,</span>
                                                  <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">HingeEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>
<span class="sd">    This is usually used for measuring whether two inputs are similar or</span>
<span class="sd">    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically</span>
<span class="sd">    used for learning nonlinear embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for :math:`n`-th sample in the mini-batch is</span>

<span class="sd">    .. math::</span>
<span class="sd">        l_n = \begin{cases}</span>
<span class="sd">            x_n, &amp; \text{if}\; y_n = 1,\\</span>
<span class="sd">            \max \{0, \Delta - x_n\}, &amp; \text{if}\; y_n = -1,</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    and the total loss functions is</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp; \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp; \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    where :math:`L = \{l_1,\dots,l_N\}^\top`.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of `1`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation</span>
<span class="sd">          operates over all the elements.</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the input</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HingeEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">hinge_embedding_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultiLabelMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class multi-classification</span>
<span class="sd">    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)</span>
<span class="sd">    and output :math:`y` (which is a 2D `Tensor` of target class indices).</span>
<span class="sd">    For each sample in the mini-batch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \</span>
<span class="sd">    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \</span>
<span class="sd">    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.</span>

<span class="sd">    :math:`y` and :math:`x` must have the same size.</span>

<span class="sd">    The criterion only considers a contiguous block of non-negative targets that</span>
<span class="sd">    starts at the front.</span>

<span class="sd">    This allows for different samples to have variable amounts of target classes.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`</span>
<span class="sd">          is the number of classes.</span>
<span class="sd">        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MultiLabelMarginLoss()</span>
<span class="sd">        &gt;&gt;&gt; x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])</span>
<span class="sd">        &gt;&gt;&gt; # for target y, only consider labels 3 and 0, not after label -1</span>
<span class="sd">        &gt;&gt;&gt; y = torch.LongTensor([[3, 0, -1, 1]])</span>
<span class="sd">        &gt;&gt;&gt; loss(x, y)</span>
<span class="sd">        &gt;&gt;&gt; # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))</span>
<span class="sd">        tensor(0.8500)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiLabelMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below 1 and an L1 term otherwise.</span>
<span class="sd">    It is less sensitive to outliers than the `MSELoss` and in some cases</span>
<span class="sd">    prevents exploding gradients (e.g. see `Fast R-CNN` paper by Ross Girshick).</span>
<span class="sd">    Also known as the Huber loss:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}</span>

<span class="sd">    where :math:`z_{i}` is given by:</span>

<span class="sd">    .. math::</span>
<span class="sd">        z_{i} =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        0.5 (x_i - y_i)^2 / beta, &amp; \text{if } |x_i - y_i| &lt; beta \\</span>
<span class="sd">        |x_i - y_i| - 0.5 * beta, &amp; \text{otherwise }</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    :math:`x` and :math:`y` arbitrary shapes with a total of :math:`n` elements each</span>
<span class="sd">    the sum operation still operates over all the elements, and divides by :math:`n`.</span>

<span class="sd">    beta is an optional parameter that defaults to 1.</span>

<span class="sd">    The division by :math:`n` can be avoided if sets ``reduction = &#39;sum&#39;``.</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">        beta (float, optional): Specifies the threshold at which to change between L1 and L2 loss.</span>
<span class="sd">            This value defaults to 1.0.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(N, *)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(N, *)`, same shape as the input</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SmoothL1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SoftMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a two-class classification</span>
<span class="sd">    logistic loss between input tensor :math:`x` and target tensor :math:`y`</span>
<span class="sd">    (containing 1 or -1).</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}</span>

<span class="sd">    Args:</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(*)` where :math:`*` means, any number of additional</span>
<span class="sd">          dimensions</span>
<span class="sd">        - Target: :math:`(*)`, same shape as the input</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then same shape as the input</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines :class:`~torch.nn.LogSoftmax` and :class:`~torch.nn.NLLLoss` in one single class.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    The `input` is expected to contain raw, unnormalized scores for each class.</span>

<span class="sd">    `input` has to be a Tensor of size either :math:`(minibatch, C)` or</span>
<span class="sd">    :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case (described later).</span>

<span class="sd">    This criterion expects a class index in the range :math:`[0, C-1]` as the</span>
<span class="sd">    `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`</span>
<span class="sd">    is specified, this criterion also accepts this class index (this index may not</span>
<span class="sd">    necessarily be in the class range).</span>

<span class="sd">    The loss can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)</span>
<span class="sd">                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)</span>

<span class="sd">    or in the case of the :attr:`weight` argument being specified:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</span>

<span class="sd">    The losses are averaged across observations for each minibatch. If the</span>
<span class="sd">    :attr:`weight` argument is specified then this is a weighted average:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss} = \frac{\sum^{N}_{i=1} loss(i, class[i])}{\sum^{N}_{i=1} weight[class[i]]}</span>

<span class="sd">    Can also be used for higher dimension inputs, such as 2D images, by providing</span>
<span class="sd">    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,</span>
<span class="sd">    where :math:`K` is the number of dimensions, and a target of appropriate shape</span>
<span class="sd">    (see below).</span>


<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each class.</span>
<span class="sd">            If given, has to be a Tensor of size `C`</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When :attr:`size_average` is</span>
<span class="sd">            ``True``, the loss is averaged over non-ignored targets.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will</span>
<span class="sd">            be applied, ``&#39;mean&#39;``: the weighted mean of the output is taken,</span>
<span class="sd">            ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in</span>
<span class="sd">            the meantime, specifying either of those two args will override</span>
<span class="sd">            :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `C = number of classes`, or</span>
<span class="sd">          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of</span>
<span class="sd">          K-dimensional loss.</span>
<span class="sd">        - Output: scalar.</span>
<span class="sd">          If :attr:`reduction` is ``&#39;none&#39;``, then the same size as the target:</span>
<span class="sd">          :math:`(N)`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case</span>
<span class="sd">          of K-dimensional loss.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(3, 5, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.empty(3, dtype=torch.long).random_(5)</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ignore_index&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CrossEntropyLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="n">ignore_index</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                               <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultiLabelSoftMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-label one-versus-all</span>
<span class="sd">    loss based on max-entropy, between input :math:`x` and target :math:`y` of size</span>
<span class="sd">    :math:`(N, C)`.</span>
<span class="sd">    For each sample in the minibatch:</span>

<span class="sd">    .. math::</span>
<span class="sd">        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})</span>
<span class="sd">                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)</span>

<span class="sd">    where :math:`i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}`,</span>
<span class="sd">    :math:`y[i] \in \left\{0, \; 1\right\}`.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.</span>
<span class="sd">        - Target: :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;reduction&#39;</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiLabelSoftMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multilabel_soft_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CosineEmbeddingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given input tensors</span>
<span class="sd">    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.</span>
<span class="sd">    This is used for measuring whether two inputs are similar or dissimilar,</span>
<span class="sd">    using the cosine distance, and is typically used for learning nonlinear</span>
<span class="sd">    embeddings or semi-supervised learning.</span>

<span class="sd">    The loss function for each sample is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">        1 - \cos(x_1, x_2), &amp; \text{if } y = 1 \\</span>
<span class="sd">        \max(0, \cos(x_1, x_2) - \text{margin}), &amp; \text{if } y = -1</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,</span>
<span class="sd">            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the</span>
<span class="sd">            default value is :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineEmbeddingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">cosine_embedding_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MarginRankingLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the loss given</span>
<span class="sd">    inputs :math:`x1`, :math:`x2`, two 1D mini-batch `Tensors`,</span>
<span class="sd">    and a label 1D mini-batch tensor :math:`y` (containing 1 or -1).</span>

<span class="sd">    If :math:`y = 1` then it assumed the first input should be ranked higher</span>
<span class="sd">    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.</span>

<span class="sd">    The loss function for each pair of samples in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x1, x2, y) = \max(0, -y * (x1 - x2) + \text{margin})</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`0`.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input1: :math:`(N)` where `N` is the batch size.</span>
<span class="sd">        - Input2: :math:`(N)`, same shape as the Input1.</span>
<span class="sd">        - Target: :math:`(N)`, same shape as the inputs.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; loss = nn.MarginRankingLoss()</span>
<span class="sd">        &gt;&gt;&gt; input1 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; input2 = torch.randn(3, requires_grad=True)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randn(3).sign()</span>
<span class="sd">        &gt;&gt;&gt; output = loss(input1, input2, target)</span>
<span class="sd">        &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MarginRankingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input1</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input2</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">margin_ranking_loss</span><span class="p">(</span><span class="n">input1</span><span class="p">,</span> <span class="n">input2</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">MultiMarginLoss</span><span class="p">(</span><span class="n">_WeightedLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that optimizes a multi-class classification hinge</span>
<span class="sd">    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and</span>
<span class="sd">    output :math:`y` (which is a 1D tensor of target class indices,</span>
<span class="sd">    :math:`0 \leq y \leq \text{x.size}(1)-1`):</span>

<span class="sd">    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar</span>
<span class="sd">    output :math:`y` is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}</span>

<span class="sd">    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`</span>
<span class="sd">    and :math:`i \neq y`.</span>

<span class="sd">    Optionally, you can give non-equal weighting on the classes by passing</span>
<span class="sd">    a 1D :attr:`weight` tensor into the constructor.</span>

<span class="sd">    The loss function then becomes:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}</span>

<span class="sd">    Args:</span>
<span class="sd">        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`</span>
<span class="sd">            are the only supported values.</span>
<span class="sd">        margin (float, optional): Has a default value of :math:`1`.</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">int</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">weight</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;only p == 1 and p == 2 supported&quot;</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">weight</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">multi_margin_loss</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span>
                                   <span class="n">weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span>


<div class="viewcode-block" id="TripletMarginLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.TripletMarginLoss.html#torch.nn.TripletMarginLoss">[docs]</a><span class="k">class</span> <span class="nc">TripletMarginLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    .. math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>


<span class="sd">    where</span>

<span class="sd">    .. math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginWithDistanceLoss`, which computes the</span>
<span class="sd">    triplet margin loss for input tensors using a custom distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Default: :math:`1`.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.</span>
<span class="sd">        swap (bool, optional): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: ``False``.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where :math:`D` is the vector dimension.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;``, or a scalar</span>
<span class="sd">            otherwise.</span>

<span class="sd">    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)</span>
<span class="sd">    &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    .. _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;swap&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
                                     <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<div class="viewcode-block" id="TripletMarginWithDistanceLoss"><a class="viewcode-back" href="../../../../generated/torch.nn.TripletMarginWithDistanceLoss.html#torch.nn.TripletMarginWithDistanceLoss">[docs]</a><span class="k">class</span> <span class="nc">TripletMarginWithDistanceLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given input</span>
<span class="sd">    tensors :math:`a`, :math:`p`, and :math:`n` (representing anchor,</span>
<span class="sd">    positive, and negative examples, respectively), and a nonnegative,</span>
<span class="sd">    real-valued function (&quot;distance function&quot;) used to compute the relationship</span>
<span class="sd">    between the anchor and positive example (&quot;positive distance&quot;) and the</span>
<span class="sd">    anchor and negative example (&quot;negative distance&quot;).</span>

<span class="sd">    The unreduced loss (i.e., with :attr:`reduction` set to ``&#39;none&#39;``)</span>
<span class="sd">    can be described as:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(a, p, n) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_i = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>

<span class="sd">    where :math:`N` is the batch size; :math:`d` is a nonnegative, real-valued function</span>
<span class="sd">    quantifying the closeness of two tensors, referred to as the :attr:`distance_function`;</span>
<span class="sd">    and :math:`margin` is a nonnegative margin representing the minimum difference</span>
<span class="sd">    between the positive and negative distances that is required for the loss to</span>
<span class="sd">    be 0.  The input tensors have :math:`N` elements each and can be of any shape</span>
<span class="sd">    that the distance function can handle.</span>

<span class="sd">    If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then:</span>

<span class="sd">    .. math::</span>
<span class="sd">        \ell(x, y) =</span>
<span class="sd">        \begin{cases}</span>
<span class="sd">            \operatorname{mean}(L), &amp;  \text{if reduction} = \text{`mean&#39;;}\\</span>
<span class="sd">            \operatorname{sum}(L),  &amp;  \text{if reduction} = \text{`sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    See also :class:`~torch.nn.TripletMarginLoss`, which computes the triplet</span>
<span class="sd">    loss for input tensors using the :math:`l_p` distance as the distance function.</span>

<span class="sd">    Args:</span>
<span class="sd">        distance_function (callable, optional): A nonnegative, real-valued function that</span>
<span class="sd">            quantifies the closeness of two tensors. If not specified,</span>
<span class="sd">            `nn.PairwiseDistance` will be used.  Default: ``None``</span>
<span class="sd">        margin (float, optional): A nonnegative margin representing the minimum difference</span>
<span class="sd">            between the positive and negative distances required for the loss to be 0. Larger</span>
<span class="sd">            margins penalize cases where the negative examples are not distant enough from the</span>
<span class="sd">            anchors, relative to the positives. Default: :math:`1`.</span>
<span class="sd">        swap (bool, optional): Whether to use the distance swap described in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. If True, and if the positive example is closer to the</span>
<span class="sd">            negative example than the anchor is, swaps the positive example and the anchor in</span>
<span class="sd">            the loss computation. Default: ``False``.</span>
<span class="sd">        reduction (string, optional): Specifies the (optional) reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Default: ``&#39;mean&#39;``</span>


<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, *)` where :math:`*` represents any number of additional dimensions</span>
<span class="sd">          as supported by the distance function.</span>
<span class="sd">        - Output: A Tensor of shape :math:`(N)` if :attr:`reduction` is ``&#39;none&#39;``, or a scalar</span>
<span class="sd">          otherwise.</span>

<span class="sd">    Examples::</span>

<span class="sd">    &gt;&gt;&gt; # Initialize embeddings</span>
<span class="sd">    &gt;&gt;&gt; embedding = nn.Embedding(1000, 128)</span>
<span class="sd">    &gt;&gt;&gt; anchor_ids = torch.randint(0, 1000, (1,), requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; positive_ids = torch.randint(0, 1000, (1,), requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; negative_ids = torch.randint(0, 1000, (1,), requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; anchor = embedding(anchor_ids)</span>
<span class="sd">    &gt;&gt;&gt; positive = embedding(positive_ids)</span>
<span class="sd">    &gt;&gt;&gt; negative = embedding(negative_ids)</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Built-in Distance Function</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=nn.PairwiseDistance())</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function</span>
<span class="sd">    &gt;&gt;&gt; def l_infinity(x1, x2):</span>
<span class="sd">    &gt;&gt;&gt;     return torch.max(torch.abs(x1 - x2), dim=1).values</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(distance_function=l_infinity, margin=1.5)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; # Custom Distance Function (Lambda)</span>
<span class="sd">    &gt;&gt;&gt; triplet_loss = \</span>
<span class="sd">    &gt;&gt;&gt;     nn.TripletMarginWithDistanceLoss(</span>
<span class="sd">    &gt;&gt;&gt;         distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        V. Balntas, et al.: Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;swap&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">distance_function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[[</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tensor</span><span class="p">],</span> <span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                 <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletMarginWithDistanceLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span> <span class="o">=</span> <span class="n">distance_function</span> <span class="k">if</span> <span class="n">distance_function</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">PairwiseDistance</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">positive</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">negative</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_with_distance_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span>
                                                   <span class="n">distance_function</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">distance_function</span><span class="p">,</span>
                                                   <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div>


<span class="k">class</span> <span class="nc">CTCLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The Connectionist Temporal Classification loss.</span>

<span class="sd">    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the</span>
<span class="sd">    probability of possible alignments of input to target, producing a loss value which is differentiable</span>
<span class="sd">    with respect to each input node. The alignment of input to target is assumed to be &quot;many-to-one&quot;, which</span>
<span class="sd">    limits the length of the target sequence such that it must be :math:`\leq` the input length.</span>

<span class="sd">    Args:</span>
<span class="sd">        blank (int, optional): blank label. Default :math:`0`.</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the output losses will be divided by the target lengths and</span>
<span class="sd">            then the mean over the batch is taken. Default: ``&#39;mean&#39;``</span>
<span class="sd">        zero_infinity (bool, optional):</span>
<span class="sd">            Whether to zero infinite losses and the associated gradients.</span>
<span class="sd">            Default: ``False``</span>
<span class="sd">            Infinite losses mainly occur when the inputs are too short</span>
<span class="sd">            to be aligned to the targets.</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Log_probs: Tensor of size :math:`(T, N, C)`,</span>
<span class="sd">          where :math:`T = \text{input length}`,</span>
<span class="sd">          :math:`N = \text{batch size}`, and</span>
<span class="sd">          :math:`C = \text{number of classes (including blank)}`.</span>
<span class="sd">          The logarithmized probabilities of the outputs (e.g. obtained with</span>
<span class="sd">          :func:`torch.nn.functional.log_softmax`).</span>
<span class="sd">        - Targets: Tensor of size :math:`(N, S)` or</span>
<span class="sd">          :math:`(\operatorname{sum}(\text{target\_lengths}))`,</span>
<span class="sd">          where :math:`N = \text{batch size}` and</span>
<span class="sd">          :math:`S = \text{max target length, if shape is } (N, S)`.</span>
<span class="sd">          It represent the target sequences. Each element in the target</span>
<span class="sd">          sequence is a class index. And the target index cannot be blank (default=0).</span>
<span class="sd">          In the :math:`(N, S)` form, targets are padded to the</span>
<span class="sd">          length of the longest sequence, and stacked.</span>
<span class="sd">          In the :math:`(\operatorname{sum}(\text{target\_lengths}))` form,</span>
<span class="sd">          the targets are assumed to be un-padded and</span>
<span class="sd">          concatenated within 1 dimension.</span>
<span class="sd">        - Input_lengths: Tuple or tensor of size :math:`(N)`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represent the lengths of the</span>
<span class="sd">          inputs (must each be :math:`\leq T`). And the lengths are specified</span>
<span class="sd">          for each sequence to achieve masking under the assumption that sequences</span>
<span class="sd">          are padded to equal lengths.</span>
<span class="sd">        - Target_lengths: Tuple or tensor of size :math:`(N)`,</span>
<span class="sd">          where :math:`N = \text{batch size}`. It represent lengths of the targets.</span>
<span class="sd">          Lengths are specified for each sequence to achieve masking under the</span>
<span class="sd">          assumption that sequences are padded to equal lengths. If target shape is</span>
<span class="sd">          :math:`(N,S)`, target_lengths are effectively the stop index</span>
<span class="sd">          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for</span>
<span class="sd">          each target in a batch. Lengths must each be :math:`\leq S`</span>
<span class="sd">          If the targets are given as a 1d tensor that is the concatenation of individual</span>
<span class="sd">          targets, the target_lengths must add up to the total length of the tensor.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then</span>
<span class="sd">          :math:`(N)`, where :math:`N = \text{batch size}`.</span>

<span class="sd">    Examples::</span>

<span class="sd">        &gt;&gt;&gt; # Target are to be padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16      # Batch size</span>
<span class="sd">        &gt;&gt;&gt; S = 30      # Target sequence length of longest target in batch (padding length)</span>
<span class="sd">        &gt;&gt;&gt; S_min = 10  # Minimum target length, for demonstration purposes</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Target are to be un-padded</span>
<span class="sd">        &gt;&gt;&gt; T = 50      # Input sequence length</span>
<span class="sd">        &gt;&gt;&gt; C = 20      # Number of classes (including blank)</span>
<span class="sd">        &gt;&gt;&gt; N = 16      # Batch size</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of input vectors, for *size = (T,N,C)</span>
<span class="sd">        &gt;&gt;&gt; input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()</span>
<span class="sd">        &gt;&gt;&gt; input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; # Initialize random batch of targets (0 = blank, 1:C = classes)</span>
<span class="sd">        &gt;&gt;&gt; target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)</span>
<span class="sd">        &gt;&gt;&gt; ctc_loss = nn.CTCLoss()</span>
<span class="sd">        &gt;&gt;&gt; loss = ctc_loss(input, target, input_lengths, target_lengths)</span>
<span class="sd">        &gt;&gt;&gt; loss.backward()</span>

<span class="sd">    Reference:</span>
<span class="sd">        A. Graves et al.: Connectionist Temporal Classification:</span>
<span class="sd">        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:</span>
<span class="sd">        https://www.cs.toronto.edu/~graves/icml_2006.pdf</span>

<span class="sd">    Note:</span>
<span class="sd">        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be</span>
<span class="sd">        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,</span>
<span class="sd">        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of</span>
<span class="sd">        dtype :attr:`torch.int32`.</span>

<span class="sd">        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.</span>


<span class="sd">    Note:</span>
<span class="sd">        In some circumstances when using the CUDA backend with CuDNN, this operator</span>
<span class="sd">        may select a nondeterministic algorithm to increase performance. If this is</span>
<span class="sd">        undesirable, you can try to make the operation deterministic (potentially at</span>
<span class="sd">        a performance cost) by setting ``torch.backends.cudnn.deterministic =</span>
<span class="sd">        True``.</span>
<span class="sd">        Please see the notes on :doc:`/notes/randomness` for background.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;blank&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">blank</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">zero_infinity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CTCLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">blank</span> <span class="o">=</span> <span class="n">blank</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span> <span class="o">=</span> <span class="n">zero_infinity</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">targets</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tensor</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">ctc_loss</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">input_lengths</span><span class="p">,</span> <span class="n">target_lengths</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">blank</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">zero_infinity</span><span class="p">)</span>

<span class="c1"># TODO: L1HingeEmbeddingCriterion</span>
<span class="c1"># TODO: MSECriterion weight</span>
<span class="c1"># TODO: ClassSimplexCriterion</span>
</pre></div>

             </article>
             
            </div>
            <footer>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Torch Contributors.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>

          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              
            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../../../../" src="../../../../_static/documentation_options.js"></script>
         <script src="../../../../_static/jquery.js"></script>
         <script src="../../../../_static/underscore.js"></script>
         <script src="../../../../_static/doctools.js"></script>
         <script src="../../../../_static/language_data.js"></script>
     

  

  <script type="text/javascript" src="../../../../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../../../../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
 
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-90545585-1', 'auto');
  ga('send', 'pageview');

</script>

<script async src="https://www.googletagmanager.com/gtag/js?id=UA-117752657-2"></script>

<script>
  window.dataLayer = window.dataLayer || [];

  function gtag(){dataLayer.push(arguments);}

  gtag('js', new Date());
  gtag('config', 'UA-117752657-2');
</script>

<img height="1" width="1" style="border-style:none;" alt="" src="https://www.googleadservices.com/pagead/conversion/795629140/?label=txkmCPmdtosBENSssfsC&amp;guid=ON&amp;script=0"/>


  <!-- Begin Footer -->

  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebookâ€™s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../../../../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../../../../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>