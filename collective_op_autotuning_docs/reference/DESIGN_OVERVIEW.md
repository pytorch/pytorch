# Collective Op Autotuning - å®ç°è·¯çº¿å›¾ & å…³é”®æ³¨æ„äº‹é¡¹

## ğŸ“‹ æ€»ç»“ (Executive Summary)

æœ¬æ–¹æ¡ˆè®¾è®¡äº†ä¸€å¥—å®Œæ•´çš„collective ops autotuningæœºåˆ¶,åœ¨ç°æœ‰custom op autotuningåŸºç¡€ä¸Šæ·»åŠ äº†åˆ†å¸ƒå¼åŒæ­¥æ”¯æŒã€‚

**æ ¸å¿ƒåˆ›æ–°ç‚¹**:
1. âœ… å¤ç”¨ç°æœ‰autotuningåŸºç¡€è®¾æ–½,æœ€å°åŒ–ä»£ç ä¿®æ”¹
2. âœ… ä¸“é—¨çš„benchmarkåŒæ­¥æœºåˆ¶ç¡®ä¿è·¨rankå‡†ç¡®è®¡æ—¶
3. âœ… æ¨¡å—åŒ–è®¾è®¡,æ˜“äºæ‰©å±•åˆ°æ›´å¤šcollective ops

---

## ğŸ¯ å®ç°ä¼˜å…ˆçº§

### P0 - å¿…é¡»å®Œæˆ (æ ¸å¿ƒåŠŸèƒ½)

1. **âœ… å·²å®Œæˆ: åˆ›å»º`collective_benchmarking.py`**
   - æ–‡ä»¶è·¯å¾„: `/data/users/tianren/pytorch/torch/_inductor/runtime/collective_benchmarking.py`
   - åŒ…å«æ ¸å¿ƒbenchmarkingé€»è¾‘å’ŒåŒæ­¥æœºåˆ¶

2. **ğŸ”² å¾…å®Œæˆ: ä¿®æ”¹`custom_op.py`**
   - æ–‡ä»¶: `/data/users/tianren/pytorch/torch/_inductor/kernel/custom_op.py`
   - å‡½æ•°: `autotune_custom_op()`
   - ä¿®æ”¹å†…å®¹:
     ```python
     # åœ¨autotune_custom_op()ä¸­æ·»åŠ :
     from torch._inductor.runtime.collective_benchmarking import is_collective_op

     is_collective = False
     process_group = None

     if op_overload:
         is_collective = is_collective_op(str(op_overload))
         if is_collective:
             for kwargs in non_tensor_args:
                 if 'group' in kwargs:
                     process_group = kwargs['group']
                     break

     # ä¼ é€’ç»™autotune_select_algorithm
     selected_result, winning_choice = autotune_select_algorithm(
         ...,
         is_collective=is_collective,
         process_group=process_group,
     )
     ```

3. **ğŸ”² å¾…å®Œæˆ: ä¿®æ”¹`select_algorithm.py`**
   - æ–‡ä»¶: `/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py`
   - ä¿®æ”¹ç‚¹:
     - `autotune_select_algorithm()`å‡½æ•°ç­¾å
     - `AlgorithmSelectorCache.__call__()`æ–¹æ³•
     - æ–°å¢`AlgorithmSelectorCache._autotune_collective()`æ–¹æ³•

### P1 - é‡è¦ä¼˜åŒ– (æ€§èƒ½æå‡)

1. **Cacheä¼˜åŒ–**
   - ä¿®æ”¹cache keyç”Ÿæˆ,åŒ…å«world_size
   - é¿å…é‡å¤autotuning

2. **Time Estimatoræ”¯æŒ**
   - æ·»åŠ å¿«é€Ÿä¼°è®¡æ¨¡å¼
   - åœ¨`collective_benchmarking.py`ä¸­å·²æœ‰æ¡†æ¶,å¯ä»¥å¯ç”¨

3. **é”™è¯¯å¤„ç†å¢å¼º**
   - æ·»åŠ timeoutæœºåˆ¶
   - å¤„ç†éƒ¨åˆ†rankå¤±è´¥çš„æƒ…å†µ

### P2 - å¯é€‰åŠŸèƒ½ (æ‰©å±•æ€§)

1. **æ”¯æŒæ›´å¤šcollective ops**
   - broadcast, scatter, gatherç­‰
   - å‚è€ƒ`COLLECTIVE_OPS`é›†åˆæ‰©å±•

2. **åˆ†å±‚autotuning**
   - å…ˆç”¨estimatorç­›é€‰,å†ç²¾ç¡®benchmark
   - å‡å°‘æ€»autotuningæ—¶é—´

3. **æ€§èƒ½ç›‘æ§**
   - æ·»åŠ loggingå’Œmetrics
   - è®°å½•autotuningç»“æœä¾›åˆ†æ

---

## âš ï¸ å…³é”®æ³¨æ„äº‹é¡¹

### 1. åŒæ­¥ç›¸å…³

**é—®é¢˜**: æ‰€æœ‰rankså¿…é¡»åŒæ—¶è¿›å…¥benchmark,å¦åˆ™ä¼šhang

**è§£å†³æ–¹æ¡ˆ**:
- âœ… åœ¨`benchmark_collective_op()`ä¸­ä½¿ç”¨`dist.barrier()`
- âœ… æ¯æ¬¡benchmarkå‰éƒ½åŒæ­¥
- âš ï¸ ç¡®ä¿æ‰€æœ‰ranksçš„choicesé¡ºåºä¸€è‡´

**ä»£ç ä½ç½®**:
```python
# collective_benchmarking.py, line ~166
for _ in range(nruns):
    dist.barrier(group=process_group)  # å…³é”®åŒæ­¥ç‚¹
    torch.cuda.synchronize()
    # ... benchmark ...
```

### 2. Process Groupä¼ é€’

**é—®é¢˜**: process_groupå¯èƒ½åœ¨kwargsä¸­,ä¹Ÿå¯èƒ½æ˜¯é»˜è®¤çš„

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¼˜å…ˆçº§:
# 1. ä»kwargsä¸­æå– 'group' æˆ– 'process_group'
# 2. å¦‚æœæ²¡æœ‰,ä½¿ç”¨ dist.group.WORLD (é»˜è®¤)
# 3. å¦‚æœdistributedæœªåˆå§‹åŒ–,æŠ›å‡ºæ¸…æ™°çš„é”™è¯¯

process_group = kwargs.get('group') or kwargs.get('process_group') or None
```

**ä»£ç ä½ç½®**: `custom_op.py`çš„detectioné˜¶æ®µ

### 3. æ—¶é—´èšåˆç­–ç•¥

**é—®é¢˜**: ä¸åŒrankså¯èƒ½æœ‰ä¸åŒçš„timing

**è§£å†³æ–¹æ¡ˆ**:
- âœ… ä½¿ç”¨`all_reduce(MAX)`è·å–æœ€æ…¢çš„rankæ—¶é—´
- åŸå› : ä¿å®ˆä¼°è®¡,ç¡®ä¿æ‰€æœ‰rankséƒ½èƒ½å®Œæˆ

**ä»£ç ä½ç½®**:
```python
# collective_benchmarking.py, line ~188
comm_time_tensor = torch.tensor([comm_time], device=device)
dist.all_reduce(comm_time_tensor, op=dist.ReduceOp.MAX)
comm_time = comm_time_tensor.item()  # æ‰€æœ‰rankséƒ½ä¼šå¾—åˆ°ç›¸åŒçš„maxå€¼
```

### 4. Cache Keyç”Ÿæˆ

**é—®é¢˜**: ä¸åŒworld_sizeå¯èƒ½æœ‰ä¸åŒçš„æœ€ä½³å®ç°

**å»ºè®®ä¿®æ”¹** (åœ¨`select_algorithm.py`):
```python
def _make_cache_key(..., is_collective=False, process_group=None):
    key = [name, str(layout), ...]

    if is_collective and dist.is_initialized():
        world_size = dist.get_world_size(process_group)
        key.append(f"ws_{world_size}")

    return tuple(key)
```

### 5. é”™è¯¯å¤„ç†

**åœºæ™¯1: æŸä¸ªrankçš„benchmarkå¤±è´¥**
```python
# å»ºè®®: è®©æ‰€æœ‰rankséƒ½æŠ›å‡ºç›¸åŒçš„é”™è¯¯,é¿å…hang
try:
    time_us = benchmarker.benchmark(...)
except Exception as e:
    # Broadcast error to all ranks
    error_flag = torch.tensor([1], device='cuda')
    dist.all_reduce(error_flag, op=dist.ReduceOp.MAX)
    raise RuntimeError(f"Benchmark failed on at least one rank: {e}")
```

**åœºæ™¯2: Distributedæœªåˆå§‹åŒ–**
```python
# åœ¨CollectiveBenchmarker.__init__ä¸­å·²æœ‰æ£€æŸ¥
if not dist.is_initialized():
    log.warning("Distributed not initialized")
    # è¿è¡Œæ—¶ä¼šæŠ›å‡ºæ¸…æ™°çš„é”™è¯¯
```

### 6. è¾“å…¥è¾“å‡ºTensorå‡†å¤‡

**é—®é¢˜**: æŸäº›collective opséœ€è¦é¢å¤–çš„output tensor

**è§£å†³æ–¹æ¡ˆ** (åœ¨`collective_benchmarking.py`ä¸­å·²å®ç°):
```python
# all_gather: éœ€è¦world_sizeå€å¤§å°çš„output
if "all_gather" in comm_func_name:
    output_tensor = torch.empty(
        world_size * input_tensor.numel(),
        dtype=input_tensor.dtype,
        device=input_tensor.device
    )

# reduce_scatter: éœ€è¦1/world_sizeå¤§å°çš„output
elif "reduce_scatter" in comm_func_name:
    output_tensor = torch.empty(
        input_tensor.numel() // world_size,
        ...
    )
```

---

## ğŸ”§ å®ç°æ­¥éª¤ (Step-by-Step)

### Step 1: ä¿®æ”¹`custom_op.py` (15åˆ†é’Ÿ)

```python
# åœ¨ autotune_custom_op() å‡½æ•°ä¸­,line ~324é™„è¿‘

# 1. æ·»åŠ import
from torch._inductor.runtime.collective_benchmarking import is_collective_op

# 2. åœ¨è°ƒç”¨autotune_select_algorithmä¹‹å‰æ·»åŠ æ£€æµ‹é€»è¾‘
is_collective = False
process_group = None

if op_overload:
    op_name = str(op_overload)
    is_collective = is_collective_op(op_name)

    if is_collective:
        # å°è¯•ä»non_tensor_argsä¸­æå–process_group
        for kwargs_dict in non_tensor_args:
            if 'group' in kwargs_dict:
                process_group = kwargs_dict['group']
                break
            elif 'process_group' in kwargs_dict:
                process_group = kwargs_dict['process_group']
                break

# 3. ä¼ é€’ç»™autotune_select_algorithm
selected_result, winning_choice = autotune_select_algorithm(
    name=name,
    choices=choices,
    input_nodes=list(inputs),
    layout=choices[0].layout,
    input_gen_fns=input_gen_fns,
    return_choice=True,
    is_collective=is_collective,      # NEW
    process_group=process_group,      # NEW
)
```

### Step 2: ä¿®æ”¹`select_algorithm.py` - Part A (20åˆ†é’Ÿ)

**ä½ç½®**: `autotune_select_algorithm()`å‡½æ•°,line ~3908

```python
def autotune_select_algorithm(
    name,
    choices,
    input_nodes,
    layout,
    *,
    input_gen_fns=None,
    return_choice=False,
    is_collective=False,      # NEW
    process_group=None,        # NEW
    **kwargs,
):
    cache = get_algorithm_selector_cache()

    if "return_multi_template" not in kwargs:
        kwargs["return_multi_template"] = (
            torch._inductor.config.benchmark_epilogue_fusion
        )

    if "precompilation_timeout_seconds" not in kwargs:
        kwargs["precompilation_timeout_seconds"] = config.precompilation_timeout_seconds

    # ä¼ é€’æ–°å‚æ•°
    return cache(
        name,
        choices,
        input_nodes,
        layout,
        input_gen_fns=input_gen_fns,
        return_choice=return_choice,
        is_collective=is_collective,      # NEW
        process_group=process_group,      # NEW
        **kwargs,
    )
```

### Step 3: ä¿®æ”¹`select_algorithm.py` - Part B (30åˆ†é’Ÿ)

**ä½ç½®**: `AlgorithmSelectorCache.__call__()`,éœ€è¦æ‰¾åˆ°å…·ä½“è¡Œæ•°

```python
class AlgorithmSelectorCache:
    def __call__(
        self,
        name,
        choices,
        input_nodes,
        layout,
        *,
        input_gen_fns=None,
        return_choice=False,
        is_collective=False,      # NEW
        process_group=None,        # NEW
        **kwargs,
    ):
        # ... existing preprocessing and cache lookup code ...

        # NEW: æ·»åŠ collective opsçš„è·¯ç”±é€»è¾‘
        if is_collective:
            import torch.distributed as dist
            from torch._inductor.runtime.collective_benchmarking import (
                CollectiveBenchmarker,
            )

            if not dist.is_initialized():
                log.warning(
                    f"Collective op '{name}' requires distributed initialization. "
                    f"Falling back to regular autotuning."
                )
                is_collective = False
            else:
                benchmarker = CollectiveBenchmarker(
                    process_group=process_group,
                    nruns=config.benchmark_kernel_nruns,
                )

                # ä½¿ç”¨specialized collective benchmarking
                result = self._autotune_collective(
                    name,
                    choices,
                    input_nodes,
                    layout,
                    benchmarker,
                    input_gen_fns,
                    return_choice=return_choice,
                    **kwargs,
                )

                # Cache the result
                # TODO: éœ€è¦ä¿®æ”¹cache keyåŒ…å«world_size

                return result

        # Regular autotuning path (existing code)
        # ... continue with existing logic ...
```

### Step 4: å®ç°`_autotune_collective()`æ–¹æ³• (45åˆ†é’Ÿ)

**ä½ç½®**: `AlgorithmSelectorCache`ç±»ä¸­æ–°å¢æ–¹æ³•

```python
class AlgorithmSelectorCache:
    # ... existing methods ...

    def _autotune_collective(
        self,
        name,
        choices,
        input_nodes,
        layout,
        benchmarker,
        input_gen_fns,
        return_choice=False,
        **kwargs,
    ):
        """Autotune collective operations with cross-rank synchronization.

        This method benchmarks collective operations ensuring all ranks
        synchronize before and during benchmarking for accurate timing.
        """
        import torch.distributed as dist

        log.info(
            f"[Collective Autotune] Starting autotuning for {name} "
            f"with {len(choices)} choices"
        )

        # 1. å‡†å¤‡è¾“å…¥æ•°æ®
        # å¤ç”¨ç°æœ‰çš„input generationé€»è¾‘
        # TODO: éœ€è¦æ‰¾åˆ°ç°æœ‰ä»£ç ä¸­ç”Ÿæˆè¾“å…¥çš„éƒ¨åˆ†

        # 2. éå†æ‰€æœ‰choicesè¿›è¡Œbenchmarking
        timings = []

        for idx, choice in enumerate(choices):
            try:
                log.debug(
                    f"[Collective Autotune] Benchmarking choice {idx}: {choice.name}"
                )

                # TODO: æ ¹æ®choiceç±»å‹å‡†å¤‡output tensor
                # å¯¹äºall_gather: output_size = input_size * world_size
                # å¯¹äºreduce_scatter: output_size = input_size / world_size

                # æ³¨æ„: è¿™é‡Œéœ€è¦å®é™…çš„input tensors,ä¸æ˜¯IR nodes
                # å¯èƒ½éœ€è¦å‚è€ƒç°æœ‰çš„benchmarkä»£ç å¦‚ä½•ç”Ÿæˆreal tensors

                # Benchmark this choice
                # time_us = benchmarker.benchmark(
                #     comm_func=choice.kernel,
                #     comm_func_name=choice.name,
                #     input_tensors=[...],  # TODO: convert IR nodes to tensors
                #     output_tensor=output_tensor,
                # )

                # timings.append((choice, time_us))

                # PLACEHOLDER: æš‚æ—¶ä½¿ç”¨infä½œä¸ºå ä½
                timings.append((choice, float('inf')))

            except Exception as e:
                log.warning(
                    f"[Collective Autotune] Choice {choice.name} failed: {e}"
                )
                timings.append((choice, float('inf')))

        # 3. é€‰æ‹©æœ€ä½³choice
        if not timings:
            raise RuntimeError(f"No valid choices for collective op {name}")

        best_choice, best_time = min(timings, key=lambda x: x[1])

        rank = dist.get_rank()
        if rank == 0:
            log.info(
                f"[Collective Autotune] {name}: "
                f"Selected {best_choice.name} with time {best_time:.2f} us"
            )

        # 4. è°ƒç”¨winning choiceç”Ÿæˆç»“æœ
        # TODO: éœ€è¦å‚è€ƒç°æœ‰ä»£ç å¦‚ä½•è°ƒç”¨choiceå¹¶è·å–ç»“æœ
        # result = self._call_choice(best_choice, input_nodes)

        # æš‚æ—¶è¿”å›None
        if return_choice:
            return None, best_choice  # TODO: fix
        else:
            return None  # TODO: fix
```

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

åˆ›å»º `test/inductor/test_collective_autotuning.py`:

```python
import unittest
import torch
import torch.distributed as dist
from torch.testing._internal.common_distributed import (
    MultiProcessTestCase,
    skip_if_lt_x_gpu,
)

class TestCollectiveAutotuning(MultiProcessTestCase):
    @skip_if_lt_x_gpu(2)
    def test_allreduce_benchmark(self):
        """Test benchmarking of all_reduce operation."""
        from torch._inductor.runtime.collective_benchmarking import (
            benchmark_collective_op,
            is_collective_op,
        )

        # Initialize distributed
        dist.init_process_group(backend='nccl')

        # Test is_collective_op detection
        self.assertTrue(
            is_collective_op("torch.ops._c10d_functional.all_reduce_.default")
        )

        # Test benchmarking
        tensor = torch.randn(1024, device='cuda')
        comm_func = torch.ops._c10d_functional.all_reduce_.default

        time_us = benchmark_collective_op(
            comm_func=comm_func,
            comm_func_name="all_reduce",
            input_tensors=[tensor],
            output_tensor=None,
            nruns=2,
        )

        self.assertGreater(time_us, 0)

        dist.destroy_process_group()
```

### é›†æˆæµ‹è¯•

```python
def test_custom_collective_op_autotuning():
    """End-to-end test of collective op autotuning."""

    # Define custom collective op
    @torch.library.custom_op("test::my_allreduce", mutates_args=())
    def my_allreduce(x: torch.Tensor):
        return torch.ops._c10d_functional.all_reduce_(x, "sum")

    # Register autotuning
    from torch._inductor.kernel.custom_op import (
        register_custom_op_autotuning,
        CustomOpConfig,
    )

    register_custom_op_autotuning(
        my_allreduce,
        configs=[CustomOpConfig()],
    )

    # Test in distributed setting
    # ...
```

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### Benchmarkå¼€é”€åˆ†æ

**Regular Op Autotuning**:
- å•ä¸ªchoice benchmark: ~1-10ms
- 10ä¸ªchoicesæ€»æ—¶é—´: ~10-100ms

**Collective Op Autotuning** (with barriers):
- å•ä¸ªchoice benchmark: ~5-20ms (åŒ…å«barrierå¼€é”€)
- 10ä¸ªchoicesæ€»æ—¶é—´: ~50-200ms
- é¢å¤–å¼€é”€ä¸»è¦æ¥è‡ª: barrieråŒæ­¥ + all_reduceèšåˆ

**ä¼˜åŒ–å»ºè®®**:
1. å‡å°‘choicesæ•°é‡ (åªé€‰æ‹©æœ€æœ‰å¸Œæœ›çš„å®ç°)
2. ä½¿ç”¨time estimatorè¿›è¡Œåˆç­›
3. Cacheç»“æœé¿å…é‡å¤autotuning

---

## ğŸ”— ç›¸å…³æ–‡ä»¶ç´¢å¼•

### å·²åˆ›å»º
- âœ… `/data/users/tianren/pytorch/torch/_inductor/runtime/collective_benchmarking.py`
- âœ… `/data/users/tianren/pytorch/COLLECTIVE_OP_AUTOTUNING_DESIGN.md`
- âœ… `/data/users/tianren/pytorch/COLLECTIVE_OP_ROADMAP.md` (æœ¬æ–‡ä»¶)

### å¾…ä¿®æ”¹
- ğŸ”² `/data/users/tianren/pytorch/torch/_inductor/kernel/custom_op.py`
  - å‡½æ•°: `autotune_custom_op()` (line ~324)

- ğŸ”² `/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py`
  - å‡½æ•°: `autotune_select_algorithm()` (line ~3908)
  - ç±»æ–¹æ³•: `AlgorithmSelectorCache.__call__()` (éœ€è¦å®šä½)
  - æ–°å¢æ–¹æ³•: `AlgorithmSelectorCache._autotune_collective()` (æ–°å»º)

### å¾…åˆ›å»º
- ğŸ”² `test/inductor/test_collective_autotuning.py` (å•å…ƒæµ‹è¯•)

---

## â“ FAQ

### Q1: ä¸ºä»€ä¹ˆä¸ç›´æ¥ä¿®æ”¹ç°æœ‰çš„benchmarkingé€»è¾‘?

**A**: ä¸ºäº†ä¿æŒä»£ç æ¸…æ™°å’Œå¯ç»´æŠ¤æ€§ã€‚Collective opsæœ‰ç‹¬ç‰¹çš„åŒæ­¥éœ€æ±‚,æ··åœ¨ä¸€èµ·ä¼šè®©ä»£ç å˜å¾—å¤æ‚ä¸”éš¾ä»¥è°ƒè¯•ã€‚

### Q2: å¦‚æœåªæœ‰éƒ¨åˆ†rankséœ€è¦autotuningæ€ä¹ˆåŠ?

**A**: ç›®å‰è®¾è®¡è¦æ±‚æ‰€æœ‰rankséƒ½å‚ä¸ã€‚å¦‚æœæŸäº›ranksä¸éœ€è¦,å¯ä»¥è€ƒè™‘:
- è®©å®ƒä»¬ä¹Ÿè¿›å…¥benchmarkä½†å¿½ç•¥ç»“æœ
- æˆ–è€…åªåœ¨éœ€è¦çš„ranksä¸Šåšautotuning,ç„¶åbroadcastç»“æœ

### Q3: ä¸åŒhardwareé…ç½®çš„ranksæ€ä¹ˆå¤„ç†?

**A**: ä½¿ç”¨`all_reduce(MAX)`ç¡®ä¿é€‰æ‹©æ‰€æœ‰rankséƒ½èƒ½æ¥å—çš„é…ç½®ã€‚å¦‚æœæ€§èƒ½å·®å¼‚å¾ˆå¤§,å»ºè®®:
- åˆ†åˆ«å¯¹ä¸åŒç±»å‹çš„ranksåšautotuning
- æˆ–è€…ä¸ºheterogeneousè®¾ç½®æ·»åŠ ä¸“é—¨çš„é€»è¾‘

### Q4: å¦‚ä½•debug collective autotuning?

**A**:
1. è®¾ç½®`TORCH_LOGS="+inductor"` æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
2. æ¯ä¸ªrankå•ç‹¬è¾“å‡ºæ—¥å¿—åˆ°ä¸åŒæ–‡ä»¶
3. ä½¿ç”¨smaller world_size (2 ranks) ç®€åŒ–è°ƒè¯•
4. æ·»åŠ é¢å¤–çš„loggingåœ¨å…³é”®åŒæ­¥ç‚¹

---

## æ€»ç»“

è¿™ä¸ªè®¾è®¡æ–¹æ¡ˆæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ã€æ¨¡å—åŒ–çš„collective ops autotuningå®ç°ã€‚å…³é”®ä¼˜åŠ¿:

âœ… **æœ€å°ä¾µå…¥**: åªéœ€ä¿®æ”¹3ä¸ªæ–‡ä»¶,æ–°å¢1ä¸ªæ¨¡å—
âœ… **å¤ç”¨ç°æœ‰**: å……åˆ†åˆ©ç”¨ç°æœ‰autotuningåŸºç¡€è®¾æ–½
âœ… **æ˜“äºæ‰©å±•**: æ¨¡å—åŒ–è®¾è®¡,å®¹æ˜“æ·»åŠ æ–°çš„collective ops
âœ… **æ€§èƒ½ä¼˜åŒ–**: å‡†ç¡®çš„è·¨rankåŒæ­¥ä¿è¯benchmarkè´¨é‡

ä¸‹ä¸€æ­¥: æŒ‰ç…§roadmapä¾æ¬¡å®ç°P0åŠŸèƒ½,ç„¶åé€æ­¥æ·»åŠ ä¼˜åŒ–å’Œæµ‹è¯•ã€‚
