# Collective Op Autotuning è®¾è®¡æ–‡æ¡£

## 1. æ¦‚è¿° (Overview)

æœ¬è®¾è®¡å®ç°äº†å¯¹åˆ†å¸ƒå¼collective operations (å¦‚`all_reduce`, `all_gather`, `reduce_scatter`ç­‰)çš„custom op autotuningæ”¯æŒã€‚ä¸å¸¸è§„opsä¸åŒ,collective opséœ€è¦åœ¨autotuningæ—¶è¿›è¡Œè·¨rankåŒæ­¥,ç¡®ä¿æ‰€æœ‰ranksåŒæ—¶å¼€å§‹benchmarkã€‚

### 1.1 ç›®æ ‡

1. âœ… å¤ç”¨ç°æœ‰çš„custom op autotuningåŸºç¡€è®¾æ–½
2. âœ… é’ˆå¯¹collective opsæ·»åŠ specialized benchmarkingæœºåˆ¶
3. âœ… ä¿è¯è·¨rankçš„åŒæ­¥å’Œå‡†ç¡®è®¡æ—¶
4. âœ… æœ€å°åŒ–å¯¹ç°æœ‰ä»£ç çš„ä¾µå…¥æ€§ä¿®æ”¹

### 1.2 å…³é”®æŒ‘æˆ˜

- **è·¨rankåŒæ­¥**: æ‰€æœ‰rankså¿…é¡»åŒæ—¶å¼€å§‹benchmarkæ‰èƒ½è·å¾—å‡†ç¡®çš„æ—¶é—´æµ‹é‡
- **æ—¶é—´èšåˆ**: éœ€è¦æ”¶é›†æ‰€æœ‰ranksçš„æ—¶é—´,é€‰æ‹©æœ€å·®æƒ…å†µ(max)ä½œä¸ºä¿å®ˆä¼°è®¡
- **å…¼å®¹æ€§**: å¿…é¡»ä¸ç°æœ‰çš„autotuningæµç¨‹æ— ç¼é›†æˆ

---

## 2. æ¶æ„è®¾è®¡ (Architecture)

### 2.1 æ ¸å¿ƒç»„ä»¶

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    1. Detection Layer                           â”‚
â”‚         æ£€æµ‹æ˜¯å¦ä¸ºcollective op (custom_op.py)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2. Routing Layer                             â”‚
â”‚    æ ¹æ®opç±»å‹è·¯ç”±åˆ°ä¸åŒçš„benchmarker (select_algorithm.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Regular         â”‚    â”‚  Collective              â”‚
â”‚  Benchmarker     â”‚    â”‚  Benchmarker (NEW)       â”‚
â”‚  (existing)      â”‚    â”‚  collective_benchmarking â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ–‡ä»¶ç»“æ„

#### æ–°å¢æ–‡ä»¶
- **`torch/_inductor/runtime/collective_benchmarking.py`** âœ… å·²åˆ›å»º
  - `is_collective_op()`: æ£€æµ‹æ˜¯å¦ä¸ºcollective op
  - `benchmark_collective_op()`: æ ¸å¿ƒbenchmarkingå‡½æ•°
  - `CollectiveBenchmarker`: å°è£…çš„benchmarkerç±»

#### éœ€è¦ä¿®æ”¹çš„æ–‡ä»¶
1. **`torch/_inductor/kernel/custom_op.py`**
   - æ·»åŠ collective opæ£€æµ‹
   - ä¼ é€’process_group metadata

2. **`torch/_inductor/select_algorithm.py`**
   - åœ¨`autotune_select_algorithm`ä¸­æ£€æµ‹collective ops
   - è·¯ç”±åˆ°`CollectiveBenchmarker`

---

## 3. è¯¦ç»†å®ç°æ–¹æ¡ˆ

### 3.1 Phase 1: Detection (æ£€æµ‹é˜¶æ®µ)

**ä½ç½®**: `torch/_inductor/kernel/custom_op.py` â†’ `autotune_custom_op()`

**ä¿®æ”¹ç‚¹**:
```python
def autotune_custom_op(...):
    # ... existing code ...
    
    # NEW: æ£€æµ‹æ˜¯å¦ä¸ºcollective op
    is_collective = False
    process_group = None
    
    # ä»op_overloadæˆ–è€…decompositionsä¸­æå–ä¿¡æ¯
    if op_overload:
        op_name = str(op_overload)
        from torch._inductor.runtime.collective_benchmarking import is_collective_op
        is_collective = is_collective_op(op_name)
        
        # å¦‚æœæ˜¯collective op,å°è¯•æå–process_group
        if is_collective:
            # ä»non_tensor_argsä¸­æå–process_group
            for kwargs_dict in non_tensor_args:
                if 'group' in kwargs_dict:
                    process_group = kwargs_dict['group']
                    break
    
    # ä¼ é€’ç»™autotune_select_algorithm
    selected_result, winning_choice = autotune_select_algorithm(
        name=name,
        choices=choices,
        input_nodes=list(inputs),
        layout=choices[0].layout,
        input_gen_fns=input_gen_fns,
        return_choice=True,
        is_collective=is_collective,  # NEW
        process_group=process_group,  # NEW
    )
```

### 3.2 Phase 2: Routing (è·¯ç”±é˜¶æ®µ)

**ä½ç½®**: `torch/_inductor/select_algorithm.py` â†’ `autotune_select_algorithm()`

**ä¿®æ”¹ç‚¹1**: ä¿®æ”¹å‡½æ•°ç­¾å
```python
def autotune_select_algorithm(
    name: str,
    choices: list,
    input_nodes: list,
    layout,
    *,
    input_gen_fns=None,
    return_choice=False,
    is_collective=False,  # NEW
    process_group=None,   # NEW
    **kwargs,
):
    cache = get_algorithm_selector_cache()
    
    # ä¼ é€’ç»™cache
    return cache(
        name,
        choices,
        input_nodes,
        layout,
        input_gen_fns=input_gen_fns,
        return_choice=return_choice,
        is_collective=is_collective,  # NEW
        process_group=process_group,  # NEW
        **kwargs,
    )
```

**ä¿®æ”¹ç‚¹2**: ä¿®æ”¹`AlgorithmSelectorCache.__call__`
```python
class AlgorithmSelectorCache:
    def __call__(
        self,
        name,
        choices,
        input_nodes,
        layout,
        *,
        input_gen_fns=None,
        is_collective=False,  # NEW
        process_group=None,   # NEW
        **kwargs,
    ):
        # ... existing cache lookup code ...
        
        if cached_result is not None:
            return cached_result
        
        # NEW: æ ¹æ®æ˜¯å¦ä¸ºcollective opé€‰æ‹©benchmarker
        if is_collective:
            from torch._inductor.runtime.collective_benchmarking import (
                CollectiveBenchmarker,
            )
            benchmarker = CollectiveBenchmarker(
                process_group=process_group,
                nruns=config.benchmark_kernel_nruns,
            )
            # ä½¿ç”¨specialized benchmarkingè·¯å¾„
            result = self._autotune_collective(
                name,
                choices,
                input_nodes,
                layout,
                benchmarker,
                input_gen_fns,
                **kwargs,
            )
        else:
            # ä½¿ç”¨ç°æœ‰çš„regular autotuningè·¯å¾„
            result = self._autotune_regular(
                name, choices, input_nodes, layout, input_gen_fns, **kwargs
            )
        
        # Cache and return
        return result
```

### 3.3 Phase 3: Benchmarking (benchmarké˜¶æ®µ)

**ä½ç½®**: `torch/_inductor/select_algorithm.py` â†’ `AlgorithmSelectorCache._autotune_collective()`

**æ–°å¢æ–¹æ³•**:
```python
class AlgorithmSelectorCache:
    def _autotune_collective(
        self,
        name,
        choices,
        input_nodes,
        layout,
        benchmarker,
        input_gen_fns,
        **kwargs,
    ):
        """Autotune collective operations with cross-rank synchronization."""
        
        # 1. ç”Ÿæˆè¾“å…¥æ•°æ® (ä¸regular autotuningç›¸åŒ)
        input_tensors = self._generate_inputs(input_nodes, input_gen_fns)
        
        # 2. éå†æ‰€æœ‰choicesè¿›è¡Œbenchmarking
        timings = []
        for choice in choices:
            try:
                # å…³é”®: æ‰€æœ‰rankså¿…é¡»åŒæ—¶è¿›å…¥è¿™ä¸ªbenchmark
                # CollectiveBenchmarkerå†…éƒ¨ä¼šåšbarrieråŒæ­¥
                
                # å‡†å¤‡è¾“å‡ºtensor (å¦‚æœéœ€è¦)
                output_tensor = self._prepare_output_tensor(choice, layout)
                
                # Benchmarkè¿™ä¸ªchoice
                time_us = benchmarker.benchmark(
                    comm_func=choice.kernel,
                    comm_func_name=choice.name,
                    input_tensors=input_tensors,
                    output_tensor=output_tensor,
                )
                
                timings.append((choice, time_us))
                
            except Exception as e:
                log.warning(f"Choice {choice.name} failed: {e}")
                timings.append((choice, float('inf')))
        
        # 3. é€‰æ‹©æœ€ä½³choice
        # æ³¨æ„: timingså·²ç»æ˜¯æ‰€æœ‰ranksçš„maxå€¼(åœ¨benchmark_collective_opä¸­å¤„ç†)
        best_choice, best_time = min(timings, key=lambda x: x[1])
        
        log.info(
            f"[Collective Autotune] {name}: "
            f"Selected {best_choice.name} with time {best_time:.2f} us"
        )
        
        # 4. è¿”å›ç»“æœ
        if kwargs.get('return_choice', False):
            return self._call_choice(best_choice, input_nodes), best_choice
        else:
            return self._call_choice(best_choice, input_nodes)
```

### 3.4 Phase 4: Synchronization (åŒæ­¥æœºåˆ¶)

**ä½ç½®**: `torch/_inductor/runtime/collective_benchmarking.py` â†’ `benchmark_collective_op()`

**å…³é”®ä»£ç ** (å·²åœ¨æ–‡ä»¶ä¸­å®ç°):

```python
def benchmark_collective_op(...):
    # ... å‡†å¤‡è¾“å…¥å‚æ•° ...
    
    # Warmup
    torch.cuda.synchronize()
    comm_func(**input_args, group=process_group)
    torch.cuda.synchronize()
    
    comm_time = 0.0
    for _ in range(nruns):
        # ğŸ”‘ å…³é”®1: Barrierç¡®ä¿æ‰€æœ‰ranksåŒæ—¶å¼€å§‹
        dist.barrier(group=process_group)
        torch.cuda.synchronize()
        
        # ğŸ”‘ å…³é”®2: ä½¿ç”¨CUDA eventsç²¾ç¡®è®¡æ—¶
        start_evt = torch.cuda.Event(enable_timing=True)
        end_evt = torch.cuda.Event(enable_timing=True)
        
        start_evt.record()
        comm_func(**input_args, group=process_group)
        end_evt.record()
        end_evt.synchronize()
        
        comm_time += start_evt.elapsed_time(end_evt)
    
    comm_time = (comm_time / nruns) * 1000.0  # ms -> us
    
    # ğŸ”‘ å…³é”®3: All-reduceè·å–æ‰€æœ‰ranksçš„æœ€å¤§æ—¶é—´
    if process_group is not None:
        comm_time_tensor = torch.tensor([comm_time], device=device)
        dist.all_reduce(comm_time_tensor, op=dist.ReduceOp.MAX, group=process_group)
        comm_time = comm_time_tensor.item()
    
    return comm_time
```

**åŒæ­¥æµç¨‹**:
```
Rank 0                  Rank 1                  Rank N
  â”‚                       â”‚                       â”‚
  â”œâ”€ barrier() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€ barrier() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€ barrier()
  â”‚  (wait for all)       â”‚  (wait for all)       â”‚  (wait for all)
  â”‚                       â”‚                       â”‚
  â”œâ”€ cuda.sync()          â”œâ”€ cuda.sync()          â”œâ”€ cuda.sync()
  â”‚                       â”‚                       â”‚
  â”œâ”€ start_event.record() â”œâ”€ start_event.record() â”œâ”€ start_event.record()
  â”œâ”€ collective_op()      â”œâ”€ collective_op()      â”œâ”€ collective_op()
  â”œâ”€ end_event.record()   â”œâ”€ end_event.record()   â”œâ”€ end_event.record()
  â”‚                       â”‚                       â”‚
  â”œâ”€ measure time: t0     â”œâ”€ measure time: t1     â”œâ”€ measure time: tN
  â”‚                       â”‚                       â”‚
  â”œâ”€ all_reduce(MAX) â”€â”€â”€â”€â”€â”¼â”€ all_reduce(MAX) â”€â”€â”€â”€â”€â”¼â”€ all_reduce(MAX)
  â”‚                       â”‚                       â”‚
  â””â”€ final_time = max(t0, t1, ..., tN) on all ranks
```

---

## 4. ä½¿ç”¨ç¤ºä¾‹

### 4.1 æ³¨å†Œcollective op autotuning

```python
import torch
from torch._inductor.kernel.custom_op import (
    register_custom_op_autotuning,
    CustomOpConfig,
)

# å®šä¹‰custom collective op
@torch.library.custom_op("mylib::my_allreduce", mutates_args=())
def my_allreduce(tensor: torch.Tensor, group_name: str = "default"):
    return torch.ops._c10d_functional.all_reduce_(
        tensor, "sum", group_name=group_name
    )

# å®ç°1: NCCLç‰ˆæœ¬
def allreduce_nccl(tensor, group_name="default"):
    return torch.ops._c10d_functional.all_reduce_(
        tensor, "sum", group_name=group_name
    )

# å®ç°2: è‡ªå®šä¹‰åˆ†æ®µallreduce
def allreduce_chunked(tensor, group_name="default", chunk_size=1024):
    # Custom implementation with chunking
    ...

# æ³¨å†Œautotuning
register_custom_op_autotuning(
    my_allreduce,
    configs=[
        CustomOpConfig(allreduce_nccl),
        CustomOpConfig(allreduce_chunked, chunk_size=1024),
        CustomOpConfig(allreduce_chunked, chunk_size=2048),
    ],
    input_gen_fns={
        "tensor": lambda fake: torch.randn_like(fake, device='cuda'),
    },
)
```

### 4.2 åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ä½¿ç”¨

```python
import torch.distributed as dist

# åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
dist.init_process_group(backend='nccl')

# ç¼–è¯‘æ¨¡å‹ (inductorä¼šè‡ªåŠ¨autotune collective ops)
model = torch.compile(model)

# è®­ç»ƒæ—¶,collective opsä¼šä½¿ç”¨autotunedå®ç°
output = model(input)  # å†…éƒ¨çš„my_allreduceä¼šè¢«autotuned
```

---

## 5. å®ç°æ³¨æ„äº‹é¡¹

### 5.1 å¿…é¡»æ³¨æ„çš„é—®é¢˜

1. **BarrieråŒæ­¥å¼€é”€**
   - æ¯æ¬¡benchmarkéƒ½éœ€è¦barrier,ä¼šæœ‰é¢å¤–å¼€é”€
   - è§£å†³æ–¹æ¡ˆ: å‡å°‘nrunsæ•°é‡,æˆ–è€…ä½¿ç”¨cachedç»“æœ

2. **Process Groupä¼ é€’**
   - å¿…é¡»ç¡®ä¿process_groupæ­£ç¡®ä¼ é€’åˆ°benchmarkingå±‚
   - å¦‚æœmissing,é»˜è®¤ä½¿ç”¨`dist.group.WORLD`

3. **é”™è¯¯å¤„ç†**
   - å¦‚æœæŸä¸ªrankçš„benchmarkå¤±è´¥,æ•´ä¸ªautotuningä¼šhang
   - è§£å†³æ–¹æ¡ˆ: æ·»åŠ timeoutæœºåˆ¶,æˆ–è€…è®©æ‰€æœ‰ranksåŒæ—¶æŠ›å‡ºå¼‚å¸¸

4. **Cache Keyç”Ÿæˆ**
   - Collective opsçš„cache keyéœ€è¦åŒ…å«world_sizeå’Œrankä¿¡æ¯
   - ä¸åŒçš„world_sizeå¯èƒ½æœ‰ä¸åŒçš„æœ€ä½³å®ç°

### 5.2 å¯é€‰ä¼˜åŒ–

1. **Time Estimator**
   - ä½¿ç”¨`dist._time_estimator`å¯ä»¥æ›´å¿«åœ°ä¼°è®¡æ—¶é—´
   - ä½†å‡†ç¡®æ€§è¾ƒä½,é€‚åˆå¿«é€ŸåŸå‹

2. **åˆ†å±‚Autotuning**
   - å…ˆç”¨estimatorå¿«é€Ÿç­›é€‰,å†ç”¨å®é™…benchmarkç²¾ç¡®æµ‹é‡
   - å¯ä»¥æ˜¾è‘—å‡å°‘æ€»autotuningæ—¶é—´

3. **Cached Resultså…±äº«**
   - Rank 0åšautotuning,ç„¶åbroadcastç»“æœç»™å…¶ä»–ranks
   - éœ€è¦ç¡®ä¿æ‰€æœ‰ranksçš„ç¡¬ä»¶é…ç½®ç›¸åŒ

---

## 6. ä¸ç°æœ‰Autotuningçš„å¯¹æ¯”

| ç»´åº¦ | Regular Autotuning | Collective Autotuning |
|------|-------------------|----------------------|
| **åŒæ­¥** | ä¸éœ€è¦ | å¿…é¡»barrieråŒæ­¥ |
| **è®¡æ—¶** | å•rank | æ‰€æœ‰ranksçš„max |
| **ç¼“å­˜** | åŸºäºshape/dtype | é¢å¤–åŒ…å«world_size |
| **å¤±è´¥å¤„ç†** | å•ranké‡è¯• | æ‰€æœ‰ranksåŒæ—¶å¤„ç† |
| **å¼€é”€** | ä½ | ä¸­ç­‰(barrierå¼€é”€) |

---

## 7. æµ‹è¯•è®¡åˆ’

### 7.1 å•å…ƒæµ‹è¯•

```python
# test/inductor/test_collective_autotuning.py
class TestCollectiveAutotuning(unittest.TestCase):
    def test_allreduce_autotuning(self):
        # æµ‹è¯•all_reduceçš„autotuning
        ...
    
    def test_allgather_autotuning(self):
        # æµ‹è¯•all_gatherçš„autotuning
        ...
    
    def test_sync_correctness(self):
        # éªŒè¯è·¨rankåŒæ­¥æ˜¯å¦æ­£ç¡®
        ...
```

### 7.2 é›†æˆæµ‹è¯•

```python
def test_end_to_end_collective_autotuning():
    # æ¨¡æ‹ŸçœŸå®çš„åˆ†å¸ƒå¼è®­ç»ƒåœºæ™¯
    # éªŒè¯autotuned collective opçš„æ­£ç¡®æ€§å’Œæ€§èƒ½
    ...
```

---

## 8. ä¸‹ä¸€æ­¥å·¥ä½œ

### 8.1 å¿…é¡»å®Œæˆ
- [ ] ä¿®æ”¹`custom_op.py`æ·»åŠ detectioné€»è¾‘
- [ ] ä¿®æ”¹`select_algorithm.py`æ·»åŠ routingé€»è¾‘
- [ ] å®ç°`_autotune_collective`æ–¹æ³•
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•

### 8.2 å¯é€‰å¢å¼º
- [ ] å®ç°time estimatorå¿«é€Ÿæ¨¡å¼
- [ ] æ·»åŠ æ›´å¤šcollective opsæ”¯æŒ(broadcast, scatterç­‰)
- [ ] å®ç°åˆ†å±‚autotuningä¼˜åŒ–
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§å’Œæ—¥å¿—

---

## 9. å‚è€ƒèµ„æ–™

1. **Autoparallel Reference**
   - https://github.com/meta-pytorch/autoparallel/blob/main/autoparallel/autobucketing_util/estimation_utils.py
   - `benchmark_comm_func`å‡½æ•°çš„å®ç°

2. **Inductor Autotuning**
   - `/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py`
   - ç°æœ‰çš„autotuningåŸºç¡€è®¾æ–½

3. **Custom Op Framework**
   - `/data/users/tianren/pytorch/torch/_inductor/kernel/custom_op.py`
   - Custom opçš„loweringå’Œautotuningæœºåˆ¶

---

## 10. é™„å½•: å…³é”®ä»£ç ç‰‡æ®µç´¢å¼•

### A. Collective Opæ£€æµ‹
- æ–‡ä»¶: `collective_benchmarking.py`
- å‡½æ•°: `is_collective_op()`
- è¡Œ: 37-47

### B. Benchmarkingæ ¸å¿ƒé€»è¾‘
- æ–‡ä»¶: `collective_benchmarking.py`
- å‡½æ•°: `benchmark_collective_op()`
- è¡Œ: 70-197

### C. åŒæ­¥æœºåˆ¶
- æ–‡ä»¶: `collective_benchmarking.py`
- å‡½æ•°: `benchmark_collective_op()` ä¸­çš„barrierå’Œall_reduceéƒ¨åˆ†
- è¡Œ: 166-193

---

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®,è¯·è”ç³»:
- Owner: PyTorch Inductor Team
- Module: `torch._inductor`
