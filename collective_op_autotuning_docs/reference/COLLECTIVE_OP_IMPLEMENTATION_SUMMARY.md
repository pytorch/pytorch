# Collective Op Autotuning - å®Œæ•´å®æ–½æ–¹æ¡ˆæ€»ç»“

## ğŸ“¦ å·²äº¤ä»˜æ–‡ä»¶æ¸…å•

### 1. æ ¸å¿ƒå®ç°
- **`/data/users/tianren/pytorch/torch/_inductor/runtime/collective_benchmarking.py`** âœ…
  - å®Œæ•´çš„collective benchmarkingå®ç°
  - âœ… è·¨rankåŒæ­¥æœºåˆ¶ (barrier + all_reduce)
  - âœ… Timeoutä¿æŠ¤ (`sync_with_timeout`, `try_collective_benchmark_with_timeout`)
  - âœ… æ”¯æŒall_reduce, all_gather, reduce_scatter, all_to_all

### 2. è®¾è®¡æ–‡æ¡£
- **`COLLECTIVE_OP_AUTOTUNING_DESIGN.md`** âœ…
  - V1åŸºç¡€æ–¹æ¡ˆçš„å®Œæ•´è®¾è®¡
  - 4ä¸ªå®ç°é˜¶æ®µè¯¦è§£
  - ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯•è®¡åˆ’

- **`COLLECTIVE_OP_ROADMAP.md`** âœ…
  - Step-by-stepå®ç°æŒ‡å—
  - ä¼˜å…ˆçº§åˆ†çº§ (P0/P1/P2)
  - å…³é”®æ³¨æ„äº‹é¡¹å’ŒFAQ
  
- **`COLLECTIVE_OP_V2_DESIGN.md`** âœ…
  - åŸºäºMultiTemplateBufferçš„å‡çº§æ–¹æ¡ˆ
  - æ”¯æŒfusionçš„å®Œæ•´è®¾è®¡
  - V1 vs V2å¯¹æ¯”åˆ†æ

---

## ğŸ¯ æ–¹æ¡ˆå¯¹æ¯” - Quick Reference

| ç‰¹æ€§ | V1 (Custom Op) | V2 (Scheduler) |
|-----|---------------|----------------|
| **å®ç°éš¾åº¦** | â­â­ ç®€å• | â­â­â­â­ ä¸­ç­‰ |
| **å¼€å‘æ—¶é—´** | 1-2 days | 3-4 days |
| **Fusionæ”¯æŒ** | âŒ | âœ… |
| **åŒæ­¥æ•ˆç‡** | ä¸­ç­‰ (æ¯ä¸ªopä¸€æ¬¡) | é«˜ (ç»Ÿä¸€ä¸€æ¬¡) |
| **é€šç”¨æ€§** | Custom ops only | æ‰€æœ‰MultiTemplateBuffer |
| **ç¨³å®šæ€§** | â­â­â­â­â­ é«˜ | â­â­â­ ä¸­ |

---

## ğŸ“‹ å®æ–½å»ºè®® - åˆ†é˜¶æ®µRoadmap

### Phase 1: V1åŸºç¡€å®ç° (P0 - å¿…é¡»å®Œæˆ)
**ç›®æ ‡**: è®©basic collective op autotuning workèµ·æ¥
**æ—¶é—´**: 1-2 days

**ä»»åŠ¡æ¸…å•**:
- [x] âœ… åˆ›å»º`collective_benchmarking.py`
- [x] âœ… å®ç°timeoutæœºåˆ¶
- [ ] ğŸ”² ä¿®æ”¹`custom_op.py`æ·»åŠ detection
- [ ] ğŸ”² ä¿®æ”¹`select_algorithm.py`æ·»åŠ routing  
- [ ] ğŸ”² åŸºç¡€æµ‹è¯•

**äº¤ä»˜ç‰©**:
- èƒ½å¤Ÿautotune simple custom collective ops (å¦‚è‡ªå®šä¹‰all_reduce)
- æœ‰timeoutä¿æŠ¤,ä¸ä¼šhang
- åŸºç¡€åŠŸèƒ½éªŒè¯é€šè¿‡

---

### Phase 2: V1ä¼˜åŒ–å’Œç¨³å®š (P1 - é‡è¦)
**ç›®æ ‡**: Production-ready V1
**æ—¶é—´**: 1-2 days

**ä»»åŠ¡æ¸…å•**:
- [ ] ğŸ”² ä¼˜åŒ–loggingå’Œmetrics
- [ ] ğŸ”² å®Œå–„é”™è¯¯å¤„ç†
- [ ] ğŸ”² ç¼–å†™comprehensive tests
- [ ] ğŸ”² æ€§èƒ½ä¼˜åŒ– (cache keyæ”¹è¿›)
- [ ] ğŸ”² æ–‡æ¡£å®Œå–„

**äº¤ä»˜ç‰©**:
- V1æ–¹æ¡ˆç¨³å®š,å¯ä»¥ä¸Šçº¿ä½¿ç”¨
- å®Œæ•´çš„æµ‹è¯•è¦†ç›–
- æ¸…æ™°çš„ä½¿ç”¨æ–‡æ¡£

---

### Phase 3: V2åŸºç¡€å®ç° (P2 - å¯é€‰)
**ç›®æ ‡**: æ”¯æŒfusionçš„é«˜çº§åŠŸèƒ½
**æ—¶é—´**: 3-4 days

**ä»»åŠ¡æ¸…å•**:
- [ ] ğŸ”² åˆ›å»º`CollectiveMultiTemplateBuffer`ç±» (ir.py)
- [ ] ğŸ”² ä¿®æ”¹`scheduler.py`æ·»åŠ pre-sync
- [ ] ğŸ”² ä¿®æ”¹`select_algorithm.py`åˆ›å»ºCollectiveç‰ˆæœ¬
- [ ] ğŸ”² å®ç°schedulerä¸­çš„distributed benchmarking
- [ ] ğŸ”² Fusionæ”¯æŒæµ‹è¯•

**äº¤ä»˜ç‰©**:
- V2æ–¹æ¡ˆåŸºç¡€åŠŸèƒ½å®Œæˆ
- æ”¯æŒcollective ops + epilogue fusion
- ç»Ÿä¸€sync windowå‡å°‘overhead

---

### Phase 4: V2ä¼˜åŒ–å’Œç”Ÿäº§åŒ– (P3 - æœªæ¥)
**ç›®æ ‡**: V2æˆä¸ºproduction default
**æ—¶é—´**: 2-3 days

**ä»»åŠ¡æ¸…å•**:
- [ ] ğŸ”² V1/V2å…±å­˜ç­–ç•¥å®ç°
- [ ] ğŸ”² é…ç½®é€‰é¡¹æ·»åŠ 
- [ ] ğŸ”² æ€§èƒ½benchmarkå’Œå¯¹æ¯”
- [ ] ğŸ”² è¿ç§»ç°æœ‰ç”¨æˆ·åˆ°V2
- [ ] ğŸ”² V1ä½œä¸ºfallbackä¿ç•™

**äº¤ä»˜ç‰©**:
- V2 production-ready
- å¹³æ»‘çš„è¿ç§»è·¯å¾„
- å®Œæ•´çš„æ€§èƒ½æ•°æ®

---

## ğŸ”‘ å…³é”®æŠ€æœ¯å†³ç­–

### 1. Timeoutæœºåˆ¶ âœ… å·²å®ç°

**é—®é¢˜**: å¦‚ä½•é¿å…å› ä¸ºæŸä¸ªrankæ— å“åº”è€Œhang?

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¸¤å±‚timeoutä¿æŠ¤
1. Pre-sync timeout (~5ms) - å¿«é€Ÿæ£€æµ‹ranksæ˜¯å¦ready
2. Benchmark timeout (~30s) - é˜²æ­¢benchmark hang

# ä½¿ç”¨async_op + polling
work = dist.all_reduce(..., async_op=True)
while not work.is_completed():
    if time.time() - start > timeout:
        return False  # Timeout, skip this benchmark
    time.sleep(0.01)
```

**å®ç°ä½ç½®**:
- `sync_with_timeout()` - Line ~270 in collective_benchmarking.py
- `try_collective_benchmark_with_timeout()` - Line ~335

---

### 2. MultiTemplateBuffer Integration

**é—®é¢˜**: å¦‚ä½•å’Œç°æœ‰çš„MultiTemplateBufferæœºåˆ¶é›†æˆ?

**V1æ–¹æ¡ˆ** (ç®€å•):
- åœ¨custom_opå±‚é¢ç›´æ¥å¤„ç†
- ä¸åˆ›å»ºMultiTemplateBuffer
- é€‚åˆ: å¿«é€ŸéªŒè¯,ç®€å•åœºæ™¯

**V2æ–¹æ¡ˆ** (å®Œæ•´):
- åˆ›å»º`CollectiveMultiTemplateBuffer`
- åœ¨scheduleré˜¶æ®µç»Ÿä¸€å¤„ç†
- é€‚åˆ: æ”¯æŒfusion,å¤šä¸ªcollective ops

**å»ºè®®**: å…ˆV1éªŒè¯,åç»­å‡çº§åˆ°V2

---

### 3. åŒæ­¥ç­–ç•¥

**V1**: æ¯ä¸ªopå•ç‹¬sync
```python
for collective_op in ops:
    sync_and_benchmark(op)  # ~50ms per op
# æ€»æ—¶é—´: N * 50ms
```

**V2**: ç»Ÿä¸€pre-sync
```python
# Pre-sync once (~5ms)
if sync_all_ranks():
    # Batch benchmark all ops
    for collective_op in ops:
        benchmark(op)  # å†…éƒ¨æœ‰barrier
# æ€»æ—¶é—´: 5ms + N * benchmark_time
```

**æ€§èƒ½å·®å¼‚**: 
- 1ä¸ªop: V1â‰ˆV2
- 3ä¸ªops: V1 ~150ms, V2 ~60ms (**èŠ‚çœ60%**)

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨ - V1

```python
import torch
from torch._inductor.kernel.custom_op import (
    register_custom_op_autotuning,
    CustomOpConfig,
)

# å®šä¹‰custom collective op
@torch.library.custom_op("mylib::my_allreduce", mutates_args=())
def my_allreduce(x: torch.Tensor, group_name: str = "default"):
    return torch.ops._c10d_functional.all_reduce_(x, "sum", group_name=group_name)

# å¤šä¸ªå®ç°
def allreduce_impl1(x, group_name="default"):
    return torch.ops._c10d_functional.all_reduce_(x, "sum", group_name=group_name)

def allreduce_impl2(x, group_name="default", chunk_size=1024):
    # Custom chunked implementation
    ...

# æ³¨å†Œautotuning
register_custom_op_autotuning(
    my_allreduce,
    configs=[
        CustomOpConfig(allreduce_impl1),
        CustomOpConfig(allreduce_impl2, chunk_size=1024),
    ],
    input_gen_fns={
        "x": lambda fake: torch.randn_like(fake, device='cuda'),
    },
)

# ä½¿ç”¨
model = torch.compile(my_model)
output = model(input)  # ç¬¬ä¸€æ¬¡ä¼šautotune, é€‰æ‹©æœ€ä¼˜å®ç°
```

### é«˜çº§ä½¿ç”¨ - V2 (with Fusion)

```python
# V2ä¼šè‡ªåŠ¨å¤„ç†fusion
class MyModel(torch.nn.Module):
    def forward(self, x):
        y = x @ self.weight
        y = my_allreduce(y)  # Collective op
        y = y + self.bias     # Potential epilogue fusion!
        return y

# Compileæ—¶:
# 1. Lowering: my_allreduce -> CollectiveMultiTemplateBuffer
# 2. Scheduler: è¯†åˆ«fusionæœºä¼š
# 3. Benchmark: æµ‹è¯• with/without bias fusion
# 4. é€‰æ‹©æœ€ä¼˜: å¯èƒ½fuseæˆä¸€ä¸ªkernel

model = torch.compile(MyModel())
```

---

## âš ï¸ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### Q1: æŸä¸ªrankä¸€ç›´æ— æ³•sync,å¯¼è‡´timeout,æ€ä¹ˆåŠ?

**A**: 
```python
# æ–¹æ¡ˆ1: Fallback to default implementation
if not sync_succeeded:
    log.warning("Using fallback implementation")
    return default_choice

# æ–¹æ¡ˆ2: è·³è¿‡è¿™ä¸ªautotuning,ä½¿ç”¨cachedç»“æœ
if timeout:
    return cached_result_or_default

# æ–¹æ¡ˆ3: è®°å½•é—®é¢˜rank,åç»­åˆ†æ
log_problematic_rank(rank_id)
```

### Q2: ä¸åŒranksçš„ç¡¬ä»¶ä¸åŒ,benchmarkç»“æœä¸ä¸€è‡´?

**A**:
```python
# ä½¿ç”¨all_reduce(MAX)è·å–æœ€æ…¢çš„timing
comm_time_tensor = torch.tensor([comm_time], device=device)
dist.all_reduce(comm_time_tensor, op=dist.ReduceOp.MAX)
# æ‰€æœ‰ranksä½¿ç”¨ç›¸åŒçš„(æœ€æ…¢çš„)timingåšå†³ç­–
```

### Q3: å¦‚ä½•debug collective autotuning?

**A**:
```python
# 1. è®¾ç½®logging level
export TORCH_LOGS="+inductor"

# 2. æ¯ä¸ªrankå•ç‹¬è¾“å‡º
log_file = f"rank_{rank}_autotune.log"

# 3. ä½¿ç”¨smaller world_size (2 ranks)
torchrun --nproc_per_node=2 test.py

# 4. æ·»åŠ é¢å¤–loggingåœ¨å…³é”®ç‚¹
log.info(f"[Rank {rank}] Before barrier...")
dist.barrier()
log.info(f"[Rank {rank}] After barrier!")
```

### Q4: V1å’ŒV2å¦‚ä½•é€‰æ‹©?

**å†³ç­–æ ‘**:
```
æ˜¯å¦éœ€è¦fusionæ”¯æŒ?
  â”œâ”€ å¦ â†’ V1 (ç®€å•å¿«é€Ÿ)
  â”‚
  â””â”€ æ˜¯ â†’ V2 (æ”¯æŒfusion)
      â”‚
      â””â”€ æœ‰å¤šä¸ªcollective ops?
          â”œâ”€ æ˜¯ â†’ V2 (ç»Ÿä¸€syncæ›´é«˜æ•ˆ)
          â””â”€ å¦ â†’ V1ä¹Ÿå¯ä»¥ (overheadç›¸è¿‘)
```

---

## ğŸ“Š æ€§èƒ½é¢„æœŸ

### Compilation Time Overhead

| åœºæ™¯ | V1 | V2 | è¯´æ˜ |
|-----|----|----|-----|
| å•ä¸ªallreduce | 50-100ms | 55-105ms | V2å¤š5ms pre-sync |
| 3ä¸ªallreduce | 150-300ms | 60-120ms | V2èŠ‚çœ60% |
| allreduce+fusion | N/A | 60-120ms | V2ç‹¬æœ‰ |

### Runtime Performance

| ä¼˜åŒ– | æ€§èƒ½æå‡ | é€‚ç”¨åœºæ™¯ |
|-----|---------|---------|
| é€‰æ‹©æœ€ä¼˜collective impl | 5-20% | æ‰€æœ‰åœºæ™¯ |
| Epilogue fusion (V2) | 5-15% | æœ‰epilogueæ—¶ |
| é¿å…å¤šæ¬¡sync (V2) | ç¼–è¯‘æ—¶60% | å¤šcollective ops |

---

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

```python
# test/inductor/test_collective_autotuning.py

class TestCollectiveAutotuning(MultiProcessTestCase):
    @skip_if_lt_x_gpu(2)
    def test_basic_allreduce(self):
        """Test basic allreduce autotuning"""
        # æµ‹è¯•åŸºç¡€åŠŸèƒ½
        ...
    
    @skip_if_lt_x_gpu(2)
    def test_timeout_protection(self):
        """Test timeout mechanism"""
        # æ¨¡æ‹ŸæŸä¸ªrank hang
        ...
    
    @skip_if_lt_x_gpu(2)
    def test_multiple_collectives(self):
        """Test multiple collective ops"""
        # æµ‹è¯•å¤šä¸ªcollective ops
        ...
```

### é›†æˆæµ‹è¯•

```python
def test_vllm_scenario():
    """End-to-end test for vLLM use case"""
    # å®Œæ•´çš„vLLM tensor parallelåœºæ™¯
    ...

def test_with_fusion():
    """Test collective op with fusion (V2)"""
    # æµ‹è¯•V2çš„fusionåŠŸèƒ½
    ...
```

---

## ğŸ“ ä¸‹ä¸€æ­¥è¡ŒåŠ¨ (Action Items)

### ç«‹å³å¼€å§‹ (This Week)
1. [ ] Reviewè®¾è®¡æ–‡æ¡£,ç¡®è®¤æ–¹æ¡ˆ
2. [ ] å¼€å§‹V1 Phase 1å®ç°
3. [ ] å‡†å¤‡æµ‹è¯•ç¯å¢ƒ (multi-GPU setup)

### çŸ­æœŸç›®æ ‡ (Next 2 Weeks)
1. [ ] å®ŒæˆV1åŸºç¡€å®ç°å’Œæµ‹è¯•
2. [ ] åœ¨vLLMåœºæ™¯éªŒè¯
3. [ ] æ”¶é›†æ€§èƒ½æ•°æ®

### ä¸­æœŸç›®æ ‡ (Next Month)
1. [ ] V1ç¨³å®šå¹¶ä¸Šçº¿
2. [ ] å¼€å§‹V2è®¾è®¡ç»†åŒ–
3. [ ] å‡†å¤‡V2å®æ–½

### é•¿æœŸç›®æ ‡ (Next Quarter)
1. [ ] V2å®ç°å®Œæˆ
2. [ ] è¿ç§»ç”¨æˆ·åˆ°V2
3. [ ] æ€§èƒ½ä¼˜åŒ–å’Œç”Ÿäº§åŒ–

---

## ğŸ“š å‚è€ƒèµ„æ–™

### ä»£ç å‚è€ƒ
1. **Autoparallel**: https://github.com/meta-pytorch/autoparallel/blob/main/autoparallel/autobucketing_util/estimation_utils.py
   - `benchmark_comm_func()` - barrier + timingå‚è€ƒ

2. **MultiTemplateBuffer**: `/data/users/tianren/pytorch/torch/_inductor/ir.py`
   - Line 5269-5350 - MultiTemplateBufferå®šä¹‰

3. **Scheduler**: `/data/users/tianren/pytorch/torch/_inductor/scheduler.py`
   - `finalize_multi_template_buffers()` - V2éœ€è¦ä¿®æ”¹çš„åœ°æ–¹

### è®¾è®¡æ–‡æ¡£
1. `COLLECTIVE_OP_AUTOTUNING_DESIGN.md` - V1å®Œæ•´è®¾è®¡
2. `COLLECTIVE_OP_ROADMAP.md` - å®æ–½æŒ‡å—
3. `COLLECTIVE_OP_V2_DESIGN.md` - V2å‡çº§æ–¹æ¡ˆ

---

## ğŸ“ æ€»ç»“

### æ ¸å¿ƒä»·å€¼
1. **è§£å†³vLLMç—›ç‚¹**: æ”¯æŒdistributed collective ops autotuning
2. **æ€§èƒ½ä¼˜åŒ–**: è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜collectiveå®ç°,æå‡5-20%
3. **Fusionæ”¯æŒ** (V2): è¿›ä¸€æ­¥5-15%æ€§èƒ½æå‡
4. **é€šç”¨æ–¹æ¡ˆ**: ä¸ä»…é™äºvLLM,é€‚ç”¨æ‰€æœ‰åˆ†å¸ƒå¼åœºæ™¯

### æŠ€æœ¯äº®ç‚¹
1. âœ… **Timeoutä¿æŠ¤**: ä¸ä¼šå› ä¸ºæŸä¸ªrankæ— å“åº”è€Œhang
2. âœ… **è·¨rankåŒæ­¥**: Barrier + all_reduceç¡®ä¿å‡†ç¡®benchmark
3. âœ… **æ¸è¿›å¼è®¾è®¡**: V1â†’V2å¹³æ»‘å‡çº§è·¯å¾„
4. âœ… **æœ€å°ä¾µå…¥**: å¤ç”¨ç°æœ‰autotuningåŸºç¡€è®¾æ–½

### å®æ–½å»ºè®®
1. **å…ˆV1åV2**: å¿«é€ŸéªŒè¯åŠŸèƒ½,å†è¿½æ±‚å®Œç¾
2. **å……åˆ†æµ‹è¯•**: Multi-GPUç¯å¢ƒå…¨é¢æµ‹è¯•
3. **æ€§èƒ½ç›‘æ§**: æ”¶é›†çœŸå®åœºæ™¯æ•°æ®
4. **æ–‡æ¡£å…ˆè¡Œ**: æ¸…æ™°çš„ä½¿ç”¨æ–‡æ¡£å¸®åŠ©adoption

---

**ç°åœ¨å¯ä»¥å¼€å§‹å®æ–½äº†!å»ºè®®ä»V1 Phase 1å¼€å§‹,1-2å¤©å®ŒæˆåŸºç¡€åŠŸèƒ½ã€‚** ğŸš€
