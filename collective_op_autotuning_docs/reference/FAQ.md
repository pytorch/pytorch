# Collective Op Autotuning - æ–¹æ¡ˆæ¾„æ¸…å’Œå¯¹æ¯”

## ğŸ” é—®é¢˜æ¾„æ¸…

### ä½ çš„æ ¸å¿ƒé—®é¢˜

1. **V1çš„"å•ç‹¬sync"æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ**
2. **V2çš„"ç»Ÿä¸€sync"ä¸ºä»€ä¹ˆæ›´é«˜æ•ˆï¼Ÿ**
3. **V2éœ€è¦custom opæ”¯æŒMultiTemplateBufferå—ï¼Ÿ**
4. **ç›®å‰SubgraphTemplateåªæ”¯æŒä¸å¸¦MultiTemplateBufferçš„è·¯å¾„ï¼Ÿ**
5. **å¦‚æœèµ°MultiTemplateBufferï¼Œfusionå°±èƒ½åšåˆ°äº†å—ï¼Ÿ**
6. **Subgraphçš„inline fusionå’ŒMultiTemplateBuffer fusionçš„åŒºåˆ«ï¼Ÿ**
7. **å„ä¸ªæ–¹æ¡ˆçš„å–èˆæ˜¯ä»€ä¹ˆï¼Ÿ**

è®©æˆ‘é€ä¸€å›ç­”ã€‚

---

## 1ï¸âƒ£ V1 "å•ç‹¬sync"çš„å«ä¹‰

### åœºæ™¯
å‡è®¾ä½ æœ‰3ä¸ªcustom collective opsåœ¨åŒä¸€ä¸ªæ¨¡å‹é‡Œ:
```python
def model(x):
    y1 = my_allreduce_1(x)      # Custom collective op 1
    y2 = my_allreduce_2(y1)     # Custom collective op 2
    y3 = my_allgather(y2)       # Custom collective op 3
    return y3
```

### V1çš„è¡Œä¸º (å½“å‰æ–¹æ¡ˆ)

**ç¼–è¯‘æ—¶** (ç¬¬ä¸€æ¬¡è¿è¡Œ):
```python
# Loweringé˜¶æ®µ - é‡åˆ°ç¬¬ä¸€ä¸ªcollective op
my_allreduce_1 loweringè§¦å‘:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2, impl3]
â”œâ”€ è°ƒç”¨ autotune_select_algorithm()
â”œâ”€ ã€åŒæ­¥ç‚¹1ã€‘æ‰€æœ‰ranks barrieråŒæ­¥
â”œâ”€ Benchmark choice 1
â”‚   â””â”€ barrier â†’ cuda.sync â†’ time â†’ barrier
â”œâ”€ Benchmark choice 2
â”‚   â””â”€ barrier â†’ cuda.sync â†’ time â†’ barrier
â”œâ”€ Benchmark choice 3
â”‚   â””â”€ barrier â†’ cuda.sync â†’ time â†’ barrier
â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ inline_subgraph_to_ir_nodes() â†’ è¿”å›IR nodes

# Loweringé˜¶æ®µ - é‡åˆ°ç¬¬äºŒä¸ªcollective op
my_allreduce_2 loweringè§¦å‘:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2, impl3]
â”œâ”€ è°ƒç”¨ autotune_select_algorithm()
â”œâ”€ ã€åŒæ­¥ç‚¹2ã€‘æ‰€æœ‰ranks barrieråŒæ­¥
â”œâ”€ Benchmark choice 1
â”‚   â””â”€ barrier â†’ cuda.sync â†’ time â†’ barrier
â”œâ”€ ...
â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ inline_subgraph_to_ir_nodes()

# Loweringé˜¶æ®µ - é‡åˆ°ç¬¬ä¸‰ä¸ªcollective op
my_allgather loweringè§¦å‘:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2]
â”œâ”€ è°ƒç”¨ autotune_select_algorithm()
â”œâ”€ ã€åŒæ­¥ç‚¹3ã€‘æ‰€æœ‰ranks barrieråŒæ­¥
â”œâ”€ Benchmark choice 1
â”‚   â””â”€ barrier â†’ cuda.sync â†’ time â†’ barrier
â”œâ”€ ...
â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ inline_subgraph_to_ir_nodes()
```

**å…³é”®ç‚¹**:
- âŒ æ¯ä¸ªcollective op **å•ç‹¬**è¿›å…¥autotuning
- âŒ æ¯ä¸ªopéƒ½è¦é‡æ–°syncæ‰€æœ‰ranks
- âŒ å¦‚æœæœ‰Nä¸ªcollective opsï¼Œéœ€è¦Næ¬¡"å¤§åŒæ­¥"

**æ—¶é—´å¼€é”€**:
```
3ä¸ªcollective ops Ã— 5ms syncå¼€é”€ = 15ms
3ä¸ªcollective ops Ã— 3 choices Ã— 10ms benchmark = 90ms
æ€»è®¡: ~105ms
```

---

## 2ï¸âƒ£ V2 "ç»Ÿä¸€sync"çš„å«ä¹‰

### V2çš„è¡Œä¸º (MultiTemplateBufferæ–¹æ¡ˆ)

**ç¼–è¯‘æ—¶** (ç¬¬ä¸€æ¬¡è¿è¡Œ):
```python
# Phase 1: Loweringé˜¶æ®µ - åˆ›å»ºMultiTemplateBuffer (ä¸benchmark)
my_allreduce_1 lowering:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2, impl3]
â”œâ”€ è°ƒç”¨ autotune_select_algorithm(return_multi_template=True)
â””â”€ è¿”å› CollectiveMultiTemplateBuffer (å»¶è¿Ÿé€‰æ‹©)
    â””â”€ åŒ…å«3ä¸ªchoicesï¼Œè¿˜æ²¡benchmark

my_allreduce_2 lowering:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2, impl3]
â””â”€ è¿”å› CollectiveMultiTemplateBuffer (å»¶è¿Ÿé€‰æ‹©)
    â””â”€ åŒ…å«3ä¸ªchoicesï¼Œè¿˜æ²¡benchmark

my_allgather lowering:
â”œâ”€ ç”Ÿæˆchoices: [impl1, impl2]
â””â”€ è¿”å› CollectiveMultiTemplateBuffer (å»¶è¿Ÿé€‰æ‹©)
    â””â”€ åŒ…å«2ä¸ªchoicesï¼Œè¿˜æ²¡benchmark

# Phase 2: Scheduleré˜¶æ®µ - ç»Ÿä¸€å¤„ç†æ‰€æœ‰MultiTemplateBuffers
scheduler.finalize_multi_template_buffers():
â”œâ”€ ã€ç¬¬1æ­¥ã€‘collect_collective_nodes()
â”‚   â””â”€ å‘ç°3ä¸ªCollectiveMultiTemplateBuffer nodes
â”‚
â”œâ”€ ã€ç¬¬2æ­¥ã€‘try_sync_collective_nodes() 
â”‚   â””â”€ ã€å”¯ä¸€çš„å¤§åŒæ­¥ã€‘5ms timeoutæ£€æµ‹æ‰€æœ‰ranksæ˜¯å¦ready
â”‚       â””â”€ æˆåŠŸ: æ‰€æœ‰rankså‡†å¤‡å¥½äº†
â”‚
â””â”€ ã€ç¬¬3æ­¥ã€‘éå†æ¯ä¸ªMultiTemplateBufferå¹¶finalize
    â”‚
    â”œâ”€ For my_allreduce_1:
    â”‚   â”œâ”€ Benchmark choice 1 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â”œâ”€ Benchmark choice 2 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â”œâ”€ Benchmark choice 3 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ finalize
    â”‚
    â”œâ”€ For my_allreduce_2:
    â”‚   â”œâ”€ Benchmark choice 1 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â”œâ”€ Benchmark choice 2 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â”œâ”€ Benchmark choice 3 (å†…éƒ¨æœ‰å°barrier)
    â”‚   â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ finalize
    â”‚
    â””â”€ For my_allgather:
        â”œâ”€ Benchmark choice 1 (å†…éƒ¨æœ‰å°barrier)
        â”œâ”€ Benchmark choice 2 (å†…éƒ¨æœ‰å°barrier)
        â””â”€ é€‰æ‹©æœ€ä¼˜ â†’ finalize
```

**å…³é”®ç‚¹**:
- âœ… æ‰€æœ‰collective opsåœ¨loweringæ—¶åªåˆ›å»ºMultiTemplateBufferï¼Œä¸benchmark
- âœ… åœ¨scheduleré˜¶æ®µ**ç»Ÿä¸€æ”¶é›†**æ‰€æœ‰collective nodes
- âœ… **åªéœ€ä¸€æ¬¡å¤§åŒæ­¥** (5ms pre-sync) æ£€æµ‹rankså°±ç»ª
- âœ… ä¹‹åæ‰€æœ‰benchmarkåœ¨å·²ç»åŒæ­¥çš„ranksä¸Šè¿›è¡Œ

**æ—¶é—´å¼€é”€**:
```
1æ¬¡å¤§åŒæ­¥: 5ms
3ä¸ªcollective ops Ã— 3 choices Ã— 10ms benchmark = 90ms
æ€»è®¡: ~95ms (æ¯”V1èŠ‚çœ10ms)
```

**ä¸ºä»€ä¹ˆæ›´é«˜æ•ˆï¼Ÿ**
å› ä¸ºé‚£ä¸ª5msçš„"pre-sync"åªæ˜¯å¿«é€Ÿæ£€æµ‹"æ‰€æœ‰ranksæ˜¯å¦å‡†å¤‡å¥½å¼€å§‹benchmark"ï¼ŒæˆåŠŸåå°±ä¸éœ€è¦æ¯ä¸ªopéƒ½é‡æ–°åè°ƒæ‰€æœ‰ranksäº†ã€‚

---

## 3ï¸âƒ£ MultiTemplateBuffer vs ç›´æ¥benchmark

### ç°çŠ¶åˆ†æ

**å½“å‰Custom Opçš„è·¯å¾„** (autotune_custom_op, Line 325-350):
```python
# Line 325: è°ƒç”¨autotune_select_algorithm
selected_result, winning_choice = autotune_select_algorithm(
    name=name,
    choices=choices,
    input_nodes=list(inputs),
    layout=choices[0].layout,
    input_gen_fns=input_gen_fns,
    return_choice=True,  # â† å…³é”®: æ²¡æœ‰return_multi_template=True
)

# Line 335-343: è·èƒœåç«‹å³inline
if winning_choice.gm is not None:
    return inline_subgraph_to_ir_nodes(winning_choice.gm, inputs, name)
```

**å…³é”®è§‚å¯Ÿ**:
- âŒ `return_multi_template` **æ²¡æœ‰è®¾ç½®ä¸ºTrue**
- âŒ æ‰€ä»¥ä¸ä¼šåˆ›å»ºMultiTemplateBuffer
- âœ… ç›´æ¥benchmarké€‰å‡ºwinnerï¼Œç„¶åinlineè¿”å›IR nodes

**è¿™æ„å‘³ç€**:
```python
# V1è·¯å¾„ (å½“å‰)
custom_op â†’ autotune_select_algorithm(return_multi_template=False)
         â†’ ç«‹å³benchmark
         â†’ é€‰å‡ºwinner
         â†’ inline_subgraph_to_ir_nodes() 
         â†’ è¿”å›IR nodes (fusable)
         â†’ åç»­å¯ä»¥epilogue fusion

# V2è·¯å¾„ (å¦‚æœæ”¹æˆreturn_multi_template=True)
custom_op â†’ autotune_select_algorithm(return_multi_template=True)
         â†’ åˆ›å»ºCollectiveMultiTemplateBuffer (å»¶è¿Ÿé€‰æ‹©)
         â†’ è¿”å›MultiTemplateBuffer
         â†’ åˆ°scheduleré˜¶æ®µæ‰finalize choice
         â†’ å¯ä»¥benchmark with/without epilogue fusion
```

---

## 4ï¸âƒ£ å…³é”®ä»£ç ä½ç½®å’Œä¿®æ”¹

### ç›®å‰çš„å®ç°è·¯å¾„

```python
# torch/_inductor/select_algorithm.py, Line ~2945
def autotune_select_algorithm(..., return_multi_template=False):
    cache = get_algorithm_selector_cache()
    
    if return_multi_template:
        # åˆ›å»ºMultiTemplateBuffer (å»¶è¿Ÿbenchmark)
        return MultiTemplateBuffer(...)
    else:
        # ç«‹å³benchmarkå¹¶è¿”å›winning choice
        return benchmark_and_select_winner(...)
```

**å½“å‰custom opè°ƒç”¨**:
```python
# custom_op.py, Line 325
autotune_select_algorithm(
    ...,
    return_choice=True,
    # âŒ æ²¡æœ‰ return_multi_template=True
)
```

### V2éœ€è¦çš„ä¿®æ”¹

**ä¿®æ”¹1: custom_op.py, Line 325**
```python
# æ·»åŠ å‚æ•°
selected_result, winning_choice = autotune_select_algorithm(
    name=name,
    choices=choices,
    input_nodes=list(inputs),
    layout=choices[0].layout,
    input_gen_fns=input_gen_fns,
    return_choice=True,
    return_multi_template=True,  # â† NEW: è¯·æ±‚MultiTemplateBuffer
    is_collective=is_collective,  # â† NEW: æ ‡è®°ä¸ºcollective
    process_group=process_group,  # â† NEW: ä¼ é€’process group
)
```

**ä¿®æ”¹2: select_algorithm.py**
```python
# Line ~2945 - AlgorithmSelectorCache.__call__
if return_multi_template:
    if is_collective and dist.is_initialized():
        # åˆ›å»ºCollectiveMultiTemplateBuffer
        return CollectiveMultiTemplateBuffer(...)
    else:
        # åˆ›å»ºæ™®é€šMultiTemplateBuffer
        return MultiTemplateBuffer(...)
```

**ä¿®æ”¹3: scheduler.py**
```python
# åœ¨finalize_multi_template_buffers()ä¸­æ·»åŠ collectiveå¤„ç†
def finalize_multi_template_buffers(self):
    # Step 1: æ”¶é›†collective nodes
    collective_nodes = self.collect_collective_nodes()
    
    # Step 2: ç»Ÿä¸€pre-sync (5ms timeout)
    if collective_nodes:
        sync_ok = self.try_sync_collective_nodes()
    
    # Step 3: Finalizeæ¯ä¸ªMultiTemplateBuffer
    for node in self.nodes:
        if isinstance(node, CollectiveMultiTemplateBuffer):
            # ä½¿ç”¨distributed benchmarking
            ...
```

---

## 5ï¸âƒ£ Fusionçš„ä¸¤ç§å½¢å¼

### Inline Fusion (å½“å‰V1ä½¿ç”¨)

**å‘ç”Ÿæ—¶æœº**: Loweringé˜¶æ®µï¼Œbenchmarkå®Œåç«‹å³å‘ç”Ÿ

**ä»£ç ä½ç½®**: custom_op.py, Line 335-343
```python
# é€‰å‡ºwinning choiceå
if winning_choice.gm is not None:
    # ç«‹å³inlineè¿™ä¸ªsubgraphåˆ°IR nodes
    return inline_subgraph_to_ir_nodes(winning_choice.gm, inputs, name)
```

**æ•ˆæœ**:
```python
# å‡è®¾winning choiceæ˜¯ä¸€ä¸ªsubgraph: all_reduce + relu
winning_choice.gm = {
    input â†’ all_reduce â†’ relu â†’ output
}

# Inlineåå˜æˆIR nodes:
return TensorBox(
    ComputedBuffer(all_reduce_ir),
    ComputedBuffer(relu_ir),
)
```

**è¿™æ ·çš„IR nodeså¯ä»¥è¢«scheduler fusion**:
```python
# å¦‚æœåç»­æœ‰epilogue
x = my_allreduce(x)  # inlineæˆIR nodes
y = x + 1            # åç»­epilogue

# Schedulerå¯ä»¥fuse: all_reduce + relu + add
```

**é™åˆ¶**:
- âœ… åªèƒ½fuse **winning choiceå†…éƒ¨**å·²ç»åŒ…å«çš„ops
- âŒ ä¸èƒ½benchmark "all_reduce vs all_reduce+epilogue"
- âŒ åªæ˜¯è®©winning choiceçš„opså˜æˆå¯fuseçš„IR nodes

### MultiTemplateBuffer Fusion (V2å¯ä»¥åšçš„)

**å‘ç”Ÿæ—¶æœº**: Scheduleré˜¶æ®µï¼Œfinalizeæ—¶

**ä»£ç ä½ç½®**: scheduler.py, finalize_multi_template_buffers()
```python
# Scheduleré˜¶æ®µè¯†åˆ«fusionæœºä¼š
if can_fuse(collective_node, epilogue_node):
    # Benchmark WITH epilogue
    time_fused = benchmark(collective_choice_fused_with_epilogue)
    
    # Benchmark WITHOUT epilogue
    time_unfused = benchmark(collective_choice_alone)
    
    if time_fused < time_unfused:
        # Fuse!
        finalize_as_fused(collective_node, epilogue_node)
```

**æ•ˆæœ**:
```python
# å¯ä»¥benchmarkå¤šç§é…ç½®
Config 1: all_reduce alone           â†’ 10ms
Config 2: all_reduce + add           â†’ 9ms  â† Better!
Config 3: all_reduce (with add later) â†’ 11ms

# é€‰æ‹©Config 2 (fused)
```

**ä¼˜åŠ¿**:
- âœ… å¯ä»¥benchmark **æœ‰æ— epilogue**çš„æ€§èƒ½å·®å¼‚
- âœ… è‡ªåŠ¨é€‰æ‹©æ˜¯å¦fusionæ›´å¿«
- âœ… æ”¯æŒæ›´å¤æ‚çš„fusion pattern

---

## 6ï¸âƒ£ æ–¹æ¡ˆå¯¹æ¯”è¡¨

| ç»´åº¦ | V1 (ç°æœ‰Inline) | V2 (MultiTemplateBuffer) |
|-----|----------------|-------------------------|
| **å®ç°ä½ç½®** | custom_op.py | custom_op.py + scheduler.py |
| **Benchmarkæ—¶æœº** | Loweringé˜¶æ®µ(ç«‹å³) | Scheduleré˜¶æ®µ(å»¶è¿Ÿ) |
| **Syncç­–ç•¥** | æ¯ä¸ªopå•ç‹¬sync | ç»Ÿä¸€pre-syncä¸€æ¬¡ |
| **Fusionç±»å‹** | Inline fusion only | Epilogue fusion benchmark |
| **Fusionèƒ½åŠ›** | Winning choiceå†…éƒ¨ + åç»­ops | å¯benchmark with/without epilogue |
| **ä»£ç ä¿®æ”¹** | å° (custom_op.py) | ä¸­ (custom_op.py + select_algorithm.py + scheduler.py) |
| **å®ç°å¤æ‚åº¦** | â­â­ ç®€å• | â­â­â­â­ ä¸­ç­‰ |
| **å¼€å‘æ—¶é—´** | 1-2å¤© | 3-4å¤© |
| **Nä¸ªcollective opså¼€é”€** | N Ã— 5ms sync | 1 Ã— 5ms sync |
| **é€‚ç”¨åœºæ™¯** | ç®€å•custom op | å¤æ‚åœºæ™¯ï¼Œå¤šcollective ops |

---

## 7ï¸âƒ£ å…·ä½“æ–¹æ¡ˆé€‰æ‹©å»ºè®®

### æ–¹æ¡ˆA: V1 - å¿«é€ŸéªŒè¯ (æ¨èå…ˆåš)

**ä»€ä¹ˆä¸æ”¹**:
- âŒ ä¸éœ€è¦`return_multi_template=True`
- âŒ ä¸éœ€è¦ä¿®æ”¹scheduler.py
- âŒ ä¿æŒç°æœ‰çš„inline fusionæœºåˆ¶

**åªéœ€è¦æ”¹**:
- âœ… åœ¨`autotune_select_algorithm`è°ƒç”¨å‰æ£€æµ‹æ˜¯å¦collective
- âœ… å¦‚æœæ˜¯collectiveï¼Œä½¿ç”¨`CollectiveBenchmarker`
- âœ… æ·»åŠ timeoutä¿æŠ¤

**ä¿®æ”¹ç‚¹**:
```python
# custom_op.py, Line 324
# æ£€æµ‹æ˜¯å¦collective
is_collective = False
process_group = None
if op_overload:
    from torch._inductor.runtime.collective_benchmarking import is_collective_op
    op_name = str(op_overload)
    is_collective = is_collective_op(op_name)
    if is_collective:
        # ä»non_tensor_argsæå–process_group
        for kwargs in non_tensor_args:
            if 'group' in kwargs:
                process_group = kwargs['group']
                break

# Line 325: ä¼ é€’collectiveä¿¡æ¯
selected_result, winning_choice = autotune_select_algorithm(
    name=name,
    choices=choices,
    input_nodes=list(inputs),
    layout=choices[0].layout,
    input_gen_fns=input_gen_fns,
    return_choice=True,
    is_collective=is_collective,  # NEW
    process_group=process_group,  # NEW
)
```

```python
# select_algorithm.py - AlgorithmSelectorCache.__call__
# åœ¨benchmarké˜¶æ®µæ£€æµ‹is_collective
if is_collective and dist.is_initialized():
    from torch._inductor.runtime.collective_benchmarking import (
        CollectiveBenchmarker
    )
    benchmarker = CollectiveBenchmarker(
        process_group=process_group,
        nruns=config.benchmark_kernel_nruns,
    )
    # ä½¿ç”¨specialized benchmarking
    # ... benchmark with sync ...
```

**ä¼˜åŠ¿**:
- âœ… æœ€å°æ”¹åŠ¨
- âœ… å¿«é€ŸéªŒè¯collective autotuningå¯è¡Œæ€§
- âœ… ä¿ç•™inline fusionèƒ½åŠ›
- âœ… 1-2å¤©å®Œæˆ

**åŠ£åŠ¿**:
- âŒ å¤šä¸ªcollective opsæ—¶sync overheadè¾ƒå¤§
- âŒ ä¸èƒ½benchmark with/without epilogue

---

### æ–¹æ¡ˆB: V2 - å®Œæ•´æ–¹æ¡ˆ (åç»­å‡çº§)

**éœ€è¦æ”¹**:
- âœ… custom_op.pyæ·»åŠ `return_multi_template=True`
- âœ… select_algorithm.pyæ”¯æŒåˆ›å»º`CollectiveMultiTemplateBuffer`
- âœ… scheduler.pyæ·»åŠ unified syncå’Œfinalizeé€»è¾‘
- âœ… ir.pyæ·»åŠ `CollectiveMultiTemplateBuffer`ç±»

**ä¿®æ”¹ç‚¹**:
```python
# custom_op.py, Line 325
selected_result, winning_choice = autotune_select_algorithm(
    ...,
    return_multi_template=True,  # NEW
    is_collective=is_collective,
    process_group=process_group,
)

# æ³¨æ„: å¦‚æœreturn_multi_template=Trueï¼Œä¸èƒ½ç«‹å³inline
# å› ä¸ºè¿”å›çš„æ˜¯MultiTemplateBufferï¼Œè¦ç­‰scheduler finalize
if return_multi_template:
    # ç›´æ¥è¿”å›MultiTemplateBuffer
    return selected_result
else:
    # åŸæœ‰çš„inlineé€»è¾‘
    if winning_choice.gm is not None:
        return inline_subgraph_to_ir_nodes(...)
```

**ä¼˜åŠ¿**:
- âœ… ç»Ÿä¸€syncï¼Œå¤šcollective opsæ›´é«˜æ•ˆ
- âœ… æ”¯æŒepilogue fusion benchmark
- âœ… æ›´é€šç”¨ï¼Œé€‚ç”¨æ‰€æœ‰MultiTemplateBufferåœºæ™¯

**åŠ£åŠ¿**:
- âŒ å®ç°å¤æ‚åº¦é«˜
- âŒ éœ€è¦ä¿®æ”¹scheduleræ ¸å¿ƒé€»è¾‘
- âŒ 3-4å¤©å¼€å‘æ—¶é—´

---

## 8ï¸âƒ£ æ¨èå®æ–½è·¯å¾„

### Phase 1: V1 åŸºç¡€ (Week 1-2)
**ç›®æ ‡**: è®©collective op autotuningåŸºç¡€åŠŸèƒ½work

**ä»»åŠ¡**:
1. âœ… å·²å®Œæˆ: `collective_benchmarking.py`
2. ğŸ”² ä¿®æ”¹`custom_op.py`æ·»åŠ detection
3. ğŸ”² ä¿®æ”¹`select_algorithm.py`ä½¿ç”¨CollectiveBenchmarker
4. ğŸ”² æµ‹è¯•vLLMåœºæ™¯

**äº¤ä»˜**:
- èƒ½autotune custom collective ops
- æœ‰timeoutä¿æŠ¤
- ä¿ç•™inline fusionèƒ½åŠ›

---

### Phase 2: è¯„ä¼°å’Œå†³ç­– (Week 3)
**ç›®æ ‡**: å†³å®šæ˜¯å¦éœ€è¦V2

**è¯„ä¼°æ ‡å‡†**:
1. **æ€§èƒ½éœ€æ±‚**: æ˜¯å¦çœŸçš„æœ‰å¤šä¸ªcollective opså¯¼è‡´sync overheadæ˜æ˜¾ï¼Ÿ
2. **Fusionéœ€æ±‚**: æ˜¯å¦éœ€è¦benchmark with/without epilogueï¼Ÿ
3. **å¼€å‘èµ„æº**: æ˜¯å¦æœ‰æ—¶é—´å®ç°V2ï¼Ÿ

**å†³ç­–**:
- å¦‚æœåªæœ‰1-2ä¸ªcollective ops â†’ V1è¶³å¤Ÿ
- å¦‚æœæœ‰3+ä¸ªcollective ops â†’ V2æœ‰æ˜æ˜¾æ”¶ç›Š
- å¦‚æœéœ€è¦fusionä¼˜åŒ– â†’ V2å¿…è¦

---

### Phase 3: V2 å®æ–½ (Week 4-5, å¯é€‰)
**å‰æ**: Phase 1ç¨³å®šï¼Œä¸”è¯„ä¼°æ˜¾ç¤ºV2æœ‰å¿…è¦

**ä»»åŠ¡**:
1. ğŸ”² åˆ›å»º`CollectiveMultiTemplateBuffer`ç±»
2. ğŸ”² ä¿®æ”¹scheduleræ·»åŠ unified sync
3. ğŸ”² å®ç°epilogue fusion benchmark
4. ğŸ”² å®Œæ•´æµ‹è¯•å’Œä¼˜åŒ–

---

## 9ï¸âƒ£ FAQ

### Q: V1çš„inline fusionå’ŒV2çš„epilogue fusionæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A**: 
```python
# V1 Inline Fusion (å‘ç”Ÿåœ¨lowering)
my_allreduce(x)  # winning choiceå·²ç»æ˜¯fused subgraph
â†“ inline
all_reduce_ir + internal_ops  # å˜æˆIR nodes
â†“ schedulerå¯ä»¥ç»§ç»­fuse
all_reduce_ir + internal_ops + epilogue_ops

# V2 Epilogue Fusion (å‘ç”Ÿåœ¨scheduler)
MultiTemplateBuffer(all_reduce)  # è¿˜æ²¡é€‰æ‹©å®ç°
â†“ schedulerè¯†åˆ«æœ‰epilogue
benchmark(all_reduce alone)          # 10ms
benchmark(all_reduce + epilogue)     # 9ms  â† ç›´æ¥æµ‹è¯•fusedç‰ˆæœ¬
â†“ é€‰æ‹©fusedç‰ˆæœ¬
finalize_as_fused()
```

V2å¯ä»¥**ç›´æ¥benchmark fusedç‰ˆæœ¬çš„æ€§èƒ½**ï¼Œè€ŒV1åªæ˜¯è®©opså˜æˆfusableç„¶åä¾èµ–schedulerå»fuseã€‚

### Q: å¦‚æœcustom opæœ¬èº«æ²¡æœ‰epilogueéœ€æ±‚ï¼ŒV2è¿˜æœ‰æ„ä¹‰å—ï¼Ÿ

**A**: å¦‚æœï¼š
- åªæœ‰1ä¸ªcollective op â†’ V2æ”¶ç›Šä¸å¤§
- æœ‰3+ä¸ªcollective ops â†’ V2çš„ç»Ÿä¸€syncä»ç„¶æœ‰ä»·å€¼ï¼ˆèŠ‚çœsync overheadï¼‰

### Q: Subgraphæ˜¯å¦æ”¯æŒMultiTemplateBufferï¼Ÿ

**A**: 
- **å½“å‰**: SubgraphTemplateç”Ÿæˆçš„choiceså¯ä»¥æ”¾å…¥MultiTemplateBufferï¼Œä½†custom_op.pyé»˜è®¤ä¸ä½¿ç”¨ï¼ˆ`return_multi_template=False`ï¼‰
- **V2éœ€è¦**: è®¾ç½®`return_multi_template=True`ï¼Œè®©SubgraphChoiceCallerç”Ÿæˆçš„choiceså»¶è¿Ÿåˆ°scheduleré€‰æ‹©

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒåŒºåˆ«

**V1 (Inline Fusion)**:
```
Lowering: collective_op â†’ benchmark â†’ inline â†’ IR nodes â†’ [scheduler fusion]
         â””â”€ æ¯ä¸ªopå•ç‹¬sync
```

**V2 (MultiTemplateBuffer)**:
```
Lowering: collective_op â†’ MultiTemplateBuffer (å»¶è¿Ÿ)
Scheduler: [unified sync] â†’ benchmark all â†’ finalize â†’ [å¯æµ‹è¯•epilogue fusion]
          â””â”€ ç»Ÿä¸€syncä¸€æ¬¡
```

### å»ºè®®

1. **ç«‹å³**: å®æ–½V1ï¼ŒéªŒè¯åŠŸèƒ½
2. **1å‘¨å**: è¯„ä¼°æ˜¯å¦éœ€è¦V2
3. **æŒ‰éœ€**: å¦‚æœæœ‰æ˜æ˜¾æ”¶ç›Šï¼Œå†å®æ–½V2

V1è¶³å¤Ÿå¤„ç†å¤§å¤šæ•°åœºæ™¯ï¼ŒV2æ˜¯æ€§èƒ½å’Œé€šç”¨æ€§çš„è¿›ä¸€æ­¥æå‡ã€‚

---

**å¸Œæœ›è¿™ä¸ªæ–‡æ¡£å½»åº•è§£ç­”äº†ä½ çš„ç–‘é—®ï¼** ğŸ‰
