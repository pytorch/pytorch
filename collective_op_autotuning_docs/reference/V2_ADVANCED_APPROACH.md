# Collective Op Autotuning - V2å‡çº§æ–¹æ¡ˆ (åŸºäºScheduler MultiTemplateBuffer)

## èƒŒæ™¯

åŸºäºä¸TLçš„è®¨è®ºå’Œå¯¹MultiTemplateBufferçš„ç†è§£,è¿™æ˜¯ä¸€ä¸ªæ›´å®Œæ•´ã€æ›´generalçš„collective op autotuningæ–¹æ¡ˆã€‚

### V1 vs V2 å¯¹æ¯”

| ç‰¹æ€§ | V1 (Custom Op Layer) | V2 (Scheduler Layer) |
|-----|---------------------|---------------------|
| **é›†æˆç‚¹** | custom_op.py (lowering) | scheduler.py (scheduler phase) |
| **è§¦å‘æ—¶æœº** | ç«‹å³åœ¨loweringæ—¶ | åœ¨scheduler finalize MultiTemplateBufferæ—¶ |
| **Fusionæ”¯æŒ** | âŒ ä¸æ”¯æŒ | âœ… æ”¯æŒepilogue fusion |
| **é€šç”¨æ€§** | ä»…custom ops | æ‰€æœ‰äº§ç”ŸMultiTemplateBufferçš„ops |
| **åŒæ­¥ç‚¹** | æ¯ä¸ªopå•ç‹¬sync | ç»Ÿä¸€åœ¨scheduleré˜¶æ®µsync (~5ms window) |
| **å®ç°å¤æ‚åº¦** | ç®€å• | ä¸­ç­‰ |

---

## V2 è®¾è®¡æ–¹æ¡ˆ

### æ ¸å¿ƒç†å¿µ

TLå»ºè®®çš„æµç¨‹:
```
Lowering â†’ Scheduler â†’ MultiTemplateBuffer gets realized â†’ 
çŸ­æš‚sync (~5ms) æ”¶é›†éœ€è¦collective benchmarkçš„nodes â†’ 
å¦‚æœåŒæ­¥å¤±è´¥ fallback â†’ 
æ‰€æœ‰ranksåŒæ—¶benchmarkå¤šä¸ªchoices
```

### æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Phase 1: Lowering                              â”‚
â”‚    Custom opé™ä½ä¸ºMultiTemplateBuffer                   â”‚
â”‚    (åŒ…å«multiple choices: Triton/ExternKernel)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Phase 2: Scheduler                             â”‚
â”‚    è¯†åˆ«æ‰€æœ‰åŒ…å«collective opsçš„MultiTemplateBuffers     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Phase 3: Pre-Benchmark Sync (NEW)              â”‚
â”‚    â± ~5ms timeout window                                â”‚
â”‚    å°è¯•åŒæ­¥æ‰€æœ‰ranks,æ”¶é›†éœ€è¦benchmarkçš„nodes            â”‚
â”‚    å¦‚æœå¤±è´¥ â†’ fallback to default                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
                â”‚ Success â”‚
                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Phase 4: finalize_multi_template_buffers() (MODIFIED)â”‚
â”‚    å¯¹äºcollective ops:                                   â”‚
â”‚    - è·¨rankåŒæ—¶benchmarkæ‰€æœ‰choices                      â”‚
â”‚    - æ”¯æŒfusion: benchmark with/without epilogue         â”‚
â”‚    - é€‰æ‹©æœ€ä¼˜choiceå¹¶finalize                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## å…³é”®ç»„ä»¶

### 1. æ–°å¢: CollectiveMultiTemplateBuffer (ir.py)

```python
# torch/_inductor/ir.py

class CollectiveMultiTemplateBuffer(MultiTemplateBuffer):
    """
    A MultiTemplateBuffer specifically for collective operations.
    
    Extends MultiTemplateBuffer to handle distributed synchronization
    and benchmarking requirements for collective ops like all_reduce, all_gather.
    """
    
    def __init__(
        self,
        layout: Layout,
        inputs: Sequence[IRNode],
        choice_timings_fn: Callable,
        unfiltered_choices: list[ChoiceCaller],
        allowed_prologue_inps: OrderedSet[str],
        process_group: Optional[dist.ProcessGroup] = None,  # NEW
        collective_op_type: str = "unknown",  # NEW: e.g., "all_reduce"
    ):
        super().__init__(
            layout, inputs, choice_timings_fn, 
            unfiltered_choices, allowed_prologue_inps
        )
        self.process_group = process_group
        self.collective_op_type = collective_op_type
        self._sync_succeeded = False  # Track if pre-sync succeeded
    
    def is_collective(self) -> bool:
        """Mark this as a collective operation buffer."""
        return True
    
    def benchmark_choices_distributed(
        self,
        hint_override: Optional[int] = None,
        timeout_seconds: float = 30.0,
    ) -> dict[ChoiceCaller, float]:
        """
        Benchmark choices with distributed synchronization.
        
        Uses CollectiveBenchmarker to ensure all ranks benchmark simultaneously.
        """
        from torch._inductor.runtime.collective_benchmarking import (
            CollectiveBenchmarker,
            try_collective_benchmark_with_timeout,
        )
        
        benchmarker = CollectiveBenchmarker(
            process_group=self.process_group,
            nruns=config.benchmark_kernel_nruns,
        )
        
        timings = {}
        for choice in self.unfiltered_choices:
            # Try benchmarking with timeout
            time_us = try_collective_benchmark_with_timeout(
                comm_func=choice.kernel,
                comm_func_name=choice.name,
                input_tensors=...,  # prepared inputs
                output_tensor=...,  # prepared output
                process_group=self.process_group,
                timeout_seconds=timeout_seconds,
            )
            
            if time_us is not None:
                timings[choice] = time_us
            else:
                # Timeout or failure, use inf to deprioritize
                timings[choice] = float('inf')
        
        return timings
```

### 2. ä¿®æ”¹: scheduler.py - æ·»åŠ Pre-Benchmark Sync

```python
# torch/_inductor/scheduler.py

class Scheduler:
    def __init__(self, nodes: list[Any]):
        # ... existing init code ...
        self.collective_nodes: list[MultiTemplateBuffer] = []
        self.collective_sync_window = 5.0  # 5ms timeout for initial sync
    
    def collect_collective_nodes(self) -> None:
        """
        Identify all MultiTemplateBuffer nodes that contain collective ops.
        
        This should be called before finalize_multi_template_buffers().
        """
        for node in self.nodes:
            if isinstance(node, SchedulerNode) and isinstance(
                node.node, (MultiTemplateBuffer, CollectiveMultiTemplateBuffer)
            ):
                # Check if this is a collective op
                multi_node = node.node
                if isinstance(multi_node, CollectiveMultiTemplateBuffer):
                    self.collective_nodes.append(multi_node)
                elif self._is_collective_multitemplate(multi_node):
                    # Convert regular MultiTemplateBuffer to Collective version
                    # if it contains collective ops
                    self.collective_nodes.append(multi_node)
    
    def _is_collective_multitemplate(self, node: MultiTemplateBuffer) -> bool:
        """Check if a MultiTemplateBuffer contains collective operations."""
        from torch._inductor.runtime.collective_benchmarking import is_collective_op
        
        # Check choices for collective ops
        for choice in node.unfiltered_choices:
            if hasattr(choice, 'kernel'):
                kernel_name = str(choice.kernel)
                if is_collective_op(kernel_name):
                    return True
        return False
    
    def try_sync_collective_nodes(self) -> bool:
        """
        Attempt to synchronize all ranks before collective benchmarking.
        
        This is the ~5ms sync window mentioned by TL. If sync fails,
        we fallback to regular autotuning without collective sync.
        
        Returns:
            True if sync succeeded, False if timeout/failure
        """
        if not self.collective_nodes:
            return True  # No collective nodes, no need to sync
        
        import torch.distributed as dist
        from torch._inductor.runtime.collective_benchmarking import sync_with_timeout
        
        if not dist.is_initialized():
            log.warning(
                "Distributed not initialized but found collective nodes. "
                "Falling back to regular autotuning."
            )
            return False
        
        rank = dist.get_rank()
        log.info(
            f"[Rank {rank}] Found {len(self.collective_nodes)} collective nodes. "
            f"Attempting sync with {self.collective_sync_window}s timeout..."
        )
        
        # Try to sync all ranks
        sync_ok = sync_with_timeout(
            process_group=None,  # Use default world group
            timeout_seconds=self.collective_sync_window,
        )
        
        if sync_ok:
            log.info(f"[Rank {rank}] Collective sync succeeded!")
            for node in self.collective_nodes:
                if isinstance(node, CollectiveMultiTemplateBuffer):
                    node._sync_succeeded = True
        else:
            log.warning(
                f"[Rank {rank}] Collective sync timeout. "
                f"Falling back to regular autotuning."
            )
        
        return sync_ok
    
    def finalize_multi_template_buffers(self) -> None:
        """
        Finalize backing choices for MultiTemplateBuffers.
        
        MODIFIED to handle collective operations specially.
        """
        # NEW: Step 1 - Collect collective nodes
        self.collect_collective_nodes()
        
        # NEW: Step 2 - Try to sync before benchmarking
        collective_sync_ok = self.try_sync_collective_nodes()
        
        # Existing code continues...
        for i, node in enumerate(self.nodes):
            if isinstance(node, SchedulerNode) and isinstance(
                node.node, MultiTemplateBuffer
            ):
                multi_node = node.node
                
                # NEW: Check if this is a collective node
                is_collective = isinstance(
                    multi_node, CollectiveMultiTemplateBuffer
                ) or multi_node in self.collective_nodes
                
                if is_collective and collective_sync_ok:
                    # Use distributed benchmarking
                    min_node_unfused, min_time = (
                        self._finalize_collective_choice(multi_node)
                    )
                else:
                    # Regular autotuning (existing code)
                    if not config.test_configs.force_extern_kernel_in_multi_template:
                        min_node_unfused, _ = multi_node.get_min_choice()
                    else:
                        # ... existing extern kernel logic ...
                        pass
                
                # ... rest of existing finalization code ...
    
    def _finalize_collective_choice(
        self, multi_node: MultiTemplateBuffer
    ) -> tuple[ChoiceCaller, float]:
        """
        Finalize choice for a collective operation MultiTemplateBuffer.
        
        Uses distributed benchmarking with synchronization.
        """
        if isinstance(multi_node, CollectiveMultiTemplateBuffer):
            # Use the specialized benchmarking method
            timings = multi_node.benchmark_choices_distributed(
                timeout_seconds=30.0  # Full benchmark timeout
            )
        else:
            # Fallback to regular timing
            timings = multi_node.choice_timings()
        
        if not timings:
            raise RuntimeError(
                f"No valid choices for collective MultiTemplateBuffer"
            )
        
        min_choice = min(timings, key=timings.get)
        min_time = timings[min_choice]
        
        return min_choice, min_time
```

### 3. ä¿®æ”¹: select_algorithm.py - åˆ›å»ºCollectiveMultiTemplateBuffer

```python
# torch/_inductor/select_algorithm.py

def autotune_select_algorithm(
    name,
    choices,
    input_nodes,
    layout,
    *,
    input_gen_fns=None,
    return_choice=False,
    is_collective=False,  # NEW
    process_group=None,   # NEW
    **kwargs,
):
    cache = get_algorithm_selector_cache()
    
    # ... existing parameter processing ...
    
    return cache(
        name,
        choices,
        input_nodes,
        layout,
        input_gen_fns=input_gen_fns,
        return_choice=return_choice,
        is_collective=is_collective,      # NEW
        process_group=process_group,      # NEW
        **kwargs,
    )


class AlgorithmSelectorCache:
    def __call__(
        self,
        name,
        choices,
        input_nodes,
        layout,
        *,
        input_gen_fns=None,
        return_choice=False,
        return_multi_template=False,
        is_collective=False,      # NEW
        process_group=None,       # NEW
        **kwargs,
    ):
        # ... existing preprocessing ...
        
        if return_multi_template:
            # NEW: Check if this should be a CollectiveMultiTemplateBuffer
            if is_collective and dist.is_initialized():
                from torch._inductor.ir import CollectiveMultiTemplateBuffer
                
                # Determine collective op type
                collective_op_type = self._infer_collective_type(choices)
                
                return torch._inductor.ir.TensorBox.create(
                    CollectiveMultiTemplateBuffer(
                        layout,
                        input_nodes,
                        get_timings,
                        choices,
                        allowed_prologue_inps,
                        process_group=process_group,          # NEW
                        collective_op_type=collective_op_type, # NEW
                    )
                )
            else:
                # Regular MultiTemplateBuffer
                return torch._inductor.ir.TensorBox.create(
                    torch._inductor.ir.MultiTemplateBuffer(
                        layout,
                        input_nodes,
                        get_timings,
                        choices,
                        allowed_prologue_inps,
                    )
                )
        
        # ... existing non-multi-template code ...
    
    def _infer_collective_type(self, choices: list[ChoiceCaller]) -> str:
        """Infer the type of collective operation from choices."""
        from torch._inductor.runtime.collective_benchmarking import is_collective_op
        
        for choice in choices:
            if hasattr(choice, 'kernel'):
                kernel_name = str(choice.kernel)
                if 'all_reduce' in kernel_name:
                    return 'all_reduce'
                elif 'all_gather' in kernel_name:
                    return 'all_gather'
                elif 'reduce_scatter' in kernel_name:
                    return 'reduce_scatter'
                elif 'all_to_all' in kernel_name:
                    return 'all_to_all'
        
        return 'unknown'
```

---

## å…³é”®ä¼˜åŠ¿

### 1. **æ”¯æŒFusion** âœ…
V2å¯ä»¥åœ¨scheduleré˜¶æ®µbenchmark with/without epilogue fusion:
```python
# åœ¨scheduler.pyçš„fusioné€»è¾‘ä¸­
if can_fuse(node1, node2):
    # å¯¹äºcollective MultiTemplateBuffer, ä¹Ÿå¯ä»¥benchmark fused version
    fused_time = benchmark_fused(node1, node2, is_collective=True)
    unfused_time = benchmark_unfused(node1, is_collective=True)
    
    if fused_time < unfused_time:
        fuse_nodes(node1, node2)
```

### 2. **ç»Ÿä¸€åŒæ­¥ç‚¹** âœ…
åªéœ€è¦ä¸€æ¬¡~5msçš„pre-sync,è€Œä¸æ˜¯æ¯ä¸ªopéƒ½sync:
```python
# V1: æ¯ä¸ªopéƒ½sync (Næ¬¡sync)
for collective_op in collective_ops:
    sync_and_benchmark(op)  # barrieræ¯æ¬¡

# V2: ç»Ÿä¸€syncä¸€æ¬¡ (1æ¬¡sync)
if try_sync_collective_nodes():  # ä¸€æ¬¡barrier
    for collective_op in collective_ops:
        benchmark(op)  # å†…éƒ¨barrier
```

### 3. **æ›´å¥½çš„Fallbackæœºåˆ¶** âœ…
å¦‚æœsyncå¤±è´¥,è‡ªåŠ¨fallbackåˆ°regular autotuning:
```python
if not sync_succeeded:
    # Fallback: ä½¿ç”¨ç¬¬ä¸€ä¸ªchoiceæˆ–è€…extern kernel
    use_fallback_choice()
    log.warning("Using fallback due to sync timeout")
```

### 4. **é€šç”¨æ€§** âœ…
ä¸ä»…é™äºcustom ops,ä»»ä½•äº§ç”ŸMultiTemplateBufferçš„opéƒ½èƒ½ä½¿ç”¨:
- Custom ops
- Matmul with collective
- Any fused collective + other ops

---

## å®ç°æ—¶é—´çº¿

### Phase 1: V1 - ç®€å•ç‰ˆæœ¬ (1-2 days)
- âœ… å·²å®Œæˆ: collective_benchmarking.py
- ğŸ”² å®ç°custom_op.pyçš„detectionå’Œrouting
- ğŸ”² å®ç°select_algorithm.pyçš„åŸºç¡€é›†æˆ
- **ç›®æ ‡**: èƒ½å¤Ÿä¸ºç®€å•çš„custom collective opsåšautotuning

### Phase 2: V1.5 - æ·»åŠ Timeout (0.5 day)
- âœ… å·²å®Œæˆ: sync_with_timeout()
- âœ… å·²å®Œæˆ: try_collective_benchmark_with_timeout()
- **ç›®æ ‡**: ä¸ä¼šå› ä¸ºæŸä¸ªrankå¡ä½è€Œhang

### Phase 3: V2 - MultiTemplateBufferé›†æˆ (3-4 days)
- ğŸ”² åˆ›å»ºCollectiveMultiTemplateBufferç±»
- ğŸ”² ä¿®æ”¹scheduler.pyæ·»åŠ pre-sync
- ğŸ”² ä¿®æ”¹select_algorithm.pyåˆ›å»ºCollectiveç‰ˆæœ¬
- ğŸ”² å®ç°distributed benchmarking in finalize_multi_template_buffers
- **ç›®æ ‡**: æ”¯æŒfusionå’Œæ›´é€šç”¨çš„åœºæ™¯

### Phase 4: V2.5 - ä¼˜åŒ–å’Œæµ‹è¯• (2-3 days)
- ğŸ”² ä¼˜åŒ–sync windowæ—¶é—´
- ğŸ”² æ·»åŠ è¯¦ç»†loggingå’Œmetrics
- ğŸ”² ç¼–å†™comprehensive tests
- ğŸ”² æ€§èƒ½ä¼˜åŒ–å’Œcache keyæ”¹è¿›

---

## ä½¿ç”¨ç¤ºä¾‹

### V2ä½¿ç”¨ - vLLMåœºæ™¯

```python
# vLLMçš„tensor parallel allreduce
import torch
import torch.distributed as dist
from torch._inductor.kernel.custom_op import (
    register_custom_op_autotuning,
    CustomOpConfig,
)

@torch.library.custom_op("vllm::allreduce_tp", mutates_args=())
def allreduce_tp(
    tensor: torch.Tensor,
    tp_group: str = "default",
) -> torch.Tensor:
    return torch.ops._c10d_functional.all_reduce_(
        tensor, "sum", group_name=tp_group
    )

# å®ç°1: Standard NCCL
def allreduce_nccl(tensor, tp_group="default"):
    return torch.ops._c10d_functional.all_reduce_(
        tensor, "sum", group_name=tp_group
    )

# å®ç°2: Ring allreduce (for large tensors)
def allreduce_ring(tensor, tp_group="default", chunk_size=1024**2):
    # Custom ring allreduce implementation
    ...

# å®ç°3: Tree allreduce (for small tensors)
def allreduce_tree(tensor, tp_group="default"):
    # Custom tree allreduce implementation
    ...

# æ³¨å†Œautotuning - ä¼šè‡ªåŠ¨åˆ›å»ºCollectiveMultiTemplateBuffer
register_custom_op_autotuning(
    allreduce_tp,
    configs=[
        CustomOpConfig(allreduce_nccl),
        CustomOpConfig(allreduce_ring, chunk_size=1024**2),
        CustomOpConfig(allreduce_tree),
    ],
    input_gen_fns={
        "tensor": lambda fake: torch.randn_like(fake, device='cuda'),
    },
)

# åœ¨æ¨¡å‹ä¸­ä½¿ç”¨
class TPLinear(torch.nn.Module):
    def forward(self, x):
        # Local matmul
        y = x @ self.weight
        
        # Collective op - ä¼šè¢«autotuned, å¹¶ä¸”å¯èƒ½å’Œå…¶ä»–ops fusion
        y = allreduce_tp(y, tp_group=self.tp_group)
        
        # Epilogue
        y = y + self.bias
        return y

# Compileæ¨¡å‹
model = torch.compile(TPLinear())

# ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶:
# 1. Loweringé˜¶æ®µ: allreduce_tp -> CollectiveMultiTemplateBuffer
# 2. Scheduleré˜¶æ®µ: è¯†åˆ«collective node
# 3. Pre-sync (~5ms): å°è¯•åŒæ­¥æ‰€æœ‰ranks
# 4. Finalize: benchmark 3ä¸ªchoices (nccl, ring, tree)
#    - å¯èƒ½è¿˜ä¼šbenchmark with bias fusion
# 5. é€‰æ‹©æœ€ä¼˜çš„å®ç°

output = model(input)
```

---

## ä¸V1çš„å…³ç³»

### å…±å­˜ç­–ç•¥

V1å’ŒV2å¯ä»¥å…±å­˜,æŒ‰éœ€ä½¿ç”¨:

```python
# V1: ç”¨äºç®€å•åœºæ™¯,custom ops without fusion
# - åœ¨custom_op.pyå±‚é¢ç›´æ¥å¤„ç†
# - é€‚åˆ: å•ä¸ªcollective op, ä¸éœ€è¦fusion

# V2: ç”¨äºå¤æ‚åœºæ™¯,éœ€è¦fusion
# - åœ¨scheduler.pyå±‚é¢å¤„ç†
# - é€‚åˆ: collective op + epilogue fusion, å¤šä¸ªcollective ops

# é€‰æ‹©ç­–ç•¥:
if config.enable_collective_multitemplate:
    # Use V2 - create CollectiveMultiTemplateBuffer
    return_multi_template = True
else:
    # Use V1 - direct benchmarking
    return_multi_template = False
```

### è¿ç§»è·¯å¾„

1. **ç°åœ¨**: å®ç°V1,éªŒè¯åŸºç¡€åŠŸèƒ½
2. **åç»­**: é€æ­¥è¿ç§»åˆ°V2,è·å¾—fusionæ”¯æŒ
3. **æœ€ç»ˆ**: V2æˆä¸ºä¸»è¦æ–¹æ¡ˆ,V1ä½œä¸ºfallback

---

## é…ç½®é€‰é¡¹

```python
# torch/_inductor/config.py

# V1ç›¸å…³
collective_autotune_timeout = 30.0  # Benchmark timeout (seconds)
collective_benchmark_nruns = 3      # Number of runs for benchmarking

# V2ç›¸å…³
enable_collective_multitemplate = True  # ä½¿ç”¨V2æ–¹æ¡ˆ
collective_pre_sync_timeout = 5.0       # Pre-sync window (seconds)
collective_fusion_enabled = True         # å…è®¸collective ops fusion
```

---

## æ€§èƒ½é¢„æœŸ

### V1 vs V2 - Overheadå¯¹æ¯”

| åœºæ™¯ | V1 Overhead | V2 Overhead | è¯´æ˜ |
|-----|------------|------------|-----|
| **å•ä¸ªallreduce** | ~50-100ms | ~55-105ms | V2å¤šä¸€æ¬¡5ms pre-sync |
| **3ä¸ªallreduce** | ~150-300ms | ~55-105ms | V2åªéœ€ä¸€æ¬¡pre-sync |
| **allreduce + fusion** | N/A (ä¸æ”¯æŒ) | ~60-120ms | V2æ”¯æŒfusion benchmark |

### V2çš„æ”¶ç›Š

å¯¹äºvLLMè¿™ç§æœ‰å¤šä¸ªcollective opsçš„åœºæ™¯:
- **3ä¸ªallreduce ops**: V1éœ€è¦3æ¬¡sync (~150ms), V2åªéœ€1æ¬¡ (~55ms)
- **èŠ‚çœ**: ~95ms per compilation
- **Fusioné¢å¤–æ”¶ç›Š**: å¦‚æœèƒ½fusion,è¿è¡Œæ—¶æ€§èƒ½æå‡5-15%

---

## æ€»ç»“

### V1 (Current)
- âœ… ç®€å•,å¿«é€Ÿå®ç°
- âœ… è¶³å¤Ÿå¤„ç†åŸºç¡€åœºæ™¯
- âŒ ä¸æ”¯æŒfusion
- âŒ å¤šä¸ªcollective opsæ—¶overheadè¾ƒå¤§

### V2 (Upgrade)
- âœ… æ”¯æŒfusion (å…³é”®!)
- âœ… æ›´é«˜æ•ˆçš„åŒæ­¥æœºåˆ¶
- âœ… æ›´é€šç”¨,é€‚ç”¨æ‰€æœ‰MultiTemplateBufferåœºæ™¯
- âœ… æ›´å¥½çš„fallbackæœºåˆ¶
- âŒ å®ç°å¤æ‚åº¦è¾ƒé«˜
- âŒ éœ€è¦ä¿®æ”¹scheduleræ ¸å¿ƒé€»è¾‘

### å»ºè®®

**åˆ†é˜¶æ®µå®æ–½**:
1. **ç«‹å³**: å®ŒæˆV1 + timeout (P0) - è®©åŸºç¡€åŠŸèƒ½work
2. **è¿‘æœŸ**: ä¼˜åŒ–å’Œæµ‹è¯•V1 (P1) - ç¨³å®šåä¸Šçº¿
3. **ä¸­æœŸ**: å®ç°V2 (P2) - è·å¾—fusionæ”¯æŒ
4. **é•¿æœŸ**: V2æˆä¸ºé»˜è®¤,V1ä¿ç•™ä½œä¸ºç®€å•åœºæ™¯çš„fast path

è¿™æ ·æ—¢èƒ½å¿«é€ŸdeliveråŠŸèƒ½,åˆä¸ºæœªæ¥çš„ä¼˜åŒ–ç•™ä¸‹ç©ºé—´ã€‚
