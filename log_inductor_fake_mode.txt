
ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬
Testing Inductor V.fake_mode with Dynamic Symbolic Shapes
ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬ğŸ”¬

This test verifies the CRITICAL assumption:
  â“ Does V.fake_mode provide symbolic shapes (SymInt)?
  â“ Will torch.cond be preserved in our implementation?


================================================================================
Test 1: Verify Inductor V.fake_mode Provides Symbolic Shapes
================================================================================

1. Creating dispatch function...

2. Compiling with torch.compile(dynamic=True)...

3. Running with test input to trigger compilation...

4. Checking correctness...
   Result shape: torch.Size([2, 256, 128])
   Match expected: True

âœ“ Test 1 PASSED: Inductor compilation works with torch.cond
================================================================================

================================================================================
Test 2: Direct V.fake_mode Access and ir_node_to_tensor()
================================================================================

This test simulates the EXACT pattern we'll use in custom_op.py
We'll create a custom lowering that inspects V.fake_mode


   Registered custom lowering for inspection

   Compiling function that uses custom op...

   Running to trigger lowering inspection...


   ğŸ“ Inside custom lowering function!
   âœ“ V.fake_mode exists: <class 'torch._subclasses.fake_tensor.FakeTensorMode'>
   âœ“ V.fake_mode.shape_env exists: <class 'torch.fx.experimental.symbolic_shapes.ShapeEnv'>

   âœ“ Found 2 tensor inputs
   âœ“ Created fake inputs using ir_node_to_tensor()

   Inspecting first fake tensor:
   - Type: <class 'torch._subclasses.fake_tensor.FakeTensor'>
   - Shape: torch.Size([2, 256, 128])
   - Device: cuda:0
   - Is FakeTensor: True

   ğŸ” CRITICAL CHECK - Dimension analysis:
   - x_fake.shape[1] = 256
   - Type: <class 'int'>
   - Type name: int
   âš ï¸  DIMENSION IS CONCRETE (int)
   - This means torch.cond will be optimized away

   Attempting to trace torch.cond dispatch...
   âœ“ Successfully traced torch.cond dispatch
   âš ï¸  torch.cond was optimized away

   Graph structure (first 20 lines):
      graph():
          %arg0_1 : [num_users=1] = placeholder[target=arg0_1]
          %arg1_1 : [num_users=1] = placeholder[target=arg1_1]
          %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {})
          %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {})
          %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {})
          return sin

================================================================================
Inspection Results Summary:
================================================================================
âœ… v_fake_mode_exists: True
âœ… fake_inputs_created: True
âŒ dim_is_symbolic: False
âœ… dim_type: <class 'int'>
âœ… dim_value: 256
âœ… torch_cond_traced: True
âŒ graph_has_cond: False

================================================================================
âš ï¸  Test 2: Some checks failed

âŒ CRITICAL: Dimensions are NOT symbolic in V.fake_mode
   This means we need to manually mark dimensions as dynamic

âŒ CRITICAL: torch.cond was optimized away
   This means the dispatch won't work as expected
================================================================================

================================================================================
FINAL RESULTS
================================================================================
Test 1 (Inductor compile): âœ… PASSED
Test 2 (V.fake_mode inspection): âŒ FAILED

âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ 
SOME TESTS FAILED
âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ âš ï¸ 

âŒ Need to investigate further or adjust approach

ğŸ“‹ Recommendations:
   1. Check if we need to manually mark dimensions as dynamic
   2. Investigate alternative approaches
   3. Consider keeping SubgraphBuffer as fallback
