# Collective Autotuning Improvements - Complete Summary

## ðŸ“‹ Overview

This document summarizes all improvements made to the collective autotuning system, including timeout detection, exception handling, and test infrastructure.

---

## ðŸŽ¯ Problem Statement

### Initial Challenge
We needed to verify that the timeout detection mechanism in `benchmark_collective_choice` works correctly when multiple ranks are doing collective autotuning. The goal is: **if ANY rank times out, ALL ranks should fallback to regular benchmarking together** to avoid deadlock.

### Additional Issues Discovered
1. **Test Infrastructure Issue**: `test_all_gather_4ranks` was hanging because `world_size` was always 2, but the test needed 4 ranks
2. **Exception Handling Issue**: When an exception occurred in one rank, only that rank would fallback, causing other ranks to wait indefinitely in collective operations

---

## âœ… Solutions Implemented

## 1. Test Infrastructure Improvements

### Problem
The original test class used a single `world_size = 2`, which didn't work for tests requiring different numbers of ranks (e.g., 4 ranks, 6 ranks, 8 ranks).

### Solution: Separate Test Classes by World Size

**File**: `/data/users/tianren/pytorch/test/inductor/test_collective_autotuning.py`

We split the test class into separate classes for different world sizes:

```python
class TestCollectiveAutotuning2Ranks(MultiProcessTestCase):
    """Test collective autotuning with 2 ranks"""
    
    @property
    def world_size(self):
        return 2

    def setUp(self):
        super().setUp()
        self._spawn_processes()

    @skip_if_lt_x_gpu(2)
    def test_equivalent_allreduce_strategies(self):
        # ... test code ...


class TestCollectiveAutotuning4Ranks(MultiProcessTestCase):
    """Test collective autotuning with 4 ranks"""
    
    @property
    def world_size(self):
        return 4

    def setUp(self):
        super().setUp()
        self._spawn_processes()

    @skip_if_lt_x_gpu(4)
    def test_all_gather_4ranks(self):
        # ... test code ...
```

**Benefits**:
- âœ… Simple and clear - class name directly indicates world_size
- âœ… Easy to extend - just add new classes for 6 ranks, 8 ranks, etc.
- âœ… No complex decorators or dynamic logic needed
- âœ… Each class is independent and self-contained

**Future Extension**:
```python
class TestCollectiveAutotuning6Ranks(MultiProcessTestCase):
    @property
    def world_size(self):
        return 6
    # ... tests requiring 6 ranks
```

---

## 2. Exception Handling Improvements

### Problem
When an exception occurred during collective benchmarking:
- Only the rank with the exception would catch it and fallback
- Other ranks would continue waiting in collective operations
- This caused **deadlock**

### Original Problematic Code
```python
try:
    # ... benchmark code ...
    return timing
except Exception:
    log.warning("Collective benchmark exception. Falling back to regular benchmarking.")
    return cls.benchmark_choice(choice, autotune_args)  # âŒ Only this rank fallbacks!
```

### Solution: Synchronized Exception Handling

**File**: `/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py`

**Key Changes**:

1. **Track exception status locally**:
```python
local_exception = False
timing = None

try:
    # ... benchmarking code ...
    # If successful, timing will be set
except Exception as e:
    local_exception = True
    log.warning("[Rank %d] Collective benchmark exception: %s", rank, str(e))
```

2. **ALL ranks participate in exception status synchronization**:
```python
def sync_exception_status(has_exception: bool) -> bool:
    """
    Synchronize exception status across all ranks to avoid deadlock.
    Returns True if any rank had an exception (all ranks should fallback).
    This MUST be called by all ranks, whether they had an exception or not.
    """
    try:
        exception_tensor = torch.tensor(
            [1.0 if has_exception else 0.0],
            dtype=torch.float32,
            device=f"cuda:{rank}",
        )
        dist.all_reduce(
            exception_tensor, op=dist.ReduceOp.MAX, group=process_group
        )
        
        any_rank_exception = exception_tensor.item() > 0.5
        return any_rank_exception
    except Exception:
        log.warning("[Rank %d] Failed to sync exception status", rank)
        return True

# ALL ranks call this, regardless of whether they had an exception
any_rank_exception = sync_exception_status(local_exception)

if any_rank_exception:
    log.warning(
        "Exception detected on one or more ranks. "
        "All ranks falling back to regular benchmarking."
    )
    return cls.benchmark_choice(choice, autotune_args)
```

**Why This Works**:
- âœ… ALL ranks participate in the `all_reduce`, even those without exceptions
- âœ… If ANY rank has `local_exception=True`, the `all_reduce` will propagate it to all ranks
- âœ… All ranks make the same fallback decision
- âœ… No deadlock because all ranks are synchronized

---

## 3. Complete `benchmark_collective_choice` Implementation

**File**: `/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py`

### Method Signature
```python
@classmethod
def benchmark_collective_choice(
    cls,
    choice: ChoiceCaller,
    autotune_args: AutotuneArgs,
) -> float:
```

### Key Features

#### A. Timeout Detection with Synchronization
```python
def sync_timeout_decision(local_timeout: bool) -> bool:
    """
    Synchronize timeout decision across all ranks.
    Returns True if any rank timed out.
    """
    try:
        timeout_tensor = torch.tensor(
            [1.0 if local_timeout else 0.0],
            dtype=torch.float32,
            device=f"cuda:{rank}",
        )
        dist.all_reduce(
            timeout_tensor, op=dist.ReduceOp.MAX, group=process_group
        )
        any_rank_timed_out = timeout_tensor.item() > 0.5
        return any_rank_timed_out
    except Exception:
        log.warning("[Rank %d] Failed to sync timeout decision", rank)
        return True

# Usage:
work = dist.barrier(group=process_group, async_op=True)
local_timeout = not work.wait(timeout)

if sync_timeout_decision(local_timeout):
    log.warning("Warmup barrier timeout detected. All ranks falling back.")
    return cls.benchmark_choice(choice, autotune_args)
```

#### B. Exception Handling with Synchronization
```python
local_exception = False
timing = None

try:
    # Warmup barrier
    # Benchmarking loop
    # All-reduce timing
    timing = time_tensor.item()
except Exception as e:
    local_exception = True
    log.warning("[Rank %d] Exception: %s", rank, str(e))

# ALL ranks sync exception status
any_rank_exception = sync_exception_status(local_exception)

if any_rank_exception:
    log.warning("Exception on one or more ranks. All ranks falling back.")
    return cls.benchmark_choice(choice, autotune_args)

if timing is not None:
    return timing
else:
    return cls.benchmark_choice(choice, autotune_args)
```

#### C. Benchmarking Loop
```python
total_time = 0.0
for i in range(nruns):
    # Barrier before each run with timeout
    work = dist.barrier(group=process_group, async_op=True)
    local_timeout = not work.wait(timeout)
    
    if sync_timeout_decision(local_timeout):
        log.warning("Barrier timeout at run %d/%d", i+1, nruns)
        return cls.benchmark_choice(choice, autotune_args)
    
    torch.cuda.synchronize()
    
    start_evt = torch.cuda.Event(enable_timing=True)
    end_evt = torch.cuda.Event(enable_timing=True)
    
    start_evt.record()
    choice.benchmark(*inputs, out=output)
    end_evt.record()
    end_evt.synchronize()
    
    total_time += start_evt.elapsed_time(end_evt)

avg_time = (total_time / nruns) * 1000.0
```

---

## ðŸ”§ Configuration

### Environment Variables

```bash
# Timeout for collective operations (seconds)
TORCHINDUCTOR_COLLECTIVE_BENCHMARK_TIMEOUT=30

# Number of benchmark runs for averaging
TORCHINDUCTOR_COLLECTIVE_BENCHMARK_NRUNS=10
```

---

## ðŸ§ª Testing Strategy

### How to Verify Timeout Detection Works

**Method**: Temporarily invert the timeout logic to force timeout detection

```python
# Normal code:
local_timeout = not work.wait(timeout)

# For testing (forces timeout):
local_timeout = work.wait(timeout)  # Remove 'not'
```

**Expected behavior when inverted**:
1. All ranks will detect timeout
2. `sync_timeout_decision()` will return True
3. All ranks will log: "Warmup barrier timeout detected"
4. All ranks will fallback to `benchmark_choice`

**How to test**:
```bash
# Run with logging enabled
TORCH_LOGS="inductor:WARNING" python test/inductor/test_collective_autotuning.py \
    TestCollectiveAutotuning2Ranks.test_equivalent_allreduce_strategies -v

# Check for timeout messages in output
grep -i "timeout detected" output.log
```

---

## ðŸ“Š Test Results

### Before Improvements
- âŒ `test_all_gather_4ranks` would hang (only 2 processes spawned, needed 4)
- âŒ Exception in one rank would cause deadlock
- âŒ No clear way to test timeout mechanism

### After Improvements
- âœ… `TestCollectiveAutotuning2Ranks` spawns 2 processes correctly
- âœ… `TestCollectiveAutotuning4Ranks` spawns 4 processes correctly
- âœ… Exception in any rank causes all ranks to fallback gracefully
- âœ… Timeout in any rank causes all ranks to fallback gracefully
- âœ… All tests pass without hanging

---

## ðŸ—ï¸ Architecture

### Synchronization Points

```
Rank 0                  Rank 1                  Rank N
  |                       |                       |
  â”œâ”€ warmup barrier â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ sync_timeout â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  |                       |                       |
  â”œâ”€ barrier (run 1) â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ sync_timeout â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  |                       |                       |
  â”œâ”€ barrier (run N) â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ sync_timeout â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ benchmark â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  |                       |                       |
  â”œâ”€ all_reduce timing â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ sync_timeout â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  |                       |                       |
  â”œâ”€ sync_exception â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”œâ”€ return timing/fall â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
```

### Fallback Decision Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Start benchmark_collective_choice   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Warmup barrier with timeout         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ sync_timeout_decision()             â”‚â—„â”€â”€â”€ ALL ranks participate
â”‚ Any rank timeout?                   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ Yes           â”‚ No
       â–¼               â–¼
   [Fallback]    [Continue]
                       â”‚
                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Benchmarking loop     â”‚
           â”‚ with timeout checks   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ All-reduce timing     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ sync_exception_status â”‚â—„â”€â”€â”€ ALL ranks participate
           â”‚ Any rank exception?   â”‚
           â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Yes       â”‚ No
               â–¼           â–¼
          [Fallback]   [Return timing]
```

---

## ðŸ“ Key Insights

### 1. **Always Synchronize Decision Making**
When doing collective operations, all ranks must participate in decision-making:
- âœ… Use `all_reduce` with MAX to detect if ANY rank has an issue
- âœ… All ranks must call the sync function, not just those with problems
- âŒ Never let one rank make a fallback decision alone

### 2. **Exception Handling Must Be Collective**
```python
# âŒ BAD: Only one rank decides
try:
    benchmark()
except Exception:
    return fallback()  # Other ranks still waiting!

# âœ… GOOD: All ranks participate in decision
local_exception = False
try:
    benchmark()
except Exception:
    local_exception = True

# ALL ranks call this
if sync_exception_status(local_exception):
    return fallback()  # All ranks fallback together
```

### 3. **Test Infrastructure Should Be Simple**
- âœ… Separate classes for different world_sizes is clearer than decorators
- âœ… Explicit is better than implicit (class name shows world_size)
- âœ… Easy to extend without complex logic

---

## ðŸš€ Future Extensions

### Adding Tests for More Ranks

```python
class TestCollectiveAutotuning6Ranks(MultiProcessTestCase):
    """Test collective autotuning with 6 ranks"""
    
    @property
    def world_size(self):
        return 6

    def setUp(self):
        super().setUp()
        self._spawn_processes()

    @skip_if_lt_x_gpu(6)
    def test_your_6rank_operation(self):
        dist.init_process_group(
            backend="nccl",
            init_method=f"file:///tmp/test_6ranks_{self.id()}",
            world_size=self.world_size,
            rank=self.rank,
        )
        # ... your test code ...
```

### Additional Collective Operations

The same pattern can be used for other collective operations:
- `all_gather`
- `reduce_scatter`
- `all_to_all`
- Custom collective patterns

---

## ðŸ“š Files Modified

1. **`torch/_inductor/select_algorithm.py`**
   - Enhanced `benchmark_collective_choice()` with proper exception handling
   - Added `sync_exception_status()` function
   - Improved error messages and logging

2. **`test/inductor/test_collective_autotuning.py`**
   - Split into `TestCollectiveAutotuning2Ranks` and `TestCollectiveAutotuning4Ranks`
   - Fixed world_size issues
   - Made test infrastructure more extensible

---

## âœ… Verification Checklist

- [x] Timeout detection works correctly (all ranks fallback together)
- [x] Exception handling works correctly (all ranks fallback together)
- [x] 2-rank tests spawn 2 processes and pass
- [x] 4-rank tests spawn 4 processes and pass
- [x] No deadlocks occur in any scenario
- [x] Code is clean and well-documented
- [x] Easy to extend for 6, 8, or more ranks

---

## ðŸŽ“ Lessons Learned

1. **Collective operations require collective decisions** - Never let one rank decide alone
2. **All ranks must participate in synchronization** - Even ranks without issues
3. **Simple, explicit designs are better** - Separate classes > complex decorators
4. **Test your error handling** - Exception paths are as important as success paths
5. **Synchronization is not just for timing** - Also needed for error handling

---

## ðŸ“ž Contact & Questions

For questions about this implementation, refer to:
- This document
- Code comments in `select_algorithm.py`
- Test examples in `test_collective_autotuning.py`

---

**Document Version**: 1.0  
**Last Updated**: 2024-11-14  
**Author**: Code improvements based on testing and debugging collective autotuning
