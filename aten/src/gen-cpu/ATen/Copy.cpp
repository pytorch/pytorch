#include "ATen/Config.h"

#include "TH/TH.h"
#if AT_CUDA_ENABLED()
#undef THNN_
#include "THC/THC.h"
#endif
#include "ATen/Utils.h"
#include "ATen/CPUByteType.h"
#include "ATen/CPUByteTensor.h"
#include "ATen/CPUCharType.h"
#include "ATen/CPUCharTensor.h"
#include "ATen/CPUDoubleType.h"
#include "ATen/CPUDoubleTensor.h"
#include "ATen/CPUFloatType.h"
#include "ATen/CPUFloatTensor.h"
#include "ATen/CPUIntType.h"
#include "ATen/CPUIntTensor.h"
#include "ATen/CPULongType.h"
#include "ATen/CPULongTensor.h"
#include "ATen/CPUShortType.h"
#include "ATen/CPUShortTensor.h"
#include "ATen/CPUHalfType.h"
#include "ATen/CPUHalfTensor.h"
#include "ATen/SparseCPUByteType.h"
#include "ATen/SparseCPUByteTensor.h"
#include "ATen/SparseCPUCharType.h"
#include "ATen/SparseCPUCharTensor.h"
#include "ATen/SparseCPUDoubleType.h"
#include "ATen/SparseCPUDoubleTensor.h"
#include "ATen/SparseCPUFloatType.h"
#include "ATen/SparseCPUFloatTensor.h"
#include "ATen/SparseCPUIntType.h"
#include "ATen/SparseCPUIntTensor.h"
#include "ATen/SparseCPULongType.h"
#include "ATen/SparseCPULongTensor.h"
#include "ATen/SparseCPUShortType.h"
#include "ATen/SparseCPUShortTensor.h"

namespace at {

Tensor & CPUByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THByteTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THByteTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THByteTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THByteTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THByteTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THByteTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THByteTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THByteTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUCharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUCharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THCharTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THCharTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THCharTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THCharTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THCharTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THCharTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THCharTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THCharTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUDoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUDoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THDoubleTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THDoubleTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THDoubleTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THDoubleTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THDoubleTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THDoubleTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THDoubleTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THDoubleTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THFloatTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THFloatTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THFloatTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THFloatTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THFloatTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THFloatTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THFloatTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THFloatTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THIntTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THIntTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THIntTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THIntTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THIntTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THIntTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THIntTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THIntTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPULongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THLongTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THLongTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THLongTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THLongTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THLongTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THLongTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THLongTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THLongTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THShortTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THShortTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THShortTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THShortTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THShortTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THShortTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THShortTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THShortTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & CPUHalfType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<CPUHalfTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {
    case TypeID::CPUByte:
        THHalfTensor_copyByte(self_->tensor, static_cast<CPUByteTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUChar:
        THHalfTensor_copyChar(self_->tensor, static_cast<CPUCharTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUDouble:
        THHalfTensor_copyDouble(self_->tensor, static_cast<CPUDoubleTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUFloat:
        THHalfTensor_copyFloat(self_->tensor, static_cast<CPUFloatTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUInt:
        THHalfTensor_copyInt(self_->tensor, static_cast<CPUIntTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPULong:
        THHalfTensor_copyLong(self_->tensor, static_cast<CPULongTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUShort:
        THHalfTensor_copyShort(self_->tensor, static_cast<CPUShortTensor*>(src.pImpl)->tensor);
        break;
    case TypeID::CPUHalf:
        THHalfTensor_copyHalf(self_->tensor, static_cast<CPUHalfTensor*>(src.pImpl)->tensor);
        break;
    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUByteType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUByteTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUCharType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUCharTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUDoubleType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUDoubleTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUFloatType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUFloatTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUIntType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUIntTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPULongType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPULongTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}
Tensor & SparseCPUShortType::s_copy_(Tensor & self, const Tensor & src, bool non_blocking) const {
  // code generated by function_wrapper
  auto self_ = checked_cast_tensor<SparseCPUShortTensor>(self.pImpl, "self", 0,false);
  (void) self_; //silence unused warning
  switch (src.type().ID()) {

    default:
      AT_ERROR("copy does not support %s to %s copy.", src.type().toString(), toString());
      break;
  }
  self.pImpl->setScalar(src.pImpl->isScalar());
  return self;
}

}
