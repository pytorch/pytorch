# Loss functions

- name: binary_cross_entropy(Tensor input, Tensor target, Tensor weight={}, bool size_average=true)
  cname: BCECriterion

- name: kl_div(Tensor input, Tensor target, bool size_average=true)
  cname: DistKLDivCriterion

- name: l1_loss(Tensor input, Tensor target, bool size_average=true)
  cname: AbsCriterion

- name: mse_loss(Tensor input, Tensor target, bool size_average=true, bool reduce=true)
  cname: MSECriterion

- name: multi_margin_loss(Tensor input, LongTensor target, Scalar p=1, Scalar margin=1, Tensor weight={}, bool size_average=true)
  cname: MultiMarginCriterion

- name: multilabel_margin_loss(Tensor input, LongTensor target, bool size_average=true)
  cname: MultiLabelMarginCriterion
  buffers: [is_target]

- name: nll_loss(Tensor input, LongTensor target, Tensor weight={}, bool size_average=true, int64_t ignore_index=-100)
  cname: ClassNLLCriterion
  buffers: [total_weight]

- name: nll_loss2d(Tensor input, LongTensor target, Tensor weight={}, bool size_average=true, int64_t ignore_index=-100)
  cname: SpatialClassNLLCriterion
  buffers: [total_weight]

- name: smooth_l1_loss(Tensor input, Tensor target, bool size_average=true)
  cname: SmoothL1Criterion

- name: soft_margin_loss(Tensor input, Tensor target, bool size_average=true)
  cname: SoftMarginCriterion

# Activation functions

- name: elu(Tensor input, Scalar alpha=1, bool inplace=false)
  cname: ELU

- name: glu(Tensor input, int64_t dim=-1)
  cname: GatedLinear
  wrap_dim:
    dim: input

- name: hardshrink(Tensor input, Scalar lambd=0.5)
  cname: HardShrink

- name: hardtanh(Tensor input, Scalar min_val=-1, Scalar max_val=1, bool inplace=false)
  cname: HardTanh

- name: leaky_relu(Tensor input, Scalar negative_slope=0.01, bool inplace=false)
  cname: LeakyReLU

- name: log_sigmoid(Tensor input)
  cname: LogSigmoid
  buffers: [buffer]

- name: log_softmax(Tensor input, int64_t dim)
  cname: LogSoftMax

- name: prelu(Tensor input, Tensor weight)
  cname: PReLU

- name: rrelu(Tensor input, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=false, bool inplace=false, Generator* generator=nullptr)
  cname: RReLU
  buffers: [noise]

- name: softmax(Tensor input, int64_t dim)
  cname: SoftMax

- name: softplus(Tensor input, Scalar beta=1, Scalar threshold=20)
  cname: SoftPlus

- name: softshrink(Tensor input, Scalar lambd=0.5)
  cname: SoftShrink

- name: threshold(Tensor input, Scalar threshold, Scalar value, bool inplace=false)
  cname: Threshold

# Pooling

- name: adaptive_max_pool2d(Tensor input, IntList[2] output_size)
  cname: SpatialAdaptiveMaxPooling

- name: avg_pool2d(Tensor input, IntList[2] kernel_size, IntList[2] stride={}, IntList[2] padding=0, bool ceil_mode=false, bool count_include_pad=false)
  cname: SpatialAveragePooling
  default_init:
    stride: kernel_size

- name: avg_pool3d(Tensor input, IntList[3] kernel_size, IntList[3] stride={}, IntList[3] padding=0, bool ceil_mode=false, bool count_include_pad=false)
  cname: VolumetricAveragePooling
  default_init:
    stride: kernel_size

- name: max_pool2d(Tensor input, IntList[2] kernel_size, IntList[2] stride={}, IntList[2] padding=0, IntList[2] dilation=1, bool ceil_mode=false)
  cname: SpatialDilatedMaxPooling
  default_init:
    stride: kernel_size

- name: max_pool3d(Tensor input, IntList[3] kernel_size, IntList[3] stride={}, IntList[3] padding=0, IntList[2] dilation=1, bool ceil_mode=false)
  cname: VolumetricDilatedMaxPooling
  default_init:
    stride: kernel_size

- name: max_unpool2d(Tensor input, LongTensor indices, IntList[2] output_size)
  cname: SpatialMaxUnpooling

- name: max_unpool3d(Tensor input, LongTensor indices, IntList[3] output_size, IntList[3] stride, IntList[3] padding)
  cname: VolumetricMaxUnpooling

# Private functions. These also exist in TH, but we want the backwards functions
# to implement derivatives.

- name: _sigmoid(Tensor input)
  cname: Sigmoid

- name: _tanh(Tensor input)
  cname: Tanh

# Batch normalization

- name: batch_norm(Tensor input, Tensor weight, Tensor bias, Tensor running_mean, Tensor running_var, bool training, double momentum, double eps)
  cname: BatchNormalization
  buffers: [save_mean, save_std]

# Convolutions

- name: conv_transpose2d(Tensor input, Tensor weight, IntList[2] kernel_size, Tensor bias={}, IntList[2] stride=1, IntList[2] padding=0, IntList[2] output_padding=0, IntList[2] dilation=1)
  cname: SpatialFullDilatedConvolution
  buffers: [columns, ones]

- name: conv_transpose3d(Tensor input, Tensor weight, Tensor bias={}, IntList[3] stride=1, IntList[3] padding=0, IntList[3] output_padding=0, IntList[3] dilation=1)
  cname: VolumetricFullDilatedConvolution
  buffers: [finput, fgrad_input]

- name: conv2d(Tensor input, Tensor weight, IntList[2] kernel_size, Tensor bias={}, IntList[2] stride=1, IntList[2] padding=0)
  cname: SpatialConvolutionMM
  buffers: [finput, fgrad_input]

- name: conv3d(Tensor input, Tensor weight, IntList[3] kernel_size, Tensor bias={}, IntList[3] stride=1, IntList[3] padding=0)
  cname: VolumetricConvolutionMM
  buffers: [finput, fgrad_input]

- name: conv_dilated2d(Tensor input, Tensor weight, IntList[2] kernel_size, Tensor bias={}, IntList[2] stride=1, IntList[2] padding=0, IntList[2] dilation=1)
  cname: SpatialDilatedConvolution
  buffers: [columns, ones]

- name: conv_dilated3d(Tensor input, Tensor weight, IntList[3] kernel_size, Tensor bias={}, IntList[3] stride=1, IntList[3] padding=0, IntList[3] dilation=1)
  cname: VolumetricDilatedConvolution
  buffers: [columns, ones]
