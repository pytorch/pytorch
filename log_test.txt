I1119 11:51:00.475000 290932 torch/_inductor/config.py:998] compile_threads set to 32
I1119 11:51:01.731000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.731000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
I1119 11:51:01.732000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
I1119 11:51:01.732000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
I1119 11:51:01.732000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
I1119 11:51:01.733000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.733000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
I1119 11:51:01.734000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
I1119 11:51:01.734000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
I1119 11:51:01.734000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
I1119 11:51:01.735000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
I1119 11:51:01.735000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
I1119 11:51:01.736000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
I1119 11:51:01.736000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
I1119 11:51:01.740000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.741000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.741000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
I1119 11:51:01.742000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
I1119 11:51:01.742000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
I1119 11:51:01.743000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.743000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.744000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
I1119 11:51:01.744000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
I1119 11:51:01.744000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
I1119 11:51:01.745000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1119 11:51:01.745000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1119 11:51:01.746000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
I1119 11:51:01.746000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
I1119 11:51:01.746000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
I1119 11:51:01.747000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
I1119 11:51:01.747000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
I1119 11:51:01.748000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
I1119 11:51:01.748000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
I1119 11:51:01.748000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
I1119 11:51:01.749000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
I1119 11:51:01.749000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
I1119 11:51:01.750000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
I1119 11:51:01.750000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
I1119 11:51:01.750000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
I1119 11:51:01.751000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
I1119 11:51:01.751000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
I1119 11:51:01.752000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
I1119 11:51:01.752000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
I1119 11:51:01.752000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
I1119 11:51:01.753000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
I1119 11:51:01.753000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
I1119 11:51:01.754000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
I1119 11:51:01.754000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
I1119 11:51:01.754000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
I1119 11:51:01.755000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
I1119 11:51:01.755000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
I1119 11:51:01.756000 290932 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
I1119 11:51:02.514000 290932 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
I1119 11:51:02.612000 290932 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[2, 256, 128][32768, 128, 1]cuda:0", L_weight_: "f32[128][1]cuda:0"):
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_weight_ = L_weight_
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:117 in test_fn, code: return dynamic_range_op(x, weight)
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         dynamic_range_139901219850496_default: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_139901219850496.default(l_x_, l_weight_);  l_x_ = l_weight_ = None
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (dynamic_range_139901219850496_default,)
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1119 11:51:02.628000 290932 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
V1119 11:51:03.233000 290932 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fj3hn373xhya3ciptis4wu3x642nfqk24y3x3ivgwx4whlt7jnxf:
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [zkcap3ge6ixwet5yfdoqyixztruq3zlb6lurucnqylfysmgyymr] gm: <lambda>()
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1, arg1_1):
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0]     dynamic_range_139901219850496 = torch.ops.test_lib.dynamic_range_139901219850496.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0]     return (dynamic_range_139901219850496,)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0]     
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ffvbi42jvc443loukbs4l7f2ktyctumpl3p52q36kphvdbcz745] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2, 256, 128]), stride=(32768, 128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [hhafgaghh5icfiiv2s3gtezjixz3usyfz36wro23umj3t2dfcyw] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4om2f62sxa7ueliq6spzazi44kbssczwh5wnqne62btfnlvidjj] torch_version: 7���vз�,��#ǲ��m���oԆF�%&��7
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bfloat16_atomic_adds_enabled]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_epilogue_fusion_enabled]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_tma_only]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_hash_with_compile_cmd]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.cutlass_prescreening]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cuda.cutlass_enabled_ops]: all
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.use_binary_remote_cache]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.upload_to_binary_remote_cache]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.binary_remote_cache_force_write]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.enable_caching_codegen]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [6trwnwm4voevl4joplmkcssruwgd46kgqfejamut6kq662kstpd] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cuda.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
V1119 11:51:03.238000 290932 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
V1119 11:51:03.239000 290932 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fj3hn373xhya3ciptis4wu3x642nfqk24y3x3ivgwx4whlt7jnxf
I1119 11:51:03.240000 290932 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fj3hn373xhya3ciptis4wu3x642nfqk24y3x3ivgwx4whlt7jnxf
V1119 11:51:03.240000 290932 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
V1119 11:51:03.241000 290932 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
I1119 11:51:03.241000 290932 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[2, 256, 128][32768, 128, 1]cuda:0", arg1_1: "f32[128][1]cuda:0"):
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:117 in test_fn, code: return dynamic_range_op(x, weight)
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         dynamic_range_139901219850496: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_139901219850496.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (dynamic_range_139901219850496,)
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
V1119 11:51:03.258000 290932 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
V1119 11:51:03.260000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:03.261000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:03.261000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %dynamic_range_139901219850496 : [num_users=1] = call_function[target=torch.ops.test_lib.dynamic_range_139901219850496.default](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:03.262000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function test_lib::dynamic_range_139901219850496 at 0x7f3b3047e200>
I1119 11:51:03.263000 290932 torch/_inductor/kernel/custom_op.py:645] [0/0] === Range-based Autotuning for dynamic_range_autotuned ===
I1119 11:51:03.263000 290932 torch/_inductor/kernel/custom_op.py:646] [0/0] Dispatch on: x[1], Ranges: [(1, 512), (513, 2048), (2049, inf)]
V1119 11:51:03.282000 290932 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1119 11:51:03.283000 290932 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1119 11:51:03.284000 290932 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1119 11:51:03.285000 290932 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1119 11:51:03.286000 290932 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1119 11:51:03.287000 290932 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py:3323: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  current_size = base.storage().size()
V1119 11:51:03.290000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:03.291000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:03.291000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:03.292000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:03.310000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:03.311000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:03.312000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:03.313000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, i2)
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:03.322000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:03.323000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0
I1119 11:51:03.330000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:03.330000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:03.331000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:03.332000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:03.332000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:03.346000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_op0 with estimated runtime 0.000214
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, %get_index), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %sin : [num_users=1] = call_method[target=sin](args = (%ops, %mul), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, %get_index_2, %sin, None), kwargs = {})
V1119 11:51:03.354000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:03.358000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.360000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.361000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.368000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.369000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ql/cqlrendtjjrfdb6zsluzidspbvrrocjgx6dspxnmzx2uans23cv2.py
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl_math.sin(tmp2)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1 = args
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, (128, ), (1, ))
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, 65536, stream=stream0)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_buf0, )
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_short_sequence_impl_0_arg1_1])
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:03.371000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.372000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ro/cronw6uhcoowhnwct7zr7ouinirkofio5u67mhl2xpugwrroauyp.py
V1119 11:51:03.378000 290932 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.378000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.379000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.379000 290932 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1119 11:51:03.581000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/ro/cronw6uhcoowhnwct7zr7ouinirkofio5u67mhl2xpugwrroauyp.py
I1119 11:51:03.582000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ro/cronw6uhcoowhnwct7zr7ouinirkofio5u67mhl2xpugwrroauyp.py
V1119 11:51:03.601000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:03.601000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:03.602000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:03.602000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:03.603000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ql/5715f3fe629e48108eb3a92deed58b696294436a05c179890827a661ed4d7469.best_config
V1119 11:51:03.612000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:03.613000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:03.614000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:03.614000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:03.616000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul,), kwargs = {}) 
V1119 11:51:03.616000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6b40e0>
V1119 11:51:03.617000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return tanh 
V1119 11:51:03.618000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, i2)
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.tanh(tmp2)
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=tanh,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, tanh, mul]),
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:03.621000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:03.622000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0
I1119 11:51:03.625000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:03.626000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:03.627000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:03.627000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:03.628000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:03.630000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_op0 with estimated runtime 0.000214
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, %get_index), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %tanh : [num_users=1] = call_method[target=tanh](args = (%ops, %mul), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, %get_index_2, %tanh, None), kwargs = {})
V1119 11:51:03.634000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:03.636000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.637000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.638000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.639000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.640000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ya/cyae4uqwec23snsr2uwnqai4nei55enh4epfwbpal2k42isgj5ft.py
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = libdevice.tanh(tmp2)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1 = args
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, (128, ), (1, ))
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, 65536, stream=stream0)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_buf0, )
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_medium_sequence_impl_1_arg1_1])
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:03.641000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.642000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/kz/ckzy7zpiflexs6ulusfnzuva2xskdsnkc2efpvioiwsqs4636ztm.py
V1119 11:51:03.645000 290932 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.645000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.646000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.646000 290932 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1119 11:51:03.780000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/kz/ckzy7zpiflexs6ulusfnzuva2xskdsnkc2efpvioiwsqs4636ztm.py
I1119 11:51:03.781000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/kz/ckzy7zpiflexs6ulusfnzuva2xskdsnkc2efpvioiwsqs4636ztm.py
V1119 11:51:03.798000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:03.799000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005632, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:03.799000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:03.800000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:03.800000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ya/ba753bea738a2072452bb15f9d34d99e800d0263e25999119199912842672d17.best_config
V1119 11:51:03.810000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:03.810000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:03.811000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:03.812000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:03.813000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:03.813000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:03.815000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:03.815000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:03.817000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return relu 
V1119 11:51:03.817000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, i2)
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.relu(tmp2)
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=relu,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, relu, mul,...,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:03.820000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:03.821000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0
I1119 11:51:03.824000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:03.825000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:03.826000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:03.826000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:03.827000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:03.829000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_op0 with estimated runtime 0.000214
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, %get_index), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %relu : [num_users=1] = call_method[target=relu](args = (%ops, %mul), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, %get_index_2, %relu, None), kwargs = {})
V1119 11:51:03.831000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:03.833000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.834000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.835000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:03.836000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.837000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ri/crinzf5wjyemqehov4iywoey3o6lps5ziuqdh4z3pgeh2wujgzbj.py
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl.full([1], 0, tl.int32)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp4 = triton_helpers.maximum(tmp3, tmp2)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp4, None)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1 = args
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, (128, ), (1, ))
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, 65536, stream=stream0)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_buf0, )
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg0_1, benchmark_dynamic_range_autotuned_range_1_512_long_sequence_impl_2_arg1_1])
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:03.838000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:03.839000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/vi/cvisccczmwe5hqhhrhlrdqthpwuok2ujh6mr4bshba5ajaiqbkk4.py
V1119 11:51:03.842000 290932 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:03.842000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.843000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:03.844000 290932 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1119 11:51:03.938000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/vi/cvisccczmwe5hqhhrhlrdqthpwuok2ujh6mr4bshba5ajaiqbkk4.py
I1119 11:51:03.938000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/vi/cvisccczmwe5hqhhrhlrdqthpwuok2ujh6mr4bshba5ajaiqbkk4.py
V1119 11:51:03.955000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:03.956000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005440, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:03.956000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:03.957000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005440, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:03.958000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ri/d89130c4cd6b5e68fb8fd880718f2cb212fc05359548f35beb92f365f031951f.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_1_512_short_sequence_impl_0", "best_kernel_desc": "CustomOp short_sequence_impl", "best_time": 0.005375999957323074}
V1119 11:51:03.976000 290932 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.69s
V1119 11:51:03.978000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:03.978000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:03.979000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:03.979000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:03.981000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:03.981000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:03.983000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:03.984000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf0,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1119 11:51:03.984000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1119 11:51:03.985000 290932 torch/_inductor/kernel/custom_op.py:546] [0/0] Inlining winning choice: dynamic_range_autotuned_range_1_512_short_sequence_impl_0 (name=dynamic_range_autotuned_range_1_512)
V1119 11:51:03.986000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:03.986000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:03.987000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:03.988000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:03.989000 290932 torch/_inductor/kernel/custom_op.py:330] [0/0] Matched choice 'dynamic_range_autotuned_range_1_512_short_sequence_impl_0' to decomposition[0] 'short_sequence_impl'
I1119 11:51:03.990000 290932 torch/_inductor/kernel/custom_op.py:696] [0/0] Range [1, 512]: Selected short_sequence_impl
V1119 11:51:04.003000 290932 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1119 11:51:04.003000 290932 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1119 11:51:04.005000 290932 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1119 11:51:04.006000 290932 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1119 11:51:04.007000 290932 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1119 11:51:04.007000 290932 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1119 11:51:04.009000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.010000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.011000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.012000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.013000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.014000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.015000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:04.016000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, i2)
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.019000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.020000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0
I1119 11:51:04.024000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.025000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.026000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.027000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.027000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.030000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_op0 with estimated runtime 0.000214
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %sin : [num_users=1] = call_method[target=sin](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, %get_index_2, %sin, None), kwargs = {})
V1119 11:51:04.032000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.033000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.034000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.036000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.037000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.038000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ql/cqlrendtjjrfdb6zsluzidspbvrrocjgx6dspxnmzx2uans23cv2.py
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl_math.sin(tmp2)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1 = args
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, (128, ), (1, ))
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, 65536, stream=stream0)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_buf0, )
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_short_sequence_impl_3_arg1_1])
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.039000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.040000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ib/cibynveaxcqzjkbgkafkzqub7pazhr6byva33yn4my4yxocdpqc6.py
V1119 11:51:04.043000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/ib/cibynveaxcqzjkbgkafkzqub7pazhr6byva33yn4my4yxocdpqc6.py
I1119 11:51:04.043000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ib/cibynveaxcqzjkbgkafkzqub7pazhr6byva33yn4my4yxocdpqc6.py
V1119 11:51:04.062000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.063000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.063000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.064000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.065000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ql/5715f3fe629e48108eb3a92deed58b696294436a05c179890827a661ed4d7469.best_config
V1119 11:51:04.074000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.075000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.076000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.076000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.078000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.078000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6b40e0>
V1119 11:51:04.080000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return tanh 
V1119 11:51:04.080000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, i2)
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.tanh(tmp2)
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=tanh,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, tanh, mul]),
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.083000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.084000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0
I1119 11:51:04.088000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.089000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.089000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.090000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.091000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.093000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_op0 with estimated runtime 0.000214
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %tanh : [num_users=1] = call_method[target=tanh](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, %get_index_2, %tanh, None), kwargs = {})
V1119 11:51:04.095000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.097000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.098000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.099000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.101000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.102000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ya/cyae4uqwec23snsr2uwnqai4nei55enh4epfwbpal2k42isgj5ft.py
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = libdevice.tanh(tmp2)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1 = args
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, (128, ), (1, ))
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, 65536, stream=stream0)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_buf0, )
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4_arg1_1])
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.103000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.104000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/7e/c7e2bk2jqevqxybklu4nmsh6gk3f33tyytygfylvg5dmu72pqvkj.py
V1119 11:51:04.107000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/7e/c7e2bk2jqevqxybklu4nmsh6gk3f33tyytygfylvg5dmu72pqvkj.py
I1119 11:51:04.108000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/7e/c7e2bk2jqevqxybklu4nmsh6gk3f33tyytygfylvg5dmu72pqvkj.py
V1119 11:51:04.125000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.126000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005728, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.126000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:04.127000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:04.128000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ya/ba753bea738a2072452bb15f9d34d99e800d0263e25999119199912842672d17.best_config
V1119 11:51:04.137000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.138000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.138000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:04.139000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:04.140000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:04.140000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.142000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.142000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:04.143000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return relu 
V1119 11:51:04.144000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, i2)
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.relu(tmp2)
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=relu,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, relu, mul,...,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.146000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.147000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0
I1119 11:51:04.151000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.151000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.152000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.153000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.153000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.155000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_op0 with estimated runtime 0.000214
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %relu : [num_users=1] = call_method[target=relu](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, %get_index_2, %relu, None), kwargs = {})
V1119 11:51:04.157000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.159000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.160000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.161000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.163000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.163000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ri/crinzf5wjyemqehov4iywoey3o6lps5ziuqdh4z3pgeh2wujgzbj.py
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl.full([1], 0, tl.int32)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp4 = triton_helpers.maximum(tmp3, tmp2)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp4, None)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1 = args
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, (128, ), (1, ))
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, 65536, stream=stream0)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_buf0, )
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg0_1, benchmark_dynamic_range_autotuned_range_513_2048_long_sequence_impl_5_arg1_1])
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.164000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.165000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/6o/c6oqmrsiineae4425pgdsf2ftznvd5n5alzozg3jd2yuwqbkaub7.py
V1119 11:51:04.168000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/6o/c6oqmrsiineae4425pgdsf2ftznvd5n5alzozg3jd2yuwqbkaub7.py
I1119 11:51:04.168000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/6o/c6oqmrsiineae4425pgdsf2ftznvd5n5alzozg3jd2yuwqbkaub7.py
V1119 11:51:04.185000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.185000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:04.186000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.186000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.187000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ri/d89130c4cd6b5e68fb8fd880718f2cb212fc05359548f35beb92f365f031951f.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_513_2048_long_sequence_impl_5", "best_kernel_desc": "CustomOp long_sequence_impl", "best_time": 0.005375999957323074}
V1119 11:51:04.205000 290932 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.20s
V1119 11:51:04.206000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.207000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.208000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:04.208000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:04.209000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:04.210000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.211000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.212000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:04.213000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return relu 
V1119 11:51:04.214000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf2,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1119 11:51:04.214000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1119 11:51:04.215000 290932 torch/_inductor/kernel/custom_op.py:546] [0/0] Inlining winning choice: dynamic_range_autotuned_range_513_2048_long_sequence_impl_5 (name=dynamic_range_autotuned_range_513_2048)
V1119 11:51:04.215000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:04.216000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:04.217000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:04.218000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.219000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.219000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:04.221000 290932 torch/_inductor/kernel/custom_op.py:330] [0/0] Matched choice 'dynamic_range_autotuned_range_513_2048_long_sequence_impl_5' to decomposition[2] 'long_sequence_impl'
I1119 11:51:04.222000 290932 torch/_inductor/kernel/custom_op.py:696] [0/0] Range [513, 2048]: Selected long_sequence_impl
V1119 11:51:04.235000 290932 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1119 11:51:04.235000 290932 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1119 11:51:04.237000 290932 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1119 11:51:04.238000 290932 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1119 11:51:04.238000 290932 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1119 11:51:04.239000 290932 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1119 11:51:04.241000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.242000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.242000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.243000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.244000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.244000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.246000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:04.246000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, i2)
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.249000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.250000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0
I1119 11:51:04.253000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.254000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.254000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.255000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.256000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.258000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_op0 with estimated runtime 0.000214
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %sin : [num_users=1] = call_method[target=sin](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, %get_index_2, %sin, None), kwargs = {})
V1119 11:51:04.260000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.262000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.263000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.264000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.265000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.266000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ql/cqlrendtjjrfdb6zsluzidspbvrrocjgx6dspxnmzx2uans23cv2.py
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl_math.sin(tmp2)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1 = args
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, (128, ), (1, ))
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, 65536, stream=stream0)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_buf0, )
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6_arg1_1])
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.267000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.268000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/2l/c2l5of6bruq2ympdx6t2h4iojdaac2xk4hofbuzkoy77q5kx46jt.py
V1119 11:51:04.270000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/2l/c2l5of6bruq2ympdx6t2h4iojdaac2xk4hofbuzkoy77q5kx46jt.py
I1119 11:51:04.271000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/2l/c2l5of6bruq2ympdx6t2h4iojdaac2xk4hofbuzkoy77q5kx46jt.py
V1119 11:51:04.287000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.288000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.289000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.289000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.290000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ql/5715f3fe629e48108eb3a92deed58b696294436a05c179890827a661ed4d7469.best_config
V1119 11:51:04.299000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.300000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.300000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.301000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.302000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %tanh : [num_users=1] = call_function[target=torch.ops.aten.tanh.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.303000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6b40e0>
V1119 11:51:04.304000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return tanh 
V1119 11:51:04.304000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, i2)
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.tanh(tmp2)
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=tanh,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, tanh, mul]),
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.307000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0
I1119 11:51:04.311000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.312000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.312000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.313000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.313000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.316000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_op0 with estimated runtime 0.000214
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %tanh : [num_users=1] = call_method[target=tanh](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, %get_index_2, %tanh, None), kwargs = {})
V1119 11:51:04.318000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.319000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.320000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.321000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.323000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.323000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ya/cyae4uqwec23snsr2uwnqai4nei55enh4epfwbpal2k42isgj5ft.py
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = libdevice.tanh(tmp2)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1 = args
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, (128, ), (1, ))
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, 65536, stream=stream0)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_buf0, )
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7_arg1_1])
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.324000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.325000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/xj/cxjatg556chb3qs3d57rhyuazyw4pn6ddi77ljtzipqvzzhfnyn6.py
V1119 11:51:04.328000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/xj/cxjatg556chb3qs3d57rhyuazyw4pn6ddi77ljtzipqvzzhfnyn6.py
I1119 11:51:04.328000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/xj/cxjatg556chb3qs3d57rhyuazyw4pn6ddi77ljtzipqvzzhfnyn6.py
V1119 11:51:04.345000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.345000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005728, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.346000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:04.346000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1119 11:51:04.347000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ya/ba753bea738a2072452bb15f9d34d99e800d0263e25999119199912842672d17.best_config
V1119 11:51:04.356000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.357000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.358000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:04.358000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:04.359000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:04.359000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.361000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.361000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:04.362000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return relu 
V1119 11:51:04.363000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, i2)
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.relu(tmp2)
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=relu,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, relu, mul,...,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.367000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0
I1119 11:51:04.371000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.372000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.372000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.373000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.373000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.375000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_op0 with estimated runtime 0.000214
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %relu : [num_users=1] = call_method[target=relu](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, %get_index_2, %relu, None), kwargs = {})
V1119 11:51:04.377000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.379000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.380000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.381000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.383000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.383000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ri/crinzf5wjyemqehov4iywoey3o6lps5ziuqdh4z3pgeh2wujgzbj.py
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl.full([1], 0, tl.int32)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp4 = triton_helpers.maximum(tmp3, tmp2)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp4, None)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1 = args
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, (128, ), (1, ))
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             stream0 = get_raw_stream(0)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             triton_poi_fused_dynamic_range_139901219850496_0.run(benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, 65536, stream=stream0)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_buf0, )
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg0_1, benchmark_dynamic_range_autotuned_range_2049_inf_long_sequence_impl_8_arg1_1])
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.384000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.385000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/gt/cgtrngcxemqr2ne42txer6cyonxmqxsqpzwaj4tu5u4mezjcnmjt.py
V1119 11:51:04.388000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/gt/cgtrngcxemqr2ne42txer6cyonxmqxsqpzwaj4tu5u4mezjcnmjt.py
I1119 11:51:04.388000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/gt/cgtrngcxemqr2ne42txer6cyonxmqxsqpzwaj4tu5u4mezjcnmjt.py
V1119 11:51:04.405000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.405000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:04.406000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.407000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:04.407000 290932 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ri/d89130c4cd6b5e68fb8fd880718f2cb212fc05359548f35beb92f365f031951f.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6", "best_kernel_desc": "CustomOp short_sequence_impl", "best_time": 0.005375999957323074}
V1119 11:51:04.425000 290932 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.19s
V1119 11:51:04.427000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.427000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.428000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.429000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.430000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.430000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.432000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:04.432000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf4,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1119 11:51:04.433000 290932 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1119 11:51:04.433000 290932 torch/_inductor/kernel/custom_op.py:546] [0/0] Inlining winning choice: dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6 (name=dynamic_range_autotuned_range_2049_inf)
V1119 11:51:04.434000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.435000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.436000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.437000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.438000 290932 torch/_inductor/kernel/custom_op.py:330] [0/0] Matched choice 'dynamic_range_autotuned_range_2049_inf_short_sequence_impl_6' to decomposition[0] 'short_sequence_impl'
I1119 11:51:04.439000 290932 torch/_inductor/kernel/custom_op.py:696] [0/0] Range [2049, inf]: Selected short_sequence_impl
I1119 11:51:04.439000 290932 torch/_inductor/kernel/custom_op.py:703] [0/0] Completed autotuning for 3 ranges
I1119 11:51:04.440000 290932 torch/_inductor/kernel/custom_op.py:708] [0/0] After merging: 2 unique implementations across 3 ranges
I1119 11:51:04.441000 290932 torch/_inductor/kernel/custom_op.py:727] [0/0] Creating runtime dispatch for 3 ranges
V1119 11:51:04.443000 290932 torch/_inductor/kernel/custom_op.py:742] [0/0] Compiling range [1, 512]: short_sequence_impl
V1119 11:51:04.447000 290932 torch/_inductor/kernel/custom_op.py:742] [0/0] Compiling range [513, 2048]: long_sequence_impl
V1119 11:51:04.451000 290932 torch/_inductor/kernel/custom_op.py:742] [0/0] Compiling range [2049, inf]: short_sequence_impl
I1119 11:51:04.455000 290932 torch/_inductor/kernel/custom_op.py:769] [0/0] Compiled 3 range implementations
V1119 11:51:04.457000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.457000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.458000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.459000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.460000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.461000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.462000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:04.463000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.464000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.465000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.465000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1119 11:51:04.466000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f3d3c62d1c0>
V1119 11:51:04.467000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1119 11:51:04.468000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.469000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.469000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ad440>
V1119 11:51:04.471000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return relu 
V1119 11:51:04.472000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1119 11:51:04.473000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1119 11:51:04.474000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1119 11:51:04.474000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1119 11:51:04.475000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f3d3c69e8e0>
V1119 11:51:04.476000 290932 torch/_inductor/graph.py:1602] [0/0] lowering %sin : [num_users=1] = call_function[target=torch.ops.aten.sin.default](args = (%mul,), kwargs = {}) 
V1119 11:51:04.477000 290932 torch/_inductor/graph.py:1291] [0/0]   via <function make_pointwise.<locals>.inner at 0x7f3d3c6ae160>
V1119 11:51:04.478000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return sin 
V1119 11:51:04.479000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
I1119 11:51:04.479000 290932 torch/_inductor/kernel/custom_op.py:783] [0/0] Created SubgraphBuffer with multi-range dispatch (3 ranges)
V1119 11:51:04.480000 290932 torch/_inductor/graph.py:1602] [0/0] lowering return (dynamic_range_139901219850496,) 
V1119 11:51:04.481000 290932 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf0,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.486000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.488000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf2,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.489000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.relu(tmp2)
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=relu,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, relu, mul,...,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.490000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf4,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.491000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf5', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.493000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf6,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=dynamic_range_139901219850496,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.494000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf7,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.495000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf8,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.497000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   name=buf9,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496]),
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.498000 290932 torch/_inductor/scheduler.py:2988] [0/0] )
V1119 11:51:04.499000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output buf6
V1119 11:51:04.500000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf9
V1119 11:51:04.501000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op9
V1119 11:51:04.501000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf8
V1119 11:51:04.502000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op8
V1119 11:51:04.503000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf7
V1119 11:51:04.503000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op7
V1119 11:51:04.504000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf5
V1119 11:51:04.505000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op5
V1119 11:51:04.505000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf4
V1119 11:51:04.506000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op4
V1119 11:51:04.507000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf3
V1119 11:51:04.507000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op3
V1119 11:51:04.508000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf2
V1119 11:51:04.508000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op2
V1119 11:51:04.509000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf1
V1119 11:51:04.510000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op1
V1119 11:51:04.510000 290932 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf0
V1119 11:51:04.511000 290932 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op0
I1119 11:51:04.512000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.513000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.514000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.515000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.516000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.517000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node op6 with estimated runtime 0.000214
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_range_1_512_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_range_1_512_arg1_1, i2)
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.521000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_range_1_512_buf0
I1119 11:51:04.527000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.527000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.528000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.529000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.529000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.532000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_range_1_512_op0 with estimated runtime 0.000214
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %sin : [num_users=1] = call_method[target=sin](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_buf0, %get_index_2, %sin, None), kwargs = {})
V1119 11:51:04.535000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.537000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.538000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.539000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.541000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.542000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_range_513_2048_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_range_513_2048_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_range_513_2048_arg1_1, i2)
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.relu(tmp2)
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=relu,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, relu, mul,...,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.546000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.547000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_range_513_2048_buf0
I1119 11:51:04.552000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.552000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.553000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.554000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.554000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.556000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_range_513_2048_op0 with estimated runtime 0.000214
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_513_2048_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_513_2048_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %relu : [num_users=1] = call_method[target=relu](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_range_513_2048_buf0, %get_index_2, %relu, None), kwargs = {})
V1119 11:51:04.559000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.560000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.561000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.563000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.564000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_1
V1119 11:51:04.565000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_range_2049_inf_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1, i2)
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       tmp3 = ops.sin(tmp2)
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return tmp3
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=sin,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139901219850496, sin, mul]),
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 117, in test_fn,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0]   }
V1119 11:51:04.569000 290932 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1119 11:51:04.570000 290932 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_range_2049_inf_buf0
I1119 11:51:04.575000 290932 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1119 11:51:04.575000 290932 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1119 11:51:04.576000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1119 11:51:04.577000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1119 11:51:04.578000 290932 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1119 11:51:04.579000 290932 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_range_2049_inf_op0 with estimated runtime 0.000214
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0] graph():
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=5] = placeholder[target=ops]
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1, %get_index), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1, %get_index_1), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %sin : [num_users=1] = call_method[target=sin](args = (%ops, %mul), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_range_2049_inf_buf0, %get_index_2, %sin, None), kwargs = {})
V1119 11:51:04.583000 290932 torch/_inductor/bounds.py:81] [0/0]     return store
V1119 11:51:04.585000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.586000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.587000 290932 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1119 11:51:04.588000 290932 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139901219850496_0
V1119 11:51:04.589000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.591000 290932 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ql/cqlrendtjjrfdb6zsluzidspbvrrocjgx6dspxnmzx2uans23cv2.py
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_0 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_0', '''
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl_math.sin(tmp2)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp3, None)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned_range_1_512(args):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_range_1_512_arg0_1, dynamic_range_autotuned_autotuned_range_1_512_arg1_1 = args
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_1_512_arg1_1, (128, ), (1, ))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_range_1_512_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_139901219850496_0.run(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, dynamic_range_autotuned_autotuned_range_1_512_arg1_1, dynamic_range_autotuned_autotuned_range_1_512_buf0, 65536, stream=stream0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_1_512_arg0_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_1_512_arg1_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_range_1_512_buf0, )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/jn/cjnc7bgnc7ltxav7xfbh62dociiec6luro3z3d2c4p2pldwg6ryt.py
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_139901219850496_default => dynamic_range_139901219850496
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_139901219850496_1 = async_compile.triton('triton_poi_fused_dynamic_range_139901219850496_1', '''
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_139901219850496_1', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_139901219850496_1(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp3 = tl.full([1], 0, tl.int32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp4 = triton_helpers.maximum(tmp3, tmp2)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp4, None)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned_range_513_2048(args):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_range_513_2048_arg0_1, dynamic_range_autotuned_autotuned_range_513_2048_arg1_1 = args
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_513_2048_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_513_2048_arg1_1, (128, ), (1, ))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_range_513_2048_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_139901219850496_1.run(dynamic_range_autotuned_autotuned_range_513_2048_arg0_1, dynamic_range_autotuned_autotuned_range_513_2048_arg1_1, dynamic_range_autotuned_autotuned_range_513_2048_buf0, 65536, stream=stream0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_513_2048_arg0_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_513_2048_arg1_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_range_513_2048_buf0, )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned_range_2049_inf(args):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1, dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1 = args
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1, (128, ), (1, ))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_range_2049_inf_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_139901219850496_default], Original ATen: [test_lib.dynamic_range_139901219850496]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_139901219850496_0.run(dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1, dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1, dynamic_range_autotuned_autotuned_range_2049_inf_buf0, 65536, stream=stream0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_2049_inf_arg0_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_2049_inf_arg1_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_range_2049_inf_buf0, )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         arg0_1, arg1_1 = args
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg0_1, (2, 256, 128), (32768, 128, 1))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg1_1, (128, ), (1, ))
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned_range_1_512
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_range_1_512_args = [arg0_1, arg1_1]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf7,) = dynamic_range_autotuned_autotuned_range_1_512(dynamic_range_autotuned_autotuned_range_1_512_args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned_range_513_2048
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_range_513_2048_args = [arg0_1, arg1_1]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf8,) = dynamic_range_autotuned_autotuned_range_513_2048(dynamic_range_autotuned_autotuned_range_513_2048_args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned_range_2049_inf
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_range_2049_inf_args = [arg0_1, arg1_1]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf9,) = dynamic_range_autotuned_autotuned_range_2049_inf(dynamic_range_autotuned_autotuned_range_2049_inf_args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             def buf6_runtime_dispatch(args):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 arg0, arg1 = args
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 dispatch_size = arg0.size(1)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 if 1 <= dispatch_size <= 512:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                     return dynamic_range_autotuned_autotuned_range_1_512(args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 elif 513 <= dispatch_size <= 2048:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                     return dynamic_range_autotuned_autotuned_range_513_2048(args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 else:
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]                     return dynamic_range_autotuned_autotuned_range_2049_inf(args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             buf6_runtime_dispatch_args = [arg0_1, arg1_1]
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf6,) = buf6_runtime_dispatch(buf6_runtime_dispatch_args)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg0_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg1_1
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (buf6, )
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1119 11:51:04.592000 290932 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1119 11:51:04.593000 290932 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/eb/cebohdpswcad4ulymal337w5jm3rtz7n7d7syr4oyyjjxfexfhja.py
V1119 11:51:04.598000 290932 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_139901219850496_1
V1119 11:51:04.598000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:04.599000 290932 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1119 11:51:04.599000 290932 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1119 11:51:04.686000 290932 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/eb/cebohdpswcad4ulymal337w5jm3rtz7n7d7syr4oyyjjxfexfhja.py
I1119 11:51:04.686000 290932 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/eb/cebohdpswcad4ulymal337w5jm3rtz7n7d7syr4oyyjjxfexfhja.py
I1119 11:51:04.691000 290932 torch/_inductor/triton_bundler.py:197] [0/0] Saving 11 statically launchable CachingAutotuners
V1119 11:51:04.691000 290932 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1119 11:51:04.692000 290932 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1119 11:51:04.692000 290932 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fj3hn373xhya3ciptis4wu3x642nfqk24y3x3ivgwx4whlt7jnxf
V1119 11:51:04.696000 290932 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 1.464s
I1119 11:51:04.697000 290932 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
I1119 11:51:04.697000 290932 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
I1119 11:51:04.698000 290932 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
I1119 11:51:04.698000 290932 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
V1119 11:51:04.742000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_0, get:
V1119 11:51:04.742000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.743000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.743000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_dynamic_range_139901219850496_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 20, nspill 8, #shared-mem 0
V1119 11:51:04.744000 290932 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/ql/5715f3fe629e48108eb3a92deed58b696294436a05c179890827a661ed4d7469.best_config
V1119 11:51:04.761000 290932 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_dynamic_range_139901219850496_1, get:
V1119 11:51:04.761000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005408, nreg 16, nspill 0, #shared-mem 0
V1119 11:51:04.762000 290932 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005376, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.762000 290932 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_dynamic_range_139901219850496_1: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005376, nreg 14, nspill 0, #shared-mem 0
V1119 11:51:04.763000 290932 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/jn/dea68d43bbf1dc95736af3afe1e8257fba70cba09e4c27e854639843c19b93ee.best_config
Running test on device: cuda

=== Verifying implementations produce expected results ===
  ✓ short_impl correct for seq_len=256
  ✓ medium_impl correct for seq_len=256
  ✓ long_impl correct for seq_len=256
  ✓ short_impl correct for seq_len=1024
  ✓ medium_impl correct for seq_len=1024
  ✓ long_impl correct for seq_len=1024
  ✓ short_impl correct for seq_len=4096
  ✓ medium_impl correct for seq_len=4096
  ✓ long_impl correct for seq_len=4096

=== Testing autotuning with compilation ===
Traceback (most recent call last):
  File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 154, in <module>
    test_dynamic_range_tuning()
  File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 123, in test_dynamic_range_tuning
    torch.testing.assert_close(
  File "/data/users/tianren/pytorch/torch/testing/_comparison.py", line 1600, in assert_close
    raise error_metas[0].to_error(msg)
AssertionError: Compiled result differs from expected
I1119 11:51:08.259000 290932 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1119 11:51:08.259000 290932 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 4, put: 11, exception: 0}
I1119 11:51:08.259000 290932 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 4, put: 11, exception: 0}
I1119 11:51:08.259000 290932 torch/_inductor/remote_cache.py:432] 
