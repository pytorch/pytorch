
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************

======================================================================
          COLLECTIVE OP AUTOTUNING - SIMPLE TEST
======================================================================

============================================================
TEST 1: Collective Op Detection
============================================================

======================================================================
          COLLECTIVE OP AUTOTUNING - SIMPLE TEST
======================================================================

============================================================
TEST 1: Collective Op Detection
============================================================
I1123 00:00:19.143000 1738822 torch/_inductor/config.py:1014] compile_threads set to 32
I1123 00:00:19.146000 1738823 torch/_inductor/config.py:1014] compile_threads set to 32
[PASS] torch.ops._c10d_functional.all_reduce_.default: True (expected True)
[PASS] torch.ops._c10d_functional.all_gather_into_tensor.default: True (expected True)
[PASS] torch.ops._c10d_functional.reduce_scatter_tensor.default: True (expected True)
[PASS] torch.ops.aten.matmul.default: False (expected False)
[PASS] torch.ops.aten.add.Tensor: False (expected False)

[PASS] All detection tests passed!

============================================================
TEST 2: Simple AllReduce (No Autotuning)
============================================================
[PASS] torch.ops._c10d_functional.all_reduce_.default: True (expected True)
[PASS] torch.ops._c10d_functional.all_gather_into_tensor.default: True (expected True)
[PASS] torch.ops._c10d_functional.reduce_scatter_tensor.default: True (expected True)
[PASS] torch.ops.aten.matmul.default: False (expected False)
[PASS] torch.ops.aten.add.Tensor: False (expected False)

[PASS] All detection tests passed!

============================================================
TEST 2: Simple AllReduce (No Autotuning)
============================================================
[Rank 1] Device: cuda:1, World Size: 2[Rank 0] Device: cuda:0, World Size: 2

[Rank 0] Input tensor sum: 16384.0
[Rank 1] Input tensor sum: 32768.0
NCCL version 2.27.5+cuda12.4
[Rank 0] Result sum: 49152.0, Expected: 49152
[Rank 0] [PASS] AllReduce result correct!

============================================================
TEST 3: Custom Collective Op with Autotuning
============================================================
[Rank 0] Starting custom op autotuning test...
[Rank 1] Result sum: 49152.0, Expected: 49152
[Rank 1] [PASS] AllReduce result correct!

============================================================
TEST 3: Custom Collective Op with Autotuning
============================================================
[Rank 1] Starting custom op autotuning test...
[rank1]:I1123 00:00:23.342000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
[rank1]:I1123 00:00:23.343000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
[rank1]:I1123 00:00:23.344000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
[rank1]:I1123 00:00:23.344000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
[rank1]:I1123 00:00:23.344000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
[rank1]:I1123 00:00:23.344000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
[rank1]:I1123 00:00:23.344000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
[rank1]:I1123 00:00:23.348000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
[rank1]:I1123 00:00:23.349000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
[rank1]:I1123 00:00:23.350000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
[rank1]:I1123 00:00:23.351000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
[rank1]:I1123 00:00:23.352000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
[rank1]:I1123 00:00:23.352000 1738823 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
[rank0]:I1123 00:00:23.361000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
[rank0]:I1123 00:00:23.362000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
[rank0]:I1123 00:00:23.363000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
[rank0]:I1123 00:00:23.366000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.366000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.366000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
[rank0]:I1123 00:00:23.366000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.367000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
[rank0]:I1123 00:00:23.368000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
[rank0]:I1123 00:00:23.369000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
[rank0]:I1123 00:00:23.370000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
[rank0]:I1123 00:00:23.370000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
[rank0]:I1123 00:00:23.370000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
[rank0]:I1123 00:00:23.370000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
[rank0]:I1123 00:00:23.370000 1738822 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
[Rank 1] Registering autotuning configs...
[Rank 1] Compiling model...
[rank1]:W1123 00:00:23.409000 1738823 torch/_logging/_internal.py:475] Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs
[Rank 0] Registering autotuning configs...
[Rank 0] Compiling model...
[rank0]:W1123 00:00:23.428000 1738822 torch/_logging/_internal.py:475] Using TORCH_LOGS environment variable for log settings, ignoring call to set_logs
[Rank 1] Input sum: 32768.0
[Rank 1] Running model (should trigger autotuning)...
[Rank 0] Input sum: 16384.0
[Rank 0] Running model (should trigger autotuning)...
[rank1]:I1123 00:00:23.512000 1738823 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
[rank0]:I1123 00:00:23.531000 1738822 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
[rank1]:I1123 00:00:23.607000 1738823 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[128, 128][128, 1]cuda:1"):
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test_simple_collective.py:161 in forward, code: return my_allreduce(x)
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         my_allreduce_default: "f32[128, 128][128, 1]cuda:1" = torch.ops.test.my_allreduce.default(l_x_);  l_x_ = None
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (my_allreduce_default,)
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
[rank1]:V1123 00:00:23.609000 1738823 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
[rank0]:I1123 00:00:23.633000 1738822 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[128, 128][128, 1]cuda:0"):
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test_simple_collective.py:161 in forward, code: return my_allreduce(x)
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         my_allreduce_default: "f32[128, 128][128, 1]cuda:0" = torch.ops.test.my_allreduce.default(l_x_);  l_x_ = None
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (my_allreduce_default,)
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
[rank0]:V1123 00:00:23.635000 1738822 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
[rank1]:V1123 00:00:24.162000 1738823 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fnpq7ffgscomjfqzejakpnmvmsjcvux5ilpelbgpr7smwwvzdqvw:
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [bjljkz6h67e44ug6xyibvgaf55shwwi3nho2vjxykyrykk4jjiv] gm: <lambda>()
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1):
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0]     my_allreduce = torch.ops.test.my_allreduce.default(arg0_1);  arg0_1 = None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0]     return (my_allreduce,)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0]     
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [muenm5ytmzjq5zkd475wowlzvjjua4mgw2eeyedktft7vmubhdk] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128, 128]), stride=(128, 1), device=device(type='cuda', index=1), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [kb5ddwiol5uxnrfap7yzzfbcgw532jks72hgxcrdjgco2qta733] torch_version: ����Dw�H�# ���gݭ��j4Ĭ5��æ
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_reduces_fx]: none
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_reduces_fx_bucket_size_determinator]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[collective_benchmark_nruns]: 10
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [5uk25jozsokfxmtekuawzrutfda644f563ena2lt6lv3xraleqh] inductor_config[collective_benchmark_timeout]: 30.0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_memory_increase_gb]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_memory_increase_ratio]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[wrap_inductor_compiled_regions]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_32bit_indexing]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cutlass.compile_opt_level]: -O1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.enable_debug_info]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.use_fast_math]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_max_profiling_configs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cutlass.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_epilogue_fusion_enabled]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_tma_only]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cutlass.cutlass_backend_min_gemm_size]: 1
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.generate_test_runner]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_op_allowlist_regex]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_op_denylist_regex]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cutlass.cutlass_instantiation_level]: 0
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_hash_with_compile_cmd]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.cutlass_prescreening]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cutlass.cutlass_enabled_ops]: all
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.use_binary_remote_cache]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.upload_to_binary_remote_cache]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.binary_remote_cache_force_write]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.enable_caching_codegen]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[xpu.arch]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[xpu.version]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [ksp2oohdz6t6g6blx6ukql4tyhznue4jedscfvuqmtwxlhv6gcf] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cutlass.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
[rank1]:V1123 00:00:24.167000 1738823 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
[rank1]:V1123 00:00:24.168000 1738823 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fnpq7ffgscomjfqzejakpnmvmsjcvux5ilpelbgpr7smwwvzdqvw
[rank1]:I1123 00:00:24.168000 1738823 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fnpq7ffgscomjfqzejakpnmvmsjcvux5ilpelbgpr7smwwvzdqvw
[rank1]:V1123 00:00:24.168000 1738823 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
[rank1]:V1123 00:00:24.168000 1738823 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
[rank1]:I1123 00:00:24.169000 1738823 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[128, 128][128, 1]cuda:1"):
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test_simple_collective.py:161 in forward, code: return my_allreduce(x)
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         my_allreduce: "f32[128, 128][128, 1]cuda:1" = torch.ops.test.my_allreduce.default(arg0_1);  arg0_1 = None
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (my_allreduce,)
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
[rank1]:V1123 00:00:24.185000 1738823 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
[rank1]:V1123 00:00:24.187000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank1]:V1123 00:00:24.187000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %my_allreduce : [num_users=1] = call_function[target=torch.ops.test.my_allreduce.default](args = (%arg0_1,), kwargs = {}) 
[rank1]:V1123 00:00:24.188000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function test::my_allreduce at 0x7f31a80b9da0>
[rank0]:V1123 00:00:24.196000 1738822 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
[rank1]:V1123 00:00:24.198000 1738823 torch/_inductor/select_algorithm.py:2750] [0/0] Max autotune selects from 3 choices.
[rank1]:V1123 00:00:24.198000 1738823 torch/_inductor/select_algorithm.py:3145] [0/0] Starting precompilation
[rank1]:I1123 00:00:24.199000 1738823 torch/_inductor/select_algorithm.py:3189] [0/0] Multithreaded precompilation for 3 choices using 3 worker threads
[rank1]:V1123 00:00:24.199000 1738823 torch/_inductor/select_algorithm.py:3259] [0/0] Waiting on futures
[rank1]:V1123 00:00:24.200000 1738823 torch/_inductor/select_algorithm.py:2980] [0/0] Precompilation elapsed time: 0.00s
[rank1]:V1123 00:00:24.200000 1738823 torch/_inductor/select_algorithm.py:2908] [0/0] Starting autotuning
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fp5akivda77vkdhsqnb3bgdi3kr2fmszivwojdtyx2oy6khjqrd3:
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [bjljkz6h67e44ug6xyibvgaf55shwwi3nho2vjxykyrykk4jjiv] gm: <lambda>()
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1):
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0]     my_allreduce = torch.ops.test.my_allreduce.default(arg0_1);  arg0_1 = None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0]     return (my_allreduce,)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0]     
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [bgjeodkdb4p2yn4yhelrjxuzh5aw6jow2kkcdpw4tvjnxff4arq] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128, 128]), stride=(128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [kb5ddwiol5uxnrfap7yzzfbcgw532jks72hgxcrdjgco2qta733] torch_version: ����Dw�H�# ���gݭ��j4Ĭ5��æ
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_reduces_fx]: none
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_reduces_fx_bucket_size_determinator]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[collective_benchmark_nruns]: 10
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [5uk25jozsokfxmtekuawzrutfda644f563ena2lt6lv3xraleqh] inductor_config[collective_benchmark_timeout]: 30.0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_memory_increase_gb]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_memory_increase_ratio]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[wrap_inductor_compiled_regions]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_32bit_indexing]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cutlass.compile_opt_level]: -O1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.enable_debug_info]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.use_fast_math]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_max_profiling_configs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cutlass.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_epilogue_fusion_enabled]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_tma_only]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cutlass.cutlass_backend_min_gemm_size]: 1
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.generate_test_runner]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_op_allowlist_regex]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cutlass.cutlass_op_denylist_regex]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cutlass.cutlass_instantiation_level]: 0
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.cutlass_hash_with_compile_cmd]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.cutlass_prescreening]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cutlass.cutlass_enabled_ops]: all
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.use_binary_remote_cache]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.upload_to_binary_remote_cache]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutlass.binary_remote_cache_force_write]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cutlass.enable_caching_codegen]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[xpu.arch]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[xpu.version]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [ksp2oohdz6t6g6blx6ukql4tyhznue4jedscfvuqmtwxlhv6gcf] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cutlass.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
[rank0]:V1123 00:00:24.201000 1738822 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fp5akivda77vkdhsqnb3bgdi3kr2fmszivwojdtyx2oy6khjqrd3
/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py:3370: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  current_size = base.storage().size()
[rank0]:I1123 00:00:24.202000 1738822 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fp5akivda77vkdhsqnb3bgdi3kr2fmszivwojdtyx2oy6khjqrd3
[rank0]:V1123 00:00:24.202000 1738822 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
[rank0]:V1123 00:00:24.202000 1738822 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
[rank1]:V1123 00:00:24.202000 1738823 torch/_inductor/select_algorithm.py:3630] [0/0] Using collective benchmarking for 3 choices on rank 1
[rank0]:I1123 00:00:24.202000 1738822 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[128, 128][128, 1]cuda:0"):
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test_simple_collective.py:161 in forward, code: return my_allreduce(x)
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         my_allreduce: "f32[128, 128][128, 1]cuda:0" = torch.ops.test.my_allreduce.default(arg0_1);  arg0_1 = None
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (my_allreduce,)
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
[rank0]:V1123 00:00:24.219000 1738822 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
[rank0]:V1123 00:00:24.221000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank0]:V1123 00:00:24.222000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %my_allreduce : [num_users=1] = call_function[target=torch.ops.test.my_allreduce.default](args = (%arg0_1,), kwargs = {}) 
[rank0]:V1123 00:00:24.222000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function test::my_allreduce at 0x7f274f4adda0>
[rank0]:V1123 00:00:24.232000 1738822 torch/_inductor/select_algorithm.py:2750] [0/0] Max autotune selects from 3 choices.
[rank0]:V1123 00:00:24.233000 1738822 torch/_inductor/select_algorithm.py:3145] [0/0] Starting precompilation
[rank0]:I1123 00:00:24.233000 1738822 torch/_inductor/select_algorithm.py:3189] [0/0] Multithreaded precompilation for 3 choices using 3 worker threads
[rank0]:V1123 00:00:24.234000 1738822 torch/_inductor/select_algorithm.py:3259] [0/0] Waiting on futures
[rank0]:V1123 00:00:24.234000 1738822 torch/_inductor/select_algorithm.py:2980] [0/0] Precompilation elapsed time: 0.00s
[rank0]:V1123 00:00:24.234000 1738822 torch/_inductor/select_algorithm.py:2908] [0/0] Starting autotuning
/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py:3370: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  current_size = base.storage().size()
[rank0]:V1123 00:00:24.237000 1738822 torch/_inductor/select_algorithm.py:3630] [0/0] Using collective benchmarking for 3 choices on rank 0
/data/users/tianren/pytorch/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank0]:V1123 00:00:24.241000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank1]:V1123 00:00:24.241000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank0]:V1123 00:00:24.241000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank0]:V1123 00:00:24.242000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f252818f1a0>
[rank1]:V1123 00:00:24.242000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank1]:V1123 00:00:24.242000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f2f1cd7b1a0>
[rank1]:V1123 00:00:24.260000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank1]:V1123 00:00:24.260000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f2f1ce37d80>
[rank0]:V1123 00:00:24.260000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank0]:V1123 00:00:24.261000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f252824fd80>
[rank1]:V1123 00:00:24.263000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f2f1cd9e840>
[rank1]:V1123 00:00:24.264000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank1]:V1123 00:00:24.264000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2f1ce54d60>
[rank0]:V1123 00:00:24.264000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f25281b6840>
[rank0]:V1123 00:00:24.265000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank0]:V1123 00:00:24.265000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2528268d60>
[rank1]:V1123 00:00:24.265000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank1]:V1123 00:00:24.265000 1738823 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank0]:V1123 00:00:24.266000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank0]:V1123 00:00:24.266000 1738822 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=wait_tensor,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf1,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf2', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.272000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf3,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf4', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.273000 1738823 torch/_inductor/scheduler.py:3079] [0/0] scheduling output benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=wait_tensor,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.273000 1738822 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank1]:V1123 00:00:24.274000 1738823 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf3
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf1,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf2', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf3,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf4', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.274000 1738822 torch/_inductor/scheduler.py:3079] [0/0] scheduling output benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0
[rank0]:V1123 00:00:24.275000 1738822 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf3
[rank1]:I1123 00:00:24.278000 1738823 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank1]:I1123 00:00:24.278000 1738823 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank1]:I1123 00:00:24.279000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank1]:I1123 00:00:24.279000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank1]:I1123 00:00:24.279000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank0]:I1123 00:00:24.279000 1738822 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank0]:I1123 00:00:24.280000 1738822 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank0]:I1123 00:00:24.280000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank0]:I1123 00:00:24.281000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank0]:I1123 00:00:24.281000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank1]:V1123 00:00:24.292000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op0 with estimated runtime 0.000054
[rank0]:V1123 00:00:24.293000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op0 with estimated runtime 0.000054
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0] graph():
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, %get_index), kwargs = {})
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, %get_index_1, %load, None), kwargs = {})
[rank1]:V1123 00:00:24.297000 1738823 torch/_inductor/bounds.py:81] [0/0]     return store
[rank1]:V1123 00:00:24.298000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0] graph():
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, %get_index), kwargs = {})
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, %get_index_1, %load, None), kwargs = {})
[rank0]:V1123 00:00:24.298000 1738822 torch/_inductor/bounds.py:81] [0/0]     return store
[rank1]:V1123 00:00:24.299000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.300000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.300000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.305000 1738823 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank0]:V1123 00:00:24.306000 1738822 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank0]:V1123 00:00:24.619000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op1 with estimated runtime 0.008496
[rank0]:V1123 00:00:24.620000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op2 with estimated runtime 0.000000
[rank1]:V1123 00:00:24.620000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op1 with estimated runtime 0.008496
[rank1]:V1123 00:00:24.621000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_op2 with estimated runtime 0.000000
[rank0]:V1123 00:00:24.621000 1738822 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank1]:V1123 00:00:24.621000 1738823 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ex/cex6raoykvwv7khoijkiv3dag4exe7pdga2pchbryrvvghefmjvy.py
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, = args
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, (128, 128), (128, 1))
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(0)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream0 = get_raw_stream(0)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, 16384, stream=stream0)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, 'sum', 'default')
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, )
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:0', dtype=torch.float32)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1])
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank0]:V1123 00:00:24.624000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.625000 1738822 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/hv/chvkmpoic7h3jiiqjohys6sakqnf7kqanhfubhld5cqqomvsqmug.py
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/lf/clf4whuc2k5pv573n76lwfoatpj4n46qq62mud6umul2tipavj2a.py
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=1, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, = args
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, (128, 128), (128, 1))
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(1):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(1)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream1 = get_raw_stream(1)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1, benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, 16384, stream=stream1)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, 'sum', 'default')
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_buf0, )
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:1', dtype=torch.float32)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([benchmark_test_my_allreduce_autotuned_allreduce_impl1_0_arg0_1])
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank1]:V1123 00:00:24.625000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.626000 1738823 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ew/cewpiqre6j3cxuzgvjwq4dkzsvdpmua6ycla6tkg5j5ihjtizeb2.py
[rank0]:V1123 00:00:24.631000 1738822 torch/_inductor/runtime/triton_heuristics.py:315] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_my_allreduce_0
[rank0]:V1123 00:00:24.631000 1738822 torch/_inductor/runtime/triton_heuristics.py:321] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
[rank0]:V1123 00:00:24.631000 1738822 torch/_inductor/runtime/triton_heuristics.py:321] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
[rank0]:V1123 00:00:24.631000 1738822 torch/_inductor/runtime/triton_heuristics.py:330] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
[rank1]:V1123 00:00:24.631000 1738823 torch/_inductor/runtime/triton_heuristics.py:315] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_my_allreduce_0
[rank1]:V1123 00:00:24.632000 1738823 torch/_inductor/runtime/triton_heuristics.py:321] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
[rank1]:V1123 00:00:24.632000 1738823 torch/_inductor/runtime/triton_heuristics.py:321] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
[rank1]:V1123 00:00:24.632000 1738823 torch/_inductor/runtime/triton_heuristics.py:330] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/1
[rank1]:V1123 00:00:24.708000 1738823 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/ew/cewpiqre6j3cxuzgvjwq4dkzsvdpmua6ycla6tkg5j5ihjtizeb2.py
[rank1]:I1123 00:00:24.708000 1738823 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/ew/cewpiqre6j3cxuzgvjwq4dkzsvdpmua6ycla6tkg5j5ihjtizeb2.py
[rank0]:V1123 00:00:24.715000 1738822 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/hv/chvkmpoic7h3jiiqjohys6sakqnf7kqanhfubhld5cqqomvsqmug.py
[rank0]:I1123 00:00:24.715000 1738822 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/hv/chvkmpoic7h3jiiqjohys6sakqnf7kqanhfubhld5cqqomvsqmug.py
[rank1]:V1123 00:00:24.725000 1738823 torch/_inductor/runtime/triton_heuristics.py:1075] [0/0] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank1]:V1123 00:00:24.725000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005376, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.726000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005408, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.726000 1738823 torch/_inductor/runtime/triton_heuristics.py:1113] [0/0] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005376, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.726000 1738823 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/lf/dfdc8b7b1e2110eacdb1f003da52f71bc3106210a88b495eea946d1f145d887e.best_config
[rank0]:V1123 00:00:24.732000 1738822 torch/_inductor/runtime/triton_heuristics.py:1075] [0/0] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank0]:V1123 00:00:24.733000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005408, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.733000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.733000 1738822 torch/_inductor/runtime/triton_heuristics.py:1113] [0/0] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005408, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.733000 1738822 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ex/9cbccc4a3e20bf6e3b54db191606b87210503fe2304850ff768db9646fb2e8fb.best_config
/data/users/tianren/pytorch/torch/distributed/c10d_logger.py:83: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  return func(*args, **kwargs)
[rank1]:V1123 00:00:24.759000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank0]:V1123 00:00:24.759000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank0]:V1123 00:00:24.761000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank1]:V1123 00:00:24.761000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank1]:V1123 00:00:24.763000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank0]:V1123 00:00:24.763000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank1]:V1123 00:00:24.764000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank0]:V1123 00:00:24.764000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank1]:V1123 00:00:24.764000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank0]:V1123 00:00:24.764000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank1]:V1123 00:00:24.765000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank0]:V1123 00:00:24.765000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank0]:V1123 00:00:24.766000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank1]:V1123 00:00:24.766000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank0]:V1123 00:00:24.768000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank1]:V1123 00:00:24.768000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank0]:V1123 00:00:24.768000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank1]:V1123 00:00:24.769000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank0]:V1123 00:00:24.770000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank1]:V1123 00:00:24.770000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank1]:I1123 00:00:24.772000 1738823 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_allreduce_impl1_0: 0.11 ms
[rank0]:I1123 00:00:24.772000 1738822 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_allreduce_impl1_0: 0.11 ms
[rank0]:V1123 00:00:24.773000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank1]:V1123 00:00:24.773000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank0]:V1123 00:00:24.774000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank1]:V1123 00:00:24.774000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank0]:V1123 00:00:24.774000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f252818f1a0>
[rank1]:V1123 00:00:24.774000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f2f1cd7b1a0>
[rank0]:V1123 00:00:24.775000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank1]:V1123 00:00:24.775000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank0]:V1123 00:00:24.775000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f252824fd80>
[rank1]:V1123 00:00:24.775000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f2f1ce37d80>
[rank0]:V1123 00:00:24.776000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f25281b6840>
[rank1]:V1123 00:00:24.776000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f2f1cd9e840>
[rank0]:V1123 00:00:24.777000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank1]:V1123 00:00:24.777000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank0]:V1123 00:00:24.777000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2528268d60>
[rank1]:V1123 00:00:24.777000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2f1ce54d60>
[rank0]:V1123 00:00:24.778000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank1]:V1123 00:00:24.778000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank0]:V1123 00:00:24.778000 1738822 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank1]:V1123 00:00:24.778000 1738823 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=wait_tensor,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.780000 1738822 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=wait_tensor,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.780000 1738823 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf1,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf2', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf1,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf2', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf3,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf4', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf3,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=wait_tensor,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf4', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.781000 1738822 torch/_inductor/scheduler.py:3079] [0/0] scheduling output benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0
[rank1]:V1123 00:00:24.781000 1738823 torch/_inductor/scheduler.py:3079] [0/0] scheduling output benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0
[rank0]:V1123 00:00:24.782000 1738822 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf3
[rank1]:V1123 00:00:24.782000 1738823 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf3
[rank0]:I1123 00:00:24.784000 1738822 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank1]:I1123 00:00:24.785000 1738823 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank0]:I1123 00:00:24.785000 1738822 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank1]:I1123 00:00:24.785000 1738823 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank0]:I1123 00:00:24.785000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank1]:I1123 00:00:24.785000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank0]:I1123 00:00:24.785000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank0]:I1123 00:00:24.786000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank1]:I1123 00:00:24.786000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank1]:I1123 00:00:24.786000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank0]:V1123 00:00:24.788000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op0 with estimated runtime 0.000054
[rank1]:V1123 00:00:24.788000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op0 with estimated runtime 0.000054
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0] graph():
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, %get_index), kwargs = {})
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, %get_index_1, %load, None), kwargs = {})
[rank0]:V1123 00:00:24.789000 1738822 torch/_inductor/bounds.py:81] [0/0]     return store
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0] graph():
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, %get_index), kwargs = {})
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, %get_index_1, %load, None), kwargs = {})
[rank1]:V1123 00:00:24.789000 1738823 torch/_inductor/bounds.py:81] [0/0]     return store
[rank0]:V1123 00:00:24.790000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.790000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.790000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.790000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.791000 1738822 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank1]:V1123 00:00:24.791000 1738823 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank0]:V1123 00:00:24.791000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op1 with estimated runtime 0.008496
[rank0]:V1123 00:00:24.792000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op2 with estimated runtime 0.000000
[rank1]:V1123 00:00:24.792000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op1 with estimated runtime 0.008496
[rank0]:V1123 00:00:24.792000 1738822 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank1]:V1123 00:00:24.792000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_op2 with estimated runtime 0.000000
[rank1]:V1123 00:00:24.792000 1738823 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ex/cex6raoykvwv7khoijkiv3dag4exe7pdga2pchbryrvvghefmjvy.py
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, = args
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, (128, 128), (128, 1))
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(0)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream0 = get_raw_stream(0)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, 16384, stream=stream0)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, 'sum', 'default')
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, )
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:0', dtype=torch.float32)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1])
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/lf/clf4whuc2k5pv573n76lwfoatpj4n46qq62mud6umul2tipavj2a.py
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=1, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, = args
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, (128, 128), (128, 1))
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(1):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(1)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream1 = get_raw_stream(1)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1, benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, 16384, stream=stream1)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, 'sum', 'default')
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_buf0, )
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:1', dtype=torch.float32)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([benchmark_test_my_allreduce_autotuned_allreduce_impl2_1_arg0_1])
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.794000 1738822 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/sy/csyyhgjpdsbf2yturwrtb57dhqyhvtsp73bmhgeb3up3qcpmvu3e.py
[rank1]:V1123 00:00:24.794000 1738823 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/3g/c3gctgjym2sqgjw7e6pfz7eczd7sk4f2ea3gtbicona7jsr7ysjt.py
[rank0]:V1123 00:00:24.796000 1738822 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/sy/csyyhgjpdsbf2yturwrtb57dhqyhvtsp73bmhgeb3up3qcpmvu3e.py
[rank0]:I1123 00:00:24.796000 1738822 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/sy/csyyhgjpdsbf2yturwrtb57dhqyhvtsp73bmhgeb3up3qcpmvu3e.py
[rank1]:V1123 00:00:24.797000 1738823 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/3g/c3gctgjym2sqgjw7e6pfz7eczd7sk4f2ea3gtbicona7jsr7ysjt.py
[rank1]:I1123 00:00:24.797000 1738823 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/3g/c3gctgjym2sqgjw7e6pfz7eczd7sk4f2ea3gtbicona7jsr7ysjt.py
[rank0]:V1123 00:00:24.813000 1738822 torch/_inductor/runtime/triton_heuristics.py:1075] [0/0] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank0]:V1123 00:00:24.813000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.813000 1738823 torch/_inductor/runtime/triton_heuristics.py:1075] [0/0] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank0]:V1123 00:00:24.813000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.813000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.813000 1738822 torch/_inductor/runtime/triton_heuristics.py:1113] [0/0] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.813000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] [0/0] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.814000 1738823 torch/_inductor/runtime/triton_heuristics.py:1113] [0/0] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.814000 1738822 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ex/9cbccc4a3e20bf6e3b54db191606b87210503fe2304850ff768db9646fb2e8fb.best_config
[rank1]:V1123 00:00:24.814000 1738823 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/lf/dfdc8b7b1e2110eacdb1f003da52f71bc3106210a88b495eea946d1f145d887e.best_config
[rank0]:V1123 00:00:24.842000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank1]:V1123 00:00:24.842000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank1]:V1123 00:00:24.843000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank0]:V1123 00:00:24.843000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank1]:V1123 00:00:24.845000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank0]:V1123 00:00:24.845000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank1]:V1123 00:00:24.846000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank0]:V1123 00:00:24.846000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank1]:V1123 00:00:24.848000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank0]:V1123 00:00:24.848000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank1]:V1123 00:00:24.848000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank0]:V1123 00:00:24.848000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank1]:V1123 00:00:24.850000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank0]:V1123 00:00:24.850000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank1]:V1123 00:00:24.852000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank0]:V1123 00:00:24.852000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank1]:V1123 00:00:24.852000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank0]:V1123 00:00:24.852000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank0]:V1123 00:00:24.854000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank1]:V1123 00:00:24.854000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank0]:I1123 00:00:24.856000 1738822 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_allreduce_impl2_1: 0.15 ms
[rank1]:I1123 00:00:24.856000 1738823 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_allreduce_impl2_1: 0.15 ms
[rank0]:V1123 00:00:24.889000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank1]:V1123 00:00:24.889000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 1/10
[rank1]:V1123 00:00:24.890000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank0]:V1123 00:00:24.890000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 2/10
[rank0]:V1123 00:00:24.891000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank1]:V1123 00:00:24.892000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 3/10
[rank1]:V1123 00:00:24.892000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank0]:V1123 00:00:24.892000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 4/10
[rank0]:V1123 00:00:24.894000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank1]:V1123 00:00:24.894000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 5/10
[rank0]:V1123 00:00:24.894000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank1]:V1123 00:00:24.894000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 6/10
[rank0]:V1123 00:00:24.895000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank1]:V1123 00:00:24.895000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 7/10
[rank1]:V1123 00:00:24.896000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank0]:V1123 00:00:24.896000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 8/10
[rank1]:V1123 00:00:24.896000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank0]:V1123 00:00:24.896000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 9/10
[rank0]:V1123 00:00:24.898000 1738822 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank1]:V1123 00:00:24.898000 1738823 torch/_inductor/select_algorithm.py:3535] [0/0] Benchmark run 10/10
[rank1]:I1123 00:00:24.899000 1738823 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_fallback_default: 0.16 ms
[rank0]:I1123 00:00:24.899000 1738822 torch/_inductor/select_algorithm.py:3593] [0/0] Collective benchmark for test::my_allreduce_autotuned_fallback_default: 0.16 ms
Autotune Choices Stats:
{"num_choices": 3, "num_triton_choices": 0, "best_kernel": "test::my_allreduce_autotuned_allreduce_impl1_0", "best_kernel_desc": "CustomOp allreduce_impl1", "best_time": 0.11365760117769241}
Autotune Choices Stats:
{"num_choices": 3, "num_triton_choices": 0, "best_kernel": "test::my_allreduce_autotuned_allreduce_impl1_0", "best_kernel_desc": "CustomOp allreduce_impl1", "best_time": 0.11365760117769241}
[rank1]:V1123 00:00:24.899000 1738823 torch/_inductor/select_algorithm.py:3068] [0/0] Autotuning elapsed time: 0.70s
[rank0]:V1123 00:00:24.900000 1738822 torch/_inductor/select_algorithm.py:3068] [0/0] Autotuning elapsed time: 0.67s
[rank1]:W1123 00:00:24.900000 1738823 torch/_inductor/select_algorithm.py:4006] [0/0] [COLLECTIVE AUTOTUNING] All timings:
[rank1]:W1123 00:00:24.900000 1738823 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_allreduce_impl1_0: 0.113658 ms ← SELECTED
[rank0]:W1123 00:00:24.900000 1738822 torch/_inductor/select_algorithm.py:4006] [0/0] [COLLECTIVE AUTOTUNING] All timings:
[rank1]:W1123 00:00:24.900000 1738823 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_allreduce_impl2_1: 0.150957 ms 
[rank0]:W1123 00:00:24.900000 1738822 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_allreduce_impl1_0: 0.113658 ms ← SELECTED
[rank1]:W1123 00:00:24.900000 1738823 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_fallback_default: 0.157398 ms 
[rank0]:W1123 00:00:24.900000 1738822 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_allreduce_impl2_1: 0.150957 ms 
[rank0]:W1123 00:00:24.900000 1738822 torch/_inductor/select_algorithm.py:4009] [0/0]   - test::my_allreduce_autotuned_fallback_default: 0.157398 ms 
[rank1]:V1123 00:00:24.901000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank0]:V1123 00:00:24.901000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
[rank1]:V1123 00:00:24.901000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank0]:V1123 00:00:24.902000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank1]:V1123 00:00:24.902000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f2f1cd7b1a0>
[rank0]:V1123 00:00:24.902000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f252818f1a0>
[rank1]:V1123 00:00:24.902000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank0]:V1123 00:00:24.902000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank1]:V1123 00:00:24.902000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f2f1ce37d80>
[rank0]:V1123 00:00:24.902000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f252824fd80>
[rank1]:V1123 00:00:24.904000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f2f1cd9e840>
[rank0]:V1123 00:00:24.904000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f25281b6840>
[rank1]:V1123 00:00:24.904000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank0]:V1123 00:00:24.904000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank1]:V1123 00:00:24.904000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2f1ce54d60>
[rank0]:V1123 00:00:24.904000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2528268d60>
[rank1]:V1123 00:00:24.905000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering return wait_tensor 
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0] Autotuning selected choice: TensorBox(StorageBox(
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]   SubgraphBuffer(
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     python_kernel_name=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     name=buf0,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     layout=FixedLayout('cuda:1', torch.float32, size=torch.Size([128, 128]), stride=(128, 1)),
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     inputs=[TensorBox(StorageBox(
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]))
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     ))],
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     constant_args=(),
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     kwargs={},
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     output_view=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     python_kernel_name=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     cpp_kernel_name=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     ordered_kwargs_for_cpp_kernel=(),
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     op_overload=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     arg_properties=[{}],
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     allarg_properties={},
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     kwarg_properties=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     unbacked_bindings={},
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     mutation_outputs=[],
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     origin_node=None,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     origins=OrderedSet([my_allreduce]),
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     ,
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]     }
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0]   )
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/select_algorithm.py:2869] [0/0] ))
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0] Autotuning selected choice: TensorBox(StorageBox(
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]   SubgraphBuffer(
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     python_kernel_name=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     name=buf0,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([128, 128]), stride=(128, 1)),
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     inputs=[TensorBox(StorageBox(
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]))
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     ))],
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     constant_args=(),
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     kwargs={},
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     output_view=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     python_kernel_name=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     cpp_kernel_name=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     ordered_kwargs_for_cpp_kernel=(),
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     op_overload=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     arg_properties=[{}],
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     allarg_properties={},
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     kwarg_properties=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     unbacked_bindings={},
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     mutation_outputs=[],
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     origin_node=None,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     origins=OrderedSet([my_allreduce]),
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     ,
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]     }
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0]   )
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/select_algorithm.py:2869] [0/0] ))
[rank1]:V1123 00:00:24.906000 1738823 torch/_inductor/kernel/custom_op.py:354] [0/0] Inlining winning choice: test::my_allreduce_autotuned_allreduce_impl1_0 (name=test::my_allreduce_autotuned)
[rank0]:V1123 00:00:24.906000 1738822 torch/_inductor/kernel/custom_op.py:354] [0/0] Inlining winning choice: test::my_allreduce_autotuned_allreduce_impl1_0 (name=test::my_allreduce_autotuned)
[rank1]:V1123 00:00:24.907000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank0]:V1123 00:00:24.907000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%arg0_1,), kwargs = {}) 
[rank1]:V1123 00:00:24.907000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f2f1cd7b1a0>
[rank0]:V1123 00:00:24.907000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function clone at 0x7f252818f1a0>
[rank1]:V1123 00:00:24.907000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank0]:V1123 00:00:24.908000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %all_reduce_ : [num_users=1] = call_function[target=torch.ops._c10d_functional.all_reduce_.default](args = (%clone, sum, default), kwargs = {}) 
[rank1]:V1123 00:00:24.908000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f2f1ce37d80>
[rank0]:V1123 00:00:24.908000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._all_reduce_ at 0x7f252824fd80>
[rank1]:V1123 00:00:24.909000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f2f1cd9e840>
[rank0]:V1123 00:00:24.909000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function copy_ at 0x7f25281b6840>
[rank1]:V1123 00:00:24.909000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank0]:V1123 00:00:24.909000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering %wait_tensor : [num_users=1] = call_function[target=torch.ops._c10d_functional.wait_tensor.default](args = (%all_reduce_,), kwargs = {}) 
[rank1]:V1123 00:00:24.909000 1738823 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2f1ce54d60>
[rank0]:V1123 00:00:24.910000 1738822 torch/_inductor/graph.py:1315] [0/0]   via <function register_comm_lowerings.<locals>._wait_tensor at 0x7f2528268d60>
[rank1]:V1123 00:00:24.911000 1738823 torch/_inductor/graph.py:1626] [0/0] lowering return (my_allreduce,) 
[rank0]:V1123 00:00:24.911000 1738822 torch/_inductor/graph.py:1626] [0/0] lowering return (my_allreduce,) 
[rank1]:V1123 00:00:24.911000 1738823 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
[rank0]:V1123 00:00:24.911000 1738822 torch/_inductor/graph.py:1522] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling SubgraphBuffer(
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=buf0,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=FixedLayout('cuda:1', torch.float32, size=torch.Size([128, 128]), stride=(128, 1)),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[TensorBox(StorageBox(
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]))
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ))],
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=(),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{}],
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={},
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[],
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce]),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=my_allreduce,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.914000 1738823 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=buf2,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='buf1', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=my_allreduce,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='buf3', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   name=buf4,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='buf1', layout=FixedLayout('cuda:1', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=my_allreduce,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='buf5', layout=NoneLayout(device=device(type='cuda', index=1), size=[0], stride=[0]))],
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling SubgraphBuffer(
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=buf0,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([128, 128]), stride=(128, 1)),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[TensorBox(StorageBox(
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]))
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ))],
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=(),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{}],
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={},
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[],
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce]),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank1]:V1123 00:00:24.915000 1738823 torch/_inductor/scheduler.py:3079] [0/0] scheduling output buf1
[rank1]:V1123 00:00:24.916000 1738823 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: buf4
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   'cuda',
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   torch.float32,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   def inner_fn(index):
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       i0, i1 = index
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return tmp0
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ranges=[128, 128],
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=my_allreduce,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.915000 1738822 torch/_inductor/scheduler.py:3008] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
[rank1]:V1123 00:00:24.916000 1738823 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: buf0
[rank1]:V1123 00:00:24.916000 1738823 torch/_inductor/scheduler.py:3242] [0/0] removed dead operation: op0
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _AllReduce_Kernel(
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.all_reduce_.default',
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=buf2,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=my_allreduce,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=('sum', 'default'),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_all_reduce_,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.all_reduce_.default,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'input', 'type': Tensor, 'default_value': None}, {'name': 'reduce_op', 'type': str, 'default_value': None}, {'name': 'group_name', 'type': str, 'default_value': None}],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'input': {'type': Tensor, 'default_value': None}, 'reduce_op': {'type': str, 'default_value': None}, 'group_name': {'type': str, 'default_value': None}},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='buf3', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0] scheduling _WaitKernel(
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name='torch.ops._c10d_functional.wait_tensor.default',
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   name=buf4,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   inputs=[ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[128, 128], stride=[128, 1]), data=Pointwise(
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     'cuda',
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     torch.float32,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     def inner_fn(index):
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         i0, i1 = index
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         tmp0 = ops.load(arg0_1, i1 + 128 * i0)
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return tmp0
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ranges=[128, 128],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origin_node=my_allreduce,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     origins=OrderedSet([my_allreduce, all_reduce_, clone]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     stack_traces = {,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]         return my_allreduce(x),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     }
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   constant_args=(),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwargs={},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   output_view=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   python_kernel_name=torch.ops._c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   cpp_kernel_name=aoti_torch_cpu__c10d_functional_wait_tensor,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ordered_kwargs_for_cpp_kernel=[],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   op_overload=_c10d_functional.wait_tensor.default,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   arg_properties=[{'name': 'tensor', 'type': Tensor, 'default_value': None}],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   allarg_properties={'tensor': {'type': Tensor, 'default_value': None}},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   kwarg_properties=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   unbacked_bindings={},
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   mutation_outputs=[MutationOutput(name='buf5', layout=NoneLayout(device=device(type='cuda', index=0), size=[0], stride=[0]))],
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origin_node=None,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   origins=OrderedSet([my_allreduce, wait_tensor]),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   stack_traces = {,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]     File "/data/users/tianren/pytorch/test_simple_collective.py", line 161, in forward,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]       return my_allreduce(x),
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   ,
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0]   }
[rank0]:V1123 00:00:24.916000 1738822 torch/_inductor/scheduler.py:3008] [0/0] )
[rank0]:V1123 00:00:24.917000 1738822 torch/_inductor/scheduler.py:3079] [0/0] scheduling output buf1
[rank0]:V1123 00:00:24.917000 1738822 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: buf4
[rank0]:V1123 00:00:24.917000 1738822 torch/_inductor/scheduler.py:3231] [0/0] removed dead buffer: buf0
[rank0]:V1123 00:00:24.917000 1738822 torch/_inductor/scheduler.py:3242] [0/0] removed dead operation: op0
[rank1]:I1123 00:00:24.919000 1738823 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank1]:I1123 00:00:24.919000 1738823 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank1]:I1123 00:00:24.920000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank1]:I1123 00:00:24.921000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank1]:I1123 00:00:24.921000 1738823 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank0]:I1123 00:00:24.921000 1738822 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 3 nodes
[rank0]:I1123 00:00:24.921000 1738822 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
[rank0]:I1123 00:00:24.922000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
[rank0]:I1123 00:00:24.923000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
[rank0]:I1123 00:00:24.923000 1738822 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
[rank1]:V1123 00:00:24.923000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op1 with estimated runtime 0.000054
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0] graph():
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, arg0_1, %get_index), kwargs = {})
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, buf1, %get_index_1, %load, None), kwargs = {})
[rank1]:V1123 00:00:24.925000 1738823 torch/_inductor/bounds.py:81] [0/0]     return store
[rank0]:V1123 00:00:24.925000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op1 with estimated runtime 0.000054
[rank1]:V1123 00:00:24.926000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.926000 1738823 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0] get_bounds:
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0] graph():
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=2] = placeholder[target=ops]
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, arg0_1, %get_index), kwargs = {})
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, buf1, %get_index_1, %load, None), kwargs = {})
[rank0]:V1123 00:00:24.927000 1738822 torch/_inductor/bounds.py:81] [0/0]     return store
[rank1]:V1123 00:00:24.928000 1738823 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank1]:V1123 00:00:24.928000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op2 with estimated runtime 0.008496
[rank0]:V1123 00:00:24.928000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.928000 1738823 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op3 with estimated runtime 0.000000
[rank0]:V1123 00:00:24.929000 1738822 torch/_inductor/codegen/triton.py:2070] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
[rank1]:V1123 00:00:24.929000 1738823 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank0]:V1123 00:00:24.930000 1738822 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_my_allreduce_0
[rank0]:V1123 00:00:24.930000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op2 with estimated runtime 0.008496
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/lf/clf4whuc2k5pv573n76lwfoatpj4n46qq62mud6umul2tipavj2a.py
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=1, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         arg0_1, = args
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(arg0_1, (128, 128), (128, 1))
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(1):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(1)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             buf1 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream1 = get_raw_stream(1)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(arg0_1, buf1, 16384, stream=stream1)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del arg0_1
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(buf1, 'sum', 'default')
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(buf1)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (buf1, )
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:1', dtype=torch.float32)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([arg0_1])
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank1]:V1123 00:00:24.930000 1738823 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.931000 1738822 torch/_inductor/scheduler.py:6016] [0/0] Generating code for node op3 with estimated runtime 0.000000
[rank1]:V1123 00:00:24.931000 1738823 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/4s/c4sqbeu5yuxbgsxdj2zmxs5hpv3wktpt2zpdmqflhwomhevq6zx3.py
[rank0]:V1123 00:00:24.931000 1738822 torch/_inductor/graph.py:2349] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] Output code: 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # AOT ID: ['0_inference']
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import torch
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import math
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import random
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import os
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import tempfile
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from math import inf, nan
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from cmath import nanj
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch import device, empty_strided
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] aten = torch.ops.aten
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] inductor_ops = torch.ops.inductor
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] _quantized = torch.ops._quantized
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile = AsyncCompile()
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/ex/cex6raoykvwv7khoijkiv3dag4exe7pdga2pchbryrvvghefmjvy.py
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] # Source node to ATen node mapping:
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] #   my_allreduce_default => my_allreduce
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_poi_fused_my_allreduce_0 = async_compile.triton('triton_poi_fused_my_allreduce_0', '''
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] import triton.language as tl
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton_heuristics.pointwise(
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     size_hints={'x': 16384}, 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     filename=__file__,
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'enable_fp_fusion': True, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]]}]},
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_my_allreduce_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 1, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 196608}},
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     min_elem_per_thread=0
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] )
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] @triton.jit
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def triton_poi_fused_my_allreduce_0(in_ptr0, out_ptr0, xnumel, XBLOCK : tl.constexpr):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xnumel = 16384
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     x0 = xindex
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x0), None)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     tl.store(out_ptr0 + (x0), tmp0, None)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] ''', device_str='cuda')
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] async_compile.wait(globals())
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] del async_compile
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] class Runner:
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def __init__(self, partitions):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = partitions
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         new_callables = []
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             new_callables.append(fn(c))
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         self.partitions = new_callables
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     def call(self, args):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         arg0_1, = args
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         args.clear()
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         assert_size_stride(arg0_1, (128, 128), (128, 1))
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.cuda.set_device(0)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             buf1 = empty_strided_cuda((128, 128), (128, 1), torch.float32)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             stream0 = get_raw_stream(0)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             triton_poi_fused_my_allreduce_0.run(arg0_1, buf1, 16384, stream=stream0)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             del arg0_1
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.all_reduce_.default(buf1, 'sum', 'default')
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             # Unsorted Source Nodes: [my_allreduce_default], Original ATen: [test.my_allreduce]
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]             torch.ops._c10d_functional.wait_tensor.default(buf1)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]         return (buf1, )
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] runner = Runner(partitions=[])
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] call = runner.call
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.utils import print_performance
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     arg0_1 = rand_strided((128, 128), (128, 1), device='cuda:0', dtype=torch.float32)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     fn = lambda: call([arg0_1])
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] if __name__ == "__main__":
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2455] [0/0] [__output_code] 
[rank1]:V1123 00:00:24.933000 1738823 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/4s/c4sqbeu5yuxbgsxdj2zmxs5hpv3wktpt2zpdmqflhwomhevq6zx3.py
[rank1]:I1123 00:00:24.933000 1738823 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/4s/c4sqbeu5yuxbgsxdj2zmxs5hpv3wktpt2zpdmqflhwomhevq6zx3.py
[rank0]:V1123 00:00:24.933000 1738822 torch/_inductor/graph.py:2466] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/2l/c2lpk563uaa6snjkvwykl7eq6vbzmdhu7tecrakoga5gwhow4e3u.py
[rank1]:I1123 00:00:24.935000 1738823 torch/_inductor/triton_bundler.py:197] [0/0] Saving 3 statically launchable CachingAutotuners
[rank1]:V1123 00:00:24.935000 1738823 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
[rank1]:V1123 00:00:24.935000 1738823 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
[rank1]:V1123 00:00:24.935000 1738823 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fnpq7ffgscomjfqzejakpnmvmsjcvux5ilpelbgpr7smwwvzdqvw
[rank0]:V1123 00:00:24.936000 1738822 torch/_inductor/graph.py:2425] [0/0] Output code written to: /tmp/torchinductor_tianren/2l/c2lpk563uaa6snjkvwykl7eq6vbzmdhu7tecrakoga5gwhow4e3u.py
[rank0]:I1123 00:00:24.936000 1738822 torch/_inductor/graph.py:2426] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/2l/c2lpk563uaa6snjkvwykl7eq6vbzmdhu7tecrakoga5gwhow4e3u.py
[rank1]:V1123 00:00:24.937000 1738823 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.775s
[rank1]:I1123 00:00:24.937000 1738823 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
[rank1]:I1123 00:00:24.937000 1738823 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
[rank1]:I1123 00:00:24.937000 1738823 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
[rank1]:I1123 00:00:24.937000 1738823 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
[rank0]:I1123 00:00:24.938000 1738822 torch/_inductor/triton_bundler.py:197] [0/0] Saving 3 statically launchable CachingAutotuners
[rank0]:V1123 00:00:24.938000 1738822 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
[rank0]:V1123 00:00:24.938000 1738822 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
[rank0]:V1123 00:00:24.938000 1738822 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fp5akivda77vkdhsqnb3bgdi3kr2fmszivwojdtyx2oy6khjqrd3
[rank0]:V1123 00:00:24.940000 1738822 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.744s
[rank0]:I1123 00:00:24.940000 1738822 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
[rank0]:I1123 00:00:24.940000 1738822 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
[rank0]:I1123 00:00:24.940000 1738822 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
[rank0]:I1123 00:00:24.940000 1738822 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
[rank1]:V1123 00:00:24.974000 1738823 torch/_inductor/runtime/triton_heuristics.py:1075] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank1]:V1123 00:00:24.974000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.974000 1738823 torch/_inductor/runtime/triton_heuristics.py:1077] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.975000 1738823 torch/_inductor/runtime/triton_heuristics.py:1113] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 10, nspill 0, #shared-mem 0
[rank1]:V1123 00:00:24.975000 1738823 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/lf/dfdc8b7b1e2110eacdb1f003da52f71bc3106210a88b495eea946d1f145d887e.best_config
[rank0]:V1123 00:00:24.981000 1738822 torch/_inductor/runtime/triton_heuristics.py:1075] Benchmark all input configs for triton_poi_fused_my_allreduce_0, get:
[rank0]:V1123 00:00:24.981000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.981000 1738822 torch/_inductor/runtime/triton_heuristics.py:1077] XBLOCK: 128, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.981000 1738822 torch/_inductor/runtime/triton_heuristics.py:1113] Best config for triton_poi_fused_my_allreduce_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005472, nreg 10, nspill 0, #shared-mem 0
[rank0]:V1123 00:00:24.982000 1738822 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/ex/9cbccc4a3e20bf6e3b54db191606b87210503fe2304850ff768db9646fb2e8fb.best_config
[Rank 1] Output sum: 49152.0, Expected: 49152
[Rank 1] [PASS] Custom collective op autotuning test passed!
[Rank 0] Output sum: 49152.0, Expected: 49152
[Rank 0] [PASS] Custom collective op autotuning test passed!

======================================================================
TEST SUMMARY
======================================================================
Test 1 (Detection):        PASSED
Test 2 (Simple AllReduce): PASSED
Test 3 (Autotuning):       PASSED
======================================================================

ALL TESTS PASSED!

I1123 00:00:27.552000 1738822 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1123 00:00:27.552000 1738822 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 3, exception: 0}
I1123 00:00:27.552000 1738822 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 3, exception: 0}
I1123 00:00:27.552000 1738822 torch/_inductor/remote_cache.py:432] 
I1123 00:00:27.702000 1738823 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1123 00:00:27.702000 1738823 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 3, exception: 0}
I1123 00:00:27.702000 1738823 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 3, exception: 0}
I1123 00:00:27.702000 1738823 torch/_inductor/remote_cache.py:432] 
[W1123 00:00:28.538184394 ProcessGroup.cpp:367] Warning: At the time of process termination, there are still 1 unwaited collective calls. Please review your program to ensure that:
1. c10d_functional.wait_tensor() is invoked on all tensors returned from c10d_functional collective,
2. c10d_functional.wait_tensor() is invoked on all output tensors of async_op=True torch.distributed collective called under `with allow_inflight_collective_as_graph_input_ctx():`,
before the output tensors of the collective are used. (function ~WorkRegistry)
[W1123 00:00:28.686366923 ProcessGroup.cpp:367] Warning: At the time of process termination, there are still 1 unwaited collective calls. Please review your program to ensure that:
1. c10d_functional.wait_tensor() is invoked on all tensors returned from c10d_functional collective,
2. c10d_functional.wait_tensor() is invoked on all output tensors of async_op=True torch.distributed collective called under `with allow_inflight_collective_as_graph_input_ctx():`,
before the output tensors of the collective are used. (function ~WorkRegistry)
