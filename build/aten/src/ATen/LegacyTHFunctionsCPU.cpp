#include <ATen/LegacyTHFunctionsCPU.h>

// @generated by aten/src/ATen/gen.py

#include <ATen/ATen.h>
#include <ATen/Utils.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/CPUGenerator.h>
#include <ATen/ExpandUtils.h>
#include <TH/TH.h>
#include <TH/THTensor.hpp>
#include <THNN/THNN.h>
#undef THNN_


namespace at {
namespace native {
namespace legacy {
namespace cpu {

namespace {
  ScalarType infer_scalar_type(const Tensor & t) {
    return t.scalar_type();
  }
  ScalarType infer_scalar_type(const TensorList & tl) {
    TORCH_CHECK(tl.size() > 0, "expected a non-empty list of Tensors");
    return tl[0].scalar_type();
  }

  TensorOptions options(ScalarType s) {
    return TensorOptions().dtype(s)
                          .device(DeviceType::CPU)
                          .layout(kStrided);
  }

  Allocator* allocator() {
    return getCPUAllocator();
  }
}

Tensor & _th_set_(Tensor & self, Storage source, int64_t storage_offset, IntArrayRef size, IntArrayRef stride) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Bool));
            THBoolTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Byte));
            THByteTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Char);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Char));
            THCharTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Double);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Double));
            THDoubleTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Float);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Float));
            THFloatTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Int);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Int));
            THIntTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Long));
            THLongTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Short);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Short));
            THShortTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Half);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::Half));
            THHalfTensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto source_ = checked_storage(source, "source", 2, DeviceType::CPU, at::scalarTypeToTypeMeta(ScalarType::BFloat16));
            THBFloat16Tensor_setStorage(self_, source_.unsafeGetStorageImpl(), storage_offset, size, stride);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_set_(Tensor & self, const Tensor & source) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Char);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Double);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Float);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Int);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Short);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::Half);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::Half);
            THHalfTensor_set(self_, source_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_set_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 2, "_th_set_", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_set(self_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_set_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) {

    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_");
    return s__th_masked_fill_(self, b_mask, value);
}
Tensor & s__th_masked_fill_(Tensor & self, const Tensor & mask, Scalar value) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toBool();
            THBoolTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toByte();
            THByteTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toChar();
            THCharTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toDouble();
            THDoubleTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toFloat();
            THFloatTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toInt();
            THIntTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toLong();
            THLongTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toShort();
            THShortTensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_", false, DeviceType::CPU, ScalarType::Byte);
            auto value_ = value.toBFloat16();
            THBFloat16Tensor_maskedFill(self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {

    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_fill_bool_");
    return s__th_masked_fill_bool_(self, b_mask, value);
}
Tensor & s__th_masked_fill_bool_(Tensor & self, const Tensor & mask, Scalar value) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toBool();
            THBoolTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toByte();
            THByteTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toChar();
            THCharTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toDouble();
            THDoubleTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toFloat();
            THFloatTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toInt();
            THIntTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toLong();
            THLongTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toShort();
            THShortTensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_fill_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto value_ = value.toBFloat16();
            THBFloat16Tensor_maskedFillBool(self_, mask_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_fill_bool_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) {

    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_scatter_");
    return s__th_masked_scatter_(self, b_mask, source);
}
Tensor & s__th_masked_scatter_(Tensor & self, const Tensor & mask, const Tensor & source) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_maskedCopy(self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {

    // DeviceGuard omitted
    Tensor b_mask;
    std::tie(b_mask) = expand_inplace(self, mask, "_th_masked_scatter_bool_");
    return s__th_masked_scatter_bool_(self, b_mask, source);
}
Tensor & s__th_masked_scatter_bool_(Tensor & self, const Tensor & mask, const Tensor & source) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::Bool);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_masked_scatter_bool_", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_maskedCopyBool(self_, mask_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_masked_scatter_bool_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_out");
    return s__th_masked_select_out(result, b_self, b_mask);
}
Tensor & s__th_masked_select_out(Tensor & result, const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THBoolTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THCharTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THDoubleTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THFloatTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THIntTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THLongTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THShortTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_out", false, DeviceType::CPU, ScalarType::Byte);
            THBFloat16Tensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_masked_select(const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select");
    return s__th_masked_select(b_self, b_mask);
}
Tensor s__th_masked_select(const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THBoolTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THCharTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THDoubleTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THFloatTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THIntTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THLongTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THShortTensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select", false, DeviceType::CPU, ScalarType::Byte);
            THBFloat16Tensor_maskedSelect(result_, self_, mask_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_bool_out");
    return s__th_masked_select_bool_out(result, b_self, b_mask);
}
Tensor & s__th_masked_select_bool_out(Tensor & result, const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THByteTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THCharTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THDoubleTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THFloatTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THIntTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THLongTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THShortTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool_out", false, DeviceType::CPU, ScalarType::Bool);
            THBFloat16Tensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_masked_select_bool(const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    Tensor b_self, b_mask;
    std::tie(b_self, b_mask) = expand_outplace(self, mask, "_th_masked_select_bool");
    return s__th_masked_select_bool(b_self, b_mask);
}
Tensor s__th_masked_select_bool(const Tensor & self, const Tensor & mask) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Byte);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THByteTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Char);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THCharTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Double);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THDoubleTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Float);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THFloatTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Int);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THIntTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Long);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THLongTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Short);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THShortTensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mask_ = checked_dense_tensor_unwrap(mask, "mask", 2, "_th_masked_select_bool", false, DeviceType::CPU, ScalarType::Bool);
            THBFloat16Tensor_maskedSelectBool(result_, self_, mask_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_masked_select_bool not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_nonzero_out(Tensor & result, const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Half);
            THHalfTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero_out", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_nonzero(result_, self_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_nonzero(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::Half: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::Half);
            THHalfTensor_nonzero(result_, self_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_nonzero", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_nonzero(result_, self_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_nonzero not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_clone(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Bool);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THBoolTensor_newClone(self_))));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Byte);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THByteTensor_newClone(self_))));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Char);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THCharTensor_newClone(self_))));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Double);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THDoubleTensor_newClone(self_))));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Float);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THFloatTensor_newClone(self_))));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Int);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THIntTensor_newClone(self_))));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Long);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THLongTensor_newClone(self_))));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Short);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THShortTensor_newClone(self_))));
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::Half);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THHalfTensor_newClone(self_))));
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_clone", false, DeviceType::CPU, ScalarType::BFloat16);
            return Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim((THBFloat16Tensor_newClone(self_))));
            break;
        }
        default:
            AT_ERROR("_th_clone not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_copy_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & source) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 4, "_th_index_copy_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_indexCopy(self_, dim, index_, source_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_copy_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_take_out(Tensor & result, const Tensor & self, const Tensor & index) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Bool);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THBoolTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Byte);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THByteTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Char);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THCharTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Double);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Float);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Int);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THIntTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_take_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take_out", false, DeviceType::CPU, ScalarType::Short);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take_out", false, DeviceType::CPU, ScalarType::Long);
            THShortTensor_take(result_, self_, index_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_take(const Tensor & self, const Tensor & index) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Bool);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THBoolTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Byte);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THByteTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Char);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THCharTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Double);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Float);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Int);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THIntTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_take(result_, self_, index_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_take", false, DeviceType::CPU, ScalarType::Short);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_take", false, DeviceType::CPU, ScalarType::Long);
            THShortTensor_take(result_, self_, index_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_take not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_put_(Tensor & self, const Tensor & index, const Tensor & source, bool accumulate) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Bool);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Byte);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Char);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Double);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Float);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Int);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_put_", false, DeviceType::CPU, ScalarType::Short);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 2, "_th_put_", false, DeviceType::CPU, ScalarType::Long);
            auto source_ = checked_dense_tensor_unwrap(source, "source", 3, "_th_put_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_put(self_, index_, source_, accumulate);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_put_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_index_fill_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toBool();
            THBoolTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toByte();
            THByteTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toChar();
            THCharTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toDouble();
            THDoubleTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toFloat();
            THFloatTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toInt();
            THIntTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            THLongTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_index_fill_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toShort();
            THShortTensor_indexFill(self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_index_fill_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {

    // DeviceGuard omitted
    if (src.dim() == 0) {
        return _th_scatter_(self, dim, index, src.item());
    }
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_scatter(self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_(Tensor & self, int64_t dim, const Tensor & index, Scalar value) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toBool();
            THBoolTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toByte();
            THByteTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toChar();
            THCharTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toDouble();
            THDoubleTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toFloat();
            THFloatTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toInt();
            THIntTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toLong();
            THLongTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_", false, DeviceType::CPU, ScalarType::Long);
            auto value_ = value.toShort();
            THShortTensor_scatterFill(self_, dim, index_, value_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_scatter_add_(Tensor & self, int64_t dim, const Tensor & index, const Tensor & src) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Long);
            auto src_ = checked_dense_tensor_unwrap(src, "src", 4, "_th_scatter_add_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_scatterAdd(self_, dim, index_, src_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_scatter_add_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_gather_out(Tensor & result, const Tensor & self, int64_t dim, const Tensor & index) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Bool);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THBoolTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Byte);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THByteTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Char);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THCharTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Double);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Float);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Int);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THIntTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_gather_out", false, DeviceType::CPU, ScalarType::Short);
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather_out", false, DeviceType::CPU, ScalarType::Long);
            THShortTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_gather(const Tensor & self, int64_t dim, const Tensor & index) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THBoolTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THByteTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THCharTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THIntTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_(index.sizes());
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gather", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            auto index_ = checked_dense_tensor_unwrap(index, "index", 3, "_th_gather", false, DeviceType::CPU, ScalarType::Long);
            THShortTensor_gather(result_, self_, dim, index_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_gather not supported on CPUType for ", dispatch_scalar_type);
    }
}
bool _th_equal(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Bool);
            return THBoolTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Byte);
            return THByteTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Char);
            return THCharTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Double);
            return THDoubleTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Float);
            return THFloatTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Int);
            return THIntTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Long);
            return THLongTensor_equal(self_, other_);
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::Short);
            return THShortTensor_equal(self_, other_);
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_equal", false, DeviceType::CPU, ScalarType::BFloat16);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_equal", false, DeviceType::CPU, ScalarType::BFloat16);
            return THBFloat16Tensor_equal(self_, other_);
            break;
        }
        default:
            AT_ERROR("_th_equal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_or_out(Tensor & result, const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = other.toBool();
            THBoolTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_or(const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = other.toBool();
            THBoolTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_bitor(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_or_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or_out");
    return s__th_or_out(result, b_self, b_other);
}
Tensor & s__th_or_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_or_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_or(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_or");
    return s__th_or(b_self, b_other);
}
Tensor s__th_or(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_or", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_or", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cbitor(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_or not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ior_(Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = other.toBool();
            THBoolTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_bitor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ior_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ior_");
    return s__th_ior_(self, b_other);
}
Tensor & s__th_ior_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ior_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ior_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cbitor(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ior_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lshift_out(Tensor & result, const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_lshift(const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_lshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift_out");
    return s__th_lshift_out(result, b_self, b_other);
}
Tensor & s__th_lshift_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_lshift(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_lshift");
    return s__th_lshift(b_self, b_other);
}
Tensor s__th_lshift(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_lshift", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_lshift", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_clshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_lshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ilshift_(Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_lshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ilshift_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_ilshift_");
    return s__th_ilshift_(self, b_other);
}
Tensor & s__th_ilshift_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_ilshift_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_clshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_ilshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_rshift_out(Tensor & result, const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_rshift(const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_rshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift_out");
    return s__th_rshift_out(result, b_self, b_other);
}
Tensor & s__th_rshift_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_rshift(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_rshift");
    return s__th_rshift(b_self, b_other);
}
Tensor s__th_rshift(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_rshift", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_rshift", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_crshift(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_rshift not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_irshift_(Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_rshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_irshift_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_irshift_");
    return s__th_irshift_(self, b_other);
}
Tensor & s__th_irshift_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_irshift_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_irshift_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_crshift(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_irshift_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_min_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min_out");
    return s__th_min_out(result, b_self, b_other);
}
Tensor & s__th_min_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_min(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_min");
    return s__th_min(b_self, b_other);
}
Tensor s__th_min(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_min", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cmin(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_min(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Bool);
            return at::scalar_tensor(convert<bool>(THBoolTensor_minall(self_)), options(ScalarType::Bool));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THByteTensor_minall(self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCharTensor_minall(self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_minall(self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_minall(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THIntTensor_minall(self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_minall(self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THShortTensor_minall(self_)), options(ScalarType::Short));
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_min_out(Tensor & min, Tensor & min_indices, const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Bool);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Byte: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Byte);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Char);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Double);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Float);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Int);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = checked_dense_tensor_unwrap(min, "min", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Short);
            auto min_indices_ = checked_dense_tensor_unwrap(min_indices, "min_indices", 0, "_th_min_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_min(const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Byte: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Char: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Double: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Float: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Int: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Long: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        case ScalarType::Short: {
            auto min_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_));
            auto min_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto min_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(min_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_min", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_min(min_, min_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(min, min_indices);
            break;
        }
        default:
            AT_ERROR("_th_min not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_max_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max_out");
    return s__th_max_out(result, b_self, b_other);
}
Tensor & s__th_max_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_max(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_max");
    return s__th_max(b_self, b_other);
}
Tensor s__th_max(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Bool);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_max", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cmax(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_max(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Bool);
            return at::scalar_tensor(convert<bool>(THBoolTensor_maxall(self_)), options(ScalarType::Bool));
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THByteTensor_maxall(self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCharTensor_maxall(self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_maxall(self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_maxall(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THIntTensor_maxall(self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_maxall(self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THShortTensor_maxall(self_)), options(ScalarType::Short));
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_max_out(Tensor & max, Tensor & max_indices, const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Bool);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Byte: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Byte);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Char);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Double);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Float);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Int);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = checked_dense_tensor_unwrap(max, "max", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Short);
            auto max_indices_ = checked_dense_tensor_unwrap(max_indices, "max_indices", 0, "_th_max_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_max(const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Byte: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Char: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Double: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Float: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Int: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Long: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        case ScalarType::Short: {
            auto max_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_));
            auto max_indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto max_indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(max_indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_max", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_max(max_, max_indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(max, max_indices);
            break;
        }
        default:
            AT_ERROR("_th_max not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_mode_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Byte);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Char);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Double);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Float);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Int);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Short);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_mode_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_mode(const Tensor & self, int64_t dim, bool keepdim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mode", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_mode(values_, indices_, self_, dim, keepdim);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_mode not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_sort_out(Tensor & values, Tensor & indices, const Tensor & self, int64_t dim, bool descending) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Byte);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Char);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Double);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Float);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Int);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = checked_dense_tensor_unwrap(values, "values", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Short);
            auto indices_ = checked_dense_tensor_unwrap(indices, "indices", 0, "_th_sort_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor &, Tensor &>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_sort(const Tensor & self, int64_t dim, bool descending) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Char: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Double: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Float: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Int: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Long: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        case ScalarType::Short: {
            auto values_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto values = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(values_));
            auto indices_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto indices = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(indices_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_sort", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_sort(values_, indices_, self_, dim, descending);
            return std::tuple<Tensor, Tensor>(values, indices);
            break;
        }
        default:
            AT_ERROR("_th_sort not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_var(const Tensor & self, bool unbiased) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_var", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_var_all(self_, unbiased)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_var", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_var_all(self_, unbiased)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_var not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_std(const Tensor & self, bool unbiased) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_std", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_std_all(self_, unbiased)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_std", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_std_all(self_, unbiased)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_std not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_renorm_out(Tensor & result, const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_renorm_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm_out", false, DeviceType::CPU, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THDoubleTensor_renorm(result_, self_, p_, dim, maxnorm_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_renorm_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm_out", false, DeviceType::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(result_, self_, p_, dim, maxnorm_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_renorm(const Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm", false, DeviceType::CPU, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THDoubleTensor_renorm(result_, self_, p_, dim, maxnorm_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm", false, DeviceType::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(result_, self_, p_, dim, maxnorm_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_renorm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_renorm_(Tensor & self, Scalar p, int64_t dim, Scalar maxnorm) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm_", false, DeviceType::CPU, ScalarType::Double);
            auto p_ = p.toDouble();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toDouble();
            THDoubleTensor_renorm(self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_renorm_", false, DeviceType::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            dim = maybe_wrap_dim(dim, self_);
            auto maxnorm_ = maxnorm.toFloat();
            THFloatTensor_renorm(self_, self_, p_, dim, maxnorm_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_renorm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_dist(const Tensor & self, const Tensor & other, Scalar p) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_dist");
    return s__th_dist(b_self, b_other, p);
}
Tensor s__th_dist(const Tensor & self, const Tensor & other, Scalar p) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dist", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_dist", false, DeviceType::CPU, ScalarType::Double);
            auto p_ = p.toDouble();
            return at::scalar_tensor(convert<double>(THDoubleTensor_dist(self_, other_, p_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dist", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_dist", false, DeviceType::CPU, ScalarType::Float);
            auto p_ = p.toFloat();
            return at::scalar_tensor(convert<float>(THFloatTensor_dist(self_, other_, p_)), options(ScalarType::Float));
            break;
        }
        default:
            AT_ERROR("_th_dist not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_histc_out(Tensor & result, const Tensor & self, int64_t bins, Scalar min, Scalar max) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_histc_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_histc_out", false, DeviceType::CPU, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THDoubleTensor_histc(result_, self_, bins, min_, max_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_histc_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_histc_out", false, DeviceType::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_histc(result_, self_, bins, min_, max_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_histc_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_histc(const Tensor & self, int64_t bins, Scalar min, Scalar max) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_histc", false, DeviceType::CPU, ScalarType::Double);
            auto min_ = min.toDouble();
            auto max_ = max.toDouble();
            THDoubleTensor_histc(result_, self_, bins, min_, max_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_histc", false, DeviceType::CPU, ScalarType::Float);
            auto min_ = min.toFloat();
            auto max_ = max.toFloat();
            THFloatTensor_histc(result_, self_, bins, min_, max_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_histc not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cumsum_out(Tensor & result, const Tensor & self, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_cumsum(const Tensor & self, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumsum", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_cumsum(result_, self_, dim);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumsum not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cumprod_out(Tensor & result, const Tensor & self, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Bool);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod_out", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_cumprod(const Tensor & self, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Bool);
            dim = maybe_wrap_dim(dim, self_);
            THBoolTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Byte);
            dim = maybe_wrap_dim(dim, self_);
            THByteTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Char);
            dim = maybe_wrap_dim(dim, self_);
            THCharTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            THDoubleTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            THFloatTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Int);
            dim = maybe_wrap_dim(dim, self_);
            THIntTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Long);
            dim = maybe_wrap_dim(dim, self_);
            THLongTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cumprod", false, DeviceType::CPU, ScalarType::Short);
            dim = maybe_wrap_dim(dim, self_);
            THShortTensor_cumprod(result_, self_, dim);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_cumprod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_trace(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THByteTensor_trace(self_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCharTensor_trace(self_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_trace(self_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_trace(self_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THIntTensor_trace(self_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_trace(self_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_trace", false, DeviceType::CPU, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THShortTensor_trace(self_)), options(ScalarType::Short));
            break;
        }
        default:
            AT_ERROR("_th_trace not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_out(Tensor & result, const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_fmod(const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_fmod(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod_out");
    return s__th_fmod_out(result, b_self, b_other);
}
Tensor & s__th_fmod_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_fmod(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_fmod");
    return s__th_fmod(b_self, b_other);
}
Tensor s__th_fmod(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_fmod", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cfmod(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_fmod not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_(Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_fmod(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_fmod_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_fmod_");
    return s__th_fmod_(self, b_other);
}
Tensor & s__th_fmod_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_fmod_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_fmod_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cfmod(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_fmod_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_out(Tensor & result, const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_remainder(const Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_remainder(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder_out");
    return s__th_remainder_out(result, b_self, b_other);
}
Tensor & s__th_remainder_out(Tensor & result, const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_remainder(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_self, b_other;
    std::tie(b_self, b_other) = expand_outplace(self, other, "_th_remainder");
    return s__th_remainder(b_self, b_other);
}
Tensor s__th_remainder(const Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 2, "_th_remainder", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cremainder(result_, self_, other_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_remainder not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_(Tensor & self, Scalar other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = other.toByte();
            THByteTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = other.toChar();
            THCharTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = other.toDouble();
            THDoubleTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = other.toFloat();
            THFloatTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = other.toInt();
            THIntTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = other.toLong();
            THLongTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = other.toShort();
            THShortTensor_remainder(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_remainder_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    Tensor b_other;
    std::tie(b_other) = expand_inplace(self, other, "_th_remainder_");
    return s__th_remainder_(self, b_other);
}
Tensor & s__th_remainder_(Tensor & self, const Tensor & other) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Byte);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Char);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Double);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Float);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Int);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Long);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_remainder_", false, DeviceType::CPU, ScalarType::Short);
            auto other_ = checked_dense_tensor_unwrap(other, "other", 3, "_th_remainder_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cremainder(self_, self_, other_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_remainder_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_dot(const Tensor & self, const Tensor & tensor) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Byte);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Byte);
            return at::scalar_tensor(convert<uint8_t>(THByteTensor_dot(self_, tensor_)), options(ScalarType::Byte));
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Char);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Char);
            return at::scalar_tensor(convert<int8_t>(THCharTensor_dot(self_, tensor_)), options(ScalarType::Char));
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Double);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Double);
            return at::scalar_tensor(convert<double>(THDoubleTensor_dot(self_, tensor_)), options(ScalarType::Double));
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Float);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Float);
            return at::scalar_tensor(convert<float>(THFloatTensor_dot(self_, tensor_)), options(ScalarType::Float));
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Int);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Int);
            return at::scalar_tensor(convert<int>(THIntTensor_dot(self_, tensor_)), options(ScalarType::Int));
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Long);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Long);
            return at::scalar_tensor(convert<int64_t>(THLongTensor_dot(self_, tensor_)), options(ScalarType::Long));
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_dot", false, DeviceType::CPU, ScalarType::Short);
            auto tensor_ = checked_dense_tensor_unwrap(tensor, "tensor", 2, "_th_dot", false, DeviceType::CPU, ScalarType::Short);
            return at::scalar_tensor(convert<int16_t>(THShortTensor_dot(self_, tensor_)), options(ScalarType::Short));
            break;
        }
        default:
            AT_ERROR("_th_dot not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_diag_out(Tensor & result, const Tensor & self, int64_t diagonal) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_diag_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_diag(const Tensor & self, int64_t diagonal) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_diag", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_diag(result_, self_, diagonal);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_diag not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm_out");
    return s__th_addmm_out(result, b_self, mat1, mat2, beta, alpha);
}
Tensor & s__th_addmm_out(Tensor & result, const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat1.size(0),mat2.size(1)}, "_th_addmm");
    return s__th_addmm(b_self, mat1, mat2, beta, alpha);
}
Tensor s__th_addmm(const Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Byte);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Byte);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Char);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Char);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Double);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Double);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Float);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Float);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Int);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Int);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Long);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Long);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::Short);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::Short);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 2, "_th_addmm", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 3, "_th_addmm", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmm(result_, self_, mat1_, mat2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmm_(Tensor & self, const Tensor & mat1, const Tensor & mat2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Char);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Char);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Double);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Double);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Float);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Float);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Int);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Int);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Long);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Long);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::Short);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::Short);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmm_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat1_ = checked_dense_tensor_unwrap(mat1, "mat1", 3, "_th_addmm_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 4, "_th_addmm_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmm(self_, self_, mat1_, mat2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv_out");
    return s__th_addmv_out(result, b_self, mat, vec, beta, alpha);
}
Tensor & s__th_addmv_out(Tensor & result, const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Byte);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Char);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Char);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Double);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Double);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Float);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Float);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Int);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Int);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Long);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Long);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Short);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Short);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addmv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {mat.size(0)}, "_th_addmv");
    return s__th_addmv(b_self, mat, vec, beta, alpha);
}
Tensor s__th_addmv(const Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Byte);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Byte);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Char);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Char);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Double);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Double);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Float);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Float);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Int);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Int);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Long);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Long);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::Short);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::Short);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 2, "_th_addmv", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 3, "_th_addmv", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmv(result_, self_, mat_, vec_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addmv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addmv_(Tensor & self, const Tensor & mat, const Tensor & vec, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Byte);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Byte);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Char);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Char);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Double);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Double);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Float);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Float);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Int);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Int);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Long);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Long);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::Short);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::Short);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addmv_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat_ = checked_dense_tensor_unwrap(mat, "mat", 3, "_th_addmv_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 4, "_th_addmv_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addmv(self_, self_, mat_, vec_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addmv_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr_out");
    return s__th_addr_out(result, b_self, vec1, vec2, beta, alpha);
}
Tensor & s__th_addr_out(Tensor & result, const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Byte);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Byte);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Char);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Char);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Double);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Double);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Float);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Float);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Int);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Int);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Long);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Long);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::Short);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::Short);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addr_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {vec1.size(0),vec2.size(0)}, "_th_addr");
    return s__th_addr(b_self, vec1, vec2, beta, alpha);
}
Tensor s__th_addr(const Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Byte);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Byte);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Char);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Char);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Double);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Double);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Float);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Float);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Int);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Int);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Long);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Long);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::Short);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::Short);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 2, "_th_addr", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 3, "_th_addr", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addr(result_, self_, vec1_, vec2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addr_(Tensor & self, const Tensor & vec1, const Tensor & vec2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Byte);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Byte);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Char);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Char);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Double);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Double);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Float);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Float);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Int);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Int);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Long);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Long);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::Short);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::Short);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addr_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec1_ = checked_dense_tensor_unwrap(vec1, "vec1", 3, "_th_addr_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 4, "_th_addr_", false, DeviceType::CPU, ScalarType::BFloat16);
            auto beta_ = beta.toBFloat16();
            auto alpha_ = alpha.toBFloat16();
            THBFloat16Tensor_addr(self_, self_, vec1_, vec2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addr_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ger_out(Tensor & result, const Tensor & self, const Tensor & vec2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Byte);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Byte);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addr(result_, result_, self_, vec2_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Char);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Char);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addr(result_, result_, self_, vec2_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Double);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Double);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addr(result_, result_, self_, vec2_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Float);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Float);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, result_, self_, vec2_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Int);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Int);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addr(result_, result_, self_, vec2_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Long);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Long);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addr(result_, result_, self_, vec2_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::Short);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::Short);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addr(result_, result_, self_, vec2_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ger_out", false, DeviceType::CPU, ScalarType::BFloat16);
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger_out", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addr(result_, result_, self_, vec2_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_ger(const Tensor & self, const Tensor & vec2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Byte);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addr(result_, result_, self_, vec2_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Char);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addr(result_, result_, self_, vec2_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Double);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addr(result_, result_, self_, vec2_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Float);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addr(result_, result_, self_, vec2_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Int);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addr(result_, result_, self_, vec2_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Long);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addr(result_, result_, self_, vec2_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::Short);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addr(result_, result_, self_, vec2_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.dim() == 0 ? 1 : self.size(0),vec2.dim() == 0 ? 1 : vec2.size(0) });
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ger", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec2_ = checked_dense_tensor_unwrap(vec2, "vec2", 2, "_th_ger", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addr(result_, result_, self_, vec2_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ger not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_mv_out(Tensor & result, const Tensor & self, const Tensor & vec) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Byte);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Byte);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addmv(result_, result_, self_, vec_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Char);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Char);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addmv(result_, result_, self_, vec_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Double);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Double);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addmv(result_, result_, self_, vec_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Float);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Float);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, result_, self_, vec_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Int);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Int);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addmv(result_, result_, self_, vec_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Long);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Long);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, result_, self_, vec_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::Short);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::Short);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addmv(result_, result_, self_, vec_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv_out", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addmv(result_, result_, self_, vec_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_mv(const Tensor & self, const Tensor & vec) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Byte);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addmv(result_, result_, self_, vec_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Char);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addmv(result_, result_, self_, vec_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Double);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addmv(result_, result_, self_, vec_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Float);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addmv(result_, result_, self_, vec_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Int);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addmv(result_, result_, self_, vec_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Long);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addmv(result_, result_, self_, vec_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::Short);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addmv(result_, result_, self_, vec_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mv", false, DeviceType::CPU, ScalarType::BFloat16);
            auto vec_ = checked_dense_tensor_unwrap(vec, "vec", 2, "_th_mv", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addmv(result_, result_, self_, vec_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mv not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_mm_out(Tensor & result, const Tensor & self, const Tensor & mat2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Byte);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addmm(result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Char);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Char);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addmm(result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Double);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Double);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addmm(result_, result_, self_, mat2_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Float);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Float);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, result_, self_, mat2_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Int);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Int);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addmm(result_, result_, self_, mat2_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Long);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Long);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::Short);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::Short);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addmm(result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_mm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm_out", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addmm(result_, result_, self_, mat2_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_mm(const Tensor & self, const Tensor & mat2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Byte);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_addmm(result_, result_, self_, mat2_, uint8_t(0), uint8_t(1));
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Char);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_addmm(result_, result_, self_, mat2_, int8_t(0), int8_t(1));
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Double);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_addmm(result_, result_, self_, mat2_, double(0), double(1));
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Float);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_addmm(result_, result_, self_, mat2_, float(0), float(1));
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Int);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_addmm(result_, result_, self_, mat2_, int(0), int(1));
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Long);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_addmm(result_, result_, self_, mat2_, int64_t(0), int64_t(1));
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::Short);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_addmm(result_, result_, self_, mat2_, int16_t(0), int16_t(1));
            return result;
            break;
        }
        case ScalarType::BFloat16: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            result.resize_({ self.size(0),mat2.size(1) });
            result.zero_();
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_mm", false, DeviceType::CPU, ScalarType::BFloat16);
            auto mat2_ = checked_dense_tensor_unwrap(mat2, "mat2", 2, "_th_mm", false, DeviceType::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_addmm(result_, result_, self_, mat2_, BFloat16(0), BFloat16(1));
            return result;
            break;
        }
        default:
            AT_ERROR("_th_mm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm_out");
    return s__th_addbmm_out(result, b_self, batch1, batch2, beta, alpha);
}
Tensor & s__th_addbmm_out(Tensor & result, const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm_out", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    Tensor b_self;
    std::tie(b_self) = expand_size(self, {batch1.size(1),batch2.size(2)}, "_th_addbmm");
    return s__th_addbmm(b_self, batch1, batch2, beta, alpha);
}
Tensor s__th_addbmm(const Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Byte);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Byte);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Char: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Char);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Char);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Double);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Double);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Float);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Float);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Int: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Int);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Int);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Long: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Long);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Long);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        case ScalarType::Short: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm", false, DeviceType::CPU, ScalarType::Short);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 2, "_th_addbmm", false, DeviceType::CPU, ScalarType::Short);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 3, "_th_addbmm", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addbmm(result_, self_, batch1_, batch2_, beta_, alpha_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_addbmm not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_addbmm_(Tensor & self, const Tensor & batch1, const Tensor & batch2, Scalar beta, Scalar alpha) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Byte);
            auto beta_ = beta.toByte();
            auto alpha_ = alpha.toByte();
            THByteTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Char);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Char);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Char);
            auto beta_ = beta.toChar();
            auto alpha_ = alpha.toChar();
            THCharTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Double);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Double);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto alpha_ = alpha.toDouble();
            THDoubleTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Float);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Float);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toFloat();
            auto alpha_ = alpha.toFloat();
            THFloatTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Int);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Int);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Int);
            auto beta_ = beta.toInt();
            auto alpha_ = alpha.toInt();
            THIntTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Long);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Long);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Long);
            auto beta_ = beta.toLong();
            auto alpha_ = alpha.toLong();
            THLongTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Short);
            auto batch1_ = checked_dense_tensor_unwrap(batch1, "batch1", 3, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Short);
            auto batch2_ = checked_dense_tensor_unwrap(batch2, "batch2", 4, "_th_addbmm_", false, DeviceType::CPU, ScalarType::Short);
            auto beta_ = beta.toShort();
            auto alpha_ = alpha.toShort();
            THShortTensor_addbmm(self_, self_, batch1_, batch2_, beta_, alpha_);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_addbmm_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_gels_out(Tensor & res1, Tensor & res2, const Tensor & self, const Tensor & A) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_gels_out", false, DeviceType::CPU, ScalarType::Double);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_gels_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gels_out", false, DeviceType::CPU, ScalarType::Double);
            auto A_ = checked_dense_tensor_unwrap(A, "A", 2, "_th_gels_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_gels(res1_, res2_, self_, A_);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_gels_out", false, DeviceType::CPU, ScalarType::Float);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_gels_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gels_out", false, DeviceType::CPU, ScalarType::Float);
            auto A_ = checked_dense_tensor_unwrap(A, "A", 2, "_th_gels_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_gels(res1_, res2_, self_, A_);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_gels(const Tensor & self, const Tensor & A) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gels", false, DeviceType::CPU, ScalarType::Double);
            auto A_ = checked_dense_tensor_unwrap(A, "A", 2, "_th_gels", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_gels(res1_, res2_, self_, A_);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_gels", false, DeviceType::CPU, ScalarType::Float);
            auto A_ = checked_dense_tensor_unwrap(A, "A", 2, "_th_gels", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_gels(res1_, res2_, self_, A_);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_gels not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_eig_out(Tensor & res1, Tensor & res2, const Tensor & self, bool eigenvectors) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_eig_out", false, DeviceType::CPU, ScalarType::Double);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_eig_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_eig_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_geev(res1_, res2_, self_, eigenvectors);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_eig_out", false, DeviceType::CPU, ScalarType::Float);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_eig_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_eig_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_geev(res1_, res2_, self_, eigenvectors);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_eig(const Tensor & self, bool eigenvectors) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_eig", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_geev(res1_, res2_, self_, eigenvectors);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_eig", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_geev(res1_, res2_, self_, eigenvectors);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_eig not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_potri_out(Tensor & output, const Tensor & self, bool upper) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_potri_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_potri_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_potri(output_, self_, upper);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_potri_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_potri_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_potri(output_, self_, upper);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_potri(const Tensor & self, bool upper) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_potri", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_potri(output_, self_, upper);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_potri", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_potri(output_, self_, upper);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_potri not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_geqrf_out(Tensor & res1, Tensor & res2, const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Double);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_geqrf(res1_, res2_, self_);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = checked_dense_tensor_unwrap(res1, "res1", 0, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Float);
            auto res2_ = checked_dense_tensor_unwrap(res2, "res2", 0, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geqrf_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_geqrf(res1_, res2_, self_);
            return std::tuple<Tensor &, Tensor &>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_geqrf(const Tensor & self) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geqrf", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_geqrf(res1_, res2_, self_);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        case ScalarType::Float: {
            auto res1_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res1 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res1_));
            auto res2_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto res2 = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(res2_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geqrf", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_geqrf(res1_, res2_, self_);
            return std::tuple<Tensor, Tensor>(res1, res2);
            break;
        }
        default:
            AT_ERROR("_th_geqrf not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_orgqr_out(Tensor & result, const Tensor & self, const Tensor & input2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Double);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_orgqr(result_, self_, input2_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Float);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_orgqr_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_orgqr(result_, self_, input2_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_orgqr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_orgqr(const Tensor & self, const Tensor & input2) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_orgqr", false, DeviceType::CPU, ScalarType::Double);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_orgqr", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_orgqr(result_, self_, input2_);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_orgqr", false, DeviceType::CPU, ScalarType::Float);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_orgqr", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_orgqr(result_, self_, input2_);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_orgqr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_ormqr_out(Tensor & result, const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Double);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Double);
            auto input3_ = checked_dense_tensor_unwrap(input3, "input3", 3, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_ormqr(result_, self_, input2_, input3_, left, transpose);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Float);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Float);
            auto input3_ = checked_dense_tensor_unwrap(input3, "input3", 3, "_th_ormqr_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_ormqr(result_, self_, input2_, input3_, left, transpose);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ormqr_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_ormqr(const Tensor & self, const Tensor & input2, const Tensor & input3, bool left, bool transpose) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ormqr", false, DeviceType::CPU, ScalarType::Double);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_ormqr", false, DeviceType::CPU, ScalarType::Double);
            auto input3_ = checked_dense_tensor_unwrap(input3, "input3", 3, "_th_ormqr", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_ormqr(result_, self_, input2_, input3_, left, transpose);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_ormqr", false, DeviceType::CPU, ScalarType::Float);
            auto input2_ = checked_dense_tensor_unwrap(input2, "input2", 2, "_th_ormqr", false, DeviceType::CPU, ScalarType::Float);
            auto input3_ = checked_dense_tensor_unwrap(input3, "input3", 3, "_th_ormqr", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_ormqr(result_, self_, input2_, input3_, left, transpose);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_ormqr not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_random_(Tensor & self, int64_t from, int64_t to, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_clampedRandom(self_, from, to, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_random_(Tensor & self, int64_t to, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_cappedRandom(self_, to, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_random_(Tensor & self, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Bool);
            THBoolTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_random(self_, generator);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_random_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_random(self_, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_random_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _th_multinomial_alias_setup_out(Tensor & J, Tensor & q, const Tensor & probs) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(J);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_dense_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Double);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Long);
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_multinomialAliasSetup(probs_, J_, q_);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_dense_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Float);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Long);
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_setup_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_multinomialAliasSetup(probs_, J_, q_);
            return std::tuple<Tensor &, Tensor &>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _th_multinomial_alias_setup(const Tensor & probs) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(probs);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto probs_ = checked_dense_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup", false, DeviceType::CPU, ScalarType::Double);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THDoubleTensor_multinomialAliasSetup(probs_, J_, q_);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        case ScalarType::Float: {
            auto probs_ = checked_dense_tensor_unwrap(probs, "probs", 1, "_th_multinomial_alias_setup", false, DeviceType::CPU, ScalarType::Float);
            auto J_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto J = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(J_));
            auto q_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto q = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(q_));
            THFloatTensor_multinomialAliasSetup(probs_, J_, q_);
            return std::tuple<Tensor, Tensor>(J, q);
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_setup not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_multinomial_alias_draw_out(Tensor & result, const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(result);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Long);
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Double);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_multinomialAliasDraw(result_, q_, J_, num_samples, generator);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = checked_dense_tensor_unwrap(result, "result", 0, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Long);
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Float);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw_out", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_multinomialAliasDraw(result_, q_, J_, num_samples, generator);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_multinomial_alias_draw(const Tensor & q, const Tensor & J, int64_t num_samples, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(q);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw", false, DeviceType::CPU, ScalarType::Double);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw", false, DeviceType::CPU, ScalarType::Long);
            THDoubleTensor_multinomialAliasDraw(result_, q_, J_, num_samples, generator);
            return result;
            break;
        }
        case ScalarType::Float: {
            auto result_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(scalarTypeToTypeMeta(ScalarType::Long), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto result = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(result_));
            auto q_ = checked_dense_tensor_unwrap(q, "q", 1, "_th_multinomial_alias_draw", false, DeviceType::CPU, ScalarType::Float);
            auto J_ = checked_dense_tensor_unwrap(J, "J", 2, "_th_multinomial_alias_draw", false, DeviceType::CPU, ScalarType::Long);
            THFloatTensor_multinomialAliasDraw(result_, q_, J_, num_samples, generator);
            return result;
            break;
        }
        default:
            AT_ERROR("_th_multinomial_alias_draw not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_uniform_(Tensor & self, double from, double to, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_uniform_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_uniform(self_, from, to, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_uniform_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_uniform(self_, from, to, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_uniform_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_normal_out(Tensor & output, const Tensor & mean, double std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_means(output_, mean_, std, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_means(output_, mean_, std, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_normal(const Tensor & mean, double std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_means(output_, mean_, std, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_means(output_, mean_, std, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_normal_out(Tensor & output, double mean, const Tensor & std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_stddevs(output_, mean, std_, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_stddevs(output_, mean, std_, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_normal(double mean, const Tensor & std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(std);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_stddevs(output_, mean, std_, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_stddevs(output_, mean, std_, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_normal_out(Tensor & output, const Tensor & mean, const Tensor & std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(output);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal_out", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_means_stddevs(output_, mean_, std_, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = checked_dense_tensor_unwrap(output, "output", 0, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal_out", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_means_stddevs(output_, mean_, std_, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_normal(const Tensor & mean, const Tensor & std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(mean);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal", false, DeviceType::CPU, ScalarType::Double);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal_means_stddevs(output_, mean_, std_, generator);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto mean_ = checked_dense_tensor_unwrap(mean, "mean", 1, "_th_normal", false, DeviceType::CPU, ScalarType::Float);
            auto std_ = checked_dense_tensor_unwrap(std, "std", 2, "_th_normal", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal_means_stddevs(output_, mean_, std_, generator);
            return output;
            break;
        }
        default:
            AT_ERROR("_th_normal not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_normal_(Tensor & self, double mean, double std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_normal_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_normal(self_, mean, std, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_normal_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_normal(self_, mean, std, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_normal_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cauchy_(Tensor & self, double median, double sigma, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cauchy_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_cauchy(self_, median, sigma, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_cauchy_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_cauchy(self_, median, sigma, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cauchy_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_log_normal_(Tensor & self, double mean, double std, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_log_normal_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_logNormal(self_, mean, std, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_log_normal_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_logNormal(self_, mean, std, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_log_normal_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_exponential_(Tensor & self, double lambd, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_exponential_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_exponential(self_, lambd, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_exponential_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_exponential(self_, lambd, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_exponential_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_geometric_(Tensor & self, double p, Generator * generator) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Byte);
            THByteTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Char);
            THCharTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Double);
            THDoubleTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Float);
            THFloatTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Int);
            THIntTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Long);
            THLongTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_th_geometric_", false, DeviceType::CPU, ScalarType::Short);
            THShortTensor_geometric(self_, p, generator);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_geometric_ not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _th_cat_out(Tensor & self, TensorList tensors, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Bool);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Bool);
            THBoolTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Byte);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Byte);
            THByteTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Char);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Char);
            THCharTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Double);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Double);
            THDoubleTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Float);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Float);
            THFloatTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Int);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Int);
            THIntTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Long);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
            THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Short);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Short);
            THShortTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::Half);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Half);
            THHalfTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 0, "_th_cat_out", false, DeviceType::CPU, ScalarType::BFloat16);
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _th_cat(TensorList tensors, int64_t dim) {

    // DeviceGuard omitted
    auto dispatch_scalar_type = infer_scalar_type(tensors);
    switch (dispatch_scalar_type) {
        case ScalarType::Bool: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<bool>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Bool);
            THBoolTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Byte: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<uint8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Byte);
            THByteTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Char: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int8_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Char);
            THCharTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Double: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Double);
            THDoubleTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Float: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Float);
            THFloatTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Int: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Int);
            THIntTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Long: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int64_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Long);
            THLongTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Short: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<int16_t>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Short);
            THShortTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::Half: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<Half>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::Half);
            THHalfTensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        case ScalarType::BFloat16: {
            auto self_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<BFloat16>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto self = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(self_));
            auto tensors_ = checked_tensor_list_unwrap(tensors,"tensors",1, Backend::CPU, ScalarType::BFloat16);
            THBFloat16Tensor_catArray(self_, tensors_.data(), tensors_.size(), dim);
            return self;
            break;
        }
        default:
            AT_ERROR("_th_cat not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_binary_cross_entropy_forward_out(Tensor & output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
    if (output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Double);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Double);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward_out", true, DeviceType::CPU, ScalarType::Double);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 4, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Float);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Float);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward_out", true, DeviceType::CPU, ScalarType::Float);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 4, "_thnn_binary_cross_entropy_forward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_binary_cross_entropy_forward(const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
    if (self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward", false, DeviceType::CPU, ScalarType::Double);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward", false, DeviceType::CPU, ScalarType::Double);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward", true, DeviceType::CPU, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_DoubleBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_binary_cross_entropy_forward", false, DeviceType::CPU, ScalarType::Float);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 2, "_thnn_binary_cross_entropy_forward", false, DeviceType::CPU, ScalarType::Float);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 3, "_thnn_binary_cross_entropy_forward", true, DeviceType::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatBCECriterion_updateOutput(globalContext().getTHCState(), self_, target_, output_, reduction, weight_ ? weight_ : NULL);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_binary_cross_entropy_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward_out", true, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward_out", true, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_binary_cross_entropy_backward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_binary_cross_entropy_backward(const Tensor & grad_output, const Tensor & self, const Tensor & target, const Tensor & weight, int64_t reduction) {
    if (grad_output.has_names() || self.has_names() || target.has_names() || weight.has_names()) {
        AT_ERROR(
            "_thnn_binary_cross_entropy_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Double);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Double);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward", true, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_DoubleBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Float);
            auto target_ = checked_dense_tensor_unwrap(target, "target", 3, "_thnn_binary_cross_entropy_backward", false, DeviceType::CPU, ScalarType::Float);
            auto weight_ = checked_dense_tensor_unwrap(weight, "weight", 4, "_thnn_binary_cross_entropy_backward", true, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatBCECriterion_updateGradInput(globalContext().getTHCState(), self_, target_, grad_output_, grad_input_, reduction, weight_ ? weight_ : NULL);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_binary_cross_entropy_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_glu_forward_out(Tensor & output, const Tensor & self, int64_t dim) {
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_glu_forward_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 2, "_thnn_glu_forward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_glu_forward_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 2, "_thnn_glu_forward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_glu_forward(const Tensor & self, int64_t dim) {
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_glu_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_glu_forward", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_DoubleGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_glu_forward", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatGatedLinear_updateOutput(globalContext().getTHCState(), self_, output_, dim);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_glu_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, int64_t dim) {
    if (grad_input.has_names() || grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_glu_backward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_glu_backward(const Tensor & grad_output, const Tensor & self, int64_t dim) {
    if (grad_output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_glu_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_glu_backward", false, DeviceType::CPU, ScalarType::Double);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_DoubleGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_glu_backward", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_glu_backward", false, DeviceType::CPU, ScalarType::Float);
            dim = maybe_wrap_dim(dim, self_);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatGatedLinear_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, dim);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_glu_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor &,Tensor &> _thnn_log_sigmoid_forward_out(Tensor & output, Tensor & buffer, const Tensor & self) {
    if (output.has_names() || buffer.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Double);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Double);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Float);
            auto output_ = checked_dense_tensor_unwrap(output, "output", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Float);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 1, "_thnn_log_sigmoid_forward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            return std::tuple<Tensor &, Tensor &>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
std::tuple<Tensor,Tensor> _thnn_log_sigmoid_forward(const Tensor & self) {
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward", false, DeviceType::CPU, ScalarType::Double);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_DoubleLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_log_sigmoid_forward", false, DeviceType::CPU, ScalarType::Float);
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            auto buffer_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto buffer = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(buffer_));
            THNN_FloatLogSigmoid_updateOutput(globalContext().getTHCState(), self_, output_, buffer_);
            return std::tuple<Tensor, Tensor>(output, buffer);
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_log_sigmoid_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || buffer.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 3, "_thnn_log_sigmoid_backward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_log_sigmoid_backward(const Tensor & grad_output, const Tensor & self, const Tensor & buffer) {
    if (grad_output.has_names() || self.has_names() || buffer.has_names()) {
        AT_ERROR(
            "_thnn_log_sigmoid_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Double);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_DoubleLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Float);
            auto buffer_ = checked_dense_tensor_unwrap(buffer, "buffer", 3, "_thnn_log_sigmoid_backward", false, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatLogSigmoid_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, buffer_);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_log_sigmoid_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softplus_forward_out(Tensor & output, const Tensor & self, Scalar beta, Scalar threshold) {
    if (output.has_names() || self.has_names()) {
        AT_ERROR(
            "_thnn_softplus_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 3, "_thnn_softplus_forward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 3, "_thnn_softplus_forward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softplus_forward(const Tensor & self, Scalar beta, Scalar threshold) {
    if (self.has_names()) {
        AT_ERROR(
            "_thnn_softplus_forward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_DoubleSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            return output;
            break;
        }
        case ScalarType::Float: {
            auto self_ = checked_dense_tensor_unwrap(self, "self", 1, "_thnn_softplus_forward", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto output = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(output_));
            THNN_FloatSoftPlus_updateOutput(globalContext().getTHCState(), self_, output_, beta_, threshold_);
            return output;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_forward not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor & _thnn_softplus_backward_out(Tensor & grad_input, const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
    if (grad_input.has_names() || grad_output.has_names() || self.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_softplus_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Double);
            THNN_DoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = checked_dense_tensor_unwrap(grad_input, "grad_input", 5, "_thnn_softplus_backward_out", false, DeviceType::CPU, ScalarType::Float);
            THNN_FloatSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward_out not supported on CPUType for ", dispatch_scalar_type);
    }
}
Tensor _thnn_softplus_backward(const Tensor & grad_output, const Tensor & self, Scalar beta, Scalar threshold, const Tensor & output) {
    if (grad_output.has_names() || self.has_names() || output.has_names()) {
        AT_ERROR(
            "_thnn_softplus_backward is not yet supported with named tensors. Please drop names via "
            "`tensor = tensor.rename(None)`, call the op with an unnamed tensor, "
            "and set names on the result of the operation.");
    }
    const OptionalDeviceGuard device_guard(device_of(self));
    auto dispatch_scalar_type = infer_scalar_type(self);
    switch (dispatch_scalar_type) {
        case ScalarType::Double: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Double);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Double);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Double);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<double>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_DoubleSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            return grad_input;
            break;
        }
        case ScalarType::Float: {
            auto grad_output_ = checked_dense_tensor_unwrap(grad_output, "grad_output", 1, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Float);
            auto self_ = checked_dense_tensor_unwrap(self, "self", 2, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Float);
            auto beta_ = beta.toDouble();
            auto threshold_ = threshold.toDouble();
            auto output_ = checked_dense_tensor_unwrap(output, "output", 5, "_thnn_softplus_backward", false, DeviceType::CPU, ScalarType::Float);
            auto grad_input_ = c10::make_intrusive<TensorImpl, UndefinedTensorImpl>(c10::Storage(caffe2::TypeMeta::Make<float>(), 0, allocator(), true),TensorTypeId::CPUTensorId).release();
            auto grad_input = Tensor(c10::intrusive_ptr<TensorImpl, UndefinedTensorImpl>::reclaim(grad_input_));
            THNN_FloatSoftPlus_updateGradInput(globalContext().getTHCState(), self_, grad_output_, grad_input_, output_, beta_, threshold_);
            return grad_input;
            break;
        }
        default:
            AT_ERROR("_thnn_softplus_backward not supported on CPUType for ", dispatch_scalar_type);
    }
}

} // namespace th
} // namespace legacy
} // namespace native
} // namespace at
