{
  "category": "confirmed_bug",
  "summary": "DTensor dispatch assertion fails when run_decompositions overrides CIA kernels with _special_op_to_preserve_cia which returns NotImplemented",
  "answer": null,
  "repro_code": "import torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.distributed.tensor.parallel import parallelize_module, ColwiseParallel, RowwiseParallel\nfrom torch.distributed._tensor import init_device_mesh\nfrom torch._decomp import get_decompositions\n\nif not dist.is_initialized():\n    dist.init_process_group(backend=\"nccl\")\n\ndevice_mesh = init_device_mesh(\"cuda\", (dist.get_world_size(),))\nrank = dist.get_rank()\n\nimport torch.utils._pytree\nimport torch.distributed.tensor._dtensor_spec\ntorch.utils._pytree.register_constant(\n    torch.distributed.tensor._dtensor_spec.DTensorSpec\n)\n\nclass ToyModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.in_proj = nn.Linear(10, 320)\n        self.out_proj = nn.Linear(320, 160)\n\n    def forward(self, x):\n        return self.out_proj(torch.relu(self.in_proj(x)))\n\nmodel = ToyModel().to(\"cuda\")\nparallelize_module(model.in_proj, device_mesh, ColwiseParallel())\nparallelize_module(model.out_proj, device_mesh, RowwiseParallel())\n\ninp = torch.rand(2, 10, device=\"cuda\")\nexported_program = torch.export.export(model, (inp,), strict=False)\n\ndecomp_table = get_decompositions([\n    torch.ops.aten.embedding_dense_backward,\n    torch.ops.aten.native_layer_norm_backward,\n    torch.ops.aten.slice_backward,\n    torch.ops.aten.select_backward,\n    torch.ops.aten.norm.ScalarOpt_dim,\n    torch.ops.aten.native_group_norm_backward,\n    torch.ops.aten.upsample_bilinear2d.vec,\n    torch.ops.aten.split.Tensor,\n    torch.ops.aten.split_with_sizes,\n])\n\ndecomposed = exported_program.run_decompositions(decomp_table)\n\ndist.destroy_process_group()",
  "repro_output": null,
  "commit_hash": "965caf0db33d31ae6303d0960f26afd40c083736",
  "fix_description": "The bug is in torch/distributed/tensor/_dispatch.py line 246. During run_decompositions(), the _override_composite_implicit_decomp context manager replaces CompositeImplicitAutograd (CIA) kernels with _special_op_to_preserve_cia (which returns NotImplemented) to indicate that certain ops should be preserved rather than decomposed. When DTensor dispatch encounters an op where sharding propagation fails (NotImplementedError) and the op has a CIA kernel, it falls back to op_call.decompose(). However, decompose() invokes the overridden CIA kernel which returns NotImplemented, causing the assertion 'assert out is not NotImplemented' to fail.\n\nThe fix changes the assertion to a conditional check: if decompose() returns NotImplemented, re-raise the original NotImplementedError instead of asserting. This allows the error to propagate properly and be handled by the caller, rather than crashing with an unhelpful AssertionError.",
  "patch_file": "fix.patch"
}