I1117 14:33:35.229000 3928440 torch/_inductor/config.py:998] compile_threads set to 32
I1117 14:33:36.539000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.540000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
I1117 14:33:36.540000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
I1117 14:33:36.540000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
I1117 14:33:36.541000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
I1117 14:33:36.541000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.542000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
I1117 14:33:36.542000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
I1117 14:33:36.542000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
I1117 14:33:36.543000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
I1117 14:33:36.543000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
I1117 14:33:36.544000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
I1117 14:33:36.544000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
I1117 14:33:36.545000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
I1117 14:33:36.549000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.549000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.550000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
I1117 14:33:36.550000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
I1117 14:33:36.550000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
I1117 14:33:36.551000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.551000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.552000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 14:33:36.552000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 14:33:36.552000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 14:33:36.553000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 14:33:36.553000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 14:33:36.554000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 14:33:36.554000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
I1117 14:33:36.554000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
I1117 14:33:36.555000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
I1117 14:33:36.555000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
I1117 14:33:36.556000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
I1117 14:33:36.556000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
I1117 14:33:36.556000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
I1117 14:33:36.557000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
I1117 14:33:36.557000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
I1117 14:33:36.558000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
I1117 14:33:36.558000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
I1117 14:33:36.558000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
I1117 14:33:36.559000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
I1117 14:33:36.559000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
I1117 14:33:36.560000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
I1117 14:33:36.560000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
I1117 14:33:36.560000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
I1117 14:33:36.561000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
I1117 14:33:36.561000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
I1117 14:33:36.562000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
I1117 14:33:36.562000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
I1117 14:33:36.562000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
I1117 14:33:36.563000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
I1117 14:33:36.563000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
I1117 14:33:36.563000 3928440 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
I1117 14:33:37.730000 3928440 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
I1117 14:33:37.828000 3928440 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[2, 256, 128][32768, 128, 1]cuda:0", L_weight_: "f32[128][1]cuda:0"):
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_weight_ = L_weight_
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:108 in test_fn, code: return dynamic_range_op(x, weight)
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         dynamic_range_139741878273824_default: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_139741878273824.default(l_x_, l_weight_);  l_x_ = l_weight_ = None
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (dynamic_range_139741878273824_default,)
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 14:33:37.830000 3928440 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
V1117 14:33:38.448000 3928440 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fkfbjaflzcyfggyqihdmq4iqil2ddwutqvarvp52n3d56es2x5ai:
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3frwywk5zeepqdekhqjnmr64oq4ljc2ciuu4o2qvkoq5nfwdhu] gm: <lambda>()
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1, arg1_1):
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0]     dynamic_range_139741878273824 = torch.ops.test_lib.dynamic_range_139741878273824.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0]     return (dynamic_range_139741878273824,)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0]     
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ffvbi42jvc443loukbs4l7f2ktyctumpl3p52q36kphvdbcz745] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2, 256, 128]), stride=(32768, 128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [hhafgaghh5icfiiv2s3gtezjixz3usyfz36wro23umj3t2dfcyw] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [sowwwtgijzs2s3vpwmuhvs253coeha2j2ty4eocnpupxckhj2lw] torch_version: Ù˜ï¿½ï¿½ï¿½ï¿½[yEï¿½ï¿½:Gfï¿½ï¿½ï¿½ï¿½ï¿½ï¿½Zï¿½?ï¿½adK?
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bfloat16_atomic_adds_enabled]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_epilogue_fusion_enabled]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_tma_only]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_hash_with_compile_cmd]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.cutlass_prescreening]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cuda.cutlass_enabled_ops]: all
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.use_binary_remote_cache]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.upload_to_binary_remote_cache]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.binary_remote_cache_force_write]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.enable_caching_codegen]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [6trwnwm4voevl4joplmkcssruwgd46kgqfejamut6kq662kstpd] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cuda.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
V1117 14:33:38.454000 3928440 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
V1117 14:33:38.455000 3928440 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fkfbjaflzcyfggyqihdmq4iqil2ddwutqvarvp52n3d56es2x5ai
I1117 14:33:38.456000 3928440 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fkfbjaflzcyfggyqihdmq4iqil2ddwutqvarvp52n3d56es2x5ai
V1117 14:33:38.456000 3928440 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
V1117 14:33:38.457000 3928440 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
I1117 14:33:38.457000 3928440 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[2, 256, 128][32768, 128, 1]cuda:0", arg1_1: "f32[128][1]cuda:0"):
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:108 in test_fn, code: return dynamic_range_op(x, weight)
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         dynamic_range_139741878273824: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_139741878273824.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (dynamic_range_139741878273824,)
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
V1117 14:33:38.476000 3928440 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
V1117 14:33:38.478000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 14:33:38.478000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 14:33:38.479000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %dynamic_range_139741878273824 : [num_users=1] = call_function[target=torch.ops.test_lib.dynamic_range_139741878273824.default](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 14:33:38.480000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function test_lib::dynamic_range_139741878273824 at 0x7f1613c66160>
I1117 14:33:38.481000 3928440 torch/_inductor/kernel/custom_op.py:713] [0/0] === Range-based Autotuning for dynamic_range_autotuned ===
I1117 14:33:38.483000 3928440 torch/_inductor/kernel/custom_op.py:714] [0/0] Dispatch on: x[1], Ranges: [(1, 512), (513, 2048), (2049, inf)]
V1117 14:33:38.511000 3928440 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 14:33:38.512000 3928440 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 14:33:38.514000 3928440 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 14:33:38.515000 3928440 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 14:33:38.516000 3928440 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 14:33:38.517000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 14:33:38.518000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 14:33:38.518000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 14:33:38.519000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f1824205080>
V1117 14:33:38.521000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 14:33:38.521000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
V1117 14:33:38.540000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 14:33:38.540000 3928440 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf0,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 14:33:38.541000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 14:33:38.542000 3928440 torch/_inductor/kernel/custom_op.py:629] [0/0] Inlining winning choice: dynamic_range_autotuned_range_1_512_long_sequence_impl_2 (name=dynamic_range_autotuned_range_1_512)
V1117 14:33:38.543000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 14:33:38.543000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f1824205080>
V1117 14:33:38.544000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 14:33:38.545000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
I1117 14:33:38.547000 3928440 torch/_inductor/kernel/custom_op.py:775] [0/0] Range [1, 512]: Selected dynamic_range_autotuned_range_1_512_long_sequence_impl_2
V1117 14:33:38.563000 3928440 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 14:33:38.564000 3928440 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 14:33:38.565000 3928440 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 14:33:38.566000 3928440 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 14:33:38.567000 3928440 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 14:33:38.568000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 14:33:38.569000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 14:33:38.570000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 14:33:38.570000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f1824205260>
V1117 14:33:38.571000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 14:33:38.571000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f1824206f20>
V1117 14:33:38.572000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 14:33:38.573000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f1824206f20>
V1117 14:33:38.573000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 14:33:38.574000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f1824205260>
V1117 14:33:38.575000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 14:33:38.575000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
V1117 14:33:38.577000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 14:33:38.578000 3928440 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf2,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 14:33:38.578000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 14:33:38.579000 3928440 torch/_inductor/kernel/custom_op.py:629] [0/0] Inlining winning choice: dynamic_range_autotuned_range_513_2048_short_sequence_impl_3 (name=dynamic_range_autotuned_range_513_2048)
V1117 14:33:38.580000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 14:33:38.580000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f1824205260>
V1117 14:33:38.581000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 14:33:38.581000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f1824206f20>
V1117 14:33:38.582000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 14:33:38.583000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f1824206f20>
V1117 14:33:38.584000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 14:33:38.584000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f1824205260>
V1117 14:33:38.585000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 14:33:38.586000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
I1117 14:33:38.588000 3928440 torch/_inductor/kernel/custom_op.py:775] [0/0] Range [513, 2048]: Selected dynamic_range_autotuned_range_513_2048_short_sequence_impl_3
V1117 14:33:38.605000 3928440 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 14:33:38.606000 3928440 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 14:33:38.607000 3928440 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 14:33:38.608000 3928440 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 14:33:38.609000 3928440 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 14:33:38.611000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 14:33:38.611000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 14:33:38.612000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 14:33:38.613000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f18241f2340>
V1117 14:33:38.613000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 14:33:38.614000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
V1117 14:33:38.615000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 14:33:38.616000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f182424ec00>
V1117 14:33:38.617000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 14:33:38.617000 3928440 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf4,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 14:33:38.618000 3928440 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 14:33:38.619000 3928440 torch/_inductor/kernel/custom_op.py:629] [0/0] Inlining winning choice: dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7 (name=dynamic_range_autotuned_range_2049_inf)
V1117 14:33:38.620000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 14:33:38.620000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f18241f2340>
V1117 14:33:38.621000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 14:33:38.622000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
V1117 14:33:38.623000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 14:33:38.623000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f182424ec00>
I1117 14:33:38.625000 3928440 torch/_inductor/kernel/custom_op.py:775] [0/0] Range [2049, inf]: Selected dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7
V1117 14:33:38.627000 3928440 torch/_inductor/kernel/custom_op.py:413] [0/0] Matched choice 'dynamic_range_autotuned_range_1_512_long_sequence_impl_2' to decomposition[2] 'long_sequence_impl'
V1117 14:33:38.628000 3928440 torch/_inductor/kernel/custom_op.py:413] [0/0] Matched choice 'dynamic_range_autotuned_range_513_2048_short_sequence_impl_3' to decomposition[0] 'short_sequence_impl'
V1117 14:33:38.630000 3928440 torch/_inductor/kernel/custom_op.py:413] [0/0] Matched choice 'dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7' to decomposition[1] 'medium_sequence_impl'
I1117 14:33:38.631000 3928440 torch/_inductor/kernel/custom_op.py:802] [0/0] âœ“ Completed autotuning for 3 ranges
I1117 14:33:38.634000 3928440 torch/_inductor/kernel/custom_op.py:823] [0/0] âœ“ Generated dispatch function saved to: /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py
I1117 14:33:38.636000 3928440 torch/_inductor/kernel/custom_op.py:828] [0/0] Creating runtime dispatch using make_fx tracing
V1117 14:33:38.638000 3928440 torch/_inductor/kernel/custom_op.py:873] [0/0] Tracing dispatch function with make_fx...
I1117 14:33:38.641000 3928440 torch/_inductor/kernel/custom_op.py:881] [0/0] Attempting to mark dimension 1 as unbacked for 'x' (tensor_inputs[0])
I1117 14:33:38.643000 3928440 torch/_inductor/kernel/custom_op.py:886] [0/0]   dispatch_fake_tensor type: <class 'torch._subclasses.fake_tensor.FakeTensor'>
I1117 14:33:38.646000 3928440 torch/_inductor/kernel/custom_op.py:887] [0/0]   dispatch_fake_tensor.size(): torch.Size([2, 256, 128])
I1117 14:33:38.648000 3928440 torch/_inductor/kernel/custom_op.py:893] [0/0]   dim_size at index 1: 256, type: <class 'int'>
I1117 14:33:38.650000 3928440 torch/_inductor/kernel/custom_op.py:894] [0/0]   hasattr(_make_unbacked_symint): False
W1117 14:33:38.652000 3928440 torch/_inductor/kernel/custom_op.py:901] [0/0] Could not mark dimension 1 as unbacked - _make_unbacked_symint not available on type <class 'int'>
V1117 14:33:38.659000 3928440 torch/_inductor/kernel/custom_op.py:922] [0/0] GraphModule created with 5 nodes
V1117 14:33:38.663000 3928440 torch/_inductor/kernel/custom_op.py:929] [0/0] Inlining dispatch graph to IR nodes...
V1117 14:33:38.666000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%arg1_1, [1, 1, -1]), kwargs = {}) 
V1117 14:33:38.666000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function view at 0x7f1824205080>
V1117 14:33:38.668000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %view), kwargs = {}) 
V1117 14:33:38.668000 3928440 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f182426a7a0>
I1117 14:33:38.671000 3928440 torch/_inductor/kernel/custom_op.py:935] [0/0] âœ“ Range-based dispatch created using make_fx + inline
V1117 14:33:38.673000 3928440 torch/_inductor/graph.py:1602] [0/0] lowering return (dynamic_range_139741878273824,) 
V1117 14:33:38.674000 3928440 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   name=buf0,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.690000 3928440 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824, mul, view]),
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.691000 3928440 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   name=buf2,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.693000 3928440 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824, mul, permu...,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.695000 3928440 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   name=buf4,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824]),
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.697000 3928440 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf5', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824, clone, mul]),
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.699000 3928440 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf6', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=dynamic_range_139741878273824,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_139741878273824, mul, view]),
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 108, in test_fn,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 14:33:38.700000 3928440 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 14:33:38.702000 3928440 torch/_inductor/scheduler.py:3059] [0/0] scheduling output buf6
V1117 14:33:38.703000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf5
V1117 14:33:38.704000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op5
V1117 14:33:38.705000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf4
V1117 14:33:38.705000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op4
V1117 14:33:38.705000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf3
V1117 14:33:38.706000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op3
V1117 14:33:38.706000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf2
V1117 14:33:38.707000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op2
V1117 14:33:38.708000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf1
V1117 14:33:38.708000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op1
V1117 14:33:38.709000 3928440 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf0
V1117 14:33:38.709000 3928440 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op0
I1117 14:33:38.722000 3928440 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 14:33:38.723000 3928440 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 14:33:38.724000 3928440 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 14:33:38.725000 3928440 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 14:33:38.726000 3928440 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 14:33:38.743000 3928440 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node op6 with estimated runtime 0.000214
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, arg0_1, %get_index), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, arg1_1, %get_index_1), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, buf6, %get_index_2, %mul, None), kwargs = {})
V1117 14:33:38.753000 3928440 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 14:33:38.758000 3928440 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 14:33:38.760000 3928440 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 14:33:38.762000 3928440 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 14:33:38.769000 3928440 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_139741878273824_0
V1117 14:33:38.772000 3928440 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 14:33:38.780000 3928440 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_139741878273824_0
V1117 14:33:38.781000 3928440 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 14:33:38.781000 3928440 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 14:33:38.782000 3928440 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1117 14:33:38.878000 3928440 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/6k/c6kdqjlruidjrwbfjw3sd6xltyvojwriuyx4yy5t3ydozdnhekun.py
I1117 14:33:38.879000 3928440 torch/_inductor/triton_bundler.py:197] [0/0] Saving 1 statically launchable CachingAutotuners
V1117 14:33:38.880000 3928440 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 14:33:38.881000 3928440 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 14:33:38.882000 3928440 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fkfbjaflzcyfggyqihdmq4iqil2ddwutqvarvp52n3d56es2x5ai
V1117 14:33:38.883000 3928440 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.435s
I1117 14:33:38.884000 3928440 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
I1117 14:33:38.884000 3928440 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
I1117 14:33:38.885000 3928440 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
I1117 14:33:38.885000 3928440 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
V1117 14:33:38.929000 3928440 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_dynamic_range_139741878273824_0, get:
V1117 14:33:38.930000 3928440 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005632, nreg 22, nspill 0, #shared-mem 0
V1117 14:33:38.930000 3928440 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 14:33:38.931000 3928440 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_dynamic_range_139741878273824_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 14:33:38.931000 3928440 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/hb/eecc0f8756eb4588fe855d8797fb33730d161dad73eac54c56d1e5c77295a1cb.best_config
Running test on device: cuda

=== Verifying all implementations produce equivalent results ===
  âœ“ short_impl correct for seq_len=256
  âœ“ medium_impl correct for seq_len=256
  âœ“ long_impl correct for seq_len=256
  âœ“ short_impl correct for seq_len=1024
  âœ“ medium_impl correct for seq_len=1024
  âœ“ long_impl correct for seq_len=1024
  âœ“ short_impl correct for seq_len=4096
  âœ“ medium_impl correct for seq_len=4096
  âœ“ long_impl correct for seq_len=4096

=== Testing autotuning with compilation ===
  âœ“ Compiled version produces correct results
  âœ“ Dispatch function generated with torch.cond at /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py

âœ… All tests passed!
I1117 14:33:41.461000 3928440 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1117 14:33:41.461000 3928440 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 1, exception: 0}
I1117 14:33:41.461000 3928440 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 1, exception: 0}
I1117 14:33:41.461000 3928440 torch/_inductor/remote_cache.py:432] 
