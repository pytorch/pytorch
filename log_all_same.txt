I1117 22:13:48.520000 369944 torch/_inductor/config.py:998] compile_threads set to 32
I1117 22:13:49.819000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.820000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
I1117 22:13:49.820000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
I1117 22:13:49.821000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
I1117 22:13:49.821000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
I1117 22:13:49.822000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.822000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
I1117 22:13:49.822000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
I1117 22:13:49.823000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:13:49.823000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:13:49.824000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:13:49.824000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
I1117 22:13:49.825000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
I1117 22:13:49.825000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
I1117 22:13:49.829000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.829000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.830000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
I1117 22:13:49.830000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:13:49.831000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
I1117 22:13:49.831000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.831000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.832000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:13:49.832000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:13:49.833000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:13:49.833000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:13:49.833000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:13:49.834000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:13:49.834000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
I1117 22:13:49.835000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
I1117 22:13:49.835000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
I1117 22:13:49.836000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
I1117 22:13:49.836000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
I1117 22:13:49.836000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
I1117 22:13:49.837000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
I1117 22:13:49.837000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
I1117 22:13:49.837000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
I1117 22:13:49.838000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
I1117 22:13:49.838000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
I1117 22:13:49.839000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
I1117 22:13:49.839000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
I1117 22:13:49.839000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
I1117 22:13:49.840000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
I1117 22:13:49.840000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
I1117 22:13:49.841000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
I1117 22:13:49.841000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
I1117 22:13:49.841000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
I1117 22:13:49.842000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
I1117 22:13:49.842000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
I1117 22:13:49.842000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
I1117 22:13:49.843000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
I1117 22:13:49.843000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
I1117 22:13:49.844000 369944 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
I1117 22:13:50.517000 369944 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
I1117 22:13:50.613000 369944 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[2, 256, 128][32768, 128, 1]cuda:0", L_weight_: "f32[128][1]cuda:0"):
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_weight_ = L_weight_
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_all_same_impl.py:62 in test_fn, code: return all_same_impl_op(x, weight)
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         all_same_impl_139706237331520_default: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.all_same_impl_139706237331520.default(l_x_, l_weight_);  l_x_ = l_weight_ = None
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (all_same_impl_139706237331520_default,)
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 22:13:50.616000 369944 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
V1117 22:13:51.224000 369944 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key fbdl7dwkjuzvu5qzkzweijo6blcwpmsssmivdtc3afve66nzqg4e:
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [54zv3wjng3nmoxs742haqlxiyujpulsyhde6q6c22lj44p2cf3a] gm: <lambda>()
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1, arg1_1):
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0]     all_same_impl_139706237331520 = torch.ops.test_lib.all_same_impl_139706237331520.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0]     return (all_same_impl_139706237331520,)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0]     
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ffvbi42jvc443loukbs4l7f2ktyctumpl3p52q36kphvdbcz745] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2, 256, 128]), stride=(32768, 128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [hhafgaghh5icfiiv2s3gtezjixz3usyfz36wro23umj3t2dfcyw] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [mg64ekekr3poks6x2x3ywwrxpdd6tk4fb4fciybz7gmmssguevi] torch_version: ï¿½ï¿½e*ï¿½ï¿½ï¿½ï¿½Ï™ï¿½ï¿½ï¿½n ï¿½ï¿½Ñ˜ï¿½ï¿½$	ï¿½l
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bfloat16_atomic_adds_enabled]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_epilogue_fusion_enabled]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_tma_only]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_hash_with_compile_cmd]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.cutlass_prescreening]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cuda.cutlass_enabled_ops]: all
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.use_binary_remote_cache]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.upload_to_binary_remote_cache]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.binary_remote_cache_force_write]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.enable_caching_codegen]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [6trwnwm4voevl4joplmkcssruwgd46kgqfejamut6kq662kstpd] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cuda.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
V1117 22:13:51.229000 369944 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
V1117 22:13:51.230000 369944 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: fbdl7dwkjuzvu5qzkzweijo6blcwpmsssmivdtc3afve66nzqg4e
I1117 22:13:51.231000 369944 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key fbdl7dwkjuzvu5qzkzweijo6blcwpmsssmivdtc3afve66nzqg4e
V1117 22:13:51.231000 369944 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
V1117 22:13:51.232000 369944 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
I1117 22:13:51.232000 369944 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[2, 256, 128][32768, 128, 1]cuda:0", arg1_1: "f32[128][1]cuda:0"):
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_all_same_impl.py:62 in test_fn, code: return all_same_impl_op(x, weight)
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         all_same_impl_139706237331520: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.all_same_impl_139706237331520.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (all_same_impl_139706237331520,)
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
V1117 22:13:51.251000 369944 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
V1117 22:13:51.253000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.254000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.255000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %all_same_impl_139706237331520 : [num_users=1] = call_function[target=torch.ops.test_lib.all_same_impl_139706237331520.default](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.255000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function test_lib::all_same_impl_139706237331520 at 0x7f0c2bf665c0>
I1117 22:13:51.256000 369944 torch/_inductor/kernel/custom_op.py:747] [0/0] === Range-based Autotuning for all_same_impl_autotuned ===
I1117 22:13:51.257000 369944 torch/_inductor/kernel/custom_op.py:748] [0/0] Dispatch on: x[1], Ranges: [(1, 512), (513, 2048), (2049, inf)]
V1117 22:13:51.270000 369944 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:13:51.270000 369944 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 22:13:51.272000 369944 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 22:13:51.273000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_1_512_simple_impl_1)
V1117 22:13:51.273000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_1_512_simple_impl_2)
V1117 22:13:51.274000 369944 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 22:13:51.274000 369944 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:13:51.275000 369944 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
/data/users/tianren/pytorch/torch/_inductor/select_algorithm.py:3323: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  current_size = base.storage().size()
V1117 22:13:51.278000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.279000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.279000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.280000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.300000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.300000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_arg1_1, i2)
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.309000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.310000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_buf0
I1117 22:13:51.318000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.318000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.319000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.320000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.320000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.334000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_op0 with estimated runtime 0.000214
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_0_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.342000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.347000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.348000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.349000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.355000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.356000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.364000 369944 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.365000 369944 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 22:13:51.365000 369944 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 22:13:51.366000 369944 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1117 22:13:51.456000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/qy/cqyq4sul4yxow6jryin2ia2t6gd5xdgo223wj35whss4xhkry7xq.py
V1117 22:13:51.474000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.475000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.475000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.476000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.477000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.486000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.487000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.488000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.488000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.490000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.490000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_arg1_1, i2)
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.493000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.494000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_buf0
I1117 22:13:51.498000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.498000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.499000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.500000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.500000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.502000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_op0 with estimated runtime 0.000214
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_1_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.504000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.506000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.507000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.508000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.509000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.510000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.516000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/wn/cwnm47qqkfxm527ruxkczcffxafphwcbo47sijskfkomfbful5un.py
V1117 22:13:51.533000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.533000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.534000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.534000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.535000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.544000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.545000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.546000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.546000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.548000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.548000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_arg1_1, i2)
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.551000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_buf0
I1117 22:13:51.555000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.556000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.556000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.557000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.558000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.560000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_op0 with estimated runtime 0.000214
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_1_512_simple_impl_2_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.562000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.564000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.565000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.566000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.567000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.568000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.572000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/xv/cxvvacvrkbqerlbpocepschpfhqkary3uo4j6uqcpx2kue74x747.py
V1117 22:13:51.589000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.589000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.590000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.590000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.591000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "all_same_impl_autotuned_range_1_512_simple_impl_0", "best_kernel_desc": "CustomOp simple_impl", "best_time": 0.00559999980032444}
V1117 22:13:51.609000 369944 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.33s
V1117 22:13:51.611000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.611000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.612000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.612000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.614000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.615000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf0,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]         return all_same_impl_op(x, weight),
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:13:51.615000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:13:51.616000 369944 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: all_same_impl_autotuned_range_1_512_simple_impl_0 (name=all_same_impl_autotuned_range_1_512)
V1117 22:13:51.617000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.617000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
I1117 22:13:51.619000 369944 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [1, 512]: Selected all_same_impl_autotuned_range_1_512_simple_impl_0
V1117 22:13:51.631000 369944 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:13:51.632000 369944 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 22:13:51.633000 369944 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 22:13:51.634000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_513_2048_simple_impl_4)
V1117 22:13:51.634000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_513_2048_simple_impl_5)
V1117 22:13:51.635000 369944 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 22:13:51.635000 369944 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:13:51.636000 369944 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1117 22:13:51.638000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.639000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.639000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.640000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.641000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.642000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_arg1_1, i2)
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.644000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.645000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_buf0
I1117 22:13:51.649000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.650000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.650000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.651000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.651000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.654000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_op0 with estimated runtime 0.000214
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_3_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.656000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.657000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.658000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.659000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.661000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.661000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.665000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/jz/cjzkg7yxoft3cy5hq3huccdnevwrxglqo7mdooz3sske7axzgvns.py
V1117 22:13:51.682000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.683000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.683000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.684000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.685000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.694000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.695000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.695000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.696000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.697000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.698000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_arg1_1, i2)
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.700000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.701000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_buf0
I1117 22:13:51.705000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.705000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.706000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.707000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.707000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.710000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_op0 with estimated runtime 0.000214
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_4_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.712000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.713000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.714000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.715000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.717000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.717000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.721000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/wa/cwauihuiff6cjp6cej43de4duf3e23naqs6xnvkrtszptgclzy7r.py
V1117 22:13:51.738000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.738000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.739000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.739000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.740000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.749000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.750000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.751000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.751000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.753000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.753000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_arg1_1, i2)
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.756000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_buf0
I1117 22:13:51.760000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.761000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.761000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.762000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.763000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.765000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_op0 with estimated runtime 0.000214
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_513_2048_simple_impl_5_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.767000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.768000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.769000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.770000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.771000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.772000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.776000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/ne/cne6opxjubtszshijrege4hwbweipfc6h74jzhjopvz5mp7dv3x2.py
V1117 22:13:51.792000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.793000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.794000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.794000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.795000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "all_same_impl_autotuned_range_513_2048_simple_impl_4", "best_kernel_desc": "CustomOp simple_impl", "best_time": 0.0055680000223219395}
V1117 22:13:51.813000 369944 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.18s
V1117 22:13:51.814000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.815000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.816000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.816000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.818000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.818000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf2,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]         return all_same_impl_op(x, weight),
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:13:51.819000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:13:51.819000 369944 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: all_same_impl_autotuned_range_513_2048_simple_impl_4 (name=all_same_impl_autotuned_range_513_2048)
V1117 22:13:51.820000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.821000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
I1117 22:13:51.822000 369944 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [513, 2048]: Selected all_same_impl_autotuned_range_513_2048_simple_impl_4
V1117 22:13:51.833000 369944 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:13:51.834000 369944 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
I1117 22:13:51.835000 369944 torch/_inductor/select_algorithm.py:3142] [0/0] Multithreaded precompilation for 4 choices using 4 worker threads
V1117 22:13:51.836000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_2049_inf_simple_impl_7)
V1117 22:13:51.836000 369944 torch/_inductor/select_algorithm.py:3186] [0/0] Skipping already seen choice: SubgraphCaller(all_same_impl_autotuned_range_2049_inf_simple_impl_8)
V1117 22:13:51.837000 369944 torch/_inductor/select_algorithm.py:3212] [0/0] Waiting on futures
V1117 22:13:51.838000 369944 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:13:51.838000 369944 torch/_inductor/select_algorithm.py:2880] [0/0] Starting autotuning
V1117 22:13:51.840000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.842000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.843000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.843000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.845000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.845000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_arg1_1, i2)
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.847000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.848000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_buf0
I1117 22:13:51.852000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.852000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.853000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.854000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.854000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.857000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_op0 with estimated runtime 0.000214
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_6_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.859000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.860000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.861000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.862000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.863000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.864000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.867000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/pl/cplmhpqr74xy4molarb2xqtdjvobsmzijky5rbvdhfni4g3whbfr.py
V1117 22:13:51.884000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.885000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.885000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.886000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.887000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.896000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.896000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.897000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.898000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.899000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.900000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_arg1_1, i2)
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.902000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.903000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_buf0
I1117 22:13:51.906000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.907000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.908000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.908000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.909000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.911000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_op0 with estimated runtime 0.000214
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_7_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.913000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.915000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.916000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.917000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.918000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.919000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.923000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/gd/cgdsu3sqqw2nmxexo2mjx7d5x7xhknmr35llmclqc5djnuxzq5lc.py
V1117 22:13:51.939000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.940000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.941000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.941000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.942000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
V1117 22:13:51.951000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:51.952000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:51.952000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:51.953000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:51.954000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:51.955000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_arg1_1, i2)
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:51.957000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:51.958000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_buf0
I1117 22:13:51.962000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:51.962000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:51.963000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:51.964000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:51.964000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:51.966000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_op0 with estimated runtime 0.000214
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_arg0_1, %get_index), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_arg1_1, %get_index_1), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, benchmark_all_same_impl_autotuned_range_2049_inf_simple_impl_8_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:51.968000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:51.970000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.971000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.972000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:51.973000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:51.974000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:51.977000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/bl/cblqmifcvblulxqvo363h7usjd7vez2mis3vyzzisv4krukkq26x.py
V1117 22:13:51.994000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] [0/0] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:51.995000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005568, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:51.995000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.996000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] [0/0] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005504, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:51.996000 369944 torch/_inductor/runtime/autotune_cache.py:310] [0/0] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
Autotune Choices Stats:
{"num_choices": 4, "num_triton_choices": 0, "best_kernel": "all_same_impl_autotuned_range_2049_inf_simple_impl_6", "best_kernel_desc": "CustomOp simple_impl", "best_time": 0.00559999980032444}
V1117 22:13:52.014000 369944 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.18s
V1117 22:13:52.016000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:13:52.016000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:13:52.017000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:52.018000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:52.019000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:13:52.020000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf4,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]         return all_same_impl_op(x, weight),
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:13:52.020000 369944 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:13:52.021000 369944 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: all_same_impl_autotuned_range_2049_inf_simple_impl_6 (name=all_same_impl_autotuned_range_2049_inf)
V1117 22:13:52.022000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:52.022000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
I1117 22:13:52.024000 369944 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [2049, inf]: Selected all_same_impl_autotuned_range_2049_inf_simple_impl_6
V1117 22:13:52.025000 369944 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'all_same_impl_autotuned_range_1_512_simple_impl_0' to decomposition[0] 'simple_impl'
V1117 22:13:52.026000 369944 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'all_same_impl_autotuned_range_513_2048_simple_impl_4' to decomposition[0] 'simple_impl'
V1117 22:13:52.027000 369944 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'all_same_impl_autotuned_range_2049_inf_simple_impl_6' to decomposition[0] 'simple_impl'
I1117 22:13:52.027000 369944 torch/_inductor/kernel/custom_op.py:819] [0/0] âœ“ Completed autotuning for 3 ranges
V1117 22:13:52.029000 369944 torch/_inductor/kernel/custom_op.py:356] [0/0] Merging range [513, 2048] into [1, 2048] (same impl: simple_impl)
V1117 22:13:52.029000 369944 torch/_inductor/kernel/custom_op.py:356] [0/0] Merging range [2049, inf] into [1, inf] (same impl: simple_impl)
I1117 22:13:52.030000 369944 torch/_inductor/kernel/custom_op.py:377] [0/0] Range merging: reduced from 3 ranges to 1 ranges
I1117 22:13:52.030000 369944 torch/_inductor/kernel/custom_op.py:824] [0/0] After merging: 1 unique implementations across 1 ranges
I1117 22:13:52.031000 369944 torch/_inductor/kernel/custom_op.py:833] [0/0] âœ“ All ranges selected the same implementation - skipping dispatch, using direct inline
I1117 22:13:52.036000 369944 torch/_inductor/kernel/custom_op.py:855] [0/0] Inlining single implementation: simple_impl
V1117 22:13:52.037000 369944 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:13:52.038000 369944 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f0e4c2aeac0>
V1117 22:13:52.040000 369944 torch/_inductor/graph.py:1602] [0/0] lowering return (all_same_impl_139706237331520,) 
V1117 22:13:52.041000 369944 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   name=buf0,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.046000 369944 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.047000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   name=buf2,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.048000 369944 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.049000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   name=buf4,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520]),
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.050000 369944 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf5', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.051000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf6', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=all_same_impl_139706237331520,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([all_same_impl_139706237331520, mul]),
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_all_same_impl.py", line 62, in test_fn,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]       return all_same_impl_op(x, weight),
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:13:52.052000 369944 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:13:52.053000 369944 torch/_inductor/scheduler.py:3059] [0/0] scheduling output buf6
V1117 22:13:52.054000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf5
V1117 22:13:52.055000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op5
V1117 22:13:52.055000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf4
V1117 22:13:52.055000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op4
V1117 22:13:52.056000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf3
V1117 22:13:52.056000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op3
V1117 22:13:52.057000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf2
V1117 22:13:52.057000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op2
V1117 22:13:52.058000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf1
V1117 22:13:52.058000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op1
V1117 22:13:52.058000 369944 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf0
V1117 22:13:52.059000 369944 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op0
I1117 22:13:52.063000 369944 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:13:52.064000 369944 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:13:52.064000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:13:52.065000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:13:52.066000 369944 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:13:52.068000 369944 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node op6 with estimated runtime 0.000214
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, arg0_1, %get_index), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, arg1_1, %get_index_1), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, buf6, %get_index_2, %mul, None), kwargs = {})
V1117 22:13:52.071000 369944 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:13:52.073000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:52.074000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:52.075000 369944 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:13:52.076000 369944 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_all_same_impl_139706237331520_0
V1117 22:13:52.078000 369944 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:13:52.082000 369944 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/h5/ch5fwn3lz2o7bkrztvjgw726jru2zzztxuzjrp4zygetqlzqr42r.py
I1117 22:13:52.086000 369944 torch/_inductor/triton_bundler.py:197] [0/0] Saving 10 statically launchable CachingAutotuners
V1117 22:13:52.086000 369944 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 22:13:52.087000 369944 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 22:13:52.087000 369944 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: fbdl7dwkjuzvu5qzkzweijo6blcwpmsssmivdtc3afve66nzqg4e
V1117 22:13:52.090000 369944 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.867s
I1117 22:13:52.091000 369944 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
I1117 22:13:52.091000 369944 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
I1117 22:13:52.092000 369944 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
I1117 22:13:52.092000 369944 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
V1117 22:13:52.133000 369944 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_all_same_impl_139706237331520_0, get:
V1117 22:13:52.134000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:52.135000 369944 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 12, nspill 0, #shared-mem 0
V1117 22:13:52.135000 369944 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_all_same_impl_139706237331520_0: XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005536, nreg 22, nspill 0, #shared-mem 0
V1117 22:13:52.136000 369944 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/ue/a42ff2dcd1de4b77795c25f90200ad9e96ef700e33e4fe884d61e751670a720c.best_config
Running test on device: cuda

=== Testing compilation ===
  âœ“ Compiled version produces correct results
  âœ“ No dispatch file generated (correctly skipped)

âœ… Test completed!
Exception ignored in atexit callback: <function shutdown_compile_workers at 0x7f0c40c059e0>
Traceback (most recent call last):
  File "/data/users/tianren/pytorch/torch/_inductor/async_compile.py", line 151, in shutdown_compile_workers
    pool.shutdown()
  File "/data/users/tianren/pytorch/torch/_inductor/compile_worker/subproc_pool.py", line 342, in shutdown
    self.process.wait(300)
  File "/home/tianren/.conda/envs/pytorch-3.12/lib/python3.12/subprocess.py", line 1264, in wait
    return self._wait(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/tianren/.conda/envs/pytorch-3.12/lib/python3.12/subprocess.py", line 2047, in _wait
    time.sleep(delay)
KeyboardInterrupt: 
I1117 22:13:52.882000 369944 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1117 22:13:52.882000 369944 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 10, exception: 0}
I1117 22:13:52.882000 369944 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 10, exception: 0}
I1117 22:13:52.882000 369944 torch/_inductor/remote_cache.py:432] 
