// @generated by tools/codegen/gen.py from DispatchKeyNativeFunctions.cpp
#include <torch/csrc/lazy/core/tensor.h>
#include <torch/csrc/lazy/core/shape_inference.h>
#include <ATen/Functions.h>
#include <ATen/MetaFunctions.h>
#include <ATen/Operators.h>
#include <torch/csrc/lazy/core/lazy_graph_executor.h>
#include <torch/csrc/lazy/core/metrics.h>
#include <torch/csrc/lazy/core/shape.h>
#include <lazy_tensor_core/csrc/ts_backend/aten_eager_fallback.h>
#include <torch/csrc/lazy/generated/LazyNativeFunctions.h>
#include <torch/csrc/lazy/generated/LazyLazyIr.h>


namespace torch {
namespace lazy {

    at::Tensor LazyNativeFunctions::_adaptive_avg_pool2d(const at::Tensor & self, at::IntArrayRef output_size) {
        
        if (force_eager_fallback(at::aten::_adaptive_avg_pool2d)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_adaptive_avg_pool2d)>::call(
                self,
                output_size
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape__adaptive_avg_pool2d(self, output_size);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::AdaptiveAvgPool2d>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(output_size.begin(), output_size.end()),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::_adaptive_avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::_adaptive_avg_pool2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_adaptive_avg_pool2d_backward)>::call(
                grad_output,
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape__adaptive_avg_pool2d_backward(grad_output, self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::AdaptiveAvgPool2dBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::_log_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
        
        if (force_eager_fallback(at::aten::_log_softmax)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_log_softmax)>::call(
                self,
                dim,
                half_to_float
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::_log_softmax(self, dim, half_to_float);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LogSoftmax>(lazy_self->GetIrValue(),
                              dim,
                              half_to_float,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::_log_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
        
        if (force_eager_fallback(at::aten::_log_softmax_backward_data)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_log_softmax_backward_data)>::call(
                grad_output,
                output,
                dim,
                input_dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(output, *common_device);
        auto out_meta = at::meta::_log_softmax_backward_data(grad_output, output, dim, input_dtype);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LogSoftmaxBackwardData>(lazy_grad_output->GetIrValue(),
                              lazy_output->GetIrValue(),
                              dim,
                              input_dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::_softmax(const at::Tensor & self, int64_t dim, bool half_to_float) {
        
        if (force_eager_fallback(at::aten::_softmax)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_softmax)>::call(
                self,
                dim,
                half_to_float
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::_softmax(self, dim, half_to_float);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Softmax>(lazy_self->GetIrValue(),
                              dim,
                              half_to_float,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::_softmax_backward_data(const at::Tensor & grad_output, const at::Tensor & output, int64_t dim, at::ScalarType input_dtype) {
        
        if (force_eager_fallback(at::aten::_softmax_backward_data)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(_softmax_backward_data)>::call(
                grad_output,
                output,
                dim,
                input_dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(output, *common_device);
        auto out_meta = at::meta::_softmax_backward_data(grad_output, output, dim, input_dtype);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SoftmaxBackwardData>(lazy_grad_output->GetIrValue(),
                              lazy_output->GetIrValue(),
                              dim,
                              input_dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::abs(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::abs)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(abs)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_abs(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Abs>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::add(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
        
        if (force_eager_fallback(at::aten::add)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(add, Tensor)>::call(
                self,
                other,
                alpha
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::add(self, other, alpha);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::AddTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::addcdiv(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
        
        if (force_eager_fallback(at::aten::addcdiv)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(addcdiv)>::call(
                self,
                tensor1,
                tensor2,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, tensor1, tensor2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_tensor1 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(tensor1, *common_device);
        torch::lazy::LazyTensorPtr lazy_tensor2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(tensor2, *common_device);
        auto out_meta = at::meta::addcdiv(self, tensor1, tensor2, value);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Addcdiv>(lazy_self->GetIrValue(),
                              lazy_tensor1->GetIrValue(),
                              lazy_tensor2->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(value),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::addcmul(const at::Tensor & self, const at::Tensor & tensor1, const at::Tensor & tensor2, const at::Scalar & value) {
        
        if (force_eager_fallback(at::aten::addcmul)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(addcmul)>::call(
                self,
                tensor1,
                tensor2,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, tensor1, tensor2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_tensor1 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(tensor1, *common_device);
        torch::lazy::LazyTensorPtr lazy_tensor2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(tensor2, *common_device);
        auto out_meta = at::meta::addcmul(self, tensor1, tensor2, value);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Addcmul>(lazy_self->GetIrValue(),
                              lazy_tensor1->GetIrValue(),
                              lazy_tensor2->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(value),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
        
        if (force_eager_fallback(at::aten::addmm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(addmm)>::call(
                self,
                mat1,
                mat2,
                beta,
                alpha
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, mat1, mat2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_mat1 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mat1, *common_device);
        torch::lazy::LazyTensorPtr lazy_mat2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mat2, *common_device);
        auto out_meta = at::meta::addmm(self, mat1, mat2, beta, alpha);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Addmm>(lazy_self->GetIrValue(),
                              lazy_mat1->GetIrValue(),
                              lazy_mat2->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(beta),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::all(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::all)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(all)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::all(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::All>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::any(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::any)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(any)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::any(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Any>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::arange_out(const at::Scalar & start, const at::Scalar & end, const at::Scalar & step, at::Tensor & out) {
        
        if (force_eager_fallback(at::aten::arange)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(arange, start_out)>::call(
                start,
                end,
                step,
                out
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(out);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_out = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(out, *common_device);
        
        auto shapes = torch::lazy::compute_shape_arange_out(start, end, step, out);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::ArangeStartOut>(torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(start),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(end),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(step),
                              lazy_out->GetIrValue(),
                                                                                      std::move(shapes));
        lazy_out->SetInPlaceIrValue(node);
        auto& result = out;
        return result;
    };

    
    at::Tensor LazyNativeFunctions::avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
        
        if (force_eager_fallback(at::aten::avg_pool2d)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(avg_pool2d)>::call(
                self,
                kernel_size,
                stride,
                padding,
                ceil_mode,
                count_include_pad,
                divisor_override
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::AvgPool2d>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(kernel_size.begin(), kernel_size.end()),
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              ceil_mode,
                              count_include_pad,
                              divisor_override,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::avg_pool2d_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, c10::optional<int64_t> divisor_override) {
        
        if (force_eager_fallback(at::aten::avg_pool2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(avg_pool2d_backward)>::call(
                grad_output,
                self,
                kernel_size,
                stride,
                padding,
                ceil_mode,
                count_include_pad,
                divisor_override
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::avg_pool2d_backward(grad_output, self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::AvgPool2dBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              std::vector<int64_t>(kernel_size.begin(), kernel_size.end()),
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              ceil_mode,
                              count_include_pad,
                              divisor_override,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::baddbmm(const at::Tensor & self, const at::Tensor & batch1, const at::Tensor & batch2, const at::Scalar & beta, const at::Scalar & alpha) {
        
        if (force_eager_fallback(at::aten::baddbmm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(baddbmm)>::call(
                self,
                batch1,
                batch2,
                beta,
                alpha
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, batch1, batch2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_batch1 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(batch1, *common_device);
        torch::lazy::LazyTensorPtr lazy_batch2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(batch2, *common_device);
        auto out_meta = at::meta::baddbmm(self, batch1, batch2, beta, alpha);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Baddbmm>(lazy_self->GetIrValue(),
                              lazy_batch1->GetIrValue(),
                              lazy_batch2->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(beta),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::bernoulli(const at::Tensor & self, c10::optional<at::Generator> generator) {
        
        if (force_eager_fallback(at::aten::bernoulli) || (generator.has_value() && generator->defined())) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(bernoulli)>::call(
                self,
                generator
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_bernoulli(self, generator);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Bernoulli>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::bernoulli_(at::Tensor & self, double p, c10::optional<at::Generator> generator) {
        
        if (force_eager_fallback(at::aten::bernoulli_) || (generator.has_value() && generator->defined())) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(bernoulli_, float)>::call(
                self,
                p,
                generator
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_bernoulli_(self, p, generator);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::BernoulliFloat>(lazy_self->GetIrValue(),
                              p,
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor LazyNativeFunctions::binary_cross_entropy(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
        
        if (force_eager_fallback(at::aten::binary_cross_entropy)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(binary_cross_entropy)>::call(
                self,
                target,
                weight,
                reduction
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, target, weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_binary_cross_entropy(self, target, weight, reduction);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::BinaryCrossEntropy>(lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::binary_cross_entropy_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction) {
        
        if (force_eager_fallback(at::aten::binary_cross_entropy_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(binary_cross_entropy_backward)>::call(
                grad_output,
                self,
                target,
                weight,
                reduction
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target, weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_binary_cross_entropy_backward(grad_output, self, target, weight, reduction);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::BinaryCrossEntropyBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::bitwise_and(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::bitwise_and)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(bitwise_and, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::bitwise_and(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::BitwiseAndTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::bitwise_or(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::bitwise_or)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(bitwise_or, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::bitwise_or(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::BitwiseOrTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::bmm(const at::Tensor & self, const at::Tensor & mat2) {
        
        if (force_eager_fallback(at::aten::bmm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(bmm)>::call(
                self,
                mat2
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, mat2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_mat2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mat2, *common_device);
        auto out_meta = at::meta::bmm(self, mat2);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Bmm>(lazy_self->GetIrValue(),
                              lazy_mat2->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::cat(at::TensorList tensors, int64_t dim) {
        
        if (force_eager_fallback(at::aten::cat)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(cat)>::call(
                tensors,
                dim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(tensors);
        TORCH_INTERNAL_ASSERT(common_device);
        
        auto lazy_tensors_tensorlist = torch::lazy::GetTensorList(tensors);
        
        auto shapes = torch::lazy::compute_shape_cat(tensors, dim);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Cat>(lazy_tensors_tensorlist,
                              dim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::clamp(const at::Tensor & self, const c10::optional<at::Scalar> & min, const c10::optional<at::Scalar> & max) {
        
        if (force_eager_fallback(at::aten::clamp)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(clamp)>::call(
                self,
                min,
                max
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::clamp(self, min, max);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Clamp>(lazy_self->GetIrValue(),
                              min ? c10::make_optional(torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(*min)) : c10::nullopt,
                              max ? c10::make_optional(torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(*max)) : c10::nullopt,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::clamp_min(const at::Tensor & self, const at::Scalar & min) {
        
        if (force_eager_fallback(at::aten::clamp_min)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(clamp_min)>::call(
                self,
                min
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_clamp_min(self, min);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::ClampMin>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(min),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::constant_pad_nd(const at::Tensor & self, at::IntArrayRef pad, const at::Scalar & value) {
        
        if (force_eager_fallback(at::aten::constant_pad_nd)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(constant_pad_nd)>::call(
                self,
                pad,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_constant_pad_nd(self, pad, value);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::ConstantPadNd>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(pad.begin(), pad.end()),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(value),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::convolution(const at::Tensor & input, const at::Tensor & weight, const c10::optional<at::Tensor> & bias, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups) {
        
        if (force_eager_fallback(at::aten::convolution)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(convolution)>::call(
                input,
                weight,
                bias,
                stride,
                padding,
                dilation,
                transposed,
                output_padding,
                groups
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(input, weight, bias);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(weight, *common_device);
            torch::lazy::LazyTensorPtr lazy_bias = torch::lazy::TryGetLtcTensor(bias.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_convolution(input, weight, bias, stride, padding, dilation, transposed, output_padding, groups);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Convolution>(lazy_input->GetIrValue(),
                              lazy_weight->GetIrValue(),
                              lazy_bias ? c10::make_optional(lazy_bias->GetIrValue()) : c10::nullopt,
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              std::vector<int64_t>(dilation.begin(), dilation.end()),
                              transposed,
                              std::vector<int64_t>(output_padding.begin(), output_padding.end()),
                              groups,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor,at::Tensor> LazyNativeFunctions::convolution_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & weight, c10::optional<at::IntArrayRef> bias_sizes, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool transposed, at::IntArrayRef output_padding, int64_t groups, ::std::array<bool,3> output_mask) {
        
        if (force_eager_fallback(at::aten::convolution_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(convolution_backward)>::call(
                grad_output,
                input,
                weight,
                bias_sizes,
                stride,
                padding,
                dilation,
                transposed,
                output_padding,
                groups,
                output_mask
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, input, weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(weight, *common_device);
        
        auto shapes = torch::lazy::compute_shape_convolution_backward(grad_output, input, weight, bias_sizes, stride, padding, dilation, transposed, output_padding, groups, output_mask);
        TORCH_INTERNAL_ASSERT(shapes.size() == 3);
        auto node = torch::lazy::MakeNode<ir::ops::ConvolutionBackward>(lazy_grad_output->GetIrValue(),
                              lazy_input->GetIrValue(),
                              lazy_weight->GetIrValue(),
                              torch::lazy::ToOptionalVector<int64_t>(bias_sizes),
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              std::vector<int64_t>(dilation.begin(), dilation.end()),
                              transposed,
                              std::vector<int64_t>(output_padding.begin(), output_padding.end()),
                              groups,
                              std::vector<bool>(output_mask.begin(), output_mask.end()),
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 3; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<3>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::cos(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::cos)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(cos)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::cos(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Cos>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::cumsum(const at::Tensor & self, int64_t dim, c10::optional<at::ScalarType> dtype) {
        
        if (force_eager_fallback(at::aten::cumsum)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(cumsum)>::call(
                self,
                dim,
                dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::cumsum(self, dim, dtype);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Cumsum>(lazy_self->GetIrValue(),
                              dim,
                              dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::div(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::div)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(div, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::div(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::DivTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::div(const at::Tensor & self, const at::Tensor & other, c10::optional<c10::string_view> rounding_mode) {
        
        if (force_eager_fallback(at::aten::div)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(div, Tensor_mode)>::call(
                self,
                other,
                rounding_mode
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::div(self, other, rounding_mode);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::DivTensorMode>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                              rounding_mode,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::elu(const at::Tensor & self, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale) {
        
        if (force_eager_fallback(at::aten::elu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(elu)>::call(
                self,
                alpha,
                scale,
                input_scale
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::elu(self, alpha, scale, input_scale);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Elu>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(scale),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(input_scale),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::elu_backward(const at::Tensor & grad_output, const at::Scalar & alpha, const at::Scalar & scale, const at::Scalar & input_scale, bool is_result, const at::Tensor & self_or_result) {
        
        if (force_eager_fallback(at::aten::elu_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(elu_backward)>::call(
                grad_output,
                alpha,
                scale,
                input_scale,
                is_result,
                self_or_result
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self_or_result);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self_or_result = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self_or_result, *common_device);
        auto out_meta = at::meta::elu_backward(grad_output, alpha, scale, input_scale, is_result, self_or_result);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::EluBackward>(lazy_grad_output->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(scale),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(input_scale),
                              is_result,
                              lazy_self_or_result->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::embedding(const at::Tensor & weight, const at::Tensor & indices, int64_t padding_idx, bool scale_grad_by_freq, bool sparse) {
        
        if (force_eager_fallback(at::aten::embedding)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(embedding)>::call(
                weight,
                indices,
                padding_idx,
                scale_grad_by_freq,
                sparse
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(weight, indices);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(weight, *common_device);
        torch::lazy::LazyTensorPtr lazy_indices = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(indices, *common_device);
        
        auto shapes = torch::lazy::compute_shape_embedding(weight, indices, padding_idx, scale_grad_by_freq, sparse);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Embedding>(lazy_weight->GetIrValue(),
                              lazy_indices->GetIrValue(),
                              padding_idx,
                              scale_grad_by_freq,
                              sparse,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::embedding_dense_backward(const at::Tensor & grad_output, const at::Tensor & indices, int64_t num_weights, int64_t padding_idx, bool scale_grad_by_freq) {
        
        if (force_eager_fallback(at::aten::embedding_dense_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(embedding_dense_backward)>::call(
                grad_output,
                indices,
                num_weights,
                padding_idx,
                scale_grad_by_freq
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, indices);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_indices = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(indices, *common_device);
        
        auto shapes = torch::lazy::compute_shape_embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::EmbeddingDenseBackward>(lazy_grad_output->GetIrValue(),
                              lazy_indices->GetIrValue(),
                              num_weights,
                              padding_idx,
                              scale_grad_by_freq,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::eq(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::eq)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(eq, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::eq(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::EqScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::eq(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::eq)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(eq, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::eq(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::EqTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::exp(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::exp)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(exp)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::exp(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Exp>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::flip(const at::Tensor & self, at::IntArrayRef dims) {
        
        if (force_eager_fallback(at::aten::flip)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(flip)>::call(
                self,
                dims
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_flip(self, dims);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Flip>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(dims.begin(), dims.end()),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::floor(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::floor)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(floor)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::floor(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Floor>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::frac(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::frac)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(frac)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::frac(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Frac>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::gather(const at::Tensor & self, int64_t dim, const at::Tensor & index, bool sparse_grad) {
        
        if (force_eager_fallback(at::aten::gather)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(gather)>::call(
                self,
                dim,
                index,
                sparse_grad
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, index);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_index = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(index, *common_device);
        auto out_meta = at::meta::gather(self, dim, index, sparse_grad);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Gather>(lazy_self->GetIrValue(),
                              dim,
                              lazy_index->GetIrValue(),
                              sparse_grad,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::ge(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::ge)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(ge, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::ge(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GeScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::ge(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::ge)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(ge, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::ge(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GeTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::gelu(const at::Tensor & self, c10::string_view approximate) {
        
        if (force_eager_fallback(at::aten::gelu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(gelu)>::call(
                self,
                approximate
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::gelu(self, approximate);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Gelu>(lazy_self->GetIrValue(),
                              approximate,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::gelu_backward(const at::Tensor & grad_output, const at::Tensor & self, c10::string_view approximate) {
        
        if (force_eager_fallback(at::aten::gelu_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(gelu_backward)>::call(
                grad_output,
                self,
                approximate
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::gelu_backward(grad_output, self, approximate);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GeluBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              approximate,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::glu(const at::Tensor & self, int64_t dim) {
        
        if (force_eager_fallback(at::aten::glu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(glu)>::call(
                self,
                dim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::glu(self, dim);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Glu>(lazy_self->GetIrValue(),
                              dim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::glu_backward(const at::Tensor & grad_output, const at::Tensor & self, int64_t dim) {
        
        if (force_eager_fallback(at::aten::glu_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(glu_backward)>::call(
                grad_output,
                self,
                dim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_glu_backward(grad_output, self, dim);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GluBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              dim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::grid_sampler_2d(const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners) {
        
        if (force_eager_fallback(at::aten::grid_sampler_2d)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(grid_sampler_2d)>::call(
                input,
                grid,
                interpolation_mode,
                padding_mode,
                align_corners
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(input, grid);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        torch::lazy::LazyTensorPtr lazy_grid = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grid, *common_device);
        
        auto shapes = torch::lazy::compute_shape_grid_sampler_2d(input, grid, interpolation_mode, padding_mode, align_corners);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GridSampler2d>(lazy_input->GetIrValue(),
                              lazy_grid->GetIrValue(),
                              interpolation_mode,
                              padding_mode,
                              align_corners,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::grid_sampler_2d_backward(const at::Tensor & grad_output, const at::Tensor & input, const at::Tensor & grid, int64_t interpolation_mode, int64_t padding_mode, bool align_corners, ::std::array<bool,2> output_mask) {
        
        if (force_eager_fallback(at::aten::grid_sampler_2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(grid_sampler_2d_backward)>::call(
                grad_output,
                input,
                grid,
                interpolation_mode,
                padding_mode,
                align_corners,
                output_mask
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, input, grid);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        torch::lazy::LazyTensorPtr lazy_grid = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grid, *common_device);
        
        auto shapes = torch::lazy::compute_shape_grid_sampler_2d_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask);
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::GridSampler2dBackward>(lazy_grad_output->GetIrValue(),
                              lazy_input->GetIrValue(),
                              lazy_grid->GetIrValue(),
                              interpolation_mode,
                              padding_mode,
                              align_corners,
                              std::vector<bool>(output_mask.begin(), output_mask.end()),
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::gt(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::gt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(gt, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::gt(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GtScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::gt(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::gt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(gt, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::gt(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::GtTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::hardsigmoid(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::hardsigmoid)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(hardsigmoid)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::hardsigmoid(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Hardsigmoid>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::index_select(const at::Tensor & self, int64_t dim, const at::Tensor & index) {
        
        if (force_eager_fallback(at::aten::index_select)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(index_select)>::call(
                self,
                dim,
                index
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, index);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_index = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(index, *common_device);
        
        auto shapes = torch::lazy::compute_shape_index_select(self, dim, index);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::IndexSelect>(lazy_self->GetIrValue(),
                              dim,
                              lazy_index->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::kl_div_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, bool log_target) {
        
        if (force_eager_fallback(at::aten::kl_div_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(kl_div_backward)>::call(
                grad_output,
                self,
                target,
                reduction,
                log_target
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
        
        auto shapes = torch::lazy::compute_shape_kl_div_backward(grad_output, self, target, reduction, log_target);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::KlDivBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              reduction,
                              log_target,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction) {
        
        if (force_eager_fallback(at::aten::l1_loss_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(l1_loss_backward)>::call(
                grad_output,
                self,
                target,
                reduction
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
        
        auto shapes = torch::lazy::compute_shape_l1_loss_backward(grad_output, self, target, reduction);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::L1LossBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              reduction,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::le(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::le)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(le, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::le(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LeScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::le(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::le)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(le, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::le(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LeTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
        
        if (force_eager_fallback(at::aten::leaky_relu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(leaky_relu)>::call(
                self,
                negative_slope
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::leaky_relu(self, negative_slope);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LeakyRelu>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(negative_slope),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::leaky_relu_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & negative_slope, bool self_is_result) {
        
        if (force_eager_fallback(at::aten::leaky_relu_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(leaky_relu_backward)>::call(
                grad_output,
                self,
                negative_slope,
                self_is_result
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::leaky_relu_backward(grad_output, self, negative_slope, self_is_result);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LeakyReluBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(negative_slope),
                              self_is_result,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::log(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::log)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(log)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::log(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Log>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::log2(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::log2)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(log2)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::log2(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Log2>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::log_sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & buffer) {
        
        if (force_eager_fallback(at::aten::log_sigmoid_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(log_sigmoid_backward)>::call(
                grad_output,
                self,
                buffer
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, buffer);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_buffer = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(buffer, *common_device);
        
        auto shapes = torch::lazy::compute_shape_log_sigmoid_backward(grad_output, self, buffer);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LogSigmoidBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_buffer->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::log_sigmoid_forward(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::log_sigmoid_forward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(log_sigmoid_forward)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_log_sigmoid_forward(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::LogSigmoidForward>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::logdet(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::logdet)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(logdet)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_logdet(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Logdet>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::lt(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::lt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(lt, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::lt(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LtScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::lt(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::lt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(lt, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::lt(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::LtTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Scalar & value) {
        
        if (force_eager_fallback(at::aten::masked_fill_)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(masked_fill_, Scalar)>::call(
                self,
                mask,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, mask);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_mask = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mask, *common_device);
        
        auto shapes = torch::lazy::compute_shape_masked_fill_(self, mask, value);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::MaskedFillScalar>(lazy_self->GetIrValue(),
                              lazy_mask->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(value),
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::masked_fill_(at::Tensor & self, const at::Tensor & mask, const at::Tensor & value) {
        
        if (force_eager_fallback(at::aten::masked_fill_)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(masked_fill_, Tensor)>::call(
                self,
                mask,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, mask, value);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_mask = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mask, *common_device);
        torch::lazy::LazyTensorPtr lazy_value = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(value, *common_device);
        
        auto shapes = torch::lazy::compute_shape_masked_fill_(self, mask, value);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::MaskedFillTensor>(lazy_self->GetIrValue(),
                              lazy_mask->GetIrValue(),
                              lazy_value->GetIrValue(),
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::max(const at::Tensor & self, int64_t dim, bool keepdim) {
        
        if (force_eager_fallback(at::aten::max)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(max, dim)>::call(
                self,
                dim,
                keepdim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::max(self, dim, keepdim);
        std::vector<Shape> shapes{Shape(std::get<0>(out_meta).scalar_type(), std::get<0>(out_meta).sizes().vec()),Shape(std::get<1>(out_meta).scalar_type(), std::get<1>(out_meta).sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::MaxDim>(lazy_self->GetIrValue(),
                              dim,
                              keepdim,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::max(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::max)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(max)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_max(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Max>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::max_pool2d_with_indices(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
        
        if (force_eager_fallback(at::aten::max_pool2d_with_indices)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(max_pool2d_with_indices)>::call(
                self,
                kernel_size,
                stride,
                padding,
                dilation,
                ceil_mode
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::max_pool2d_with_indices(self, kernel_size, stride, padding, dilation, ceil_mode);
        std::vector<Shape> shapes{Shape(std::get<0>(out_meta).scalar_type(), std::get<0>(out_meta).sizes().vec()),Shape(std::get<1>(out_meta).scalar_type(), std::get<1>(out_meta).sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::MaxPool2dWithIndices>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(kernel_size.begin(), kernel_size.end()),
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              std::vector<int64_t>(dilation.begin(), dilation.end()),
                              ceil_mode,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::max_pool2d_with_indices_backward(const at::Tensor & grad_output, const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode, const at::Tensor & indices) {
        
        if (force_eager_fallback(at::aten::max_pool2d_with_indices_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(max_pool2d_with_indices_backward)>::call(
                grad_output,
                self,
                kernel_size,
                stride,
                padding,
                dilation,
                ceil_mode,
                indices
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, indices);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_indices = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(indices, *common_device);
        auto out_meta = at::meta::max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::MaxPool2dWithIndicesBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              std::vector<int64_t>(kernel_size.begin(), kernel_size.end()),
                              std::vector<int64_t>(stride.begin(), stride.end()),
                              std::vector<int64_t>(padding.begin(), padding.end()),
                              std::vector<int64_t>(dilation.begin(), dilation.end()),
                              ceil_mode,
                              lazy_indices->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::maximum(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::maximum)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(maximum)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::maximum(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Maximum>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::mean(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
        
        if (force_eager_fallback(at::aten::mean)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(mean)>::call(
                self,
                dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_mean(self, dtype);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Mean>(lazy_self->GetIrValue(),
                              dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::mean(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
        
        if (force_eager_fallback(at::aten::mean)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(mean, dim)>::call(
                self,
                dim,
                keepdim,
                dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::mean(self, dim, keepdim, dtype);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::MeanDim>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(dim.begin(), dim.end()),
                              keepdim,
                              dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::min(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::min)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(min)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_min(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Min>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::minimum(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::minimum)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(minimum)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::minimum(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Minimum>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::mm(const at::Tensor & self, const at::Tensor & mat2) {
        
        if (force_eager_fallback(at::aten::mm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(mm)>::call(
                self,
                mat2
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, mat2);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_mat2 = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mat2, *common_device);
        auto out_meta = at::meta::mm(self, mat2);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Mm>(lazy_self->GetIrValue(),
                              lazy_mat2->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::mul(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::mul)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(mul, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::mul(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::MulTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::mv(const at::Tensor & self, const at::Tensor & vec) {
        
        if (force_eager_fallback(at::aten::mv)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(mv)>::call(
                self,
                vec
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, vec);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_vec = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(vec, *common_device);
        
        auto shapes = torch::lazy::compute_shape_mv(self, vec);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Mv>(lazy_self->GetIrValue(),
                              lazy_vec->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::native_dropout(const at::Tensor & input, double p, c10::optional<bool> train) {
        
        if (force_eager_fallback(at::aten::native_dropout)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(native_dropout)>::call(
                input,
                p,
                train
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(input);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        
        auto shapes = torch::lazy::compute_shape_native_dropout(input, p, train);
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::NativeDropout>(lazy_input->GetIrValue(),
                              p,
                              train,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::native_dropout_backward(const at::Tensor & grad_output, const at::Tensor & mask, double scale) {
        
        if (force_eager_fallback(at::aten::native_dropout_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(native_dropout_backward)>::call(
                grad_output,
                mask,
                scale
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, mask);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_mask = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mask, *common_device);
        
        auto shapes = torch::lazy::compute_shape_native_dropout_backward(grad_output, mask, scale);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NativeDropoutBackward>(lazy_grad_output->GetIrValue(),
                              lazy_mask->GetIrValue(),
                              scale,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor,at::Tensor> LazyNativeFunctions::native_layer_norm(const at::Tensor & input, at::IntArrayRef normalized_shape, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, double eps) {
        
        if (force_eager_fallback(at::aten::native_layer_norm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(native_layer_norm)>::call(
                input,
                normalized_shape,
                weight,
                bias,
                eps
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(input, weight, bias);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
            torch::lazy::LazyTensorPtr lazy_bias = torch::lazy::TryGetLtcTensor(bias.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_native_layer_norm(input, normalized_shape, weight, bias, eps);
        TORCH_INTERNAL_ASSERT(shapes.size() == 3);
        auto node = torch::lazy::MakeNode<ir::ops::NativeLayerNorm>(lazy_input->GetIrValue(),
                              std::vector<int64_t>(normalized_shape.begin(), normalized_shape.end()),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              lazy_bias ? c10::make_optional(lazy_bias->GetIrValue()) : c10::nullopt,
                              eps,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 3; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<3>(lazy_tensors);
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor,at::Tensor> LazyNativeFunctions::native_layer_norm_backward(const at::Tensor & grad_out, const at::Tensor & input, at::IntArrayRef normalized_shape, const at::Tensor & mean, const at::Tensor & rstd, const c10::optional<at::Tensor> & weight, const c10::optional<at::Tensor> & bias, ::std::array<bool,3> output_mask) {
        
        if (force_eager_fallback(at::aten::native_layer_norm_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(native_layer_norm_backward)>::call(
                grad_out,
                input,
                normalized_shape,
                mean,
                rstd,
                weight,
                bias,
                output_mask
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_out, input, mean, rstd, weight, bias);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_out = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_out, *common_device);
        torch::lazy::LazyTensorPtr lazy_input = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(input, *common_device);
        torch::lazy::LazyTensorPtr lazy_mean = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(mean, *common_device);
        torch::lazy::LazyTensorPtr lazy_rstd = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(rstd, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
            torch::lazy::LazyTensorPtr lazy_bias = torch::lazy::TryGetLtcTensor(bias.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_native_layer_norm_backward(grad_out, input, normalized_shape, mean, rstd, weight, bias, output_mask);
        TORCH_INTERNAL_ASSERT(shapes.size() == 3);
        auto node = torch::lazy::MakeNode<ir::ops::NativeLayerNormBackward>(lazy_grad_out->GetIrValue(),
                              lazy_input->GetIrValue(),
                              std::vector<int64_t>(normalized_shape.begin(), normalized_shape.end()),
                              lazy_mean->GetIrValue(),
                              lazy_rstd->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              lazy_bias ? c10::make_optional(lazy_bias->GetIrValue()) : c10::nullopt,
                              std::vector<bool>(output_mask.begin(), output_mask.end()),
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 3; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<3>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::ne(const at::Tensor & self, const at::Scalar & other) {
        
        if (force_eager_fallback(at::aten::ne)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(ne, Scalar)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::ne(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NeScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(other),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::ne(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::ne)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(ne, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::ne(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NeTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::neg(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::neg)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(neg)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::neg(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Neg>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::nll_loss2d_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
        
        if (force_eager_fallback(at::aten::nll_loss2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(nll_loss2d_backward)>::call(
                grad_output,
                self,
                target,
                weight,
                reduction,
                ignore_index,
                total_weight
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target, weight, total_weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        torch::lazy::LazyTensorPtr lazy_total_weight = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(total_weight, *common_device);
        
        auto shapes = torch::lazy::compute_shape_nll_loss2d_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NllLoss2dBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                              ignore_index,
                              lazy_total_weight->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::nll_loss2d_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
        
        if (force_eager_fallback(at::aten::nll_loss2d_forward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(nll_loss2d_forward)>::call(
                self,
                target,
                weight,
                reduction,
                ignore_index
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, target, weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        
        auto shapes = torch::lazy::compute_shape_nll_loss2d_forward(self, target, weight, reduction, ignore_index);
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::NllLoss2dForward>(lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                              ignore_index,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::nll_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index, const at::Tensor & total_weight) {
        
        if (force_eager_fallback(at::aten::nll_loss_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(nll_loss_backward)>::call(
                grad_output,
                self,
                target,
                weight,
                reduction,
                ignore_index,
                total_weight
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target, weight, total_weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        torch::lazy::LazyTensorPtr lazy_total_weight = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(total_weight, *common_device);
        auto out_meta = at::meta::nll_loss_backward(grad_output, self, target, weight, reduction, ignore_index, total_weight);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NllLossBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                              ignore_index,
                              lazy_total_weight->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::nll_loss_forward(const at::Tensor & self, const at::Tensor & target, const c10::optional<at::Tensor> & weight, int64_t reduction, int64_t ignore_index) {
        
        if (force_eager_fallback(at::aten::nll_loss_forward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(nll_loss_forward)>::call(
                self,
                target,
                weight,
                reduction,
                ignore_index
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, target, weight);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
            torch::lazy::LazyTensorPtr lazy_weight = torch::lazy::TryGetLtcTensor(weight.value_or(at::Tensor()));
        auto out_meta = at::meta::nll_loss_forward(self, target, weight, reduction, ignore_index);
        std::vector<Shape> shapes{Shape(std::get<0>(out_meta).scalar_type(), std::get<0>(out_meta).sizes().vec()),Shape(std::get<1>(out_meta).scalar_type(), std::get<1>(out_meta).sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::NllLossForward>(lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              lazy_weight ? c10::make_optional(lazy_weight->GetIrValue()) : c10::nullopt,
                              reduction,
                              ignore_index,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::norm(const at::Tensor & self, const c10::optional<at::Scalar> & p, at::IntArrayRef dim, bool keepdim) {
        
        if (force_eager_fallback(at::aten::norm)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(norm, ScalarOpt_dim)>::call(
                self,
                p,
                dim,
                keepdim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::norm(self, p, dim, keepdim);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::NormScalaroptDim>(lazy_self->GetIrValue(),
                              p ? c10::make_optional(torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(*p)) : c10::nullopt,
                              std::vector<int64_t>(dim.begin(), dim.end()),
                              keepdim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::pow(const at::Tensor & self, const at::Tensor & exponent) {
        
        if (force_eager_fallback(at::aten::pow)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(pow, Tensor_Tensor)>::call(
                self,
                exponent
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, exponent);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_exponent = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(exponent, *common_device);
        auto out_meta = at::meta::pow(self, exponent);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::PowTensorTensor>(lazy_self->GetIrValue(),
                              lazy_exponent->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::pow(const at::Tensor & self, const at::Scalar & exponent) {
        
        if (force_eager_fallback(at::aten::pow)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(pow, Tensor_Scalar)>::call(
                self,
                exponent
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::pow(self, exponent);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::PowTensorScalar>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(exponent),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::random_(at::Tensor & self, int64_t from, c10::optional<int64_t> to, c10::optional<at::Generator> generator) {
        
        if (force_eager_fallback(at::aten::random_) || (generator.has_value() && generator->defined())) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(random_, from)>::call(
                self,
                from,
                to,
                generator
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_random_(self, from, to, generator);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::RandomFrom>(lazy_self->GetIrValue(),
                              from,
                              to,
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::random_(at::Tensor & self, int64_t to, c10::optional<at::Generator> generator) {
        
        if (force_eager_fallback(at::aten::random_) || (generator.has_value() && generator->defined())) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(random_, to)>::call(
                self,
                to,
                generator
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_random_(self, to, generator);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::RandomTo>(lazy_self->GetIrValue(),
                              to,
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::random_(at::Tensor & self, c10::optional<at::Generator> generator) {
        
        if (force_eager_fallback(at::aten::random_) || (generator.has_value() && generator->defined())) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(random_)>::call(
                self,
                generator
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_random_(self, generator);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Random>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor LazyNativeFunctions::reciprocal(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::reciprocal)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(reciprocal)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::reciprocal(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Reciprocal>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::relu(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::relu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(relu)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_relu(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Relu>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::relu_(at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::relu_)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(relu_)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_relu_(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Relu>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

    
    at::Tensor LazyNativeFunctions::remainder(const at::Tensor & self, const at::Tensor & other) {
        
        if (force_eager_fallback(at::aten::remainder)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(remainder, Tensor)>::call(
                self,
                other
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::remainder(self, other);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::RemainderTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::repeat(const at::Tensor & self, at::IntArrayRef repeats) {
        
        if (force_eager_fallback(at::aten::repeat)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(repeat)>::call(
                self,
                repeats
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_repeat(self, repeats);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Repeat>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(repeats.begin(), repeats.end()),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::rsqrt(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::rsqrt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(rsqrt)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::rsqrt(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Rsqrt>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::scatter_add(const at::Tensor & self, int64_t dim, const at::Tensor & index, const at::Tensor & src) {
        
        if (force_eager_fallback(at::aten::scatter_add)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(scatter_add)>::call(
                self,
                dim,
                index,
                src
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, index, src);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_index = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(index, *common_device);
        torch::lazy::LazyTensorPtr lazy_src = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(src, *common_device);
        auto out_meta = at::meta::scatter_add(self, dim, index, src);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::ScatterAdd>(lazy_self->GetIrValue(),
                              dim,
                              lazy_index->GetIrValue(),
                              lazy_src->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sgn(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::sgn)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sgn)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::sgn(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Sgn>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sigmoid(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::sigmoid)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sigmoid)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::sigmoid(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Sigmoid>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sigmoid_backward(const at::Tensor & grad_output, const at::Tensor & output) {
        
        if (force_eager_fallback(c10::Symbol::fromQualString("aten::sigmoid_backward"))) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sigmoid_backward)>::call(
                grad_output,
                output
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(output, *common_device);
        auto out_meta = at::meta::sigmoid_backward(grad_output, output);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SigmoidBackward>(lazy_grad_output->GetIrValue(),
                              lazy_output->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::silu(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::silu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(silu)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::silu(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Silu>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::smooth_l1_loss(const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
        
        if (force_eager_fallback(at::aten::smooth_l1_loss)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(smooth_l1_loss)>::call(
                self,
                target,
                reduction,
                beta
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, target);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
        auto out_meta = at::meta::smooth_l1_loss(self, target, reduction, beta);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SmoothL1Loss>(lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              reduction,
                              beta,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::smooth_l1_loss_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Tensor & target, int64_t reduction, double beta) {
        
        if (force_eager_fallback(at::aten::smooth_l1_loss_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(smooth_l1_loss_backward)>::call(
                grad_output,
                self,
                target,
                reduction,
                beta
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self, target);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_target = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(target, *common_device);
        
        auto shapes = torch::lazy::compute_shape_smooth_l1_loss_backward(grad_output, self, target, reduction, beta);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SmoothL1LossBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              lazy_target->GetIrValue(),
                              reduction,
                              beta,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::softplus(const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
        
        if (force_eager_fallback(at::aten::softplus)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(softplus)>::call(
                self,
                beta,
                threshold
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::softplus(self, beta, threshold);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Softplus>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(beta),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(threshold),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::softplus_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & beta, const at::Scalar & threshold) {
        
        if (force_eager_fallback(at::aten::softplus_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(softplus_backward)>::call(
                grad_output,
                self,
                beta,
                threshold
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::softplus_backward(grad_output, self, beta, threshold);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SoftplusBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(beta),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(threshold),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::sort(const at::Tensor & self, int64_t dim, bool descending) {
        
        if (force_eager_fallback(at::aten::sort)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sort)>::call(
                self,
                dim,
                descending
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_sort(self, dim, descending);
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::Sort>(lazy_self->GetIrValue(),
                              dim,
                              descending,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sqrt(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::sqrt)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sqrt)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::sqrt(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Sqrt>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::stack(at::TensorList tensors, int64_t dim) {
        
        if (force_eager_fallback(at::aten::stack)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(stack)>::call(
                tensors,
                dim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(tensors);
        TORCH_INTERNAL_ASSERT(common_device);
        
        auto lazy_tensors_tensorlist = torch::lazy::GetTensorList(tensors);
        
        auto shapes = torch::lazy::compute_shape_stack(tensors, dim);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Stack>(lazy_tensors_tensorlist,
                              dim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::std(const at::Tensor & self, bool unbiased) {
        
        if (force_eager_fallback(at::aten::std)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(std)>::call(
                self,
                unbiased
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_std(self, unbiased);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Std>(lazy_self->GetIrValue(),
                              unbiased,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::std(const at::Tensor & self, at::IntArrayRef dim, bool unbiased, bool keepdim) {
        
        if (force_eager_fallback(at::aten::std)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(std, dim)>::call(
                self,
                dim,
                unbiased,
                keepdim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_std(self, dim, unbiased, keepdim);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::StdDim>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(dim.begin(), dim.end()),
                              unbiased,
                              keepdim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::std(const at::Tensor & self, c10::optional<at::IntArrayRef> dim, c10::optional<int64_t> correction, bool keepdim) {
        
        if (force_eager_fallback(at::aten::std)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(std, correction)>::call(
                self,
                dim,
                correction,
                keepdim
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_std(self, dim, correction, keepdim);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::StdCorrection>(lazy_self->GetIrValue(),
                              torch::lazy::ToOptionalVector<int64_t>(dim),
                              correction,
                              keepdim,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sub(const at::Tensor & self, const at::Tensor & other, const at::Scalar & alpha) {
        
        if (force_eager_fallback(at::aten::sub)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(sub, Tensor)>::call(
                self,
                other,
                alpha
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self, other);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        torch::lazy::LazyTensorPtr lazy_other = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(other, *common_device);
        auto out_meta = at::meta::sub(self, other, alpha);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SubTensor>(lazy_self->GetIrValue(),
                              lazy_other->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(alpha),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sum(const at::Tensor & self, c10::optional<at::ScalarType> dtype) {
        
        if (force_eager_fallback(at::aten::sum)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(sum)>::call(
                self,
                dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_sum(self, dtype);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Sum>(lazy_self->GetIrValue(),
                              dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::sum(const at::Tensor & self, at::IntArrayRef dim, bool keepdim, c10::optional<at::ScalarType> dtype) {
        
        if (force_eager_fallback(at::aten::sum)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP2(sum, dim_IntList)>::call(
                self,
                dim,
                keepdim,
                dtype
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::sum(self, dim, keepdim, dtype);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::SumDimIntlist>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(dim.begin(), dim.end()),
                              keepdim,
                              dtype,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::tanh(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::tanh)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(tanh)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::tanh(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Tanh>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::tanh_backward(const at::Tensor & grad_output, const at::Tensor & output) {
        
        if (force_eager_fallback(at::aten::tanh_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(tanh_backward)>::call(
                grad_output,
                output
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(output, *common_device);
        auto out_meta = at::meta::tanh_backward(grad_output, output);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::TanhBackward>(lazy_grad_output->GetIrValue(),
                              lazy_output->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::threshold(const at::Tensor & self, const at::Scalar & threshold, const at::Scalar & value) {
        
        if (force_eager_fallback(at::aten::threshold)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(threshold)>::call(
                self,
                threshold,
                value
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::threshold(self, threshold, value);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Threshold>(lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(threshold),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(value),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::threshold_backward(const at::Tensor & grad_output, const at::Tensor & self, const at::Scalar & threshold) {
        
        if (force_eager_fallback(at::aten::threshold_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(threshold_backward)>::call(
                grad_output,
                self,
                threshold
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output, self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::threshold_backward(grad_output, self, threshold);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::ThresholdBackward>(lazy_grad_output->GetIrValue(),
                              lazy_self->GetIrValue(),
                              torch::lazy::LazyGraphExecutor::Get()->GetIrValueForScalarFromCodegen(threshold),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    ::std::tuple<at::Tensor,at::Tensor> LazyNativeFunctions::topk(const at::Tensor & self, int64_t k, int64_t dim, bool largest, bool sorted) {
        
        if (force_eager_fallback(at::aten::topk)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(topk)>::call(
                self,
                k,
                dim,
                largest,
                sorted
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::topk(self, k, dim, largest, sorted);
        std::vector<Shape> shapes{Shape(std::get<0>(out_meta).scalar_type(), std::get<0>(out_meta).sizes().vec()),Shape(std::get<1>(out_meta).scalar_type(), std::get<1>(out_meta).sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 2);
        auto node = torch::lazy::MakeNode<ir::ops::Topk>(lazy_self->GetIrValue(),
                              k,
                              dim,
                              largest,
                              sorted,
                                                                                      std::move(shapes));
        std::vector<torch::lazy::LazyTensorPtr> lazy_tensors;
        for (int i = 0; i < 2; i++) {
            lazy_tensors.push_back(torch::lazy::LazyTensor::Create(torch::lazy::Value(node, i), *common_device));
        }
        auto result = torch::lazy::TupleAtenFromLtcTensors<2>(lazy_tensors);
        return result;
    };

    
    at::Tensor LazyNativeFunctions::trace(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::trace)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(trace)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_trace(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Trace>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::tril(const at::Tensor & self, int64_t diagonal) {
        
        if (force_eager_fallback(at::aten::tril)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(tril)>::call(
                self,
                diagonal
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::tril(self, diagonal);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Tril>(lazy_self->GetIrValue(),
                              diagonal,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::triu(const at::Tensor & self, int64_t diagonal) {
        
        if (force_eager_fallback(at::aten::triu)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(triu)>::call(
                self,
                diagonal
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::triu(self, diagonal);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Triu>(lazy_self->GetIrValue(),
                              diagonal,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::trunc(const at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::trunc)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(trunc)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::trunc(self);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Trunc>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::upsample_bilinear2d(const at::Tensor & self, at::IntArrayRef output_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
        
        if (force_eager_fallback(at::aten::upsample_bilinear2d)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(upsample_bilinear2d)>::call(
                self,
                output_size,
                align_corners,
                scales_h,
                scales_w
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::upsample_bilinear2d(self, output_size, align_corners, scales_h, scales_w);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::UpsampleBilinear2d>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(output_size.begin(), output_size.end()),
                              align_corners,
                              scales_h,
                              scales_w,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::upsample_bilinear2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, bool align_corners, c10::optional<double> scales_h, c10::optional<double> scales_w) {
        
        if (force_eager_fallback(at::aten::upsample_bilinear2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(upsample_bilinear2d_backward)>::call(
                grad_output,
                output_size,
                input_size,
                align_corners,
                scales_h,
                scales_w
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        auto out_meta = at::meta::upsample_bilinear2d_backward(grad_output, output_size, input_size, align_corners, scales_h, scales_w);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::UpsampleBilinear2dBackward>(lazy_grad_output->GetIrValue(),
                              std::vector<int64_t>(output_size.begin(), output_size.end()),
                              std::vector<int64_t>(input_size.begin(), input_size.end()),
                              align_corners,
                              scales_h,
                              scales_w,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::upsample_nearest2d(const at::Tensor & self, at::IntArrayRef output_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
        
        if (force_eager_fallback(at::aten::upsample_nearest2d)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(upsample_nearest2d)>::call(
                self,
                output_size,
                scales_h,
                scales_w
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        auto out_meta = at::meta::upsample_nearest2d(self, output_size, scales_h, scales_w);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::UpsampleNearest2d>(lazy_self->GetIrValue(),
                              std::vector<int64_t>(output_size.begin(), output_size.end()),
                              scales_h,
                              scales_w,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor LazyNativeFunctions::upsample_nearest2d_backward(const at::Tensor & grad_output, at::IntArrayRef output_size, at::IntArrayRef input_size, c10::optional<double> scales_h, c10::optional<double> scales_w) {
        
        if (force_eager_fallback(at::aten::upsample_nearest2d_backward)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(upsample_nearest2d_backward)>::call(
                grad_output,
                output_size,
                input_size,
                scales_h,
                scales_w
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(grad_output);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_grad_output = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(grad_output, *common_device);
        auto out_meta = at::meta::upsample_nearest2d_backward(grad_output, output_size, input_size, scales_h, scales_w);
        std::vector<Shape> shapes{Shape(out_meta.scalar_type(), out_meta.sizes().vec())};
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::UpsampleNearest2dBackward>(lazy_grad_output->GetIrValue(),
                              std::vector<int64_t>(output_size.begin(), output_size.end()),
                              std::vector<int64_t>(input_size.begin(), input_size.end()),
                              scales_h,
                              scales_w,
                                                                                      std::move(shapes));
        auto result = torch::lazy::CreateAtenFromLtcTensor(
                torch::lazy::LazyTensor::Create(std::move(node), *common_device));
        return result;
    };

    
    at::Tensor & LazyNativeFunctions::zero_(at::Tensor & self) {
        
        if (force_eager_fallback(at::aten::zero_)) {
            return at::native::call_fallback_fn<&ltc_eager_fallback, ATEN_OP(zero_)>::call(
                self
            );
        }

        TORCH_LAZY_FN_COUNTER("lazy::");
        auto common_device = torch::lazy::GetBackendDevice(self);
        TORCH_INTERNAL_ASSERT(common_device);
        
        torch::lazy::LazyTensorPtr lazy_self = torch::lazy::GetLtcTensorOrCreateForWrappedNumber(self, *common_device);
        
        auto shapes = torch::lazy::compute_shape_zero_(self);
        TORCH_INTERNAL_ASSERT(shapes.size() == 1);
        auto node = torch::lazy::MakeNode<ir::ops::Zero>(lazy_self->GetIrValue(),
                                                                                      std::move(shapes));
        lazy_self->SetInPlaceIrValue(node);
        auto& result = self;
        return result;
    };

} // namespace lazy
} // namespace torch
