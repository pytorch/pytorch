"""
Chrome Trace Graph Builder for PyTorch Profiling

This module provides a robust parser and analyzer for Chrome trace JSON files
generated by PyTorch profilers. It maps operations to source code lines and
tracks forward/backward passes through sequence numbers.

Example Usage:
    from trace_graph import TraceGraph

    # Build graph from trace file
    graph = TraceGraph("my_trace.json")

    # Get operations for a specific source line
    ops = graph.get_operations_for_line("myfile.py:42")
    for tid, thread_ops in ops.items():
        print(f"Thread {tid}:")
        for op in thread_ops["forward"]:
            print(f"  Forward: {op['name']} ({op['time_ms']:.3f} ms)")
        for op in thread_ops["backward"]:
            print(f"  Backward: {op['name']} ({op['time_ms']:.3f} ms)")

    # Get all annotations and their operations
    all_ops = graph.get_all_line_operations()

    # Access thread information
    for tid, info in graph.threads.items():
        is_gpu = "GPU" if info["has_gpu_kernels"] else "CPU"
        print(f"Thread {tid} [{is_gpu}]: {info['total_time_ms']:.2f} ms")

    # Print formatted analysis
    graph.print_analysis(
        gpu_only=False,
        include_thread_summary=True,
        include_line_details=True,
        include_grand_total=True,
    )

Key Features:
    - Parses Chrome trace JSON (phase 'X' complete events)
    - Builds parent-child trees via time containment
    - Maps PyTorch operations (aten::*) to GPU kernels via correlation IDs
    - Links forward and backward passes using sequence numbers
    - Classifies threads as CPU or GPU based on correlation IDs
    - Parses source line annotations from auto_profiler
"""

import json
from collections import defaultdict

try:
    from auto_profiler import ANNOTATION_MARKER
except ImportError:
    ANNOTATION_MARKER = "@@"


class TraceEvent:
    """Represents a single event in the Chrome trace."""

    def __init__(self, evt_dict, idx):
        self.idx = idx
        self.name = evt_dict.get("name", "")
        self.ts = evt_dict.get("ts", 0)
        self.dur = evt_dict.get("dur", 0)
        self.tid = evt_dict.get("tid", 0)
        self.pid = evt_dict.get("pid", 0)
        self.args = evt_dict.get("args", {})

        self.children = []
        self.parent = None

        self.is_line_annotation = self.name.startswith(f"{ANNOTATION_MARKER}:")
        self.is_backward = any(x in self.name for x in ["Backward", "AccumulateGrad"])

        self.annotation_filename = None
        self.annotation_line_number = None
        self.annotation_code = None
        self.annotation_loop = None

        if self.is_line_annotation:
            self._parse_annotation()

    def _parse_annotation(self):
        """Parse annotation format: {MARKER}:<filename>:<line_number>:<code>[:<loopstr>]"""
        parts = self.name.split(":")

        if len(parts) < 4:
            return

        self.annotation_filename = parts[1]
        self.annotation_line_number = parts[2]

        remaining = parts[3:]

        if remaining and remaining[-1].startswith("LOOP="):
            self.annotation_loop = remaining[-1].replace("LOOP=", "")
            self.annotation_code = ":".join(remaining[:-1])
        else:
            self.annotation_code = ":".join(remaining)

    def format_annotation(self, include_code=True):
        """
        Format the annotation for display.

        Args:
            include_code: If True, include the code snippet

        Returns:
            Formatted string like "so2_layers.py:42" or "so2_layers.py:42: x = torch.matmul(a, b)"
        """
        if not self.is_line_annotation:
            return self.name

        base = f"{self.annotation_filename}:{self.annotation_line_number}"

        if self.annotation_loop is not None:
            base += f" (LOOP={self.annotation_loop})"

        if include_code and self.annotation_code:
            base += f": {self.annotation_code}"

        return base

    @property
    def end_ts(self):
        return self.ts + self.dur

    def contains(self, other):
        """Check if this event completely contains another event in time."""
        return other.ts >= self.ts and other.end_ts <= self.end_ts


class TraceGraph:
    """Graph structure of trace events with parent-child relationships."""

    def __init__(self, trace_file):
        with open(trace_file, "r") as f:
            trace_data = json.load(f)

        # Parse events
        self.events = []
        for idx, evt_dict in enumerate(trace_data.get("traceEvents", [])):
            if evt_dict.get("ph") == "X":  # Complete events only
                self.events.append(TraceEvent(evt_dict, idx))

        # Build parent-child relationships based on time containment
        # CRITICAL: We need this to find cudaLaunchKernel events (which lack sequence numbers)
        # as children of aten:: operations (which have sequence numbers)
        self._build_tree()

        # Map Sequence numbers
        self._map_sequence_numbers()

        # Find LINE annotations
        self._find_line_annotations()

        # Analyze threads
        self._analyze_threads()

        # Map cudaLaunchKernel → GPU kernels
        self._map_launch_to_kernel()

    def _build_tree(self):
        """Build parent-child tree based on time containment, filtered by process ID."""
        for child in self.events:
            best_parent = None
            best_range = float("inf")

            for potential_parent in self.events:
                if potential_parent.idx == child.idx:
                    continue

                if potential_parent.pid != child.pid:
                    continue

                if potential_parent.contains(child):
                    range_size = potential_parent.dur
                    if range_size < best_range:
                        best_range = range_size
                        best_parent = potential_parent

            if best_parent:
                child.parent = best_parent
                best_parent.children.append(child)

    def _map_sequence_numbers(self):
        """Map Sequence numbers from forward to backward operations."""
        self.seq_to_forward = defaultdict(list)
        self.seq_to_backward = defaultdict(list)

        for evt in self.events:
            seq_num = evt.args.get("Sequence number")
            if seq_num is not None:
                if evt.is_backward:
                    self.seq_to_backward[seq_num].append(evt)
                else:
                    self.seq_to_forward[seq_num].append(evt)

    def _find_line_annotations(self):
        """
        Find all annotations and create unique keys for each occurrence.

        Annotation format: {MARKER}:<filename>:<line_number>:<code>[:<loopstr>]
        Example: @@:so2_layers.py:42:x = torch.matmul(a, b)

        Each occurrence gets a unique key with occurrence number as prefix:
        0:so2_layers.py:42 (first occurrence)
        1:so2_layers.py:42 (second occurrence)
        2:so2_layers.py:42 (third occurrence)
        """
        self.line_annotations = {}  # unique_key -> event
        self.line_to_base_id = {}  # maps unique_key back to base line_id (filename:line)

        # Track occurrence count for each base line ID
        occurrence_count = defaultdict(int)

        for evt in self.events:
            if evt.is_line_annotation:
                # Parse the annotation format: {MARKER}:<filename>:<line_number>:<code>[:<loopstr>]
                parts = evt.name.split(":")

                if len(parts) < 4:
                    continue  # Malformed annotation

                filename = parts[1]
                line_number = parts[2]

                # Base ID without occurrence number (for grouping)
                base_id = f"{filename}:{line_number}"

                # Get the occurrence number for this base_id and increment
                occurrence_num = occurrence_count[base_id]
                occurrence_count[base_id] += 1

                # Unique key with occurrence number as prefix
                unique_key = f"{occurrence_num}:{base_id}"

                # Store the event with its unique key
                self.line_annotations[unique_key] = evt
                self.line_to_base_id[unique_key] = base_id

    def _analyze_threads(self):
        """Analyze all threads and collect their characteristics."""
        self.threads = defaultdict(
            lambda: {"operations": [], "total_time_ms": 0, "has_gpu_kernels": False}
        )

        for evt in self.events:
            if not evt.is_line_annotation and evt.dur > 0:
                self.threads[evt.tid]["operations"].append(evt.name)
                self.threads[evt.tid]["total_time_ms"] += evt.dur / 1000

                # A thread is a GPU thread if it contains GPU kernel operations
                # GPU kernels run on CUDA streams and have a "stream" argument
                if evt.args.get("stream") is not None:
                    self.threads[evt.tid]["has_gpu_kernels"] = True

    def _map_launch_to_kernel(self):
        """
        Map kernel launch operations to their corresponding GPU kernels using correlation IDs.

        KEY INSIGHT: Instead of using fragile regex patterns, we use correlation IDs:
        - GPU kernels have correlation IDs
        - cudaLaunchKernel/cuLaunchKernel have correlation IDs
        - Correlation ID links launch → kernel

        This is more robust than regex and architecture-independent!
        """
        # Build lookup: correlation_id → GPU kernel event
        # GPU kernels are on GPU streams (different threads than CPU)
        gpu_kernels_by_correlation = {}

        for evt in self.events:
            correlation = evt.args.get("correlation")
            if correlation is None:
                continue

            # Check if this is a kernel launch (CPU-side operation)
            if "cudaLaunchKernel" in evt.name or "cuLaunchKernel" in evt.name:
                continue  # Skip launches, we're looking for actual GPU kernels

            # Everything else with a correlation ID is a GPU kernel or related operation
            gpu_kernels_by_correlation[correlation] = evt

        # Map: launch event → GPU kernel
        self.launch_to_kernel = {}

        # Find all kernel launches and map them to their GPU kernels
        for evt in self.events:
            # Check if this is a kernel launch (Runtime API or Driver API)
            if "cudaLaunchKernel" in evt.name or "cuLaunchKernel" in evt.name:
                correlation = evt.args.get("correlation")
                if correlation is not None:
                    # Map this launch to its GPU kernel via correlation ID
                    self.launch_to_kernel[evt] = gpu_kernels_by_correlation.get(
                        correlation
                    )

    def get_operations_for_line(self, line_id):
        """Get all operations (forward + backward) for a specific LINE occurrence, grouped by thread.

        line_id format: filename:line_number:timestamp (unique key for each occurrence)
        """
        line_evt = self.line_annotations.get(line_id)
        if not line_evt:
            return {}

        # Find all nested (child) line annotations within THIS occurrence's time window
        nested_annotations = []
        for other_line_id, other_evt in self.line_annotations.items():
            if other_line_id == line_id:
                continue
            # Check if other annotation is nested within this line occurrence's time window
            if other_evt.ts >= line_evt.ts and other_evt.end_ts <= line_evt.end_ts:
                nested_annotations.append(other_evt)

        # Collect sequence numbers from operations in THIS occurrence's time window
        # EXCLUDING operations that fall within nested child annotations
        forward_seq_numbers = set()

        for evt in self.events:
            if evt.is_line_annotation or evt.is_backward:
                continue

            # Check if event is within this line occurrence's time window
            if evt.ts >= line_evt.ts and evt.end_ts <= line_evt.end_ts:
                # Check if event is within any nested child annotation
                is_in_nested = False
                for nested in nested_annotations:
                    if evt.ts >= nested.ts and evt.end_ts <= nested.end_ts:
                        is_in_nested = True
                        break

                # Only count if not in a nested annotation
                if not is_in_nested:
                    seq_num = evt.args.get("Sequence number")
                    if seq_num is not None:
                        forward_seq_numbers.add(seq_num)

        # Step 2: Collect all operations with these sequence numbers
        results = defaultdict(lambda: {"forward": [], "backward": []})
        seen = set()

        def add_event_with_attached_kernel(evt, direction, original_seq_num):
            """
            Add event to results, and if it's a kernel launch, also add its GPU kernel.
            Then recurse to children.

            CRITICAL: We recurse through children because cudaLaunchKernel events
            (which contain correlation IDs) do NOT have sequence numbers, but are
            children of aten:: operations (which do have sequence numbers).
            """
            key = (evt.tid, evt.name, evt.ts)
            if key in seen or evt.is_line_annotation:
                return

            # Add this event
            if evt.dur >= 0.001:
                seen.add(key)
                op_info = {
                    "name": evt.name,
                    "time_ms": evt.dur / 1000,
                    "seq_num": original_seq_num,
                }
                results[evt.tid][direction].append(op_info)

                # If this is a kernel launch, add the attached GPU kernel
                if evt in self.launch_to_kernel:
                    gpu_kernel = self.launch_to_kernel[evt]
                    gpu_key = (gpu_kernel.tid, gpu_kernel.name, gpu_kernel.ts)
                    if gpu_key not in seen and gpu_kernel.dur >= 0.001:
                        seen.add(gpu_key)
                        gpu_op_info = {
                            "name": gpu_kernel.name,
                            "time_ms": gpu_kernel.dur / 1000,
                            "seq_num": original_seq_num,
                        }
                        results[gpu_kernel.tid][direction].append(gpu_op_info)

            # Recurse to children to find cudaLaunchKernel (which lack sequence numbers)
            for child in evt.children:
                if not child.is_line_annotation:
                    add_event_with_attached_kernel(child, direction, original_seq_num)

        # Step 3: Process forward operations
        for seq_num in forward_seq_numbers:
            for evt in self.events:
                if evt.is_backward or evt.is_line_annotation:
                    continue

                if evt.args.get("Sequence number") == seq_num:
                    add_event_with_attached_kernel(evt, "forward", seq_num)

        # Step 4: Process backward operations
        for seq_num in forward_seq_numbers:
            for evt in self.events:
                if not evt.is_backward:
                    continue

                if evt.args.get("Sequence number") == seq_num:
                    add_event_with_attached_kernel(evt, "backward", seq_num)

        return dict(results)

    def get_all_line_operations(self):
        """Get operations for all LINEs."""
        return {
            line_id: self.get_operations_for_line(line_id)
            for line_id in sorted(self.line_annotations.keys())
        }

    def print_analysis(
        self,
        gpu_only=False,
        width=120,
        include_thread_summary=True,
        include_line_details=True,
        include_grand_total=True,
    ):
        """
        Print analysis of trace with configurable formatting.

        Args:
            gpu_only: If True, only show GPU thread operations
            width: Width of output (default 120 characters)
            include_thread_summary: Print thread classification summary
            include_line_details: Print detailed operations for each line
            include_grand_total: Print grand total at the end
        """
        sep = "=" * width
        subsep = "-" * width

        # Thread summary
        if include_thread_summary:
            print(sep)
            print("THREAD ANALYSIS")
            print(sep)
            for tid in sorted(
                self.threads.keys(), key=lambda x: (isinstance(x, str), x)
            ):
                thread_info = self.threads[tid]
                thread_type = "GPU" if thread_info["has_gpu_kernels"] else "CPU"
                print(
                    f"Thread {str(tid):>5s} [{thread_type}]: {thread_info['total_time_ms']:8.3f} ms, "
                    f"{len(thread_info['operations'])} operations"
                )

        # Get all line operations
        line_ops = self.get_all_line_operations()

        # Print line details
        if include_line_details:
            print(f"\n{sep}")
            print("OPERATIONS MAPPED TO SOURCE LINES (GROUPED BY THREAD)")
            print(sep)

            grand_total_forward_ms = 0.0
            grand_total_backward_ms = 0.0

            # Sort lines by their first timestamp (execution order) instead of lexicographically
            def get_first_timestamp(line_id):
                """Get the first timestamp for a line annotation."""
                line_evt = self.line_annotations.get(line_id)
                if not line_evt:
                    return float("inf")
                return line_evt.ts

            sorted_line_ids = sorted(line_ops.keys(), key=get_first_timestamp)

            for line_id in sorted_line_ids:
                ops_by_thread = line_ops[line_id]
                if not ops_by_thread:
                    continue

                line_evt = self.line_annotations[line_id]
                line_desc = line_evt.format_annotation(include_code=True)

                print(f"\n{sep}")
                print(f"{line_desc}")
                print(sep)

                # Track line totals
                line_total_forward = 0.0
                line_total_backward = 0.0

                # Process each thread
                for tid in sorted(ops_by_thread.keys()):
                    thread_ops = ops_by_thread[tid]
                    thread_type = (
                        "GPU" if self.threads[tid]["has_gpu_kernels"] else "CPU"
                    )

                    # Skip CPU threads if gpu_only is True
                    if gpu_only and thread_type == "CPU":
                        continue

                    # Forward operations
                    if thread_ops["forward"]:
                        fwd_agg = {}
                        for op in thread_ops["forward"]:
                            name = op["name"]
                            if name not in fwd_agg:
                                fwd_agg[name] = {"time_ms": 0, "count": 0}
                            fwd_agg[name]["time_ms"] += op["time_ms"]
                            fwd_agg[name]["count"] += 1

                        print(f"\n  Thread {tid} [{thread_type}] - FORWARD:")
                        print(f"  {'-' * (width - 4)}")
                        print(f"  {'Operation':<80} {'Time (ms)':>15} {'Calls':>8}")
                        print(f"  {'-' * (width - 4)}")

                        sorted_ops = sorted(
                            fwd_agg.items(), key=lambda x: x[1]["time_ms"], reverse=True
                        )
                        for op_name, op_data in sorted_ops:
                            truncated = op_name[:80] if len(op_name) > 80 else op_name
                            print(
                                f"  {truncated:<80} {op_data['time_ms']:>15.3f} {op_data['count']:>8}"
                            )

                        total = sum(op["time_ms"] for op in fwd_agg.values())
                        line_total_forward += total
                        print(f"  {'-' * (width - 4)}")
                        print(f"  {'Subtotal':<80} {total:>15.3f}")

                    # Backward operations
                    if thread_ops["backward"]:
                        bwd_agg = {}
                        for op in thread_ops["backward"]:
                            name = op["name"]
                            if name not in bwd_agg:
                                bwd_agg[name] = {"time_ms": 0, "count": 0}
                            bwd_agg[name]["time_ms"] += op["time_ms"]
                            bwd_agg[name]["count"] += 1

                        print(f"\n  Thread {tid} [{thread_type}] - BACKWARD:")
                        print(f"  {'-' * (width - 4)}")
                        print(f"  {'Operation':<80} {'Time (ms)':>15} {'Calls':>8}")
                        print(f"  {'-' * (width - 4)}")

                        sorted_ops = sorted(
                            bwd_agg.items(), key=lambda x: x[1]["time_ms"], reverse=True
                        )
                        for op_name, op_data in sorted_ops:
                            truncated = op_name[:80] if len(op_name) > 80 else op_name
                            print(
                                f"  {truncated:<80} {op_data['time_ms']:>15.3f} {op_data['count']:>8}"
                            )

                        total = sum(op["time_ms"] for op in bwd_agg.values())
                        line_total_backward += total
                        print(f"  {'-' * (width - 4)}")
                        print(f"  {'Subtotal':<80} {total:>15.3f}")

                # Print line total
                line_total = line_total_forward + line_total_backward
                print(f"\n  {'LINE TOTAL':<80} {line_total:>15.3f} ms")
                grand_total_forward_ms += line_total_forward
                grand_total_backward_ms += line_total_backward

            # Print grand total
            if include_grand_total:
                grand_total_ms = grand_total_forward_ms + grand_total_backward_ms
                print(f"\n\n{sep}")
                print(f"GRAND TOTAL (ALL LINES)")
                print(sep)
                print(f"{'Forward:':<20} {grand_total_forward_ms:>15.3f} ms")
                print(f"{'Backward:':<20} {grand_total_backward_ms:>15.3f} ms")
                print(subsep)
                print(f"{'Total:':<20} {grand_total_ms:>15.3f} ms")
                print(sep)
