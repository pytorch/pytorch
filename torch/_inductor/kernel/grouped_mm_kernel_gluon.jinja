import triton

from triton.language.core import _aggregate as aggregate

from triton.experimental import gluon
from triton.experimental.gluon import language as gl

from triton.experimental.gluon.language.nvidia.blackwell import (
    tensor_memory_descriptor,
    TensorMemoryLayout,
    allocate_tensor_memory,
    get_tmem_reg_layout,
    tma,
    mbarrier,
    tcgen05_mma,
    tcgen05_commit,
    fence_async_shared,
)

import triton.profiler.language as pl


{%- if not USE_UPDATE_TENSOR_DESCRIPTOR %}
@gluon.jit
def to_ragged_indices(slice_off, slice_size, row):
    billion = 0x40000000  # == 2**30
    x = billion - slice_size + row
    y = slice_off + slice_size
    return billion, y, x
{%- endif %}


@aggregate
class PartitionArgs:
    a_desc: tma.tensor_descriptor
    b_desc: tma.tensor_descriptor
    c_desc: tma.tensor_descriptor
    offs_ptr: gl.tensor
    c_ptr: gl.tensor
    c_stride_0: gl.constexpr
    c_stride_1: gl.constexpr
    c_layout: gl.constexpr
    a_smem_bufs: gl.shared_memory_descriptor
    b_smem_bufs: gl.shared_memory_descriptor
    acc_tmem_bufs: tensor_memory_descriptor
    load_buf_empty_bars: gl.shared_memory_descriptor
    load_buf_full_bars: gl.shared_memory_descriptor
    acc_buf_empty_bars: gl.shared_memory_descriptor
    acc_buf_full_bars: gl.shared_memory_descriptor
    G: gl.constexpr
    M: gl.constexpr
    N: gl.constexpr
    K: gl.constexpr
    BLOCK_M: gl.constexpr
    BLOCK_N: gl.constexpr
    BLOCK_K: gl.constexpr
    store_layout: gl.constexpr
    NUM_BLOCKS: gl.constexpr
    NUM_STORE_WARPS: gl.constexpr

    @gluon.constexpr_function
    def __init__(
        self,
        a_desc,
        b_desc,
        c_desc,
        offs_ptr,
        c_ptr,
        c_stride_0,
        c_stride_1,
        c_layout,
        a_smem_bufs,
        b_smem_bufs,
        acc_tmem_bufs,
        load_buf_empty_bars,
        load_buf_full_bars,
        acc_buf_empty_bars,
        acc_buf_full_bars,
        G,
        M,
        N,
        K,
        BLOCK_M,
        BLOCK_N,
        BLOCK_K,
        store_layout,
        NUM_BLOCKS,
        NUM_STORE_WARPS,
    ):
        self.a_desc = a_desc
        self.b_desc = b_desc
        self.c_desc = c_desc
        self.offs_ptr = offs_ptr
        self.c_ptr = c_ptr
        self.c_stride_0 = gl.constexpr(c_stride_0)
        self.c_stride_1 = gl.constexpr(c_stride_1)
        self.c_layout = gl.constexpr(c_layout)
        self.a_smem_bufs = a_smem_bufs
        self.b_smem_bufs = b_smem_bufs
        self.acc_tmem_bufs = acc_tmem_bufs
        self.load_buf_empty_bars = load_buf_empty_bars
        self.load_buf_full_bars = load_buf_full_bars
        self.acc_buf_empty_bars = acc_buf_empty_bars
        self.acc_buf_full_bars = acc_buf_full_bars
        self.G = gl.constexpr(G)
        self.M = gl.constexpr(M)
        self.N = gl.constexpr(N)
        self.K = gl.constexpr(K)
        self.BLOCK_M = gl.constexpr(BLOCK_M)
        self.BLOCK_N = gl.constexpr(BLOCK_N)
        self.BLOCK_K = gl.constexpr(BLOCK_K)
        self.store_layout = gl.constexpr(store_layout)
        self.NUM_BLOCKS = gl.constexpr(NUM_BLOCKS)
        self.NUM_STORE_WARPS = gl.constexpr(NUM_STORE_WARPS)


# Counter abstraction for tracking barrier index and phase.
@aggregate
class Counter:
    index: gl.tensor
    phase: gl.tensor
    num_barriers: gl.constexpr

    @gluon.constexpr_function
    def __init__(self, index, phase, num_barriers):
        self.index = index
        self.phase = phase
        self.num_barriers = gl.constexpr(num_barriers)

    @gluon.jit
    def create(phase, num_barriers: gl.constexpr):
        return Counter(gl.to_tensor(0), gl.to_tensor(phase), num_barriers)

    @gluon.must_use_result
    @gluon.jit
    def next(self):
        incr = self.index + 1
        rollover = incr == self.num_barriers
        index = gl.where(rollover, 0, incr)
        phase = gl.where(rollover, self.phase ^ 1, self.phase)
        return Counter(index, phase, self.num_barriers)


@gluon.jit
def load_partition(pargs: PartitionArgs):
    pl.enter_scope("load_partition")

    tidx = gl.program_id(0)

    G: gl.constexpr = pargs.G
    M: gl.constexpr = pargs.M
    N: gl.constexpr = pargs.N
    K: gl.constexpr = pargs.K
    BLOCK_M: gl.constexpr = pargs.BLOCK_M
    BLOCK_N: gl.constexpr = pargs.BLOCK_N
    BLOCK_K: gl.constexpr = pargs.BLOCK_K

    load_buf_empty_bars = pargs.load_buf_empty_bars
    load_buf_full_bars = pargs.load_buf_full_bars
    state = Counter.create(1, load_buf_empty_bars.shape[0])

    iterated_tiles = 0

    n_start_offset = 0
    m_end_offset = 0
    for g in range(G):
        m_start_offset = m_end_offset
        m_end_offset = gl.load(pargs.offs_ptr + g)
        m_size = m_end_offset - m_start_offset
        n_size = N
        k_size = K

        num_m_tiles = gl.cdiv(m_size, BLOCK_M)
        num_n_tiles = gl.cdiv(n_size, BLOCK_N)
        num_tiles = num_m_tiles * num_n_tiles

        while tidx >= iterated_tiles and tidx < iterated_tiles + num_tiles:
            gidx = tidx - iterated_tiles
            tile_m_idx = gidx % num_m_tiles
            tile_n_idx = gidx // num_m_tiles
            m_tile_offset = tile_m_idx * BLOCK_M
            n_tile_offset = tile_n_idx * BLOCK_N
            m_offset = m_start_offset + m_tile_offset
            n_offset = n_start_offset + n_tile_offset

            for k in range(0, k_size, BLOCK_K):
                load_buf_empty_bar = load_buf_empty_bars.index(state.index)
                load_buf_full_bar = load_buf_full_bars.index(state.index)
                a_smem = pargs.a_smem_bufs.index(state.index)
                b_smem = pargs.b_smem_bufs.index(state.index)

                with pl.scope("load_barrier_wait_empty"):
                    mbarrier.wait(load_buf_empty_bar, state.phase)

                with pl.scope("load_barrier_expect"):
                    mbarrier.expect(
                        load_buf_full_bar,
                        pargs.a_desc.block_type.nbytes + pargs.b_desc.block_type.nbytes,
                    )

                with pl.scope("load_tma_async"):
                    tma.async_copy_global_to_shared(
                        pargs.a_desc, [m_offset, k], load_buf_full_bar, a_smem
                    )
                    tma.async_copy_global_to_shared(
                        pargs.b_desc, [g, n_offset, k], load_buf_full_bar, b_smem
                    )

                state = state.next()

            tidx += pargs.NUM_BLOCKS

        iterated_tiles += num_tiles

    pl.exit_scope("load_partition")


@gluon.jit
def compute_partition(pargs: PartitionArgs):
    pl.enter_scope("compute_partition")

    tidx = gl.program_id(0)

    G: gl.constexpr = pargs.G
    M: gl.constexpr = pargs.M
    N: gl.constexpr = pargs.N
    K: gl.constexpr = pargs.K
    BLOCK_M: gl.constexpr = pargs.BLOCK_M
    BLOCK_N: gl.constexpr = pargs.BLOCK_N
    BLOCK_K: gl.constexpr = pargs.BLOCK_K

    load_buf_empty_bars = pargs.load_buf_empty_bars
    load_buf_full_bars = pargs.load_buf_full_bars
    load_state = Counter.create(0, load_buf_empty_bars.shape[0])

    acc_buf_empty_bars = pargs.acc_buf_empty_bars
    acc_buf_full_bars = pargs.acc_buf_full_bars
    acc_state = Counter.create(1, acc_buf_empty_bars.shape[0])

    iterated_tiles = 0

    n_start_offset = 0
    m_end_offset = 0
    for g in range(G):
        m_start_offset = m_end_offset
        m_end_offset = gl.load(pargs.offs_ptr + g)
        m_size = m_end_offset - m_start_offset
        n_size = N
        k_size = K

        num_m_tiles = gl.cdiv(m_size, BLOCK_M)
        num_n_tiles = gl.cdiv(n_size, BLOCK_N)
        num_tiles = num_m_tiles * num_n_tiles

        while tidx >= iterated_tiles and tidx < iterated_tiles + num_tiles:
            gidx = tidx - iterated_tiles
            tile_m_idx = gidx % num_m_tiles
            tile_n_idx = gidx // num_m_tiles
            m_tile_offset = tile_m_idx * BLOCK_M
            n_tile_offset = tile_n_idx * BLOCK_N
            m_offset = m_start_offset + m_tile_offset
            n_offset = n_start_offset + n_tile_offset

            acc_buf_empty_bar = acc_buf_empty_bars.index(acc_state.index)

            with pl.scope("compute_barrier_wait_acc_empty"):
                mbarrier.wait(acc_buf_empty_bar, acc_state.phase)

            acc = pargs.acc_tmem_bufs.index(acc_state.index)
            use_acc = False

            for k in range(0, k_size, BLOCK_K):
                load_buf_empty_bar = load_buf_empty_bars.index(load_state.index)
                load_buf_full_bar = load_buf_full_bars.index(load_state.index)

                with pl.scope("compute_barrier_wait_load_full"):
                    mbarrier.wait(load_buf_full_bar, load_state.phase)

                with pl.scope("compute_data_prep"):
                    a_smem = pargs.a_smem_bufs.index(load_state.index)
                    b_smem = pargs.b_smem_bufs.index(load_state.index)
                    b = b_smem.reshape([BLOCK_N, BLOCK_K]).permute((1, 0))

                with pl.scope("compute_mma"):
                    tcgen05_mma(a_smem, b, acc, use_acc=use_acc)

                with pl.scope("compute_commit_empty"):
                    tcgen05_commit(load_buf_empty_bar)

                use_acc = True
                load_state = load_state.next()

            acc_buf_full_bar = acc_buf_full_bars.index(acc_state.index)

            with pl.scope("compute_commit_acc_full"):
                tcgen05_commit(acc_buf_full_bar)

            acc_state = acc_state.next()

            tidx += pargs.NUM_BLOCKS

        iterated_tiles += num_tiles

    pl.exit_scope("compute_partition")


@gluon.jit
def store_partition(pargs: PartitionArgs):
    pl.enter_scope("store_partition")

    c_desc = pargs.c_desc

    tidx = gl.program_id(0)

    G: gl.constexpr = pargs.G
    M: gl.constexpr = pargs.M
    N: gl.constexpr = pargs.N
    K: gl.constexpr = pargs.K
    BLOCK_M: gl.constexpr = pargs.BLOCK_M
    BLOCK_N: gl.constexpr = pargs.BLOCK_N
    BLOCK_K: gl.constexpr = pargs.BLOCK_K
    NUM_STORE_WARPS: gl.constexpr = pargs.NUM_STORE_WARPS

    acc_buf_empty_bars = pargs.acc_buf_empty_bars
    acc_buf_full_bars = pargs.acc_buf_full_bars
    acc_state = Counter.create(0, acc_buf_empty_bars.shape[0])
    acc_reg_layout: gl.constexpr = get_tmem_reg_layout(
        gl.float32,
        [BLOCK_M, BLOCK_N],
        TensorMemoryLayout(block=[BLOCK_M, BLOCK_N], col_stride=1),
        NUM_STORE_WARPS,
        instr_variant="32x32b",
    )

    # Shared memory is always allocated with 2D shape [BLOCK_M, BLOCK_N]
    # Always use 2D c_layout for shared memory, regardless of descriptor layout
    c_smem = gl.allocate_shared_memory(
        pargs.c_desc.dtype, [BLOCK_M, BLOCK_N], pargs.c_layout
    )

    iterated_tiles = 0

    n_start_offset = 0
    m_end_offset = 0
    for g in range(G):
        m_start_offset = m_end_offset
        m_end_offset = gl.load(pargs.offs_ptr + g)
        m_size = m_end_offset - m_start_offset
        n_size = N
        k_size = K

        num_m_tiles = gl.cdiv(m_size, BLOCK_M)
        num_n_tiles = gl.cdiv(n_size, BLOCK_N)
        num_tiles = num_m_tiles * num_n_tiles

{%- if USE_UPDATE_TENSOR_DESCRIPTOR %}
        if num_tiles > 0:
            with pl.scope("store_update_descriptor"):
                tma.update_tensor_descriptor(
                    c_desc,
                    base=pargs.c_ptr
                    + m_start_offset * pargs.c_stride_0
                    + n_start_offset * pargs.c_stride_1,
                    shape=[m_size, n_size],
                )
{%- endif %}

        while tidx >= iterated_tiles and tidx < iterated_tiles + num_tiles:
            gidx = tidx - iterated_tiles
            tile_m_idx = gidx % num_m_tiles
            tile_n_idx = gidx // num_m_tiles
{%- if USE_UPDATE_TENSOR_DESCRIPTOR %}
            m_offset = tile_m_idx * BLOCK_M
            n_offset = tile_n_idx * BLOCK_N
 {%- else %}
            m_tile_offset = tile_m_idx * BLOCK_M
            n_tile_offset = tile_n_idx * BLOCK_N
            m_offset = m_start_offset + m_tile_offset
            n_offset = n_start_offset + n_tile_offset
{%- endif %}

            # fixme: add epilogue here!

            acc_buf_full_bar = acc_buf_full_bars.index(acc_state.index)

            with pl.scope("store_barrier_wait_acc_full"):
                mbarrier.wait(acc_buf_full_bar, acc_state.phase)

            with pl.scope("store_tmem_load"):
                acc_tmem = pargs.acc_tmem_bufs.index(acc_state.index)
                acc = acc_tmem.load(acc_reg_layout)

            acc_buf_empty_bar = acc_buf_empty_bars.index(acc_state.index)
            acc_state = acc_state.next()

            with pl.scope("store_convert"):
                c = acc.to(c_desc.dtype)

            with pl.scope("store_tma_wait"):
                tma.store_wait(pendings=0)

            with pl.scope("store_smem_write"):
                c_smem.store(c)

            # After usage of acc, we're sure that smem/tmem buffer
            # is free for reuse
            with pl.scope("store_barrier_arrive"):
                mbarrier.arrive(acc_buf_empty_bar, count=1)

            with pl.scope("store_fence_and_tma"):
                fence_async_shared()
{%- if USE_UPDATE_TENSOR_DESCRIPTOR %}
                tma.async_copy_shared_to_global(c_desc, [m_offset, n_offset], c_smem)
{%- else %}
                c0, c1, c2 = to_ragged_indices(m_start_offset, m_size, m_tile_offset)
                c_smem_4d = c_smem.reshape([1, 1, BLOCK_M, BLOCK_N])
                tma.async_copy_shared_to_global(c_desc, [c0, c1, c2, n_offset], c_smem_4d)
{%- endif %}

            tidx += pargs.NUM_BLOCKS

        iterated_tiles += num_tiles

    with pl.scope("store_final_tma_wait"):
        tma.store_wait(pendings=0)

    pl.exit_scope("store_partition")


@gluon.jit
def grouped_mm_kernel(
    a_ptr,
    b_ptr,
    offs_ptr,
    c_ptr,
    a_layout: gl.constexpr,
    b_layout: gl.constexpr,
    c_layout: gl.constexpr,
    c_layout_ragged: gl.constexpr,
    dtype: gl.constexpr,
    M: gl.constexpr,
    N: gl.constexpr,
    K: gl.constexpr,
    a_stride_0: gl.constexpr,
    a_stride_1: gl.constexpr,
    b_stride_0: gl.constexpr,
    b_stride_1: gl.constexpr,
    b_stride_2: gl.constexpr,
    c_stride_0: gl.constexpr,
    c_stride_1: gl.constexpr,
    BLOCK_M: gl.constexpr,
    BLOCK_N: gl.constexpr,
    BLOCK_K: gl.constexpr,
    G: gl.constexpr,
    store_layout: gl.constexpr,
    NUM_BLOCKS: gl.constexpr,
    NUM_LOAD_BUFFERS: gl.constexpr,
    NUM_ACC_BUFFERS: gl.constexpr,
    NUM_LOAD_WARPS: gl.constexpr,
    NUM_COMPUTE_WARPS: gl.constexpr,
    NUM_LOAD_THREAD_REGISTERS: gl.constexpr,
    NUM_COMPUTE_THREAD_REGISTERS: gl.constexpr,
    num_warps: gl.constexpr,
):
    NUM_STORE_WARPS: gl.constexpr = num_warps

    a_desc = tma.make_tensor_descriptor(
        a_ptr,
        shape=[M, K],
        strides=[a_stride_0, a_stride_1],
        block_shape=[BLOCK_M, BLOCK_K],
        layout=a_layout,
    )

    b_desc = tma.make_tensor_descriptor(
        b_ptr,
        shape=[G, N, K],
        strides=[b_stride_0, b_stride_1, b_stride_2],
        block_shape=[1, BLOCK_N, BLOCK_K],
        layout=b_layout,
    )

{%- if USE_UPDATE_TENSOR_DESCRIPTOR %}
    c_desc = tma.make_tensor_descriptor(
        c_ptr,
        shape=[M, N],
        strides=[c_stride_0, c_stride_1],
        block_shape=[BLOCK_M, BLOCK_N],
        layout=c_layout,
    )
{%- else %}
    billion: gl.constexpr = 0x40000000  # == 2**30
    max_int: gl.constexpr = 0x7fff0000

    # C tensor ragged descriptor: prepend 2 transformation dimensions
    # Use 4D layout to match the 4D block_shape
    c_desc = tma.make_tensor_descriptor(
        c_ptr,
        shape=[max_int, max_int, billion, N],
        strides=[2**34 - c_stride_0, c_stride_0, c_stride_0, c_stride_1],
        block_shape=[1, 1, BLOCK_M, BLOCK_N],
        layout=c_layout_ragged,
    )
{%- endif %}

    a_smem_bufs = gl.allocate_shared_memory(
        dtype, [NUM_LOAD_BUFFERS, BLOCK_M, BLOCK_K], a_layout
    )
    b_smem_bufs = gl.allocate_shared_memory(
        dtype, [NUM_LOAD_BUFFERS, 1, BLOCK_N, BLOCK_K], b_layout
    )
    load_buf_empty_bars = gl.allocate_shared_memory(
        gl.int64, [NUM_LOAD_BUFFERS, 1], mbarrier.MBarrierLayout()
    )
    load_buf_full_bars = gl.allocate_shared_memory(
        gl.int64, [NUM_LOAD_BUFFERS, 1], mbarrier.MBarrierLayout()
    )
    for i in gl.static_range(NUM_LOAD_BUFFERS):
        mbarrier.init(load_buf_empty_bars.index(i), count=1)
        mbarrier.init(load_buf_full_bars.index(i), count=1)

    tmem_layout: gl.constexpr = TensorMemoryLayout(
        block=[BLOCK_M, BLOCK_N], col_stride=1
    )
    acc_tmem_bufs = allocate_tensor_memory(
        gl.float32, [NUM_ACC_BUFFERS, BLOCK_M, BLOCK_N], tmem_layout
    )

    acc_buf_empty_bars = gl.allocate_shared_memory(
        gl.int64, [NUM_ACC_BUFFERS, 1], mbarrier.MBarrierLayout()
    )
    acc_buf_full_bars = gl.allocate_shared_memory(
        gl.int64, [NUM_ACC_BUFFERS, 1], mbarrier.MBarrierLayout()
    )
    for i in gl.static_range(NUM_ACC_BUFFERS):
        mbarrier.init(acc_buf_empty_bars.index(i), count=1)
        mbarrier.init(acc_buf_full_bars.index(i), count=1)

    pargs = PartitionArgs(
        a_desc,
        b_desc,
        c_desc,
        offs_ptr,
        c_ptr,
        c_stride_0,
        c_stride_1,
        c_layout,
        a_smem_bufs,
        b_smem_bufs,
        acc_tmem_bufs,
        load_buf_empty_bars,
        load_buf_full_bars,
        acc_buf_empty_bars,
        acc_buf_full_bars,
        G,
        M,
        N,
        K,
        BLOCK_M,
        BLOCK_N,
        BLOCK_K,
        store_layout,
        NUM_BLOCKS,
        NUM_STORE_WARPS,
    )

    gl.warp_specialize(
        [
            (store_partition, (pargs,)),
            (load_partition, (pargs,)),
            (compute_partition, (pargs,)),
        ],
        [NUM_LOAD_WARPS, NUM_COMPUTE_WARPS],
        [NUM_LOAD_THREAD_REGISTERS, NUM_COMPUTE_THREAD_REGISTERS],
    )
