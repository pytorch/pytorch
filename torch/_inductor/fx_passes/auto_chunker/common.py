# mypy: allow-untyped-defs
import dataclasses
from typing import Optional

from torch.fx import Node


class CantChunk(RuntimeError):
    pass


@dataclasses.dataclass
class ChunkingMeta:
    # The value of the node should be scaled by the specified scalar
    # tensor. Need this sine we pretend tangent to be 1 first and
    # scale the affected tensor later. Need propagate such information
    # to downstream.
    scale_by: Optional[Node] = None

    # The dimension of the current tensor that get chunked.
    # Can be None if the current tensor is not chunked. E.g., when
    # the current tensor is a scalar tensor generated by summing a chunked tensor.
    #
    # To recover a tensor with non-None chunk_dim, we need concat each
    # chunk at the 'chunk_dim' dimension.
    chunk_dim: Optional[int] = None

    # The original tensor is the sum of each tensor in the chunk subgraph.
    # chunk_dim should be None if need_sum is True
    #
    # Note for some special cases like the tangent placeholder node, both
    # chunk_dim can be None and need_sum can be false, but scale_by
    # in that case is the tangent node itself.
    need_sum: bool = False

    def copy(self, **kwargs):
        meta = ChunkingMeta(**self.__dict__)
        for k, v in kwargs.items():
            setattr(meta, k, v)
        return meta

    @staticmethod
    def equal(lhs_meta, rhs_meta, skip_scale_by=False):
        if skip_scale_by:
            lhs_meta = lhs_meta.copy(scale_by=None)
            rhs_meta = rhs_meta.copy(scale_by=None)
        return lhs_meta == rhs_meta

    def chunk_by_dim(self, dim: int):
        return self.chunk_dim == dim

    @staticmethod
    def is_nop(meta):
        return meta is None or meta == ChunkingMeta()
