docker_config_defaults: &docker_config_defaults
  user: jenkins
  aws_auth:
    # This IAM user only allows read-only access to ECR
    aws_access_key_id: AKIAJ2J6FIG5OSZTQ3IA
    aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_ONLY}

pytorch_linux_cpu_build_test_defaults: &pytorch_linux_cpu_build_test_defaults
  resource_class: large
  working_directory: /var/lib/jenkins/workspace
  steps:
  - checkout
  - run:
      name: Build
      command: |
        export SCCACHE_BUCKET=ossci-compiler-cache
        export SCCACHE_MAX_JOBS=`expr $(nproc) - 1`
        export MEMORY_LIMIT_MAX_JOBS=8  # the "large" resource class on CircleCI has 32 CPU cores, if we use all of them we'll OOM
        export MAX_JOBS=$(( ${SCCACHE_MAX_JOBS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${SCCACHE_MAX_JOBS} ))
        # This IAM user allows write access to S3 bucket for sccache
        export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
        export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
        git submodule update --init
        .jenkins/pytorch/build.sh
        .jenkins/pytorch/test.sh

pytorch_linux_cpu_build_defaults: &pytorch_linux_cpu_build_defaults
  resource_class: large
  working_directory: /var/lib/jenkins/workspace
  steps:
  - checkout
  - run:
      name: Build
      command: |
        export SCCACHE_BUCKET=ossci-compiler-cache
        export SCCACHE_MAX_JOBS=`expr $(nproc) - 1`
        export MEMORY_LIMIT_MAX_JOBS=8  # the "large" resource class on CircleCI has 32 CPU cores, if we use all of them we'll OOM
        export MAX_JOBS=$(( ${SCCACHE_MAX_JOBS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${SCCACHE_MAX_JOBS} ))
        # This IAM user allows write access to S3 bucket for sccache
        export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
        export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
        git submodule update --init
        .jenkins/pytorch/build.sh
        mkdir -p pytorch-ci-env/
        cp -r /opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch pytorch-ci-env/torch
        cp -r build/bin pytorch-ci-env/cpp_test_bin
        if [ -d "../cpp-build" ]; then
          cp -r ../cpp-build pytorch-ci-env/cpp-build
        fi
  - persist_to_workspace:
      root: /var/lib/jenkins/workspace/pytorch-ci-env
      paths:
        - "*"

pytorch_linux_cpu_test_defaults: &pytorch_linux_cpu_test_defaults
  machine:
    image: default
  resource_class: large
  steps:
  - checkout
  - run:
      name: Prepare workspace
      command: |
        sudo mkdir -p /opt/workspace
        sudo chmod -R 777 /opt/workspace
  - attach_workspace:
      at: /opt/workspace
  - run:
      name: Build
      command: |
        set -x
        sudo pip install awscli
        sudo apt-get update
        sudo apt-get remove linux-image-generic linux-headers-generic linux-generic
        sudo apt-get install linux-headers-$(uname -r)
        sudo apt-get install linux-image-generic
        sudo pkill -SIGHUP dockerd
        # This IAM user only allows read-only access to ECR
        export AWS_ACCESS_KEY_ID=AKIAJ2J6FIG5OSZTQ3IA
        export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_ONLY}
        eval $(aws ecr get-login --region us-east-1 --no-include-email)
        docker pull ${DOCKER_IMAGE}
        id=$(docker run -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})
        pwd
        echo "declare -x PYTHON_VERSION=${PYTHON_VERSION}" > /home/circleci/project/env
        echo "declare -x SCCACHE_BUCKET=${SCCACHE_BUCKET}" >> /home/circleci/project/env
        # This IAM user allows write access to S3 bucket for sccache
        echo "declare -x AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA" >> /home/circleci/project/env
        echo "declare -x AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}" >> /home/circleci/project/env
        mkdir -p /home/circleci/project/build
        cp -r /opt/workspace/cpp_test_bin /home/circleci/project/build/bin
        docker cp /home/circleci/project/. "$id:/var/lib/jenkins/workspace"
        echo "mkdir -p /opt/conda/lib/python${PYTHON_VERSION}/site-packages" | docker exec -u jenkins -i "$id" bash
        docker cp "/opt/workspace/torch" "$id:/opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch"
        if [ -d "/opt/workspace/cpp-build" ]; then
          docker cp "/opt/workspace/cpp-build" "$id:/var/lib/jenkins/cpp-build"
        fi
        (echo "source ./workspace/env" && echo 'sudo chown -R jenkins workspace /opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch && cd workspace && git submodule update --init && .jenkins/pytorch/test.sh') | docker exec -u jenkins -i "$id" bash

pytorch_linux_cuda_build_defaults: &pytorch_linux_cuda_build_defaults
  resource_class: large
  working_directory: /var/lib/jenkins/workspace
  steps:
  - checkout
  - run:
      name: Build
      command: |
        export SCCACHE_BUCKET=ossci-compiler-cache
        export TORCH_CUDA_ARCH_LIST=5.2
        export SCCACHE_MAX_JOBS=`expr $(nproc) - 1`
        export MEMORY_LIMIT_MAX_JOBS=8  # the "large" resource class on CircleCI has 32 CPU cores, if we use all of them we'll OOM
        export MAX_JOBS=$(( ${SCCACHE_MAX_JOBS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${SCCACHE_MAX_JOBS} ))
        # This IAM user allows write access to S3 bucket for sccache
        export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
        export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
        git submodule update --init
        .jenkins/pytorch/build.sh
        mkdir -p pytorch-ci-env/
        cp -r /opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch pytorch-ci-env/torch
        cp -r build/bin pytorch-ci-env/cpp_test_bin
        if [ -d "../cpp-build" ]; then
          cp -r ../cpp-build pytorch-ci-env/cpp-build
        fi
  - persist_to_workspace:
      root: /var/lib/jenkins/workspace/pytorch-ci-env
      paths:
        - "*"

pytorch_linux_cuda_test_defaults: &pytorch_linux_cuda_test_defaults
  machine:
    image: default
  resource_class: gpu.small
  steps:
  - checkout
  - run:
      name: Prepare workspace
      command: |
        sudo mkdir -p /opt/workspace
        sudo chmod -R 777 /opt/workspace
  - attach_workspace:
      at: /opt/workspace
  - run:
      name: Build
      command: |
        set -x
        sudo pip install awscli
        curl -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
        echo "deb https://nvidia.github.io/libnvidia-container/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
        echo "deb https://nvidia.github.io/nvidia-container-runtime/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
        echo "deb https://nvidia.github.io/nvidia-docker/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
        sudo apt-get update
        sudo apt-get remove linux-image-generic linux-headers-generic linux-generic
        sudo apt-get install linux-headers-$(uname -r)
        sudo apt-get install linux-image-generic
        wget 'https://s3.amazonaws.com/ossci-linux/nvidia_driver/NVIDIA-Linux-x86_64-396.26.run'
        sudo /bin/bash ./NVIDIA-Linux-x86_64-396.26.run -s --no-drm
        sudo apt-get install -y nvidia-docker2
        sudo pkill -SIGHUP dockerd
        nvidia-smi
        # This IAM user only allows read-only access to ECR
        export AWS_ACCESS_KEY_ID=AKIAJ2J6FIG5OSZTQ3IA
        export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_ONLY}
        eval $(aws ecr get-login --region us-east-1 --no-include-email)
        docker pull ${DOCKER_IMAGE}
        id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})
        pwd
        echo "declare -x PYTHON_VERSION=${PYTHON_VERSION}" > /home/circleci/project/env
        echo "declare -x SCCACHE_BUCKET=${SCCACHE_BUCKET}" >> /home/circleci/project/env
        # This IAM user allows write access to S3 bucket for sccache
        echo "declare -x AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA" >> /home/circleci/project/env
        echo "declare -x AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}" >> /home/circleci/project/env
        mkdir -p /home/circleci/project/build
        cp -r /opt/workspace/cpp_test_bin /home/circleci/project/build/bin
        docker cp /home/circleci/project/. "$id:/var/lib/jenkins/workspace"
        echo "mkdir -p /opt/conda/lib/python${PYTHON_VERSION}/site-packages" | docker exec -u jenkins -i "$id" bash
        docker cp "/opt/workspace/torch" "$id:/opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch"
        if [ -d "/opt/workspace/cpp-build" ]; then
          docker cp "/opt/workspace/cpp-build" "$id:/var/lib/jenkins/cpp-build"
        fi
        (echo "source ./workspace/env" && echo 'sudo chown -R jenkins workspace /opt/conda/lib/python${PYTHON_VERSION}/site-packages/torch && cd workspace && git submodule update --init && .jenkins/pytorch/test.sh') | docker exec -u jenkins -i "$id" bash


version: 2
jobs:
  pytorch_linux_trusty_py2_7_9_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py2.7.9:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_py2_7_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py2.7:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_py3_5_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py3.5:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_py3_6_gcc4_8_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py3.6-gcc4.8:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_py3_6_gcc5_4_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py3.6-gcc5.4:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_py3_6_gcc7_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-py3.6-gcc7:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_trusty_pynightly_build_test:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-trusty-pynightly:226
        <<: *docker_config_defaults
    <<: *pytorch_linux_cpu_build_test_defaults

  pytorch_linux_xenial_py3_clang5_asan_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-asan:226
        <<: *docker_config_defaults
    environment:
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cpu_build_defaults

  pytorch_linux_xenial_py3_clang5_asan_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-asan:226"
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cpu_test_defaults

  pytorch_linux_xenial_cuda8_cudnn6_py3_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda8-cudnn6-py3:226
        <<: *docker_config_defaults
    environment:
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_build_defaults

  pytorch_linux_xenial_cuda8_cudnn6_py3_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda8-cudnn6-py3:226"
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_test_defaults

  pytorch_linux_xenial_cuda9_cudnn7_py2_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py2:226
        <<: *docker_config_defaults
    environment:
      PYTHON_VERSION: "2.7"
    <<: *pytorch_linux_cuda_build_defaults

  pytorch_linux_xenial_cuda9_cudnn7_py2_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py2:226"
      PYTHON_VERSION: "2.7"
    <<: *pytorch_linux_cuda_test_defaults

  pytorch_linux_xenial_cuda9_cudnn7_py3_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py3:226
        <<: *docker_config_defaults
    environment:
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_build_defaults

  pytorch_linux_xenial_cuda9_cudnn7_py3_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9-cudnn7-py3:226"
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_test_defaults

  pytorch_linux_xenial_cuda9_2_cudnn7_py3_gcc7_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9.2-cudnn7-py3-gcc7:226
        <<: *docker_config_defaults
    environment:
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_build_defaults

  pytorch_linux_xenial_cuda9_2_cudnn7_py3_gcc7_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-cuda9.2-cudnn7-py3-gcc7:226"
      PYTHON_VERSION: "3.6"
    <<: *pytorch_linux_cuda_test_defaults

  pytorch_macos_10_13_py3_build:
    macos:
      xcode: "9.0"
    steps:
      - checkout
      - run:
          name: Build
          environment:
            BUILD_ENVIRONMENT: pytorch-macos-10.13-py3
          command: |
            set -ex
            export SCCACHE_BUCKET=ossci-compiler-cache
            # This IAM user allows write access to S3 bucket for sccache
            export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
            git submodule update --init
            chmod a+x .jenkins/pytorch/macos-build.sh
            .jenkins/pytorch/macos-build.sh
      - persist_to_workspace:
          root: /Users/distiller/pytorch-ci-env
          paths:
            - "*"

  pytorch_macos_10_13_py3_test:
    macos:
      xcode: "9.0"
    steps:
      - checkout
      - run:
          name: Prepare workspace
          command: |
            sudo mkdir -p /Users/distiller/pytorch-ci-env
            sudo chmod -R 777 /Users/distiller/pytorch-ci-env
      - attach_workspace:
          at: /Users/distiller/pytorch-ci-env
      - run:
          name: Build
          environment:
            BUILD_ENVIRONMENT: pytorch-macos-10.13-py3
          command: |
            set -ex
            git submodule update --init
            chmod a+x .jenkins/pytorch/macos-test.sh
            .jenkins/pytorch/macos-test.sh

  pytorch_macos_10_13_cuda9_2_cudnn7_py3_build:
    macos:
      xcode: "9.0"
    steps:
      - checkout
      - run:
          name: Build
          environment:
            JOB_BASE_NAME: pytorch-macos-10.13-cuda9.2-cudnn7-py3-build
            BUILD_ENVIRONMENT: pytorch-macos-10.13-cuda9.2-cudnn7-py3
          command: |
            set -ex

            # Install CUDA 9.2
            sudo rm -rf ~/cuda_9.2.64_mac_installer.app || true
            curl https://s3.amazonaws.com/ossci-macos/cuda_9.2.64_mac_installer.zip -o ~/cuda_9.2.64_mac_installer.zip
            unzip ~/cuda_9.2.64_mac_installer.zip -d ~/
            sudo ~/cuda_9.2.64_mac_installer.app/Contents/MacOS/CUDAMacOSXInstaller --accept-eula --no-window
            sudo cp /usr/local/cuda/lib/libcuda.dylib /Developer/NVIDIA/CUDA-9.2/lib/libcuda.dylib
            sudo rm -rf /usr/local/cuda || true

            # Install cuDNN 7.1 for CUDA 9.2
            curl https://s3.amazonaws.com/ossci-macos/cudnn-9.2-osx-x64-v7.1.tgz -o ~/cudnn-9.2-osx-x64-v7.1.tgz
            rm -rf ~/cudnn-9.2-osx-x64-v7.1 && mkdir ~/cudnn-9.2-osx-x64-v7.1
            tar -xzvf ~/cudnn-9.2-osx-x64-v7.1.tgz -C ~/cudnn-9.2-osx-x64-v7.1
            sudo cp ~/cudnn-9.2-osx-x64-v7.1/cuda/include/cudnn.h /Developer/NVIDIA/CUDA-9.2/include/
            sudo cp ~/cudnn-9.2-osx-x64-v7.1/cuda/lib/libcudnn* /Developer/NVIDIA/CUDA-9.2/lib/
            sudo chmod a+r /Developer/NVIDIA/CUDA-9.2/include/cudnn.h /Developer/NVIDIA/CUDA-9.2/lib/libcudnn*

            export SCCACHE_BUCKET=ossci-compiler-cache
            # This IAM user allows write access to S3 bucket for sccache
            export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
            git submodule update --init
            chmod a+x .jenkins/pytorch/macos-build.sh
            .jenkins/pytorch/macos-build.sh

  caffe2_py2_cuda8_0_cudnn6_ubuntu16_04_build:
    docker:
      - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/caffe2/py2-cuda8.0-cudnn6-ubuntu16.04:189
        <<: *docker_config_defaults
    resource_class: large
    working_directory: /var/lib/jenkins/workspace
    steps:
    - checkout
    - run:
        name: Build
        command: |
          export SCCACHE_BUCKET=ossci-compiler-cache
          # This IAM user allows write access to S3 bucket for sccache
          export AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA
          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}
          export SCCACHE_MAX_JOBS=`expr $(nproc) - 1`
          export MEMORY_LIMIT_MAX_JOBS=8  # the "large" resource class on CircleCI has 32 CPU cores, if we use all of them we'll OOM
          export MAX_JOBS=$(( ${SCCACHE_MAX_JOBS} > ${MEMORY_LIMIT_MAX_JOBS} ? ${MEMORY_LIMIT_MAX_JOBS} : ${SCCACHE_MAX_JOBS} ))

          set -ex

          # Need to checkout fetch PRs for onnxbot tracking PRs
          git submodule update --init third_party/onnx || true
          cd third_party/onnx && git fetch --tags --progress origin +refs/pull/*:refs/remotes/origin/pr/* && cd -

          # Reinitialize submodules
          git submodule update --init --recursive

          # Ensure jenkins can write to the ccache root dir.
          sudo chown jenkins:jenkins "${HOME}/.ccache"

          # Make ccache log to the workspace, so we can archive it after the build
          mkdir -p build
          ccache -o log_file=$PWD/build/ccache.log

          # Configure additional cmake arguments
          cmake_args=()
          cmake_args+=("$CMAKE_ARGS")

          if [[ $BUILD_ENVIRONMENT == *aten* ]]; then
            cmake_args+=("-DBUILD_ATEN=ON")
          fi

          # conda must be added to the path for Anaconda builds (this location must be
          # the same as that in install_anaconda.sh used to build the docker image)
          if [[ "${BUILD_ENVIRONMENT}" == conda* ]]; then
            export PATH=/opt/conda/bin:$PATH
            sudo chown -R jenkins:jenkins '/opt/conda'
          fi

          # Build
          if test -x ".jenkins/caffe2/build.sh"; then
            ./.jenkins/caffe2/build.sh ${cmake_args[@]}
          else
            ./.jenkins/build.sh ${cmake_args[@]}
          fi

          # Show sccache stats if it is running
          if pgrep sccache > /dev/null; then
            sccache --show-stats
          fi
    - persist_to_workspace:
        root: /usr/local/caffe2
        paths:
          - "*"

  caffe2_py2_cuda8_0_cudnn6_ubuntu16_04_test:
    environment:
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/caffe2/py2-cuda8.0-cudnn6-ubuntu16.04:189"
    machine:
      image: default
    resource_class: gpu.small
    steps:
    - checkout
    - run:
        name: Prepare workspace
        command: |
          sudo mkdir -p /opt/workspace
          sudo chmod -R 777 /opt/workspace
    - attach_workspace:
        at: /opt/workspace
    - run:
        name: Build
        command: |
          set -x
          sudo pip install awscli
          curl -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
          echo "deb https://nvidia.github.io/libnvidia-container/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
          echo "deb https://nvidia.github.io/nvidia-container-runtime/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
          echo "deb https://nvidia.github.io/nvidia-docker/ubuntu14.04/amd64 /" | sudo tee -a /etc/apt/sources.list.d/nvidia-docker.list
          sudo apt-get update
          sudo apt-get remove linux-image-generic linux-headers-generic linux-generic
          sudo apt-get install linux-headers-$(uname -r)
          sudo apt-get install linux-image-generic
          wget 'https://s3.amazonaws.com/ossci-linux/nvidia_driver/NVIDIA-Linux-x86_64-396.26.run'
          sudo /bin/bash ./NVIDIA-Linux-x86_64-396.26.run -s --no-drm
          sudo apt-get install -y nvidia-docker2
          sudo pkill -SIGHUP dockerd
          nvidia-smi
          # This IAM user only allows read-only access to ECR
          export AWS_ACCESS_KEY_ID=AKIAJ2J6FIG5OSZTQ3IA
          export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_ONLY}
          eval $(aws ecr get-login --region us-east-1 --no-include-email)
          docker pull ${DOCKER_IMAGE}
          id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})
          pwd
          echo "declare -x SCCACHE_BUCKET=${SCCACHE_BUCKET}" > /home/circleci/project/env
          # This IAM user allows write access to S3 bucket for sccache
          echo "declare -x AWS_ACCESS_KEY_ID=AKIAJJZUW4G2ASX5W7KA" >> /home/circleci/project/env
          echo "declare -x AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET}" >> /home/circleci/project/env

          # TODO: merge this into Caffe2 build.sh
          cat >/home/circleci/project/ci_build_script.sh <<EOL

          set -ex

          # libdc1394 (dependency of OpenCV) expects /dev/raw1394 to exist...
          sudo ln /dev/null /dev/raw1394

          # Hotfix, use hypothesis 3.44.6 on Ubuntu 14.04
          # See comments on https://github.com/HypothesisWorks/hypothesis-python/commit/eadd62e467d6cee6216e71b391951ec25b4f5830
          if [[ "$BUILD_ENVIRONMENT" == *ubuntu14.04* ]]; then
            sudo pip uninstall -y hypothesis
            sudo pip install hypothesis==3.44.6
          fi

          # conda must be added to the path for Anaconda builds (this location must be
          # the same as that in install_anaconda.sh used to build the docker image)
          if [[ "${BUILD_ENVIRONMENT}" == conda* ]]; then
            export PATH=/opt/conda/bin:$PATH
          fi

          # Build
          if test -x ".jenkins/caffe2/test.sh"; then
            ./.jenkins/caffe2/test.sh
          else
            ./.jenkins/test.sh
          fi

          # Remove benign core dumps.
          # These are tests for signal handling (including SIGABRT).
          rm -f ./crash/core.fatal_signal_as.*
          rm -f ./crash/core.logging_test.*

          EOL
          chmod +x /home/circleci/project/ci_build_script.sh
          docker cp /home/circleci/project/. "$id:/var/lib/jenkins/workspace"
          echo "mkdir -p /usr/local/caffe2" | docker exec -u jenkins -i "$id" bash
          docker cp /opt/workspace/. "$id:/usr/local/caffe2"
          (echo "source ./workspace/env" && echo 'sudo chown -R jenkins workspace && cd workspace && ./ci_build_script.sh') | docker exec -u jenkins -i "$id" bash

workflows:
  version: 2
  build:
    jobs:
      - pytorch_linux_trusty_py2_7_9_build_test
      - pytorch_linux_trusty_py2_7_build_test
      - pytorch_linux_trusty_py3_5_build_test
      - pytorch_linux_trusty_py3_6_gcc4_8_build_test
      - pytorch_linux_trusty_py3_6_gcc5_4_build_test
      - pytorch_linux_trusty_py3_6_gcc7_build_test
      - pytorch_linux_trusty_pynightly_build_test
      - pytorch_linux_xenial_py3_clang5_asan_build
      - pytorch_linux_xenial_py3_clang5_asan_test:
          requires:
            - pytorch_linux_xenial_py3_clang5_asan_build
      - pytorch_linux_xenial_cuda8_cudnn6_py3_build
      - pytorch_linux_xenial_cuda8_cudnn6_py3_test:
          requires:
            - pytorch_linux_xenial_cuda8_cudnn6_py3_build
      - pytorch_linux_xenial_cuda9_cudnn7_py2_build
      - pytorch_linux_xenial_cuda9_cudnn7_py2_test:
          requires:
            - pytorch_linux_xenial_cuda9_cudnn7_py2_build
      - pytorch_linux_xenial_cuda9_cudnn7_py3_build
      - pytorch_linux_xenial_cuda9_cudnn7_py3_test:
          requires:
            - pytorch_linux_xenial_cuda9_cudnn7_py3_build
      - pytorch_linux_xenial_cuda9_2_cudnn7_py3_gcc7_build
      - pytorch_linux_xenial_cuda9_2_cudnn7_py3_gcc7_test:
          requires:
            - pytorch_linux_xenial_cuda9_2_cudnn7_py3_gcc7_build
      - pytorch_macos_10_13_py3_build
      - pytorch_macos_10_13_py3_test:
          requires:
            - pytorch_macos_10_13_py3_build
      - pytorch_macos_10_13_cuda9_2_cudnn7_py3_build
      - caffe2_py2_cuda8_0_cudnn6_ubuntu16_04_build
      - caffe2_py2_cuda8_0_cudnn6_ubuntu16_04_test:
          requires:
            - caffe2_py2_cuda8_0_cudnn6_ubuntu16_04_build
