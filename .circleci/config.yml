# WARNING: DO NOT EDIT THIS FILE DIRECTLY!!!
# See the README.md in this directory.

# IMPORTANT: To update Docker image version, please follow
# the instructions at
# https://github.com/pytorch/pytorch/wiki/Docker-image-build-on-CircleCI

version: 2.1

parameters:
  run_binary_tests:
    type: boolean
    default: false
  run_build:
    type: boolean
    default: true

docker_config_defaults: &docker_config_defaults
  user: jenkins
  aws_auth:
    # This IAM user only allows read-write access to ECR
    aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_ECR_READ_WRITE_V4}
    aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_ECR_READ_WRITE_V4}

executors:
  windows-with-nvidia-gpu:
    machine:
      resource_class: windows.gpu.nvidia.medium
      image: windows-server-2019-nvidia:stable
      shell: bash.exe

  windows-xlarge-cpu-with-nvidia-cuda:
    machine:
      resource_class: windows.xlarge
      image: windows-server-2019-vs2019:stable
      shell: bash.exe

  windows-medium-cpu-with-nvidia-cuda:
    machine:
      resource_class: windows.medium
      image: windows-server-2019-vs2019:stable
      shell: bash.exe
commands:

  calculate_docker_image_tag:
    description: "Calculates the docker image tag"
    steps:
      - run:
          name: "Calculate docker image hash"
          command: |
            DOCKER_TAG=$(git rev-parse HEAD:.circleci/docker)
            echo "DOCKER_TAG=${DOCKER_TAG}" >> "${BASH_ENV}"

  designate_upload_channel:
    description: "inserts the correct upload channel into ${BASH_ENV}"
    steps:
      - run:
          name: adding UPLOAD_CHANNEL to BASH_ENV
          command: |
            our_upload_channel=nightly
            # On tags upload to test instead
            if [[ -n "${CIRCLE_TAG}" ]]; then
              our_upload_channel=test
            fi
            echo "export UPLOAD_CHANNEL=${our_upload_channel}" >> ${BASH_ENV}

  # This system setup script is meant to run before the CI-related scripts, e.g.,
  # installing Git client, checking out code, setting up CI env, and
  # building/testing.
  setup_linux_system_environment:
    steps:
      - run:
          name: Set Up System Environment
          no_output_timeout: "1h"
          command: .circleci/scripts/setup_linux_system_environment.sh

  setup_ci_environment:
    steps:
      - run:
          name: Set Up CI Environment After attach_workspace
          no_output_timeout: "1h"
          command: .circleci/scripts/setup_ci_environment.sh

  brew_update:
    description: "Update Homebrew and install base formulae"
    steps:
      - run:
          name: Update Homebrew
          no_output_timeout: "10m"
          command: |
            set -ex

            # Update repositories manually.
            # Running `brew update` produces a comparison between the
            # current checkout and the updated checkout, which takes a
            # very long time because the existing checkout is 2y old.
            for path in $(find /usr/local/Homebrew -type d -name .git)
            do
            cd $path/..
            git fetch --depth=1 origin
            git reset --hard origin/master
            done

            export HOMEBREW_NO_AUTO_UPDATE=1

            # Install expect and moreutils so that we can call `unbuffer` and `ts`.
            # moreutils installs a `parallel` executable by default, which conflicts
            # with the executable from the GNU `parallel`, so we must unlink GNU
            # `parallel` first, and relink it afterwards.
            brew unlink parallel
            brew install moreutils
            brew link parallel --overwrite
            brew install expect

  brew_install:
    description: "Install Homebrew formulae"
    parameters:
      formulae:
        type: string
        default: ""
    steps:
      - run:
          name: Install << parameters.formulae >>
          no_output_timeout: "10m"
          command: |
            set -ex
            export HOMEBREW_NO_AUTO_UPDATE=1
            brew install << parameters.formulae >>

  run_brew_for_macos_build:
    steps:
      - brew_update
      - brew_install:
          formulae: libomp

  run_brew_for_ios_build:
    steps:
      - brew_update
      - brew_install:
          formulae: libtool

  optional_merge_target_branch:
    steps:
      - run:
          name: (Optional) Merge target branch
          no_output_timeout: "10m"
          command: |
            if [[ -n "$CIRCLE_PULL_REQUEST" && "$CIRCLE_BRANCH" != "nightly" ]]; then
              PR_NUM=$(basename $CIRCLE_PULL_REQUEST)
              CIRCLE_PR_BASE_BRANCH=$(curl -s https://api.github.com/repos/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME/pulls/$PR_NUM | jq -r '.base.ref')
              if [[ "${BUILD_ENVIRONMENT}" == *"xla"* || "${BUILD_ENVIRONMENT}" == *"gcc5"* ]] ; then
                set -x
                git config --global user.email "circleci.ossci@gmail.com"
                git config --global user.name "CircleCI"
                git config remote.origin.url https://github.com/pytorch/pytorch.git
                git config --add remote.origin.fetch +refs/heads/master:refs/remotes/origin/master
                git fetch --tags --progress https://github.com/pytorch/pytorch.git +refs/heads/master:refs/remotes/origin/master --depth=100 --quiet
                # PRs generated from ghstack has format CIRCLE_PR_BASE_BRANCH=gh/xxx/1234/base
                if [[ "${CIRCLE_PR_BASE_BRANCH}" == "gh/"* ]]; then
                  CIRCLE_PR_BASE_BRANCH=master
                fi
                export GIT_MERGE_TARGET=`git log -n 1 --pretty=format:"%H" origin/$CIRCLE_PR_BASE_BRANCH`
                echo "GIT_MERGE_TARGET: " ${GIT_MERGE_TARGET}
                export GIT_COMMIT=${CIRCLE_SHA1}
                echo "GIT_COMMIT: " ${GIT_COMMIT}
                git checkout -f ${GIT_COMMIT}
                git reset --hard ${GIT_COMMIT}
                git merge --allow-unrelated-histories --no-edit --no-ff ${GIT_MERGE_TARGET}
                echo "Merged $CIRCLE_PR_BASE_BRANCH branch before building in environment $BUILD_ENVIRONMENT"
                set +x
              else
                echo "No need to merge with $CIRCLE_PR_BASE_BRANCH, skipping..."
              fi
            else
              echo "This is not a pull request, skipping..."
            fi

  upload_binary_size_for_android_build:
    description: "Upload binary size data for Android build"
    parameters:
      build_type:
        type: string
        default: ""
      artifacts:
        type: string
        default: ""
    steps:
      - run:
          name: "Binary Size - Install Dependencies"
          no_output_timeout: "5m"
          command: |
            retry () {
              $* || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 && $*) || (sleep 8 && $*)
            }
            retry pip3 install requests
      - run:
          name: "Binary Size - Untar Artifacts"
          no_output_timeout: "5m"
          command: |
            # The artifact file is created inside docker container, which contains the result binaries.
            # Now unpackage it into the project folder. The subsequent script will scan project folder
            # to locate result binaries and report their sizes.
            # If artifact file is not provided it assumes that the project folder has been mounted in
            # the docker during build and already contains the result binaries, so this step can be skipped.
            export ARTIFACTS="<< parameters.artifacts >>"
            if [ -n "${ARTIFACTS}" ]; then
              tar xf "${ARTIFACTS}" -C ~/project
            fi
      - run:
          name: "Binary Size - Upload << parameters.build_type >>"
          no_output_timeout: "5m"
          command: |
            cd ~/project
            export ANDROID_BUILD_TYPE="<< parameters.build_type >>"
            export COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
            python3 .circleci/scripts/upload_binary_size_to_scuba.py android

##############################################################################
# Binary build (nightlies nightly build) defaults
# The binary builds use the docker executor b/c at time of writing the machine
# executor is limited to only two cores and is painfully slow (4.5+ hours per
# GPU build). But the docker executor cannot be run with --runtime=nvidia, and
# so the binary test/upload jobs must run on a machine executor. The package
# built in the build job is persisted to the workspace, which the test jobs
# expect. The test jobs just run a few quick smoke tests (very similar to the
# second-round-user-facing smoke tests above) and then upload the binaries to
# their final locations. The upload part requires credentials that should only
# be available to org-members.
#
# binary_checkout MUST be run before other commands here. This is because the
# other commands are written in .circleci/scripts/*.sh , so the pytorch source
# code must be downloaded on the machine before they can be run. We cannot
# inline all the code into this file, since that would cause the yaml size to
# explode past 4 MB (all the code in the command section is just copy-pasted to
# everywhere in the .circleci/config.yml file where it appears).
##############################################################################

# Checks out the Pytorch and Builder repos (always both of them), and places
# them in the right place depending on what executor we're running on. We curl
# our .sh file from the interweb to avoid yaml size bloat. Note that many jobs
# do not need both the pytorch and builder repos, so this is a little wasteful
# (smoke tests and upload jobs do not need the pytorch repo).
binary_checkout: &binary_checkout
  name: Checkout pytorch/builder repo
  command: .circleci/scripts/binary_checkout.sh

# Parses circleci arguments in a consistent way, essentially routing to the
# correct pythonXgccXcudaXos build we want
binary_populate_env: &binary_populate_env
  name: Set up binary env variables
  command: .circleci/scripts/binary_populate_env.sh

binary_install_miniconda: &binary_install_miniconda
  name: Install miniconda
  no_output_timeout: "1h"
  command: .circleci/scripts/binary_install_miniconda.sh

# This section is used in the binary_test and smoke_test jobs. It expects
# 'binary_populate_env' to have populated /home/circleci/project/env and it
# expects another section to populate /home/circleci/project/ci_test_script.sh
# with the code to run in the docker
binary_run_in_docker: &binary_run_in_docker
  name: Run in docker
  # This step only runs on circleci linux machine executors that themselves
  # need to start docker images
  command: .circleci/scripts/binary_run_in_docker.sh
##############################################################################
# Build parameters
##############################################################################
pytorch_params: &pytorch_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
    use_cuda_docker_runtime:
      type: string
      default: ""
    build_only:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    BUILD_ONLY: << parameters.build_only >>
  resource_class: << parameters.resource_class >>

pytorch_ios_params: &pytorch_ios_params
  parameters:
    build_environment:
      type: string
      default: ""
    ios_arch:
      type: string
      default: ""
    ios_platform:
      type: string
      default: ""
    op_list:
      type: string
      default: ""
    use_metal:
      type: string
      default: "0"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    IOS_ARCH: << parameters.ios_arch >>
    IOS_PLATFORM: << parameters.ios_platform >>
    SELECTED_OP_LIST: << parameters.op_list >>
    USE_PYTORCH_METAL: << parameters.use_metal >>

pytorch_windows_params: &pytorch_windows_params
  parameters:
    executor:
      type: string
      default: "windows-xlarge-cpu-with-nvidia-cuda"
    build_environment:
      type: string
      default: ""
    test_name:
      type: string
      default: ""
    cuda_version:
      type: string
      default: "10.1"
    python_version:
      type: string
      default: "3.6"
    vc_version:
      type: string
      default: "14.16"
    vc_year:
      type: string
      default: "2019"
    vc_product:
      type: string
      default: "BuildTools"
    use_cuda:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: <<parameters.build_environment>>
    SCCACHE_BUCKET: "ossci-compiler-cache"
    CUDA_VERSION: <<parameters.cuda_version>>
    PYTHON_VERSION: <<parameters.python_version>>
    VC_VERSION: <<parameters.vc_version>>
    VC_YEAR: <<parameters.vc_year>>
    VC_PRODUCT: <<parameters.vc_product>>
    USE_CUDA: <<parameters.use_cuda>>
    TORCH_CUDA_ARCH_LIST: "7.5"
    JOB_BASE_NAME: <<parameters.test_name>>
    JOB_EXECUTOR: <<parameters.executor>>
binary_linux_build_params: &binary_linux_build_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    libtorch_variant:
      type: string
      default: ""
    resource_class:
      type: string
      default: "2xlarge+"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
    ANACONDA_USER: pytorch
  resource_class: << parameters.resource_class >>
  docker:
    - image: << parameters.docker_image >>

binary_linux_test_upload_params: &binary_linux_test_upload_params
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    libtorch_variant:
      type: string
      default: ""
    resource_class:
      type: string
      default: "medium"
    use_cuda_docker_runtime:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    LIBTORCH_VARIANT: << parameters.libtorch_variant >>
  resource_class: << parameters.resource_class >>

binary_mac_params: &binary_mac_params
  parameters:
    build_environment:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>

binary_windows_params: &binary_windows_params
  parameters:
    build_environment:
      type: string
      default: ""
    executor:
      type: string
      default: "windows-xlarge-cpu-with-nvidia-cuda"
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    BUILD_FOR_SYSTEM: windows
    JOB_EXECUTOR: <<parameters.executor>>

promote_common: &promote_common
  docker:
    - image: pytorch/release
  parameters:
    package_name:
      description: "package name to promote"
      type: string
      default: ""
  environment:
    PACKAGE_NAME: << parameters.package_name >>
    ANACONDA_API_TOKEN: ${CONDA_PYTORCHBOT_TOKEN}
    AWS_ACCESS_KEY_ID: ${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}
    AWS_SECRET_ACCESS_KEY: ${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}
##############################################################################
# Job specs
##############################################################################
jobs:
  pytorch_linux_build:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:202007-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - optional_merge_target_branch
    - setup_ci_environment
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -e
          if [[ "${DOCKER_IMAGE}" == *rocm3.9* ]]; then
            export DOCKER_TAG="f3d89a32912f62815e4feaeed47e564e887dffd6"
          fi
          if [[ ${BUILD_ENVIRONMENT} == *"pure_torch"* ]]; then
            echo 'BUILD_CAFFE2=OFF' >> "${BASH_ENV}"
          fi
          if [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
            echo 'ATEN_THREADING=TBB' >> "${BASH_ENV}"
            echo 'USE_TBB=1' >> "${BASH_ENV}"
          elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
            echo 'ATEN_THREADING=NATIVE' >> "${BASH_ENV}"
          fi
          echo "Parallel backend flags: "${PARALLEL_FLAGS}
          # Pull Docker image and run build
          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}:${DOCKER_TAG}
          time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null
          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})

          git submodule sync && git submodule update -q --init --recursive

          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh && find ${BUILD_ROOT} -type f -name "*.a" -or -name "*.o" -delete") | docker exec -u jenkins -i "$id" bash) 2>&1'

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Copy dist folder back
          docker cp $id:/var/lib/jenkins/workspace/dist /home/circleci/project/. || echo "Dist folder not found"

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            # Note [Special build images]
            # The xla build uses the same docker image as
            # pytorch_linux_bionic_py3_6_clang9_build. In the push step, we have to
            # distinguish between them so the test can pick up the correct image.
            output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
            if [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-xla
            elif [[ ${BUILD_ENVIRONMENT} == *"libtorch"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-libtorch
            elif [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-paralleltbb
            elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-parallelnative
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_64"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v7a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v8a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_32"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-vulkan-x86_32"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-vulkan-x86_32
            elif [[ ${BUILD_ENVIRONMENT} == *"vulkan-linux"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-vulkan
            else
              export COMMIT_DOCKER_IMAGE=$output_image
            fi
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            time docker push ${COMMIT_DOCKER_IMAGE}
          fi
    - store_artifacts:
        path: /home/circleci/project/dist

  pytorch_linux_test:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:202007-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Download Docker image
        no_output_timeout: "90m"
        command: |
          set -e
          export PYTHONUNBUFFERED=1
          if [[ "${DOCKER_IMAGE}" == *rocm3.9* ]]; then
            export DOCKER_TAG="f3d89a32912f62815e4feaeed47e564e887dffd6"
          fi
          # See Note [Special build images]
          output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
          if [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-xla
          elif [[ ${BUILD_ENVIRONMENT} == *"libtorch"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-libtorch
          elif [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-paralleltbb
          elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-parallelnative
          elif [[ ${BUILD_ENVIRONMENT} == *"vulkan-linux"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-vulkan
          else
            export COMMIT_DOCKER_IMAGE=$output_image
          fi
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

          if [[ ${BUILD_ENVIRONMENT} == *"paralleltbb"* ]]; then
            echo 'ATEN_THREADING=TBB' >> "${BASH_ENV}"
            echo 'USE_TBB=1' >> "${BASH_ENV}"
          elif [[ ${BUILD_ENVIRONMENT} == *"parallelnative"* ]]; then
            echo 'ATEN_THREADING=NATIVE' >> "${BASH_ENV}"
          fi
          echo "Parallel backend flags: "${PARALLEL_FLAGS}

          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

          # TODO: Make this less painful
          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --gpus all --shm-size=2g -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          elif [[ ${BUILD_ENVIRONMENT} == *"rocm"* ]]; then
            hostname
            export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=8g --ipc=host --device /dev/kfd --device /dev/dri --group-add video -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --shm-size=1g --ipc=host -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi
          echo "id=${id}" >> "${BASH_ENV}"

    - run:
        name: Check for no AVX instruction by default
        no_output_timeout: "20m"
        command: |
          set -e
          is_vanilla_build() {
            if [ "${BUILD_ENVIRONMENT}" == "pytorch-linux-bionic-py3.6-clang9-test" ]; then
              return 0
            fi
            if [ "${BUILD_ENVIRONMENT}" == "pytorch-linux-xenial-py3.6-gcc5.4-test" ]; then
              return 0
            fi
            return 1
          }

          if is_vanilla_build; then
            echo "apt-get update && apt-get install -y qemu-user gdb" | docker exec -u root -i "$id" bash
            echo "cd workspace/build; qemu-x86_64 -g 2345 -cpu Broadwell -E ATEN_CPU_CAPABILITY=default ./bin/basic --gtest_filter=BasicTest.BasicTestCPU & gdb ./bin/basic -ex 'set pagination off' -ex 'target remote :2345' -ex 'continue' -ex 'bt' -ex='set confirm off' -ex 'quit \$_isvoid(\$_exitcode)'" | docker exec -u jenkins -i "$id" bash
          else
            echo "Skipping for ${BUILD_ENVIRONMENT}"
          fi
    - run:
        name: Run tests
        no_output_timeout: "90m"
        command: |
          set -e

          cat >docker_commands.sh \<<EOL
          # =================== The following code will be executed inside Docker container ===================
          set -ex
          export SCRIBE_GRAPHQL_ACCESS_TOKEN="${SCRIBE_GRAPHQL_ACCESS_TOKEN}"
          ${PARALLEL_FLAGS}
          cd workspace
          EOL
          if [[ ${BUILD_ENVIRONMENT} == *"multigpu"* ]]; then
            echo ".jenkins/pytorch/multigpu-test.sh" >> docker_commands.sh
          elif [[ ${BUILD_ENVIRONMENT} == *onnx* ]]; then
            echo "pip install click mock tabulate networkx==2.0" >> docker_commands.sh
            echo "pip -q install --user \"file:///var/lib/jenkins/workspace/third_party/onnx#egg=onnx\"" >> docker_commands.sh
            echo ".jenkins/caffe2/test.sh" >> docker_commands.sh
          else
            echo ".jenkins/pytorch/test.sh" >> docker_commands.sh
          fi
          echo "(cat docker_commands.sh | docker exec -u jenkins -i "$id" bash) 2>&1" > command.sh
          unbuffer bash command.sh | ts
    - run:
        name: Report results
        no_output_timeout: "5m"
        command: |
          set -e
          docker stats --all --no-stream

          cat >docker_commands.sh \<<EOL
          # =================== The following code will be executed inside Docker container ===================
          set -ex
          export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}
          export SCRIBE_GRAPHQL_ACCESS_TOKEN="${SCRIBE_GRAPHQL_ACCESS_TOKEN}"
          export CIRCLE_TAG="${CIRCLE_TAG:-}"
          export CIRCLE_SHA1="$CIRCLE_SHA1"
          export CIRCLE_PR_NUMBER="${CIRCLE_PR_NUMBER:-}"
          export CIRCLE_BRANCH="$CIRCLE_BRANCH"
          export CIRCLE_JOB="$CIRCLE_JOB"
          export CIRCLE_WORKFLOW_ID="$CIRCLE_WORKFLOW_ID"
          cd workspace
          python test/print_test_stats.py --upload-to-s3 test
          EOL
          echo "(cat docker_commands.sh | docker exec -u jenkins -i "$id" bash) 2>&1" > command.sh
          unbuffer bash command.sh | ts

          echo "Retrieving test reports"
          docker cp $id:/var/lib/jenkins/workspace/test/test-reports ./ || echo 'No test reports found!'
          if [[ ${BUILD_ENVIRONMENT} == *"coverage"* ]]; then
              echo "Retrieving C++ coverage report"
              docker cp $id:/var/lib/jenkins/workspace/build/coverage.info ./test
          fi
          if [[ ${BUILD_ENVIRONMENT} == *"coverage"* || ${BUILD_ENVIRONMENT} == *"onnx"* ]]; then
              echo "Retrieving Python coverage report"
              docker cp $id:/var/lib/jenkins/workspace/test/.coverage ./test
              docker cp $id:/var/lib/jenkins/workspace/test/coverage.xml ./test
              python3 -mpip install codecov
              python3 -mcodecov
          fi
        when: always
    - store_test_results:
        path: test-reports

  pytorch_windows_build:
    <<: *pytorch_windows_params
    parameters:
      executor:
        type: string
        default: "windows-xlarge-cpu-with-nvidia-cuda"
      build_environment:
        type: string
        default: ""
      test_name:
        type: string
        default: ""
      cuda_version:
        type: string
        default: "10.1"
      python_version:
        type: string
        default: "3.6"
      vc_version:
        type: string
        default: "14.16"
      vc_year:
        type: string
        default: "2019"
      vc_product:
        type: string
        default: "BuildTools"
      use_cuda:
        type: string
        default: ""
    executor: <<parameters.executor>>
    steps:
      - checkout
      - run:
          name: Install Cuda
          no_output_timeout: 30m
          command: |
            if [[ "${USE_CUDA}" == "1" ]]; then
              .circleci/scripts/windows_cuda_install.sh
            fi
      - run:
          name: Install Cudnn
          command : |
            if [[ "${USE_CUDA}" == "1" ]]; then
              .circleci/scripts/windows_cudnn_install.sh
            fi
      - run:
          name: Build
          no_output_timeout: "90m"
          command: |
            set -e
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}
            set -x
            .jenkins/pytorch/win-build.sh
      - persist_to_workspace:
          root: "C:/w"
          paths: build-results
      - store_artifacts:
          path: C:/w/build-results

  pytorch_windows_test:
    <<: *pytorch_windows_params
    parameters:
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
      build_environment:
        type: string
        default: ""
      test_name:
        type: string
        default: ""
      cuda_version:
        type: string
        default: "10.1"
      python_version:
        type: string
        default: "3.6"
      vc_version:
        type: string
        default: "14.16"
      vc_year:
        type: string
        default: "2019"
      vc_product:
        type: string
        default: "BuildTools"
      use_cuda:
        type: string
        default: ""
    executor: <<parameters.executor>>
    steps:
      - checkout
      - attach_workspace:
          at: c:/users/circleci/workspace
      - run:
          name: Install Cuda
          no_output_timeout: 30m
          command: |
            if [[ "${CUDA_VERSION}" != "cpu" ]]; then
              if [[ "${CUDA_VERSION}" != "10" || "${JOB_EXECUTOR}" != "windows-with-nvidia-gpu" ]]; then
                .circleci/scripts/windows_cuda_install.sh
              fi
            fi
      - run:
          name: Install Cudnn
          command : |
            if [[ "${CUDA_VERSION}" != "cpu" ]]; then
              .circleci/scripts/windows_cudnn_install.sh
            fi
      - run:
          name: Test
          no_output_timeout: "30m"
          command: |
            set -e
            export IN_CI=1
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_WIN_BUILD_V1}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_WIN_BUILD_V1}
            set -x
            .jenkins/pytorch/win-test.sh
      - store_test_results:
          path: test/test-reports
  binary_linux_build:
    <<: *binary_linux_build_params
    steps:
    - checkout
    - calculate_docker_image_tag
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
            source "/pytorch/.circleci/scripts/binary_linux_build.sh"
            # Preserve build log
            if [ -f /pytorch/build/.ninja_log ]; then
              cp /pytorch/build/.ninja_log /final_pkgs
            fi
    - run:
        name: Output binary sizes
        no_output_timeout: "1m"
        command: |
            ls -lah /final_pkgs
    - run:
        name: save binary size
        no_output_timeout: "5m"
        command: |
            source /env
            cd /pytorch && export COMMIT_TIME=$(git log --max-count=1 --format=%ct || echo 0)
            python3 -mpip install requests && \
            SCRIBE_GRAPHQL_ACCESS_TOKEN=${SCRIBE_GRAPHQL_ACCESS_TOKEN} \
            python3 /pytorch/.circleci/scripts/upload_binary_size_to_scuba.py || exit 0
    - persist_to_workspace:
        root: /
        paths: final_pkgs

    - store_artifacts:
        path: /final_pkgs

    # This should really just be another step of the binary_linux_build job above.
    # This isn't possible right now b/c the build job uses the docker executor
    # (otherwise they'd be really really slow) but this one uses the macine
    # executor (b/c we have to run the docker with --runtime=nvidia and we can't do
    # that on the docker executor)
  binary_linux_test:
    <<: *binary_linux_test_upload_params
    machine:
        image: ubuntu-1604:202007-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - attach_workspace:
        at: /home/circleci/project
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Prepare test code
        no_output_timeout: "1h"
        command: .circleci/scripts/binary_linux_test.sh
    - run:
        <<: *binary_run_in_docker

  binary_upload:
    parameters:
      package_type:
        type: string
        description: "What type of package we are uploading (eg. wheel, libtorch, conda)"
        default: "wheel"
      upload_subfolder:
        type: string
        description: "What subfolder to put our package into (eg. cpu, cudaX.Y, etc.)"
        default: "cpu"
    docker:
      - image: continuumio/miniconda3
    environment:
      - DRY_RUN: disabled
      - PACKAGE_TYPE: "<< parameters.package_type >>"
      - UPLOAD_SUBFOLDER: "<< parameters.upload_subfolder >>"
    steps:
      - attach_workspace:
          at: /tmp/workspace
      - checkout
      - designate_upload_channel
      - run:
          name: Install dependencies
          no_output_timeout: "1h"
          command: |
            conda install -yq anaconda-client
            pip install -q awscli
      - run:
          name: Do upload
          no_output_timeout: "1h"
          command: |
            AWS_ACCESS_KEY_ID="${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}" \
              AWS_SECRET_ACCESS_KEY="${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}" \
              ANACONDA_API_TOKEN="${CONDA_PYTORCHBOT_TOKEN}" \
              .circleci/scripts/binary_upload.sh

  # Nighlty build smoke tests defaults
  # These are the second-round smoke tests. These make sure that the binaries are
  # correct from a user perspective, testing that they exist from the cloud are
  # are runnable. Note that the pytorch repo is never cloned into these jobs
  ##############################################################################
  smoke_linux_test:
    <<: *binary_linux_test_upload_params
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -ex
          cat >/home/circleci/project/ci_test_script.sh \<<EOL
          # The following code will be executed inside Docker container
          set -eux -o pipefail
          /builder/smoke_test.sh
          # The above code will be executed inside Docker container
          EOL
    - run:
        <<: *binary_run_in_docker

  smoke_mac_test:
    <<: *binary_linux_test_upload_params
    macos:
      xcode: "12.0"
    steps:
      - checkout
      - run:
          <<: *binary_checkout
      - run:
          <<: *binary_populate_env
      - brew_update
      - run:
          <<: *binary_install_miniconda
      - run:
          name: Build
          no_output_timeout: "1h"
          command: |
            set -ex
            source "/Users/distiller/project/env"
            export "PATH=$workdir/miniconda/bin:$PATH"
            # TODO unbuffer and ts this, but it breaks cause miniconda overwrites
            # tclsh. But unbuffer and ts aren't that important so they're just
            # disabled for now
            ./builder/smoke_test.sh

  binary_mac_build:
    <<: *binary_mac_params
    macos:
      xcode: "12.0"
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - brew_update
    - run:
        <<: *binary_install_miniconda

    - run:
        name: Build
        no_output_timeout: "90m"
        command: |
          # Do not set -u here; there is some problem with CircleCI
          # variable expansion with PROMPT_COMMAND
          set -ex -o pipefail
          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_build.sh"
          cat "$script"
          source "$script"

    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          # Do not set -u here; there is some problem with CircleCI
          # variable expansion with PROMPT_COMMAND
          set -ex -o pipefail
          script="/Users/distiller/project/pytorch/.circleci/scripts/binary_macos_test.sh"
          cat "$script"
          source "$script"

    - persist_to_workspace:
        root: /Users/distiller/project
        paths: final_pkgs

  binary_ios_build:
    <<: *pytorch_ios_params
    macos:
      xcode: "12.0"
    steps:
    - attach_workspace:
        at: ~/workspace
    - checkout
    - run_brew_for_ios_build
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          script="/Users/distiller/project/.circleci/scripts/binary_ios_build.sh"
          cat "$script"
          source "$script"
    - run:
        name: Test
        no_output_timeout: "30m"
        command: |
          script="/Users/distiller/project/.circleci/scripts/binary_ios_test.sh"
          cat "$script"
          source "$script"
    - persist_to_workspace:
        root: /Users/distiller/workspace/
        paths: ios

  binary_ios_upload:
    <<: *pytorch_ios_params
    macos:
      xcode: "12.0"
    steps:
    - attach_workspace:
        at: ~/workspace
    - checkout
    - run_brew_for_ios_build
    - run:
        name: Upload
        no_output_timeout: "1h"
        command: |
          script="/Users/distiller/project/.circleci/scripts/binary_ios_upload.sh"
          cat "$script"
          source "$script"

  binary_windows_build:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-xlarge-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - checkout
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/c/w/p/.circleci/scripts/binary_windows_build.sh"
          cat "$script"
          source "$script"
    - persist_to_workspace:
        root: "C:/w"
        paths: final_pkgs

  binary_windows_test:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    - checkout
    - attach_workspace:
        at: c:/users/circleci/project
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"
          cat "$script"
          source "$script"

  smoke_windows_test:
    <<: *binary_windows_params
    parameters:
      build_environment:
        type: string
        default: ""
      executor:
        type: string
        default: "windows-medium-cpu-with-nvidia-cuda"
    executor: <<parameters.executor>>
    steps:
    - checkout
    - run:
        <<: *binary_checkout
    - run:
        <<: *binary_populate_env
    - run:
        name: Test
        no_output_timeout: "1h"
        command: |
          set -eux -o pipefail
          export TEST_NIGHTLY_PACKAGE=1
          script="/c/w/p/.circleci/scripts/binary_windows_test.sh"
          cat "$script"
          source "$script"

  anaconda_prune:
    parameters:
      packages:
        type: string
        description: "What packages are we pruning? (quoted, space-separated string. eg. 'pytorch', 'torchvision torchaudio', etc.)"
        default: "pytorch"
      channel:
        type: string
        description: "What channel are we pruning? (eq. pytorch-nightly)"
        default: "pytorch-nightly"
    docker:
      - image: continuumio/miniconda3
    environment:
      - PACKAGES: "<< parameters.packages >>"
      - CHANNEL: "<< parameters.channel >>"
    steps:
      - checkout
      - run:
          name: Install dependencies
          no_output_timeout: "1h"
          command: |
            conda install -yq anaconda-client
      - run:
          name: Prune packages
          no_output_timeout: "1h"
          command: |
              ANACONDA_API_TOKEN="${CONDA_PYTORCHBOT_TOKEN}" \
              scripts/release/anaconda-prune/run.sh

  pytorch_doc_push:
    resource_class: medium
    machine:
      image: ubuntu-1604:202007-01
    parameters:
      branch:
        type: string
        default: "master"
    steps:
    - attach_workspace:
        at: /tmp/workspace
    - run:
        name: Generate netrc
        command: |
          # set credentials for https pushing
          cat > ~/.netrc \<<DONE
            machine github.com
            login pytorchbot
            password ${GITHUB_PYTORCHBOT_TOKEN}
          DONE
    - run:
        name: Docs push
        command: |
          pushd /tmp/workspace
          git push -u origin "<< parameters.branch >>"

  pytorch_python_doc_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-python-doc-push
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Doc Build and Push
        no_output_timeout: "1h"
        command: |
          set -ex
          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          tag=${CIRCLE_TAG:1:5}
          target=${tag:-master}
          echo "building for ${target}"
          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && '"export CIRCLE_SHA1='$CIRCLE_SHA1'"' && . ./.circleci/scripts/python_doc_push_script.sh docs/'$target' '$target' site") | docker exec -u jenkins -i "$id" bash) 2>&1'

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_artifacts
          docker cp $id:/var/lib/jenkins/workspace/pytorch.github.io/docs/master ~/workspace/build_artifacts
          docker cp $id:/var/lib/jenkins/workspace/pytorch.github.io /tmp/workspace

          # Save the docs build so we can debug any problems
          export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug
          docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}
          time docker push ${DEBUG_COMMIT_DOCKER_IMAGE}
    - persist_to_workspace:
        root: /tmp/workspace
        paths:
          - .
    - store_artifacts:
        path: ~/workspace/build_artifacts/master
        destination: docs

  pytorch_cpp_doc_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-cpp-doc-push
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Doc Build and Push
        no_output_timeout: "1h"
        command: |
          set -ex
          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          tag=${CIRCLE_TAG:1:5}
          target=${tag:-master}
          echo "building for ${target}"
          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})

          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && '"export CIRCLE_SHA1='$CIRCLE_SHA1'"' && . ./.circleci/scripts/cpp_doc_push_script.sh docs/"$target" master") | docker exec -u jenkins -i "$id" bash) 2>&1'

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_artifacts
          docker cp $id:/var/lib/jenkins/workspace/cppdocs/ /tmp/workspace

          # Save the docs build so we can debug any problems
          export DEBUG_COMMIT_DOCKER_IMAGE=${COMMIT_DOCKER_IMAGE}-debug
          docker commit "$id" ${DEBUG_COMMIT_DOCKER_IMAGE}
          time docker push ${DEBUG_COMMIT_DOCKER_IMAGE}

    - persist_to_workspace:
        root: /tmp/workspace
        paths:
          - .

  pytorch_macos_10_13_py3_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-build
    macos:
      xcode: "12.0"
    steps:
      - checkout
      - run_brew_for_macos_build
      - run:
          name: Build
          no_output_timeout: "1h"
          command: |
            set -e
            export IN_CI=1

            # Install sccache
            sudo curl --retry 3 https://s3.amazonaws.com/ossci-macos/sccache --output /usr/local/bin/sccache
            sudo chmod +x /usr/local/bin/sccache
            export SCCACHE_BUCKET=ossci-compiler-cache-circleci-v2

            # This IAM user allows write access to S3 bucket for sccache
            set +x
            export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_SCCACHE_S3_BUCKET_V4}
            export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_SCCACHE_S3_BUCKET_V4}
            set -x

            chmod a+x .jenkins/pytorch/macos-build.sh
            unbuffer .jenkins/pytorch/macos-build.sh 2>&1 | ts

      - persist_to_workspace:
          root: /Users/distiller/workspace/
          paths:
            - miniconda3

  pytorch_macos_10_13_py3_test:
    environment:
      BUILD_ENVIRONMENT: pytorch-macos-10.13-py3-test
    macos:
      xcode: "12.0"
    steps:
      - checkout
      - attach_workspace:
          at: ~/workspace
      - run_brew_for_macos_build
      - run:
          name: Test
          no_output_timeout: "1h"
          command: |
            set -e
            export IN_CI=1

            chmod a+x .jenkins/pytorch/macos-test.sh
            unbuffer .jenkins/pytorch/macos-test.sh 2>&1 | ts
      - store_test_results:
          path: test/test-reports

  pytorch_android_gradle_build:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c"
      PYTHON_VERSION: "3.6"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: pytorch android gradle build
        no_output_timeout: "1h"
        command: |
          set -eux
          docker_image_commit=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}

          docker_image_libtorch_android_x86_32=${docker_image_commit}-android-x86_32
          docker_image_libtorch_android_x86_64=${docker_image_commit}-android-x86_64
          docker_image_libtorch_android_arm_v7a=${docker_image_commit}-android-arm-v7a
          docker_image_libtorch_android_arm_v8a=${docker_image_commit}-android-arm-v8a

          echo "docker_image_commit: "${docker_image_commit}
          echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}
          echo "docker_image_libtorch_android_x86_64: "${docker_image_libtorch_android_x86_64}
          echo "docker_image_libtorch_android_arm_v7a: "${docker_image_libtorch_android_arm_v7a}
          echo "docker_image_libtorch_android_arm_v8a: "${docker_image_libtorch_android_arm_v8a}

          # x86_32
          time docker pull ${docker_image_libtorch_android_x86_32} >/dev/null
          export id_x86_32=$(docker run --env-file "${BASH_ENV}" -e GRADLE_OFFLINE=1 --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32})

          export COMMAND='((echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # arm-v7a
          time docker pull ${docker_image_libtorch_android_arm_v7a} >/dev/null
          export id_arm_v7a=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v7a})

          export COMMAND='((echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_arm_v7a" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_android_install_arm_v7a
          docker cp $id_arm_v7a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v7a

          # x86_64
          time docker pull ${docker_image_libtorch_android_x86_64} >/dev/null
          export id_x86_64=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_64})

          export COMMAND='((echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_x86_64" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_android_install_x86_64
          docker cp $id_x86_64:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_x86_64

          # arm-v8a
          time docker pull ${docker_image_libtorch_android_arm_v8a} >/dev/null
          export id_arm_v8a=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_arm_v8a})

          export COMMAND='((echo "sudo chown -R jenkins workspace") | docker exec -u jenkins -i "$id_arm_v8a" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_android_install_arm_v8a
          docker cp $id_arm_v8a:/var/lib/jenkins/workspace/build_android/install ~/workspace/build_android_install_arm_v8a

          docker cp ~/workspace/build_android_install_arm_v7a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v7a
          docker cp ~/workspace/build_android_install_x86_64 $id_x86_32:/var/lib/jenkins/workspace/build_android_install_x86_64
          docker cp ~/workspace/build_android_install_arm_v8a $id_x86_32:/var/lib/jenkins/workspace/build_android_install_arm_v8a

          # run gradle buildRelease
          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_android_artifacts
          docker cp $id_x86_32:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_artifacts/

          output_image=$docker_image_libtorch_android_x86_32-gradle
          docker commit "$id_x86_32" ${output_image}
          time docker push ${output_image}
    - upload_binary_size_for_android_build:
        build_type: prebuilt
        artifacts: /home/circleci/workspace/build_android_artifacts/artifacts.tgz
    - store_artifacts:
        path: ~/workspace/build_android_artifacts/artifacts.tgz
        destination: artifacts.tgz

  pytorch_android_publish_snapshot:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-publish-snapshot
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c"
      PYTHON_VERSION: "3.6"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: pytorch android gradle build
        no_output_timeout: "1h"
        command: |
          set -eux
          docker_image_commit=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}

          docker_image_libtorch_android_x86_32_gradle=${docker_image_commit}-android-x86_32-gradle

          echo "docker_image_commit: "${docker_image_commit}
          echo "docker_image_libtorch_android_x86_32_gradle: "${docker_image_libtorch_android_x86_32_gradle}

          # x86_32
          time docker pull ${docker_image_libtorch_android_x86_32_gradle} >/dev/null
          export id_x86_32=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32_gradle})

          export COMMAND='((echo "sudo chown -R jenkins workspace" && echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export SONATYPE_NEXUS_USERNAME=${SONATYPE_NEXUS_USERNAME}" && echo "export SONATYPE_NEXUS_PASSWORD=${SONATYPE_NEXUS_PASSWORD}" && echo "export ANDROID_SIGN_KEY=${ANDROID_SIGN_KEY}" && echo "export ANDROID_SIGN_PASS=${ANDROID_SIGN_PASS}" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/publish_android_snapshot.sh") | docker exec -u jenkins -i "$id_x86_32" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          output_image=${docker_image_libtorch_android_x86_32_gradle}-publish-snapshot
          docker commit "$id_x86_32" ${output_image}
          time docker push ${output_image}

  pytorch_android_gradle_build-x86_32:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-build-only-x86_32
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c"
      PYTHON_VERSION: "3.6"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - checkout
    - setup_ci_environment
    - run:
        name: pytorch android gradle build only x86_32 (for PR)
        no_output_timeout: "1h"
        command: |
          set -e
          docker_image_libtorch_android_x86_32=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}-android-x86_32
          echo "docker_image_libtorch_android_x86_32: "${docker_image_libtorch_android_x86_32}

          # x86
          time docker pull ${docker_image_libtorch_android_x86_32} >/dev/null
          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${docker_image_libtorch_android_x86_32})

          export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo "export GRADLE_OFFLINE=1" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          mkdir -p ~/workspace/build_android_x86_32_artifacts
          docker cp $id:/var/lib/jenkins/workspace/android/artifacts.tgz ~/workspace/build_android_x86_32_artifacts/

          output_image=${docker_image_libtorch_android_x86_32}-gradle
          docker commit "$id" ${output_image}
          time docker push ${output_image}
    - upload_binary_size_for_android_build:
        build_type: prebuilt-single
        artifacts: /home/circleci/workspace/build_android_x86_32_artifacts/artifacts.tgz
    - store_artifacts:
        path: ~/workspace/build_android_x86_32_artifacts/artifacts.tgz
        destination: artifacts.tgz

  pytorch_android_gradle_custom_build_single:
    environment:
      BUILD_ENVIRONMENT: pytorch-linux-xenial-py3-clang5-android-ndk-r19c-gradle-custom-build-single
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3-clang5-android-ndk-r19c"
      PYTHON_VERSION: "3.6"
    resource_class: large
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - checkout
    - calculate_docker_image_tag
    - setup_ci_environment
    - run:
        name: pytorch android gradle custom build single architecture (for PR)
        no_output_timeout: "1h"
        command: |
          set -e
          # Unlike other gradle jobs, it's not worth building libtorch in a separate CI job and share via docker, because:
          # 1) Not shareable: it's custom selective build, which is different from default libtorch mobile build;
          # 2) Not parallelizable by architecture: it only builds libtorch for one architecture;

          echo "DOCKER_IMAGE: ${DOCKER_IMAGE}:${DOCKER_TAG}"
          time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null

          git submodule sync && git submodule update -q --init --recursive
          VOLUME_MOUNTS="-v /home/circleci/project/:/var/lib/jenkins/workspace"
          export id=$(docker run --env-file "${BASH_ENV}" ${VOLUME_MOUNTS} --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})

          export COMMAND='((echo "export GRADLE_OFFLINE=1" && echo "sudo chown -R jenkins workspace && cd workspace && ./.circleci/scripts/build_android_gradle.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Skip docker push as this job is purely for size analysis purpose.
          # Result binaries are already in `/home/circleci/project/` as it's mounted instead of copied.

    - upload_binary_size_for_android_build:
        build_type: custom-build-single

  pytorch_ios_build:
    <<: *pytorch_ios_params
    macos:
      xcode: "12.0"
    steps:
      - checkout
      - run_brew_for_ios_build
      - run:
          name: Run Fastlane
          no_output_timeout: "1h"
          command: |
            set -e
            PROJ_ROOT=/Users/distiller/project
            cd ${PROJ_ROOT}/ios/TestApp
            # install fastlane
            sudo gem install bundler && bundle install
            # install certificates
            echo ${IOS_CERT_KEY} >> cert.txt
            base64 --decode cert.txt -o Certificates.p12
            rm cert.txt
            bundle exec fastlane install_cert
            # install the provisioning profile
            PROFILE=PyTorch_CI_2021.mobileprovision
            PROVISIONING_PROFILES=~/Library/MobileDevice/Provisioning\ Profiles
            mkdir -pv "${PROVISIONING_PROFILES}"
            cd "${PROVISIONING_PROFILES}"
            echo ${IOS_SIGN_KEY} >> cert.txt
            base64 --decode cert.txt -o ${PROFILE}
            rm cert.txt
      - run:
          name: Build
          no_output_timeout: "1h"
          command: |
            set -e
            export IN_CI=1
            WORKSPACE=/Users/distiller/workspace
            PROJ_ROOT=/Users/distiller/project
            export TCLLIBPATH="/usr/local/lib"

            # Install conda
            curl --retry 3 -o ~/conda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
            chmod +x ~/conda.sh
            /bin/bash ~/conda.sh -b -p ~/anaconda
            export PATH="~/anaconda/bin:${PATH}"
            source ~/anaconda/bin/activate

            # Install dependencies
            retry () {
                $*  || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 && $*) || (sleep 8 && $*)
            }

            retry conda install numpy ninja pyyaml mkl mkl-include setuptools cmake cffi requests typing_extensions --yes

            # sync submodules
            cd ${PROJ_ROOT}
            git submodule sync
            git submodule update --init --recursive

            # export
            export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-"$(dirname $(which conda))/../"}

            # run build script
            chmod a+x ${PROJ_ROOT}/scripts/build_ios.sh
            echo "IOS_ARCH: ${IOS_ARCH}"
            echo "IOS_PLATFORM: ${IOS_PLATFORM}"
            echo "USE_PYTORCH_METAL": "${USE_METAL}"

            #check the custom build flag
            echo "SELECTED_OP_LIST: ${SELECTED_OP_LIST}"
            if [ -n "${SELECTED_OP_LIST}" ]; then
                export SELECTED_OP_LIST="${PROJ_ROOT}/ios/TestApp/custom_build/${SELECTED_OP_LIST}"
            fi
            export IOS_ARCH=${IOS_ARCH}
            export IOS_PLATFORM=${IOS_PLATFORM}
            if [ ${IOS_PLATFORM} != "SIMULATOR" ]; then
              export USE_PYTORCH_METAL=${USE_METAL}
            fi
            unbuffer ${PROJ_ROOT}/scripts/build_ios.sh 2>&1 | ts
      - run:
          name: Run Build Test
          no_output_timeout: "30m"
          command: |
            set -e
            PROJ_ROOT=/Users/distiller/project
            PROFILE=PyTorch_CI_2021
            # run the ruby build script
            if ! [ -x "$(command -v xcodebuild)" ]; then
              echo 'Error: xcodebuild is not installed.'
              exit 1
            fi
            echo ${IOS_DEV_TEAM_ID}
            if [ ${IOS_PLATFORM} != "SIMULATOR" ]; then
              ruby ${PROJ_ROOT}/scripts/xcode_build.rb -i ${PROJ_ROOT}/build_ios/install -x ${PROJ_ROOT}/ios/TestApp/TestApp.xcodeproj -p ${IOS_PLATFORM} -c ${PROFILE} -t ${IOS_DEV_TEAM_ID}
            else
              ruby ${PROJ_ROOT}/scripts/xcode_build.rb -i ${PROJ_ROOT}/build_ios/install -x ${PROJ_ROOT}/ios/TestApp/TestApp.xcodeproj -p ${IOS_PLATFORM}
            fi
            if ! [ "$?" -eq "0" ]; then
              echo 'xcodebuild failed!'
              exit 1
            fi
      - run:
          name: Run Simulator Tests
          no_output_timeout: "2h"
          command: |
            set -e
            if [ ${IOS_PLATFORM} != "SIMULATOR" ]; then
              echo "not SIMULATOR build, skip it."
              exit 0
            fi
            WORKSPACE=/Users/distiller/workspace
            PROJ_ROOT=/Users/distiller/project
            source ~/anaconda/bin/activate
            pip install torch torchvision --progress-bar off
            #run unit test
            cd ${PROJ_ROOT}/ios/TestApp/benchmark
            python trace_model.py
            ruby setup.rb
            cd ${PROJ_ROOT}/ios/TestApp
            instruments -s -devices
            fastlane scan
  pytorch_linux_bazel_build:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Bazel Build
        no_output_timeout: "1h"
        command: |
          set -e
          # Pull Docker image and run build
          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}:${DOCKER_TAG}
          time docker pull ${DOCKER_IMAGE}:${DOCKER_TAG} >/dev/null
          export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${DOCKER_IMAGE}:${DOCKER_TAG})

          echo "Do NOT merge master branch into $CIRCLE_BRANCH in environment $BUILD_ENVIRONMENT"

          git submodule sync && git submodule update -q --init --recursive

          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'

          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            # Augment our output image name with bazel to avoid collisions
            output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-bazel-${CIRCLE_SHA1}
            export COMMIT_DOCKER_IMAGE=$output_image
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            time docker push ${COMMIT_DOCKER_IMAGE}
          fi

  pytorch_linux_bazel_test:
    <<: *pytorch_params
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Test
        no_output_timeout: "90m"
        command: |
          set -e
          output_image=${DOCKER_IMAGE}:${DOCKER_TAG}-bazel-${CIRCLE_SHA1}
          export COMMIT_DOCKER_IMAGE=$output_image
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}

          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null

          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --gpus all -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run --env-file "${BASH_ENV}" --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi

          retrieve_test_reports() {
            echo "retrieving test reports"
            docker cp -L $id:/var/lib/jenkins/workspace/bazel-testlogs ./ || echo 'No test reports found!'
          }
          trap "retrieve_test_reports" ERR

          if [[ ${BUILD_ENVIRONMENT} == *"multigpu"* ]]; then
            export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/multigpu-test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          else
            export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          fi
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          retrieve_test_reports
          docker stats --all --no-stream
    - store_test_results:
        path: bazel-testlogs

  pytorch_doc_test:
    environment:
      BUILD_ENVIRONMENT: pytorch-doc-test
      DOCKER_IMAGE: "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/pytorch-linux-xenial-py3.6-gcc5.4"
    resource_class: medium
    machine:
      image: ubuntu-1604:202007-01
    steps:
    - checkout
    - calculate_docker_image_tag
    - setup_linux_system_environment
    - setup_ci_environment
    - run:
        name: Doc test
        no_output_timeout: "30m"
        command: |
          set -ex
          export COMMIT_DOCKER_IMAGE=${DOCKER_IMAGE}:${DOCKER_TAG}-${CIRCLE_SHA1}
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          time docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          export id=$(docker run --cap-add=SYS_PTRACE --security-opt seccomp=unconfined -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          export COMMAND='((echo "sudo chown -R jenkins workspace && cd workspace && . ./.jenkins/pytorch/docs-test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

  promote_s3:
    <<: *promote_common
    steps:
      - checkout
      - run:
          name: Running promote script
          command: |
            scripts/release/promote/wheel_to_s3.sh

  promote_conda:
    <<: *promote_common
    steps:
      - checkout
      - run:
          name: Running promote script
          command: |
            scripts/release/promote/conda_to_conda.sh

  # update_s3_htmls job
  # These jobs create html files for every cpu/cu## folder in s3. The html
  # files just store the names of all the files in that folder (which are
  # binary files (.whl files)). This is to allow pip installs of the latest
  # version in a folder without having to know the latest date. Pip has a flag
  # -f that you can pass an html file listing a bunch of packages, and pip will
  # then install the one with the most recent version.
  update_s3_htmls: &update_s3_htmls
    machine:
      image: ubuntu-1604:202007-01
    resource_class: medium
    steps:
    - checkout
    - setup_linux_system_environment
    - run:
        <<: *binary_checkout
    # N.B. we do not run binary_populate_env. The only variable we need is
    # PIP_UPLOAD_FOLDER (which is 'nightly/' for the nightlies and '' for
    # releases, and sometimes other things for special cases). Instead we
    # expect PIP_UPLOAD_FOLDER to be passed directly in the env. This is
    # because, unlike all the other binary jobs, these jobs only get run once,
    # in a separate workflow. They are not a step in other binary jobs like
    # build, test, upload.
    #
    # You could attach this to every job, or include it in the upload step if
    # you wanted. You would need to add binary_populate_env in this case to
    # make sure it has the same upload folder as the job it's attached to. This
    # function is idempotent, so it won't hurt anything; it's just a little
    # unnescessary"
    - run:
        name: define PIP_UPLOAD_FOLDER
        command: |
          our_upload_folder=nightly/
          # On tags upload to test instead
          if [[ -n "${CIRCLE_TAG}" ]]; then
            our_upload_folder=test/
          fi
          echo "export PIP_UPLOAD_FOLDER=${our_upload_folder}" >> ${BASH_ENV}
    - run:
        name: Update s3 htmls
        no_output_timeout: "1h"
        command: |
          set +x
          echo "declare -x \"AWS_ACCESS_KEY_ID=${PYTORCH_BINARY_AWS_ACCESS_KEY_ID}\"" >> /home/circleci/project/env
          echo "declare -x \"AWS_SECRET_ACCESS_KEY=${PYTORCH_BINARY_AWS_SECRET_ACCESS_KEY}\"" >> /home/circleci/project/env
          source /home/circleci/project/env
          set -eux -o pipefail
          retry () {
              $*  || (sleep 1 && $*) || (sleep 2 && $*) || (sleep 4 && $*) || (sleep 8 && $*)
          }
          retry pip install awscli==1.6
          "/home/circleci/project/builder/cron/update_s3_htmls.sh"

# There is currently no testing for libtorch TODO
#  binary_linux_libtorch_3.6m_cpu_test:
#    environment:
#      BUILD_ENVIRONMENT: "libtorch 3.6m cpu"
#    resource_class: gpu.medium
#    <<: *binary_linux_test
#
#  binary_linux_libtorch_3.6m_cu90_test:
#    environment:
#      BUILD_ENVIRONMENT: "libtorch 3.6m cu90"
#    resource_class: gpu.medium
#    <<: *binary_linux_test
#
  docker_build_job:
      parameters:
        image_name:
          type: string
          default: ""
      machine:
        image: ubuntu-1604:202007-01
      resource_class: large
      environment:
        IMAGE_NAME: << parameters.image_name >>
        # Enable 'docker manifest'
        DOCKER_CLI_EXPERIMENTAL: "enabled"
        DOCKER_BUILDKIT: 1
      steps:
        - checkout
        - calculate_docker_image_tag
        - run:
            name: Check if image should be built
            command: |
              set +x
              export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
              export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}
              eval $(aws ecr get-login --no-include-email --region us-east-1)
              set -x
              # Check if image already exists, if it does then skip building it
              if docker manifest inspect "308535385114.dkr.ecr.us-east-1.amazonaws.com/pytorch/${IMAGE_NAME}:${DOCKER_TAG}"; then
                circleci-agent step halt
                # circleci-agent step halt doesn't actually halt the step so we need to
                # explicitly exit the step here ourselves before it causes too much trouble
                exit 0
              fi
              # Covers the case where a previous tag doesn't exist for the tree
              # this is only really applicable on trees that don't have `.circleci/docker` at its merge base, i.e. nightly
              if ! git rev-parse "$(git merge-base HEAD << pipeline.git.base_revision >>):.circleci/docker"; then
                echo "Directory '.circleci/docker' not found in tree << pipeline.git.base_revision >>, you should probably rebase onto a more recent commit"
                exit 1
              fi
              PREVIOUS_DOCKER_TAG=$(git rev-parse "$(git merge-base HEAD << pipeline.git.base_revision >>):.circleci/docker")
              # If no image exists but the hash is the same as the previous hash then we should error out here
              if [[ "${PREVIOUS_DOCKER_TAG}" = "${DOCKER_TAG}" ]]; then
                echo "ERROR: Something has gone wrong and the previous image isn't available for the merge-base of your branch"
                echo "       contact the PyTorch team to restore the original images"
                exit 1
              fi
        - run:
            name: build_docker_image_<< parameters.image_name >>
            no_output_timeout: "1h"
            command: |
              set +x
              export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
              export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}
              set -x
              cd .circleci/docker && ./build_docker.sh
  docker_for_ecr_gc_build_job:
      machine:
        image: ubuntu-1604:202007-01
      steps:
        - checkout
        - run:
            name: build_docker_image_for_ecr_gc
            no_output_timeout: "1h"
            command: |
              cd .circleci/ecr_gc_docker
              docker build . -t 308535385114.dkr.ecr.us-east-1.amazonaws.com/gc/ecr
              set +x
              export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
              export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}
              eval $(aws ecr get-login --no-include-email --region us-east-1)
              set -x
              docker push 308535385114.dkr.ecr.us-east-1.amazonaws.com/gc/ecr
  ecr_gc_job:
      parameters:
        project:
          type: string
          default: "pytorch"
        tags_to_keep:  # comma separate values
          type: string
      environment:
        PROJECT: << parameters.project >>
        # TODO: Remove legacy image tags once we feel comfortable with new docker image tags
        IMAGE_TAG: << parameters.tags_to_keep >>
      docker:
        - image: 308535385114.dkr.ecr.us-east-1.amazonaws.com/gc/ecr
          aws_auth:
            aws_access_key_id: ${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
            aws_secret_access_key: ${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}

      steps:
        - checkout
        - run:
            # NOTE: see 'docker_build_job' for how these tags actually get built
            name: dynamically generate tags to keep
            no_output_timeout: "1h"
            command: |
              GENERATED_IMAGE_TAG=$(\
                git log --oneline --pretty='%H' .circleci/docker \
                  | xargs -I '{}' git rev-parse '{}:.circleci/docker' \
                  | paste -sd "," -)
              echo "export GENERATED_IMAGE_TAG='${GENERATED_IMAGE_TAG}'" >> ${BASH_ENV}
        - run:
            name: garbage collecting for ecr images
            no_output_timeout: "1h"
            command: |
              set +x
              export AWS_ACCESS_KEY_ID=${CIRCLECI_AWS_ACCESS_KEY_FOR_DOCKER_BUILDER_V1}
              export AWS_SECRET_ACCESS_KEY=${CIRCLECI_AWS_SECRET_KEY_FOR_DOCKER_BUILDER_V1}
              set -x
              /usr/bin/gc.py --filter-prefix ${PROJECT}  --ignore-tags "${IMAGE_TAG},${GENERATED_IMAGE_TAG}"
##############################################################################
# Workflows
##############################################################################
workflows:
  binary_builds:
    jobs:
      - binary_linux_build:
          name: binary_linux_conda_3_6_cpu_devtoolset7_nightly_build
          build_environment: "conda 3.6 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_build:
          name: binary_linux_conda_3_7_cpu_devtoolset7_nightly_build
          build_environment: "conda 3.7 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_build:
          name: binary_linux_conda_3_8_cpu_devtoolset7_nightly_build
          build_environment: "conda 3.8 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_build:
          name: binary_linux_conda_3_9_cpu_devtoolset7_nightly_build
          build_environment: "conda 3.9 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_build:
          name: binary_linux_conda_3_6_cu101_devtoolset7_nightly_build
          build_environment: "conda 3.6 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda101"
      - binary_linux_build:
          name: binary_linux_conda_3_7_cu101_devtoolset7_nightly_build
          build_environment: "conda 3.7 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda101"
      - binary_linux_build:
          name: binary_linux_conda_3_8_cu101_devtoolset7_nightly_build
          build_environment: "conda 3.8 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda101"
      - binary_linux_build:
          name: binary_linux_conda_3_9_cu101_devtoolset7_nightly_build
          build_environment: "conda 3.9 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda101"
      - binary_linux_build:
          name: binary_linux_conda_3_6_cu102_devtoolset7_nightly_build
          build_environment: "conda 3.6 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda102"
      - binary_linux_build:
          name: binary_linux_conda_3_7_cu102_devtoolset7_nightly_build
          build_environment: "conda 3.7 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda102"
      - binary_linux_build:
          name: binary_linux_conda_3_8_cu102_devtoolset7_nightly_build
          build_environment: "conda 3.8 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda102"
      - binary_linux_build:
          name: binary_linux_conda_3_9_cu102_devtoolset7_nightly_build
          build_environment: "conda 3.9 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda102"
      - binary_linux_build:
          name: binary_linux_conda_3_6_cu110_devtoolset7_nightly_build
          build_environment: "conda 3.6 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda110"
      - binary_linux_build:
          name: binary_linux_conda_3_7_cu110_devtoolset7_nightly_build
          build_environment: "conda 3.7 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda110"
      - binary_linux_build:
          name: binary_linux_conda_3_8_cu110_devtoolset7_nightly_build
          build_environment: "conda 3.8 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda110"
      - binary_linux_build:
          name: binary_linux_conda_3_9_cu110_devtoolset7_nightly_build
          build_environment: "conda 3.9 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          docker_image: "pytorch/conda-builder:cuda110"
      - binary_mac_build:
          name: binary_macos_conda_3_9_cpu_nightly_build
          build_environment: "conda 3.9 cpu"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
      - binary_linux_test:
          name: binary_linux_conda_3_6_cpu_devtoolset7_nightly_test
          build_environment: "conda 3.6 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_6_cpu_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_test:
          name: binary_linux_conda_3_7_cpu_devtoolset7_nightly_test
          build_environment: "conda 3.7 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_7_cpu_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_test:
          name: binary_linux_conda_3_8_cpu_devtoolset7_nightly_test
          build_environment: "conda 3.8 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_8_cpu_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_test:
          name: binary_linux_conda_3_9_cpu_devtoolset7_nightly_test
          build_environment: "conda 3.9 cpu devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_9_cpu_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cpu"
      - binary_linux_test:
          name: binary_linux_conda_3_6_cu101_devtoolset7_nightly_test
          build_environment: "conda 3.6 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_6_cu101_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda101"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_7_cu101_devtoolset7_nightly_test
          build_environment: "conda 3.7 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_7_cu101_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda101"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_8_cu101_devtoolset7_nightly_test
          build_environment: "conda 3.8 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_8_cu101_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda101"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_9_cu101_devtoolset7_nightly_test
          build_environment: "conda 3.9 cu101 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_9_cu101_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda101"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_6_cu102_devtoolset7_nightly_test
          build_environment: "conda 3.6 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_6_cu102_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda102"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_7_cu102_devtoolset7_nightly_test
          build_environment: "conda 3.7 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_7_cu102_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda102"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_8_cu102_devtoolset7_nightly_test
          build_environment: "conda 3.8 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_8_cu102_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda102"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_9_cu102_devtoolset7_nightly_test
          build_environment: "conda 3.9 cu102 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_9_cu102_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda102"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_6_cu110_devtoolset7_nightly_test
          build_environment: "conda 3.6 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_6_cu110_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda110"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_7_cu110_devtoolset7_nightly_test
          build_environment: "conda 3.7 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_7_cu110_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda110"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_8_cu110_devtoolset7_nightly_test
          build_environment: "conda 3.8 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_8_cu110_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda110"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
      - binary_linux_test:
          name: binary_linux_conda_3_9_cu110_devtoolset7_nightly_test
          build_environment: "conda 3.9 cu110 devtoolset7"
          filters:
            branches:
              only:
                - /.*/
            tags:
              only:
                - /v[0-9]+(\.[0-9]+)*-rc[0-9]+/
          requires:
            - binary_linux_conda_3_9_cu110_devtoolset7_nightly_build
          docker_image: "pytorch/conda-builder:cuda110"
          use_cuda_docker_runtime: "1"
          resource_class: gpu.medium
  ecr_gc:
    triggers:
      - schedule:
          cron: "45 * * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - docker_for_ecr_gc_build_job
      - ecr_gc_job:
            name: ecr_gc_job_for_pytorch
            project: pytorch
            tags_to_keep: "271,262,256,278,282,291,300,323,327,347,389,401,402,403,405,a8006f9a-272d-4478-b137-d121c6f05c83,6e7b11da-a919-49e5-b2ba-da66e3d4bb0a,f990c76a-a798-42bb-852f-5be5006f8026,e43973a9-9d5a-4138-9181-a08a0fc55e2f,8fcf46ef-4a34-480b-a8ee-b0a30a4d3e59,9a3986fa-7ce7-4a36-a001-3c9bef9892e2,1bc00f11-e0f3-4e5c-859f-15937dd938cd,209062ef-ab58-422a-b295-36c4eed6e906,be76e8fd-44e2-484d-b090-07e0cc3a56f0,fff7795428560442086f7b2bb6004b65245dc11a,ab1632df-fa59-40e6-8c23-98e004f61148"
            requires:
              - docker_for_ecr_gc_build_job
      - ecr_gc_job:
            name: ecr_gc_job_for_caffe2
            project: caffe2
            tags_to_keep: "376,373,369,348,345,336,325,324,315,306,301,287,283,276,273,266,253,248,238,230,213"
            requires:
              - docker_for_ecr_gc_build_job
      - ecr_gc_job:
            name: ecr_gc_job_for_translate
            project: translate
            tags_to_keep: "8"
            requires:
              - docker_for_ecr_gc_build_job
      - ecr_gc_job:
            name: ecr_gc_job_for_tensorcomp
            project: tensorcomp
            tags_to_keep: "34"
            requires:
              - docker_for_ecr_gc_build_job
  # Promotion workflow
  promote:
    jobs:
      # Requires manual approval by someone in org-member
      # CircleCI security context
      - promote_approval:
          context: org-member
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /v[0-9]+(\.[0-9]+)*/
          type: approval
      - promote_s3:
          context: org-member
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /v[0-9]+(\.[0-9]+)*/
          name: promote_s3_libtorch
          package_name: libtorch
          requires:
            - promote_approval
      - promote_s3:
          context: org-member
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /v[0-9]+(\.[0-9]+)*/
          name: promote_s3_torch
          package_name: torch
          requires:
            - promote_approval
      - promote_conda:
          context: org-member
          filters:
            branches:
              ignore: /.*/
            tags:
              only: /v[0-9]+(\.[0-9]+)*/
          name: promote_conda_pytorch
          package_name: pytorch
          requires:
            - promote_approval
