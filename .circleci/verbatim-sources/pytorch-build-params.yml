pytorch_common: &pytorch_common
  parameters:
    build_environment:
      type: string
      default: ""
    docker_image:
      type: string
      default: ""
    resource_class:
      type: string
      default: "large"
    python_version:
      type: string
      default: ""
    use_cuda_docker_runtime:
      type: string
      default: ""
    multi_gpu:
      type: string
      default: ""
  environment:
    BUILD_ENVIRONMENT: << parameters.build_environment >>
    DOCKER_IMAGE: << parameters.docker_image >>
    PYTHON_VERSION: << parameters.python_version >>
    USE_CUDA_DOCKER_RUNTIME: << parameters.use_cuda_docker_runtime >>
    MULTI_GPU: << parameters.multi_gpu >>
  resource_class: << parameters.resource_class >>

jobs:
  pytorch_linux_build:
    <<: *pytorch_common
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - checkout
    - run:
        <<: *setup_ci_environment
    - run:
        name: Build
        no_output_timeout: "1h"
        command: |
          set -e
          # Pull Docker image and run build
          echo "DOCKER_IMAGE: "${DOCKER_IMAGE}
          docker pull ${DOCKER_IMAGE} >/dev/null
          export id=$(docker run -t -d -w /var/lib/jenkins ${DOCKER_IMAGE})

          git submodule sync && git submodule update -q --init --recursive

          docker cp /home/circleci/project/. $id:/var/lib/jenkins/workspace

          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          fi

          # dispatch aten ops statically for mobile
          if [[ ${BUILD_ENVIRONMENT} == *"android"* ]]; then
            NAMED_FLAG="export USE_STATIC_DISPATCH=1"
          fi

          export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/build.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

          # Push intermediate Docker image for next phase to use
          if [ -z "${BUILD_ONLY}" ]; then
            # Note [Special build images]
            # The namedtensor and xla builds use the same docker image as
            # pytorch-linux-trusty-py3.6-gcc5.4-build. In the push step, we have to
            # distinguish between them so the test can pick up the correct image.
            output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
            if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-xla
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_64"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_64
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v7a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v7a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-arm-v8a"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-arm-v8a
            elif [[ ${BUILD_ENVIRONMENT} == *"android-ndk-r19c-x86_32"* ]]; then
              export COMMIT_DOCKER_IMAGE=$output_image-android-x86_32
            else
              export COMMIT_DOCKER_IMAGE=$output_image
            fi
            docker commit "$id" ${COMMIT_DOCKER_IMAGE}
            docker push ${COMMIT_DOCKER_IMAGE}
          fi

  pytorch_linux_test:
    <<: *pytorch_common
    machine:
      image: ubuntu-1604:201903-01
    steps:
    # See Note [Workspace for CircleCI scripts] in job-specs-setup.yml
    - attach_workspace:
        at: ~/workspace
    - run:
        <<: *should_run_job
    - run:
        <<: *setup_linux_system_environment
    - run:
        <<: *setup_ci_environment
    - run:
        name: Test
        no_output_timeout: "90m"
        command: |
          set -e
          # See Note [Special build images]
          output_image=${DOCKER_IMAGE}-${CIRCLE_SHA1}
          if [[ ${BUILD_ENVIRONMENT} == *"namedtensor"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-namedtensor
            NAMED_FLAG="export BUILD_NAMEDTENSOR=1"
          elif [[ ${BUILD_ENVIRONMENT} == *"xla"* ]]; then
            export COMMIT_DOCKER_IMAGE=$output_image-xla
          else
            export COMMIT_DOCKER_IMAGE=$output_image
          fi
          echo "DOCKER_IMAGE: "${COMMIT_DOCKER_IMAGE}
          docker pull ${COMMIT_DOCKER_IMAGE} >/dev/null
          if [ -n "${USE_CUDA_DOCKER_RUNTIME}" ]; then
            export id=$(docker run --runtime=nvidia -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          else
            export id=$(docker run -t -d -w /var/lib/jenkins ${COMMIT_DOCKER_IMAGE})
          fi
          if [ -n "${MULTI_GPU}" ]; then
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"' && echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/multigpu-test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          else
            export COMMAND='((echo "export BUILD_ENVIRONMENT=${BUILD_ENVIRONMENT}" && echo '"$NAMED_FLAG"'&& echo "source ./workspace/env" && echo "sudo chown -R jenkins workspace && cd workspace && .jenkins/pytorch/test.sh") | docker exec -u jenkins -i "$id" bash) 2>&1'
          fi
          echo ${COMMAND} > ./command.sh && unbuffer bash ./command.sh | ts

