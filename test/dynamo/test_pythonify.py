# Owner(s): ["module: dynamo"]
"""
End-to-end tests for the pythonify feature of torch.compile.

These tests verify that torch.compile with pythonify=path generates a Python
file that can be exec'd to produce the same results as the compiled model.

The pythonify feature takes all runtime machinery normally embedded inside
torch.compile and makes it explicit as generated Python code that can be
inspected, debugged, and executed.
"""

import inspect
import os
import tempfile
import unittest

import torch
import torch.nn as nn
from torch.testing._internal.common_utils import run_tests, TestCase


class TestPythonifyEndToEnd(TestCase):
    """
    End-to-end tests for pythonify feature.

    These tests verify the complete workflow:
    1. Create a model
    2. Compile with torch.compile(pythonify=path)
    3. Execute the generated Python file via exec()
    4. Verify outputs match the compiled model
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_end_to_end(self):
        """
        Basic end-to-end test for pythonify.

        This test matches the example from the requirements specification:
        create a simple model, compile it with pythonify, exec the generated
        file, and verify outputs match.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "Generated by torch.compile(pythonify=...)",
                code,
                "Expected header comment in generated file",
            )
            self.assertIn(
                "CompiledFunction",
                code,
                "Expected CompiledFunction class in generated file",
            )
            self.assertIn(
                "obj_from_id",
                code,
                "Expected obj_from_id helper for object ID extraction",
            )

            torch.manual_seed(0)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                f"Forward outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_end_to_end_multiple_runs(self):
        """
        Test that exec'd code can be run multiple times with different inputs.

        The generated code should be reusable with different input tensors
        as long as they have the same shape and dtype.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x_initial)

            with open(path) as f:
                code = f.read()

            for i in range(3):
                x_test = torch.randn(batch, features)
                y_expected = model(x_test)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["model"] = model
                namespace["torch"] = torch

                exec(code, namespace)
                y_actual = namespace["y"]

                self.assertTrue(
                    torch.allclose(y_expected, y_actual),
                    f"Run {i}: outputs should match",
                )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_uses_object_id_for_parameters(self):
        """
        Test that generated code uses obj_from_id() for parameter extraction.

        The pythonify implementation should use object ID-based parameter
        extraction instead of relying on the model variable being in scope.
        This is more robust because:
        1. No need to pass model through exec scope
        2. Guaranteed to get the exact tensor that was compiled
        3. Works with any model structure
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))
                self.bias = nn.Parameter(torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("obj_from_id", code)
            self.assertIn("import ctypes", code)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with object ID extraction",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_generated_code_is_valid_python(self):
        """
        Test that generated code is syntactically valid Python.

        The generated file should be parseable by Python's compile() function.
        This catches syntax errors before attempting to exec().
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(2, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")
        finally:
            os.unlink(path)


class TestPythonifyModelVariants(TestCase):
    """
    Tests for pythonify with different model architectures.

    Note: Tests for models with nested modules like nn.Linear are not yet
    supported because nested parameter paths (e.g., self.linear.weight) are
    not fully implemented in the object ID extraction path. These tests are
    skipped pending implementation of nested module parameter support.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_simple_parameter_model(self):
        """
        Test pythonify with a model with simple (non-nested) parameters.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(8, 4))
                self.bias = nn.Parameter(torch.zeros(4))

            def forward(self, x):
                return x.matmul(self.weight) + self.bias

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Simple parameter model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_elementwise_ops(self):
        """
        Test pythonify with elementwise operations (add, mul, relu).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x):
                y = x.matmul(self.W)
                return torch.relu(y)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Elementwise ops model outputs should match",
            )
        finally:
            os.unlink(path)


class TestPythonifyBackward(TestCase):
    """
    Tests for backward pass functionality in pythonify.

    These tests verify that loss.backward() works correctly when using
    the exec'd pythonify output. The backward kernel must be compiled
    eagerly (not lazily) so it can be captured in the generated Python file.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_basic(self):
        """
        Basic test that backward pass works via exec'd pythonify output.

        This test verifies that:
        1. The generated code includes backward kernel
        2. Calling loss.backward() on exec'd output computes gradients correctly
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            original_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)
            self.assertIn("compiled_fn_backward", code)

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(original_grad, x2.grad),
                f"Gradients should match. Expected {original_grad}, got {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_forward_output_matches(self):
        """
        Test that forward output from exec'd code matches compiled output.

        This is a simpler backward test that verifies the forward pass
        produces matching results when requires_grad=True (which triggers
        backward kernel compilation).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(99)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)

            with open(path) as f:
                code = f.read()

            torch.manual_seed(99)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match with requires_grad=True",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_kernel_present(self):
        """
        Test that backward kernel is present in generated code.

        Verifies that when compiling with requires_grad=True, the backward
        kernel is eagerly compiled and included in the generated Python file.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(88)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)
            self.assertIn("compiled_fn_backward", code)
            backward_kernel_count = code.count("# AOT ID:")
            self.assertEqual(
                backward_kernel_count,
                2,
                f"Expected 2 AOT IDs (forward + backward), got {backward_kernel_count}",
            )
        finally:
            os.unlink(path)


class TestPythonifyInference(TestCase):
    """
    Tests for inference-only mode in pythonify.

    Verifies that models without requires_grad work correctly and that
    the generated code handles absence of backward kernel gracefully.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_no_requires_grad(self):
        """
        Test that inference mode (requires_grad=False) works correctly.

        When inputs don't require gradients, the backward kernel is not
        compiled. The generated code should work correctly without errors.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "def forward",
                code,
                "Expected forward method to be present",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Forward outputs should match in inference mode",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_with_torch_no_grad(self):
        """
        Test that torch.no_grad() context works correctly.

        Even if the model has parameters that could have gradients,
        when executed within torch.no_grad(), no backward kernel should
        be needed.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            with torch.no_grad():
                y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            with torch.no_grad():
                exec(code, namespace)
                y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Forward outputs should match with no_grad",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_multiple_runs(self):
        """
        Test that even if backward is not available, forward still works.

        This verifies that the absence of backward kernel doesn't break
        forward pass execution, even when run multiple times.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            for _ in range(3):
                x_test = torch.randn(batch, features, requires_grad=False)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["model"] = model
                namespace["torch"] = torch

                exec(code, namespace)
                y_test = namespace["y"]

                self.assertEqual(y_test.shape, (batch, features))
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_backward_raises_clear_error(self):
        """
        Test that backward on inference-compiled model raises clear error.

        When a model is compiled in inference mode (e.g., with torch.no_grad()),
        there's no backward kernel. If the user later tries to call backward
        with requires_grad inputs, they should get a clear error message
        explaining the issue, not a generic PyTorch NotImplementedError.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            with torch.no_grad():
                torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "def backward",
                code,
                "Expected backward method to be present for fallback",
            )
            self.assertIn(
                "Backward pass not available",
                code,
                "Expected fallback error message in generated code",
            )

            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertEqual(y2.shape, (batch, features))
            self.assertTrue(y2.requires_grad)

            loss = y2.mean()
            with self.assertRaises(RuntimeError) as context:
                loss.backward()

            error_msg = str(context.exception)
            self.assertIn(
                "Backward pass not available",
                error_msg,
                "Expected clear error message about missing backward kernel",
            )
            self.assertIn(
                "inference mode",
                error_msg,
                "Expected error message to mention inference mode",
            )

        finally:
            os.unlink(path)


class TestPythonifyCodeQuality(TestCase):
    """
    Tests for the quality and structure of generated code.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_section_headers(self):
        """
        Test that generated code has clear section headers.

        The generated code should be well-organized with section headers
        like "Argument Extraction", "Guard Evaluation", etc.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x):
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("Argument Extraction", code)
            self.assertIn("Guard Evaluation", code)
            self.assertIn("AOT Autograd Function Definition", code)
            self.assertIn("Invoke Compiled Callable", code)
            self.assertIn("Expose Result", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_inductor_kernel(self):
        """
        Test that generated code includes Inductor kernel source.

        When Inductor backend is used, the generated code should include
        the Inductor-generated kernel code with the 'call' function.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(8, 4))

            def forward(self, x):
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("Inductor Compiled Kernel", code)
            self.assertIn("def call(", code)
            self.assertIn("compiled_fn = call", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_object_id_warning(self):
        """
        Test that generated code includes a prominent warning about object ID limitations.

        When object IDs are used for parameter/buffer access, the generated file
        should contain a clear warning explaining that the file is process-local
        and should not be saved for later use.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(8, 4))

            def forward(self, x):
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify the warning header is present
            self.assertIn(
                "WARNING: PROCESS-LOCAL FILE - DO NOT SAVE OR REUSE",
                code,
                "Expected prominent warning about process-local nature",
            )
            self.assertIn(
                "NOT PERSISTABLE",
                code,
                "Expected warning about non-persistability",
            )
            self.assertIn(
                "SAME-PROCESS ONLY",
                code,
                "Expected warning about same-process execution",
            )
            self.assertIn(
                "KEEP MODEL ALIVE",
                code,
                "Expected warning about keeping model alive",
            )
            self.assertIn(
                "CRASH RISK",
                code,
                "Expected warning about crash risk from invalid object IDs",
            )
            self.assertIn(
                "object IDs (memory addresses)",
                code,
                "Expected explanation of what object IDs are",
            )
            self.assertIn(
                "torch.export()",
                code,
                "Expected mention of torch.export() as alternative",
            )
        finally:
            os.unlink(path)


class TestPythonifyNestedModules(TestCase):
    """
    Tests for pythonify with models containing nested modules.

    These tests verify that parameter extraction works correctly for models
    with nested nn.Module structures. The object ID extraction must traverse
    the nested module hierarchy to capture the correct tensor references.

    NOTE: Currently pythonify assumes a fixed argument ordering (inputs first,
    then parameters, then buffers). This works for models where the input is
    the first operand in operations. Models like nn.Linear (where weight is
    used first internally) may not work correctly with the current implementation.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_simple_param_model(self):
        """
        Test pythonify with a simple model using a direct parameter.

        This tests the basic case where a parameter is directly on the model
        (not in a nested submodule) and is used as the second operand in matmul.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify generated code uses obj_from_id for parameter extraction
            self.assertIn("obj_from_id", code)
            self.assertIn("import ctypes", code)

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Simple param model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_model_with_add_and_matmul(self):
        """
        Test pythonify with model using x + x.matmul(W) pattern.

        This is the canonical example from the pythonify requirements.
        The input x is used as the first operand in both the add and matmul.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Model with add and matmul outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_model_with_multiple_params(self):
        """
        Test pythonify with model having multiple parameters used in order.

        Both parameters are used after the input, so the ordering is
        (input, param1, param2).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W1).matmul(self.W2)

        torch.manual_seed(456)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify obj_from_id is used
            self.assertIn("obj_from_id", code)

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Multiple param model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_code_uses_object_id(self):
        """
        Test that generated code properly uses obj_from_id for parameters.

        Verifies the core functionality that allows pythonify to work
        without needing the model object passed through exec scope.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight) + self.bias

        torch.manual_seed(789)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify obj_from_id helper is present
            self.assertIn("def obj_from_id(obj_id):", code)
            self.assertIn("ctypes.cast(obj_id, ctypes.py_object).value", code)

            # Verify obj_from_id is used for parameter extraction
            self.assertIn("obj_from_id(", code)

            # Execute the generated code WITHOUT passing model
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            # Note: model is NOT in namespace - obj_from_id should handle it

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Object ID extraction should work without model in scope",
            )
        finally:
            os.unlink(path)


class TestPythonifyBuffers(TestCase):
    """
    Tests for pythonify with models that use register_buffer().

    Buffers are like parameters but not trained. They are saved/loaded with
    the model state_dict and should be captured via object ID just like
    parameters. This test class verifies that various buffer configurations
    work correctly with pythonify.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_scalar(self):
        """
        Test pythonify with a scalar buffer (0-dimensional tensor).

        Scalar buffers are commonly used for things like running mean/variance
        in batch normalization or scale factors. This test verifies that scalar
        buffers are correctly captured and used in the generated code.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "obj_from_id",
                code,
                "Expected obj_from_id helper for object ID extraction",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Scalar buffer model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_tensor(self):
        """
        Test pythonify with a tensor buffer (1D or higher dimensional).

        Tensor buffers are commonly used for things like positional encodings,
        lookup tables, or fixed masks. This test verifies that tensor buffers
        are correctly captured and used in the generated code.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("bias", torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("obj_from_id", code)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Tensor buffer model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_mixed_params_and_buffers(self):
        """
        Test pythonify with a model containing both parameters and buffers.

        This is a common pattern where trainable parameters are combined with
        non-trainable buffers (e.g., batch norm has weight/bias params plus
        running_mean/running_var buffers).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))
                self.register_buffer("scale", torch.tensor(0.5))
                self.register_buffer("shift", torch.ones(features) * 0.1)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                y = x.matmul(self.W) + self.bias
                return y * self.scale + self.shift

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("obj_from_id", code)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Mixed params and buffers model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_only_model(self):
        """
        Test pythonify with a model that has only buffers and no parameters.

        This tests the edge case where there are no trainable parameters,
        only registered buffers used for computation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("W", torch.randn(features, features))
                self.register_buffer("bias", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("obj_from_id", code)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Buffer-only model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_backward(self):
        """
        Test that backward pass works correctly with buffers.

        Buffers themselves don't require gradients, but they can be part of
        computations that have gradient flow through them. This test verifies
        that the backward pass works correctly when buffers are involved.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)
        x1_data = x1.data.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            original_x_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)

            x2 = x1_data.clone().requires_grad_(True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match with buffer",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(original_x_grad, x2.grad, atol=1e-5),
                f"Gradients should match. Expected {original_x_grad}, got {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_multiple_runs(self):
        """
        Test that exec'd code with buffers works for multiple runs.

        Verifies that the generated code can be executed multiple times
        with different inputs while correctly using the buffers each time.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(1.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x_initial)

            with open(path) as f:
                code = f.read()

            for i in range(3):
                x_test = torch.randn(batch, features)
                y_expected = model(x_test)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["torch"] = torch

                exec(code, namespace)
                y_actual = namespace["y"]

                self.assertTrue(
                    torch.allclose(y_expected, y_actual, atol=1e-5),
                    f"Run {i}: outputs should match with buffer",
                )
        finally:
            os.unlink(path)


class TestPythonifyTritonNamingCollision(TestCase):
    """
    Tests for Triton kernel naming collision fix.

    When both forward and backward Inductor kernels generate Triton functions
    with the same name (e.g., `triton_poi_fused_mul_0`), the backward kernel
    definition overwrites the forward kernel when the generated file is exec'd.

    The fix adds a `_bwd` suffix to Triton function names in backward kernels
    to avoid collisions.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_triton_kernel_names_are_unique(self):
        """
        Verify that forward and backward Triton kernels have unique names.
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            kernel_defs = re.findall(r"^def (triton_\w+)\(", code, re.MULTILINE)
            self.assertGreater(
                len(kernel_defs), 0, "Should have at least one Triton kernel"
            )

            unique_defs = set(kernel_defs)
            self.assertEqual(
                len(kernel_defs),
                len(unique_defs),
                f"Kernel names should be unique, but found duplicates: {kernel_defs}",
            )

            bwd_kernels = [k for k in kernel_defs if k.endswith("_bwd")]
            self.assertGreater(
                len(bwd_kernels),
                0,
                "Backward kernels should have _bwd suffix to avoid collisions",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_model_with_buffer_forward_backward(self):
        """
        End-to-end test for model with buffer, verifying both forward and backward work.
        """
        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss1 = y1.mean()
            loss1.backward()

            compiled_grad = model.W.grad.clone()
            model.W.grad = None

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["f_locals"] = namespace
            namespace["x"] = x

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2, atol=1e-5),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(compiled_grad, model.W.grad, atol=1e-5),
                "Backward gradients should match",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_triton_rename_patterns(self):
        """
        Test that all Triton renaming patterns are applied correctly.

        This test verifies that:
        1. Variable assignments are renamed: `triton_xxx_bwd = async_compile.triton(...)`
        2. String arguments are renamed: `async_compile.triton('triton_xxx_bwd', ...)`
        3. Function definitions are renamed: `def triton_xxx_bwd(...)`
        4. Function calls are renamed: `triton_xxx_bwd.run(...)`
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            bwd_assigns = re.findall(
                r"^(triton_\w+_bwd)\s*=\s*async_compile\.triton\(",
                code,
                re.MULTILINE,
            )
            self.assertGreater(
                len(bwd_assigns), 0, "Should have _bwd variable assignments"
            )

            bwd_string_args = re.findall(
                r"async_compile\.triton\(\s*'(triton_\w+_bwd)'", code
            )
            self.assertGreater(
                len(bwd_string_args), 0, "Should have _bwd string arguments"
            )

            bwd_defs = re.findall(r"^def (triton_\w+_bwd)\(", code, re.MULTILINE)
            self.assertGreater(
                len(bwd_defs), 0, "Should have _bwd function definitions"
            )

            bwd_calls = re.findall(r"\b(triton_\w+_bwd)\.run\(", code)
            self.assertGreater(len(bwd_calls), 0, "Should have _bwd function calls")

        finally:
            os.unlink(path)


if __name__ == "__main__":
    run_tests()
