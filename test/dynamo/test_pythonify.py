# Owner(s): ["module: dynamo"]
"""
End-to-end tests for the pythonify feature of torch.compile.

These tests verify that torch.compile with pythonify=path generates a Python
file that can be exec'd to produce the same results as the compiled model.

The pythonify feature takes all runtime machinery normally embedded inside
torch.compile and makes it explicit as generated Python code that can be
inspected, debugged, and executed.
"""

import inspect
import os
import tempfile
import unittest

import torch
import torch.nn as nn
from torch.testing._internal.common_utils import run_tests, TestCase


class TestPythonifyEndToEnd(TestCase):
    """
    End-to-end tests for pythonify feature.

    These tests verify the complete workflow:
    1. Create a model
    2. Compile with torch.compile(pythonify=path)
    3. Execute the generated Python file via exec()
    4. Verify outputs match the compiled model
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_end_to_end(self):
        """
        Basic end-to-end test for pythonify.

        This test matches the example from the requirements specification:
        create a simple model, compile it with pythonify, exec the generated
        file, and verify outputs match.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "Generated by torch.compile(pythonify=...)",
                code,
                "Expected header comment in generated file",
            )
            self.assertIn(
                "CompiledFunction",
                code,
                "Expected CompiledFunction class in generated file",
            )
            # The golden path uses module-based access (model.W) instead of
            # ctypes-based obj_from_id(). Verify the golden path documentation
            # is present in the header and that model.W access is used.
            self.assertIn(
                "GOLDEN PATH: Module-Based Parameter Access",
                code,
                "Expected golden path documentation header",
            )
            self.assertIn(
                "model.W",
                code,
                "Expected module-based parameter access (model.W) in golden path",
            )

            torch.manual_seed(0)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                f"Forward outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_end_to_end_multiple_runs(self):
        """
        Test that exec'd code can be run multiple times with different inputs.

        The generated code should be reusable with different input tensors
        as long as they have the same shape and dtype.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x_initial)

            with open(path) as f:
                code = f.read()

            for i in range(3):
                x_test = torch.randn(batch, features)
                y_expected = model(x_test)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["model"] = model
                namespace["torch"] = torch

                exec(code, namespace)
                y_actual = namespace["y"]

                self.assertTrue(
                    torch.allclose(y_expected, y_actual),
                    f"Run {i}: outputs should match",
                )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_uses_object_id_for_parameters(self):
        """
        Test that generated code uses module-based attribute access for parameters.

        The pythonify implementation now uses module-based parameter extraction
        (the "golden path") when model context is available. This requires the
        model to be in the exec namespace but provides:
        1. Process-portable code (no memory address dependencies)
        2. Live parameter access (mutations are visible)
        3. Cleaner generated code aligned with typical nn.Module usage

        When model_name is provided (the default), parameters are accessed as
        model.W rather than obj_from_id(...). The object-id fallback is only
        used when module context is unavailable.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))
                self.bias = nn.Parameter(torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # The golden path now uses module-based access instead of obj_from_id.
            # Generated code should contain model.W and model.bias for parameter access.
            self.assertIn("model.W", code)
            self.assertIn("model.bias", code)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with module-based parameter extraction",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_generated_code_is_valid_python(self):
        """
        Test that generated code is syntactically valid Python.

        The generated file should be parseable by Python's compile() function.
        This catches syntax errors before attempting to exec().
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(2, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")
        finally:
            os.unlink(path)


class TestPythonifyDebugPrints(TestCase):
    """
    Tests for the debug print functionality in pythonify code generation.

    The emit_debug_prints flag causes the generated code to include print()
    statements at key points to trace execution flow. This is useful for
    debugging issues like "not enough values to unpack" errors.
    """

    def test_debug_prints_emitted_when_enabled(self):
        """
        Test that debug print statements are included when emit_debug_prints=True.

        When the flag is enabled, the generated code should contain [DEBUG]
        print statements at key points:
        - Argument extraction
        - Forward/backward function entry
        - Compiled function invocation
        - Result exposure
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import (
            generate_python_code,
            PythonCodeGenVisitor,
        )

        ir = RuntimeWrapperIR()
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg1"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(
            result_name="result",
            expose_as="y",
        ))

        code_with_debug = generate_python_code(ir, emit_debug_prints=True)

        self.assertIn('[DEBUG]', code_with_debug)
        self.assertIn('Extracted arg1', code_with_debug)
        self.assertIn('Final result y', code_with_debug)

        code_without_debug = generate_python_code(ir, emit_debug_prints=False)

        self.assertNotIn('[DEBUG]', code_without_debug)

    def test_debug_prints_not_emitted_by_default(self):
        """
        Test that debug prints are NOT emitted by default.

        The default behavior should not include debug prints to keep
        generated code clean and readable.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code

        ir = RuntimeWrapperIR()
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        code = generate_python_code(ir)

        self.assertNotIn('[DEBUG]', code)

    def test_debug_prints_in_forward_method(self):
        """
        Test that debug prints are emitted inside forward() method.

        When an AOT Autograd wrapper is generated with emit_debug_prints=True,
        the forward() method should contain debug prints showing:
        - Function entry with args length and shapes
        - Before calling compiled_fn
        - After receiving result from compiled_fn
        """
        from torch._dynamo.pythonify.ir import (
            AOTAutogradWrapperNode,
            KernelLoadNode,
            KernelType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import (
            PythonCodeGenVisitor,
        )

        ir = RuntimeWrapperIR()
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": False},
        ))
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_backward",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn_backward",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": True},
        ))
        ir.add_node(AOTAutogradWrapperNode(
            class_name="CompiledFunction",
            num_inputs=2,
        ))

        visitor = PythonCodeGenVisitor(emit_debug_prints=True)
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn('CompiledFunction.forward() called with', code)
        self.assertIn('len(args)=', code)
        self.assertIn('Args shapes', code)

    def test_debug_prints_in_backward_method(self):
        """
        Test that debug prints are emitted inside backward() method.

        When backward kernel is present and emit_debug_prints=True,
        the backward() method should contain debug prints.
        """
        from torch._dynamo.pythonify.ir import (
            AOTAutogradWrapperNode,
            KernelLoadNode,
            KernelType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import (
            PythonCodeGenVisitor,
        )

        ir = RuntimeWrapperIR()
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": False},
        ))
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_backward",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn_backward",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": True},
        ))
        ir.add_node(AOTAutogradWrapperNode(
            class_name="CompiledFunction",
            num_inputs=2,
        ))

        visitor = PythonCodeGenVisitor(emit_debug_prints=True)
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn('CompiledFunction.backward() called with', code)
        self.assertIn('len(grad_outputs)=', code)

    def test_debug_prints_valid_python_syntax(self):
        """
        Test that generated code with debug prints is valid Python.

        The debug print statements should not break the syntax of the
        generated code. The code should compile without errors.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code

        ir = RuntimeWrapperIR()
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg1"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(
            result_name="result",
            expose_as="y",
        ))

        code = generate_python_code(ir, emit_debug_prints=True)

        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code with debug prints has syntax error: {e}")


class TestPythonifyModelVariants(TestCase):
    """
    Tests for pythonify with different model architectures.

    Note: Tests for models with nested modules like nn.Linear are not yet
    supported because nested parameter paths (e.g., self.linear.weight) are
    not fully implemented in the object ID extraction path. These tests are
    skipped pending implementation of nested module parameter support.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_simple_parameter_model(self):
        """
        Test pythonify with a model with simple (non-nested) parameters.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(8, 4))
                self.bias = nn.Parameter(torch.zeros(4))

            def forward(self, x):
                return x.matmul(self.weight) + self.bias

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Simple parameter model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_elementwise_ops(self):
        """
        Test pythonify with elementwise operations (add, mul, relu).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x):
                y = x.matmul(self.W)
                return torch.relu(y)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Elementwise ops model outputs should match",
            )
        finally:
            os.unlink(path)


class TestPythonifyBackward(TestCase):
    """
    Tests for backward pass functionality in pythonify.

    These tests verify that loss.backward() works correctly when using
    the exec'd pythonify output. The backward kernel must be compiled
    eagerly (not lazily) so it can be captured in the generated Python file.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_basic(self):
        """
        Basic test that backward pass works via exec'd pythonify output.

        This test verifies that:
        1. The generated code includes backward kernel
        2. Calling loss.backward() on exec'd output computes gradients correctly
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            original_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)
            self.assertIn("compiled_fn_backward", code)

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(original_grad, x2.grad),
                f"Gradients should match. Expected {original_grad}, got {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_forward_output_matches(self):
        """
        Test that forward output from exec'd code matches compiled output.

        This is a simpler backward test that verifies the forward pass
        produces matching results when requires_grad=True (which triggers
        backward kernel compilation).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(99)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)

            with open(path) as f:
                code = f.read()

            torch.manual_seed(99)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match with requires_grad=True",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_backward_kernel_present(self):
        """
        Test that backward kernel is present in generated code.

        Verifies that when compiling with requires_grad=True, the backward
        kernel is eagerly compiled and included in the generated Python file.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(88)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)
            self.assertIn("compiled_fn_backward", code)
            backward_kernel_count = code.count("# AOT ID:")
            self.assertEqual(
                backward_kernel_count,
                2,
                f"Expected 2 AOT IDs (forward + backward), got {backward_kernel_count}",
            )
        finally:
            os.unlink(path)


class TestPythonifyInference(TestCase):
    """
    Tests for inference-only mode in pythonify.

    Verifies that models without requires_grad work correctly and that
    the generated code handles absence of backward kernel gracefully.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_no_requires_grad(self):
        """
        Test that inference mode (requires_grad=False) works correctly.

        When inputs don't require gradients, the backward kernel is not
        compiled. The generated code should work correctly without errors.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "def forward",
                code,
                "Expected forward method to be present",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Forward outputs should match in inference mode",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_with_torch_no_grad(self):
        """
        Test that torch.no_grad() context works correctly.

        Even if the model has parameters that could have gradients,
        when executed within torch.no_grad(), no backward kernel should
        be needed.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            with torch.no_grad():
                y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            with torch.no_grad():
                exec(code, namespace)
                y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Forward outputs should match with no_grad",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_multiple_runs(self):
        """
        Test that even if backward is not available, forward still works.

        This verifies that the absence of backward kernel doesn't break
        forward pass execution, even when run multiple times.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            for _ in range(3):
                x_test = torch.randn(batch, features, requires_grad=False)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["model"] = model
                namespace["torch"] = torch

                exec(code, namespace)
                y_test = namespace["y"]

                self.assertEqual(y_test.shape, (batch, features))
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_inference_backward_raises_clear_error(self):
        """
        Test that backward on inference-compiled model raises clear error.

        When a model is compiled in inference mode (e.g., with torch.no_grad()),
        there's no backward kernel. If the user later tries to call backward
        with requires_grad inputs, they should get a clear error message
        explaining the issue, not a generic PyTorch NotImplementedError.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            with torch.no_grad():
                torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "def backward",
                code,
                "Expected backward method to be present for fallback",
            )
            self.assertIn(
                "Backward pass not available",
                code,
                "Expected fallback error message in generated code",
            )

            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertEqual(y2.shape, (batch, features))
            self.assertTrue(y2.requires_grad)

            loss = y2.mean()
            with self.assertRaises(RuntimeError) as context:
                loss.backward()

            error_msg = str(context.exception)
            self.assertIn(
                "Backward pass not available",
                error_msg,
                "Expected clear error message about missing backward kernel",
            )
            self.assertIn(
                "inference mode",
                error_msg,
                "Expected error message to mention inference mode",
            )

        finally:
            os.unlink(path)


class TestPythonifyCodeQuality(TestCase):
    """
    Tests for the quality and structure of generated code.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_section_headers(self):
        """
        Test that generated code has clear section headers.

        The generated code should be well-organized with section headers
        like "Argument Extraction", "Guard Evaluation", etc.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x):
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("Argument Extraction", code)
            self.assertIn("Guard Evaluation", code)
            self.assertIn("AOT Autograd Function Definition", code)
            self.assertIn("Invoke Compiled Callable", code)
            self.assertIn("Expose Result", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_inductor_kernel(self):
        """
        Test that generated code includes Inductor kernel source.

        When Inductor backend is used, the generated code should include
        the Inductor-generated kernel code with the 'call' function.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(8, 4))

            def forward(self, x):
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("Inductor Compiled Kernel", code)
            self.assertIn("def call(", code)
            self.assertIn("compiled_fn = call", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_generated_code_has_object_id_warning(self):
        """
        Test that generated code includes a prominent warning about object ID limitations.

        When object IDs are used for parameter/buffer access, the generated file
        should contain a clear warning explaining that the file is process-local
        and should not be saved for later use.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(8, 4))

            def forward(self, x):
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model()
        x = torch.randn(2, 8)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify the warning header is present
            self.assertIn(
                "WARNING: PROCESS-LOCAL FILE - DO NOT SAVE OR REUSE",
                code,
                "Expected prominent warning about process-local nature",
            )
            self.assertIn(
                "NOT PERSISTABLE",
                code,
                "Expected warning about non-persistability",
            )
            self.assertIn(
                "SAME-PROCESS ONLY",
                code,
                "Expected warning about same-process execution",
            )
            self.assertIn(
                "KEEP MODEL ALIVE",
                code,
                "Expected warning about keeping model alive",
            )
            self.assertIn(
                "CRASH RISK",
                code,
                "Expected warning about crash risk from invalid object IDs",
            )
            self.assertIn(
                "object IDs (memory addresses)",
                code,
                "Expected explanation of what object IDs are",
            )
            self.assertIn(
                "torch.export()",
                code,
                "Expected mention of torch.export() as alternative",
            )
        finally:
            os.unlink(path)


class TestPythonifyNestedModules(TestCase):
    """
    Tests for pythonify with models containing nested modules.

    These tests verify that parameter extraction works correctly for models
    with nested nn.Module structures. The object ID extraction must traverse
    the nested module hierarchy to capture the correct tensor references.

    NOTE: Currently pythonify assumes a fixed argument ordering (inputs first,
    then parameters, then buffers). This works for models where the input is
    the first operand in operations. Models like nn.Linear (where weight is
    used first internally) may not work correctly with the current implementation.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_simple_param_model(self):
        """
        Test pythonify with a simple model using a direct parameter.

        This tests the basic case where a parameter is directly on the model
        (not in a nested submodule) and is used as the second operand in matmul.
        With the golden path, parameters are accessed via model.W instead of obj_from_id.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify generated code uses golden path (module-based access)
            self.assertIn("model.W", code)
            self.assertIn("GOLDEN PATH", code)

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Simple param model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_model_with_add_and_matmul(self):
        """
        Test pythonify with model using x + x.matmul(W) pattern.

        This is the canonical example from the pythonify requirements.
        The input x is used as the first operand in both the add and matmul.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Model with add and matmul outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_model_with_multiple_params(self):
        """
        Test pythonify with model having multiple parameters used in order.

        Both parameters are used after the input, so the ordering is
        (input, param1, param2).

        The pythonify implementation now uses module-based parameter extraction
        (the "golden path") when model context is available. Parameters are
        accessed as model.W1 and model.W2 rather than obj_from_id(...).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W1).matmul(self.W2)

        torch.manual_seed(456)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # The golden path now uses module-based access instead of obj_from_id.
            # Generated code should contain model.W1 and model.W2 for parameter access.
            self.assertIn("model.W1", code)
            self.assertIn("model.W2", code)

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Multiple param model outputs should match with module-based extraction",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_code_uses_module_based_access(self):
        """
        Test that generated code uses the golden path (module-based access).

        The golden path accesses parameters via model attributes (model.weight)
        instead of ctypes-based obj_from_id. This is the preferred approach for
        nn.Module pythonify as it's process-portable and uses live values.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight) + self.bias

        torch.manual_seed(789)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path documentation header is present
            self.assertIn(
                "GOLDEN PATH: Module-Based Parameter Access",
                code,
                "Expected golden path documentation in generated code",
            )

            # Verify module-based access is used for parameters
            self.assertIn("model.weight", code)
            self.assertIn("model.bias", code)

            # Verify obj_from_id is NOT used (golden path doesn't need it)
            self.assertNotIn("def obj_from_id(obj_id):", code)

            # Execute the generated code WITH model in namespace
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Module-based access should work with model in scope",
            )
        finally:
            os.unlink(path)


class TestPythonifyGoldenPath(TestCase):
    """
    Tests for the golden path of nn.Module pythonify: module-based parameter extraction.

    The golden path accesses parameters via model attributes (e.g., model.W, model.layer.weight)
    instead of ctypes-based obj_from_id(). This approach is:
    - Process-portable: No memory address dependencies
    - Live access: Parameter mutations after compilation are visible
    - Safer: No ctypes or memory manipulation
    - Natural: Aligns with typical nn.Module usage patterns

    These tests comprehensively verify that:
    1. Generated code uses model.X patterns for parameter access
    2. Generated code does NOT use ctypes or obj_from_id
    3. The model must be in the exec namespace
    4. Execution produces correct results matching the compiled model
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_basic_parameter_extraction(self):
        """
        Test that golden path extracts parameters via model attributes.

        The generated code should access parameters as model.W, model.bias, etc.
        instead of using obj_from_id(). This is the fundamental behavior of the
        golden path.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code:\n{code[:2000]}...")

            # Verify golden path patterns are present
            self.assertIn(
                "model.W",
                code,
                "Expected model.W attribute access in golden path",
            )
            self.assertIn(
                "model.bias",
                code,
                "Expected model.bias attribute access in golden path",
            )

            # Verify obj_from_id is NOT used (golden path should not need it)
            # Note: ctypes may still appear in Inductor kernel boilerplate,
            # but the parameter extraction path should not use obj_from_id.
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT generate obj_from_id helper",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Golden path should NOT call obj_from_id",
            )

            # Verify documentation header
            self.assertIn(
                "GOLDEN PATH",
                code,
                "Expected golden path documentation in generated code",
            )

            # Execute with model in namespace
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                f"Exec'd output should match compiled output. "
                f"Expected {y_compiled}, got {y_exec}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_requires_model_in_namespace(self):
        """
        Test that golden path requires model to be in exec namespace.

        Since the generated code accesses model.W directly, the model must
        be present in the namespace when exec() is called. Without it,
        a NameError should be raised.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path is used
            self.assertIn("model.W", code)

            # Try executing WITHOUT model in namespace - should fail
            namespace = {"x": x, "torch": torch}

            with self.assertRaises(NameError) as ctx:
                exec(code, namespace)

            # The error should mention 'model' is not defined
            self.assertIn(
                "model",
                str(ctx.exception),
                "NameError should mention 'model' is not defined",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_multiple_parameters(self):
        """
        Test golden path with multiple parameters.

        The generated code should use model.X access for all parameters,
        not just one. This verifies the path works for models with
        several parameters.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.W3 = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                y = x.matmul(self.W1)
                y = y.matmul(self.W2)
                y = y.matmul(self.W3)
                return y + self.bias

        torch.manual_seed(456)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify all parameters use module-based access
            self.assertIn("model.W1", code)
            self.assertIn("model.W2", code)
            self.assertIn("model.W3", code)
            self.assertIn("model.bias", code)

            # Verify obj_from_id fallback is not used
            # Note: ctypes may appear in Inductor kernel boilerplate
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Multiple parameters should work correctly with golden path",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_with_buffer(self):
        """
        Test golden path with a registered buffer.

        Buffers should also be accessed via model.buffer_name, not obj_from_id.
        This verifies the golden path works for both parameters and buffers.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(789)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify parameter uses module-based access
            self.assertIn("model.W", code)

            # Verify buffer uses module-based access
            self.assertIn("model.scale", code)

            # Verify obj_from_id fallback is not used
            # Note: ctypes may appear in Inductor kernel boilerplate
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Buffer should work correctly with golden path",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_documentation_header(self):
        """
        Test that golden path includes proper documentation in generated code.

        The generated code should include a header section explaining:
        - That parameters are accessed via model attributes
        - That the model must be in the exec namespace
        - Advantages of the golden path approach
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(111)
        features = 4
        model = Model(features)
        x = torch.randn(2, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path documentation is present
            self.assertIn(
                "GOLDEN PATH: Module-Based Parameter Access",
                code,
                "Expected golden path header",
            )

            # Verify documentation mentions key benefits
            self.assertIn(
                "process-portable",
                code.lower(),
                "Expected mention of process-portability",
            )

            # Verify documentation mentions model must be in scope
            self.assertIn(
                "model",
                code,
                "Expected mention of model in documentation",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_output_correctness(self):
        """
        Test that golden path produces numerically correct outputs.

        This test uses a more complex forward pass to verify that
        the golden path correctly handles all intermediate computations
        when accessing parameters via model attributes.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.randn(features))
                self.register_buffer("scale", torch.tensor(0.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                y = x.matmul(self.W)
                y = y + self.bias
                y = y * self.scale
                return torch.relu(y)

        torch.manual_seed(222)
        features = 8
        batch = 4
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Execute with model in namespace
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            # Verify exact match (not just allclose)
            self.assertTrue(
                torch.equal(y_compiled, y_exec),
                f"Exec'd output should exactly match compiled output. "
                f"Max diff: {(y_compiled - y_exec).abs().max().item()}",
            )

            # Also verify against eager mode
            y_eager = model(x)
            self.assertTrue(
                torch.allclose(y_eager, y_exec, rtol=1e-4, atol=1e-4),
                f"Exec'd output should match eager mode. "
                f"Max diff: {(y_eager - y_exec).abs().max().item()}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_live_attribute_access(self):
        """
        Test that golden path uses live attribute access, not stale cached IDs.

        This is the key differentiator between the golden path and the object-id
        fallback approach. With object-id (ctypes) approach, parameter values are
        looked up by memory address at compile time, so mutations after compilation
        are NOT visible. With the golden path (module-based access), the code
        accesses model.W at runtime, so mutations ARE visible.

        This test proves live access by:
        1. Compiling a model with pythonify
        2. Mutating parameter values AFTER compilation
        3. Executing the generated code
        4. Verifying the output uses the MUTATED values, not the original values
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(999)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            # Step 1: Compile the model and generate pythonify code
            # Record the output with ORIGINAL parameter values
            y_with_original_params = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code snippet:\n{code[:1500]}...")

            # Verify we're using golden path (model.X access, not obj_from_id)
            self.assertIn(
                "model.W",
                code,
                "Golden path should use model.W attribute access",
            )
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Golden path should NOT call obj_from_id",
            )

            # Step 2: MUTATE the parameter values AFTER compilation
            # This is the critical step - if the code uses stale IDs, it would
            # still see the old values. If it uses live attribute access, it
            # will see these new values.
            original_W = model.W.data.clone()
            original_bias = model.bias.data.clone()

            # Set W to all ones and bias to all twos
            model.W.data.fill_(1.0)
            model.bias.data.fill_(2.0)

            print(f"[DEBUG] Original W[0,0]: {original_W[0, 0].item():.4f}")
            print(f"[DEBUG] Mutated W[0,0]: {model.W.data[0, 0].item():.4f}")
            print(f"[DEBUG] Original bias[0]: {original_bias[0].item():.4f}")
            print(f"[DEBUG] Mutated bias[0]: {model.bias.data[0].item():.4f}")

            # Step 3: Compute expected output with MUTATED values
            # This is what we expect if live attribute access is working
            with torch.no_grad():
                y_expected_with_mutated = x.matmul(model.W) + model.bias

            # Step 4: Execute the generated code with the MUTATED model
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model  # This model has MUTATED parameters
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            print(f"[DEBUG] y_with_original_params[0]: {y_with_original_params[0]}")
            print(f"[DEBUG] y_expected_with_mutated[0]: {y_expected_with_mutated[0]}")
            print(f"[DEBUG] y_exec[0]: {y_exec[0]}")

            # Step 5: Verify the exec'd code uses MUTATED values
            # If live access works, y_exec should match y_expected_with_mutated
            # If stale IDs were used, y_exec would match y_with_original_params
            self.assertTrue(
                torch.allclose(y_exec, y_expected_with_mutated, rtol=1e-4, atol=1e-4),
                f"Exec'd output should use MUTATED parameter values (live access). "
                f"Expected {y_expected_with_mutated}, got {y_exec}. "
                f"Max diff: {(y_exec - y_expected_with_mutated).abs().max().item()}",
            )

            # Also verify it does NOT match the original compiled output
            # (since we mutated the parameters after compilation)
            self.assertFalse(
                torch.allclose(y_exec, y_with_original_params, rtol=1e-4, atol=1e-4),
                f"Exec'd output should NOT match original compiled output "
                f"(parameters were mutated). This proves live access is working. "
                f"Original: {y_with_original_params}, exec'd: {y_exec}",
            )

            print("[DEBUG] SUCCESS: Live attribute access verified!")
            print("[DEBUG] The generated code correctly sees mutated parameter values,")
            print("[DEBUG] proving it uses model.W at runtime, not cached object IDs.")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_live_access_with_buffer(self):
        """
        Test that buffers also use live attribute access in the golden path.

        Similar to test_golden_path_live_attribute_access, but specifically
        tests buffers (registered via register_buffer) to ensure they also
        benefit from live attribute access.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(1.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(888)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            # Compile with original scale=1.0
            y_with_scale_1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path is used
            self.assertIn("model.W", code)
            self.assertIn("model.scale", code)
            self.assertNotIn("def obj_from_id", code)

            # MUTATE the buffer after compilation
            model.scale.fill_(3.0)

            print(f"[DEBUG] Mutated scale to: {model.scale.item()}")

            # Compute expected output with mutated scale
            with torch.no_grad():
                y_expected_with_scale_3 = x.matmul(model.W) * model.scale

            # Execute generated code with mutated model
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            # Verify exec'd code sees the MUTATED buffer value
            self.assertTrue(
                torch.allclose(y_exec, y_expected_with_scale_3, rtol=1e-4, atol=1e-4),
                f"Exec'd output should use MUTATED buffer value (scale=3.0). "
                f"Expected {y_expected_with_scale_3}, got {y_exec}",
            )

            # Verify it does NOT match the original output (scale=1.0)
            self.assertFalse(
                torch.allclose(y_exec, y_with_scale_1, rtol=1e-4, atol=1e-4),
                f"Exec'd output should NOT match original (scale=1.0). "
                f"This proves live buffer access is working.",
            )

            print("[DEBUG] SUCCESS: Live buffer access verified!")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_no_ctypes_import(self):
        """
        Test that golden path does NOT import ctypes for pure nn.Module parameter access.

        This is a code hygiene check. When the golden path (module-based parameter access)
        is fully used, the generated code should NOT contain:
        1. `import ctypes` - not needed when accessing model.X attributes
        2. `def obj_from_id` - the legacy fallback helper
        3. `obj_from_id(` - any calls to the fallback helper

        The ctypes import is specifically used for the obj_from_id helper which
        converts memory addresses back to Python objects. The golden path eliminates
        the need for this by accessing parameters directly via model attributes.

        Note: Some Inductor kernel loading paths may still use ctypes (for C++ kernels
        or binary inline kernels), but simple Triton-based models should not require it.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(333)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code length: {len(code)} chars")
            print(f"[DEBUG] Code preview:\n{code[:2000]}...")

            # Primary verification: golden path should NOT use obj_from_id
            # (which would require ctypes)
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT define obj_from_id helper. "
                "If present, the code is using the legacy ctypes-based fallback.",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Golden path should NOT call obj_from_id. "
                "Parameters should be accessed via model.X attributes.",
            )

            # Secondary verification: verify golden path patterns ARE present
            # (this confirms we're testing the right path)
            self.assertIn(
                "model.W",
                code,
                "Expected model.W attribute access (golden path pattern)",
            )
            self.assertIn(
                "model.bias",
                code,
                "Expected model.bias attribute access (golden path pattern)",
            )

            # Code hygiene check: for simple Triton-based models,
            # ctypes should not be imported at all. The ctypes import is added
            # specifically for the obj_from_id helper. If ctypes IS imported
            # but obj_from_id is NOT present, it's likely from kernel loading
            # (which is acceptable for this test).
            #
            # However, if both ctypes and obj_from_id are absent, we have
            # verified full golden path hygiene.
            if "import ctypes" in code:
                # ctypes is imported - verify it's NOT for obj_from_id
                # (since we already checked obj_from_id is absent above)
                print(
                    "[DEBUG] Note: ctypes is imported but obj_from_id is absent."
                )
                print(
                    "[DEBUG] This is likely for Inductor kernel loading, not parameter access."
                )
            else:
                # Best case: no ctypes at all - fully clean golden path
                print("[DEBUG] EXCELLENT: No ctypes import in generated code!")
                print(
                    "[DEBUG] The golden path completely eliminates ctypes dependency "
                    "for parameter access."
                )

            # Verify the code is still functional
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-4, atol=1e-4),
                f"Generated code should produce correct output. "
                f"Expected {y_compiled}, got {y_exec}",
            )

            print("[DEBUG] SUCCESS: Golden path verified with no ctypes for parameter access!")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_golden_path_no_ctypes_complex_model(self):
        """
        Test that golden path avoids ctypes for a more complex model with buffers.

        This extends the basic ctypes hygiene test with:
        - Multiple parameters at different levels of complexity
        - A registered buffer
        - More complex forward pass operations

        The goal is to verify the golden path remains clean even with more
        intricate model structures.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear1_W = nn.Parameter(torch.randn(features, features))
                self.linear1_bias = nn.Parameter(torch.zeros(features))
                self.linear2_W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(0.5))
                self.register_buffer("shift", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                y = x.matmul(self.linear1_W) + self.linear1_bias
                y = torch.relu(y)
                y = y.matmul(self.linear2_W)
                y = y * self.scale + self.shift
                return y

        torch.manual_seed(444)
        features = 8
        batch = 4
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify NO obj_from_id (ctypes-based fallback) is used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Complex model golden path should NOT define obj_from_id",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Complex model golden path should NOT call obj_from_id",
            )

            # Verify all parameters and buffers use golden path
            for attr in ["linear1_W", "linear1_bias", "linear2_W", "scale", "shift"]:
                self.assertIn(
                    f"model.{attr}",
                    code,
                    f"Expected model.{attr} in golden path code",
                )

            # Verify golden path documentation is present
            self.assertIn(
                "GOLDEN PATH",
                code,
                "Expected golden path documentation header",
            )

            # Verify execution correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-4, atol=1e-4),
                "Complex model golden path should produce correct output",
            )

            print("[DEBUG] SUCCESS: Complex model golden path verified!")

        finally:
            os.unlink(path)


class TestNestedModuleHierarchies(TestCase):
    """
    Tests for nested module hierarchies with the golden path.

    These tests specifically verify that the nested_path field is correctly
    populated and used for deeply nested module structures. The golden path
    should generate code like `model.encoder.layer1.weight` for parameters
    accessed via `self.encoder.layer1.weight`.

    This is distinct from TestNestedModulesSequential which tests nn.Sequential
    and similar patterns. This class focuses on:
    1. Explicit verification of nested_path population in the IR
    2. Deeply nested hierarchies (3+ levels)
    3. Named submodules (not just indexed containers)
    4. Mixed parameters and buffers at different nesting levels
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_two_levels(self):
        """
        Test nested_path for two-level module hierarchy.

        Verifies that `self.encoder.weight` generates `model.encoder.weight`.
        This is the simplest nested case beyond flat modules.
        """
        torch.set_default_device("cuda")

        class Encoder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Encoder(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.encoder(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2000 chars):\n{code[:2000]}...")

            # Verify nested path pattern is used
            # The generated code should access model.encoder.weight
            self.assertIn(
                "model.encoder.weight",
                code,
                "Expected model.encoder.weight for two-level nested parameter",
            )

            # Verify golden path is used (no obj_from_id)
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT generate obj_from_id helper",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Golden path should NOT call obj_from_id",
            )

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                f"Two-level nested path should produce correct output. "
                f"Expected {y_compiled}, got {y_exec}",
            )

            print("[DEBUG] SUCCESS: Two-level nested path verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_three_levels(self):
        """
        Test nested_path for three-level module hierarchy.

        Verifies that `self.encoder.layer1.weight` generates
        `model.encoder.layer1.weight`. This tests deeper nesting.
        """
        torch.set_default_device("cuda")

        class Layer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Encoder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layer1 = Layer(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.layer1(x)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Encoder(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.encoder(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2000 chars):\n{code[:2000]}...")

            # Verify three-level nested path pattern is used
            self.assertIn(
                "model.encoder.layer1.weight",
                code,
                "Expected model.encoder.layer1.weight for three-level nested parameter",
            )

            # Verify golden path is used
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Three-level nested path should produce correct output",
            )

            print("[DEBUG] SUCCESS: Three-level nested path verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_four_levels(self):
        """
        Test nested_path for four-level module hierarchy (very deep).

        Verifies that `self.model.encoder.layer1.weight` generates
        `model.model.encoder.layer1.weight`. This tests very deep nesting.
        """
        torch.set_default_device("cuda")

        class Transform(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Layer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.transform = Transform(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.transform(x)

        class Encoder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layer1 = Layer(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.layer1(x)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Encoder(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.encoder(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2500 chars):\n{code[:2500]}...")

            # Verify four-level nested path pattern is used
            self.assertIn(
                "model.encoder.layer1.transform.weight",
                code,
                "Expected model.encoder.layer1.transform.weight for four-level nested parameter",
            )

            # Verify golden path is used
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Four-level nested path should produce correct output",
            )

            print("[DEBUG] SUCCESS: Four-level nested path verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_multiple_branches(self):
        """
        Test nested_path with multiple parallel branches at different levels.

        Verifies that when a model has multiple nested submodules (e.g.,
        encoder and decoder), each branch generates the correct path.
        """
        torch.set_default_device("cuda")

        class Submodule(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Submodule(features)
                self.decoder = Submodule(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                encoded = self.encoder(x)
                decoded = self.decoder(encoded)
                return decoded

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2000 chars):\n{code[:2000]}...")

            # Verify both branches have correct nested paths
            self.assertIn(
                "model.encoder.weight",
                code,
                "Expected model.encoder.weight for encoder branch",
            )
            self.assertIn(
                "model.decoder.weight",
                code,
                "Expected model.decoder.weight for decoder branch",
            )

            # Verify golden path is used
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Multiple branches should produce correct output",
            )

            print("[DEBUG] SUCCESS: Multiple branches nested path verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_with_buffers(self):
        """
        Test nested_path for buffers at different nesting levels.

        Verifies that nested buffers (e.g., `self.encoder.scale`) generate
        `model.encoder.scale` in the generated code.
        """
        torch.set_default_device("cuda")

        class Layer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight) * self.scale

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layer = Layer(features)
                self.register_buffer("shift", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.layer(x) + self.shift

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2000 chars):\n{code[:2000]}...")

            # Verify nested parameter path
            self.assertIn(
                "model.layer.weight",
                code,
                "Expected model.layer.weight for nested parameter",
            )

            # Verify nested buffer path
            self.assertIn(
                "model.layer.scale",
                code,
                "Expected model.layer.scale for nested buffer",
            )

            # Verify top-level buffer path
            self.assertIn(
                "model.shift",
                code,
                "Expected model.shift for top-level buffer",
            )

            # Verify golden path is used
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec),
                "Nested buffers should produce correct output",
            )

            print("[DEBUG] SUCCESS: Nested buffers path verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_live_access(self):
        """
        Test that nested paths support live attribute access.

        Mutates a deeply nested parameter after compilation and verifies
        the executed code sees the mutated value (proving live access,
        not stale object IDs).
        """
        torch.set_default_device("cuda")

        class Inner(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Outer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.inner = Inner(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.inner(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Outer(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            # Compile and get original output
            y_original = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify nested path is used
            self.assertIn(
                "model.inner.weight",
                code,
                "Expected model.inner.weight for nested path",
            )

            # MUTATE the nested parameter AFTER compilation
            with torch.no_grad():
                model.inner.weight.fill_(0.5)

            # Execute with mutated model
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            # Calculate expected output with mutated weight
            y_expected = x.matmul(model.inner.weight)

            # Verify exec'd code sees the MUTATED value
            self.assertTrue(
                torch.allclose(y_expected, y_exec),
                "Exec'd code should see mutated nested parameter value",
            )

            # Verify it's different from the original output
            self.assertFalse(
                torch.allclose(y_original, y_exec),
                "Mutated output should differ from original (proving live access)",
            )

            print("[DEBUG] SUCCESS: Nested path live access verified!")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_path_complex_hierarchy(self):
        """
        Test nested_path with a complex model hierarchy.

        This tests a realistic model structure similar to transformers:
        - encoder with multiple layers
        - each layer has attention and feedforward components
        - each component has its own parameters
        """
        torch.set_default_device("cuda")

        class Attention(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.query_weight = nn.Parameter(torch.randn(features, features))
                self.key_weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                q = x.matmul(self.query_weight)
                k = x.matmul(self.key_weight)
                return q + k

        class FeedForward(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(0.1))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight) * self.scale

        class Layer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.attention = Attention(features)
                self.ff = FeedForward(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                x = x + self.attention(x)
                x = x + self.ff(x)
                return x

        class Encoder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layer0 = Layer(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.layer0(x)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Encoder(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.encoder(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 3000 chars):\n{code[:3000]}...")

            # Verify all deeply nested paths are correct
            expected_paths = [
                "model.encoder.layer0.attention.query_weight",
                "model.encoder.layer0.attention.key_weight",
                "model.encoder.layer0.ff.weight",
                "model.encoder.layer0.ff.scale",
            ]

            for expected_path in expected_paths:
                self.assertIn(
                    expected_path,
                    code,
                    f"Expected {expected_path} in complex hierarchy",
                )

            # Verify golden path is used
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-4, atol=1e-4),
                "Complex hierarchy should produce correct output",
            )

            print("[DEBUG] SUCCESS: Complex hierarchy nested paths verified!")
        finally:
            os.unlink(path)


class TestPythonifyBuffers(TestCase):
    """
    Tests for pythonify with models that use register_buffer().

    Buffers are like parameters but not trained. They are saved/loaded with
    the model state_dict and should be accessed via module-based attribute
    traversal using the golden path (e.g., model.buffer_name). This test class
    verifies that various buffer configurations work correctly with pythonify
    using the golden path approach.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_scalar(self):
        """
        Test pythonify with a scalar buffer (0-dimensional tensor).

        Scalar buffers are commonly used for things like running mean/variance
        in batch normalization or scale factors. This test verifies that scalar
        buffers are correctly captured and used in the generated code.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns are used
            self.assertIn(
                "model.W",
                code,
                "Expected model.W access for parameter (golden path)",
            )
            self.assertIn(
                "model.scale",
                code,
                "Expected model.scale access for buffer (golden path)",
            )
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Scalar buffer model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_tensor(self):
        """
        Test pythonify with a tensor buffer (1D or higher dimensional).

        Tensor buffers are commonly used for things like positional encodings,
        lookup tables, or fixed masks. This test verifies that tensor buffers
        are correctly captured and used in the generated code.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("bias", torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns are used
            self.assertIn("model.W", code, "Expected model.W access (golden path)")
            self.assertIn("model.bias", code, "Expected model.bias access (golden path)")
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Tensor buffer model outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_mixed_params_and_buffers(self):
        """
        Test pythonify with a model containing both parameters and buffers.

        This is a common pattern where trainable parameters are combined with
        non-trainable buffers (e.g., batch norm has weight/bias params plus
        running_mean/running_var buffers).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))
                self.register_buffer("scale", torch.tensor(0.5))
                self.register_buffer("shift", torch.ones(features) * 0.1)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                y = x.matmul(self.W) + self.bias
                return y * self.scale + self.shift

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns for all params and buffers
            self.assertIn("model.W", code, "Expected model.W access (golden path)")
            self.assertIn("model.bias", code, "Expected model.bias access (golden path)")
            self.assertIn("model.scale", code, "Expected model.scale access (golden path)")
            self.assertIn("model.shift", code, "Expected model.shift access (golden path)")
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Mixed params and buffers model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_only_model(self):
        """
        Test pythonify with a model that has only buffers and no parameters.

        This tests the edge case where there are no trainable parameters,
        only registered buffers used for computation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("W", torch.randn(features, features))
                self.register_buffer("bias", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns for buffers
            self.assertIn("model.W", code, "Expected model.W access for buffer (golden path)")
            self.assertIn("model.bias", code, "Expected model.bias access for buffer (golden path)")
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Buffer-only model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_backward(self):
        """
        Test that backward pass works correctly with buffers.

        Buffers themselves don't require gradients, but they can be part of
        computations that have gradient flow through them. This test verifies
        that the backward pass works correctly when buffers are involved.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)
        x1_data = x1.data.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            original_x_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("def backward", code)

            x2 = x1_data.clone().requires_grad_(True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match with buffer",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(original_x_grad, x2.grad, atol=1e-5),
                f"Gradients should match. Expected {original_x_grad}, got {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_buffer_multiple_runs(self):
        """
        Test that exec'd code with buffers works for multiple runs.

        Verifies that the generated code can be executed multiple times
        with different inputs while correctly using the buffers each time.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(1.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x_initial)

            with open(path) as f:
                code = f.read()

            for i in range(3):
                x_test = torch.randn(batch, features)
                y_expected = model(x_test)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_test
                namespace["torch"] = torch

                exec(code, namespace)
                y_actual = namespace["y"]

                self.assertTrue(
                    torch.allclose(y_expected, y_actual, atol=1e-5),
                    f"Run {i}: outputs should match with buffer",
                )
        finally:
            os.unlink(path)


class TestPythonifyTritonNamingCollision(TestCase):
    """
    Tests for Triton kernel naming collision fix.

    When both forward and backward Inductor kernels generate Triton functions
    with the same name (e.g., `triton_poi_fused_mul_0`), the backward kernel
    definition overwrites the forward kernel when the generated file is exec'd.

    The fix adds a `_bwd` suffix to Triton function names in backward kernels
    to avoid collisions.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_triton_kernel_names_are_unique(self):
        """
        Verify that forward and backward Triton kernels have unique names.
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            kernel_defs = re.findall(r"^def (triton_\w+)\(", code, re.MULTILINE)
            self.assertGreater(
                len(kernel_defs), 0, "Should have at least one Triton kernel"
            )

            unique_defs = set(kernel_defs)
            self.assertEqual(
                len(kernel_defs),
                len(unique_defs),
                f"Kernel names should be unique, but found duplicates: {kernel_defs}",
            )

            bwd_kernels = [k for k in kernel_defs if k.endswith("_bwd")]
            self.assertGreater(
                len(bwd_kernels),
                0,
                "Backward kernels should have _bwd suffix to avoid collisions",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_model_with_buffer_forward_backward(self):
        """
        End-to-end test for model with buffer, verifying both forward and backward work.
        """
        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss1 = y1.mean()
            loss1.backward()

            compiled_grad = model.W.grad.clone()
            model.W.grad = None

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["f_locals"] = namespace
            namespace["x"] = x

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2, atol=1e-5),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(compiled_grad, model.W.grad, atol=1e-5),
                "Backward gradients should match",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_triton_rename_patterns(self):
        """
        Test that all Triton renaming patterns are applied correctly.

        This test verifies that:
        1. Variable assignments are renamed: `triton_xxx_bwd = async_compile.triton(...)`
        2. String arguments are renamed: `async_compile.triton('triton_xxx_bwd', ...)`
        3. Function definitions are renamed: `def triton_xxx_bwd(...)`
        4. Function calls are renamed: `triton_xxx_bwd.run(...)`
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)
            loss = y1.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            bwd_assigns = re.findall(
                r"^(triton_\w+_bwd)\s*=\s*async_compile\.triton\(",
                code,
                re.MULTILINE,
            )
            self.assertGreater(
                len(bwd_assigns), 0, "Should have _bwd variable assignments"
            )

            bwd_string_args = re.findall(
                r"async_compile\.triton\(\s*'(triton_\w+_bwd)'", code
            )
            self.assertGreater(
                len(bwd_string_args), 0, "Should have _bwd string arguments"
            )

            bwd_defs = re.findall(r"^def (triton_\w+_bwd)\(", code, re.MULTILINE)
            self.assertGreater(
                len(bwd_defs), 0, "Should have _bwd function definitions"
            )

            bwd_calls = re.findall(r"\b(triton_\w+_bwd)\.run\(", code)
            self.assertGreater(len(bwd_calls), 0, "Should have _bwd function calls")

        finally:
            os.unlink(path)


class TestInductorCallingConvention(TestCase):
    """
    Tests that verify the Inductor kernel calling convention expectations.

    This test class documents and verifies the calling convention that Inductor-
    generated kernels expect. The pythonify feature must generate invocation
    code that matches these expectations.

    Inductor Kernel Calling Convention:
    ===================================

    1. **Function Signature**: `def call(args):` or `def call(self, args):`
       - Takes an `args` parameter which is a list of tensors
       - NOT `def call(*args)` or `def call(arg1, arg2)`
       - The `self` parameter is present when using graph partitioning

    2. **Argument Unpacking**: `primals_1, primals_2, ... = args`
       - Arguments are unpacked from the list at the start of the function
       - Names follow the pattern: primals_1, primals_2, etc.
       - The number of args depends on inputs + parameters + buffers

    3. **Argument Order**: Determined by graph_input_names from the FX graph
       - Order is: user inputs, then parameters, then buffers
       - This must match the order pythonify uses in ArgumentExtractionNodes

    4. **Return Value**: Returns a tuple
       - For forward pass: (output, saved_tensors...)
       - The first element is the user-visible output
       - Additional elements are tensors saved for backward

    5. **args.clear()**: Inductor clears the args list after unpacking
       - This is for memory efficiency
       - The list is modified in-place

    Example generated Inductor code:
        def call(args):
            primals_1, primals_2 = args
            args.clear()
            assert_size_stride(primals_1, (3, 4), (4, 1))
            assert_size_stride(primals_2, (4, 4), (4, 1))
            # ... kernel execution ...
            return (buf0, buf1)  # output + saved tensors
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inductor_call_signature(self):
        """
        Verify that Inductor generates `def call(args):` or `def call(self, args):` signature.

        The pythonify code must invoke compiled_fn(list_of_args), NOT
        compiled_fn(arg1, arg2, ...).

        Note: When using graph partitioning, the signature is `def call(self, args):`.
        Otherwise it's `def call(args):`. Both take args as a list.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            call_signatures = re.findall(r"def call\(([^)]+)\):", code)
            self.assertGreater(
                len(call_signatures),
                0,
                "Expected to find 'def call(...):' in generated code",
            )

            for sig in call_signatures:
                sig_stripped = sig.strip()
                is_valid_sig = sig_stripped in ("args", "self, args")
                self.assertTrue(
                    is_valid_sig,
                    f"Expected 'def call(args):' or 'def call(self, args):' signature, "
                    f"got 'def call({sig_stripped}):'",
                )
                self.assertIn(
                    "args",
                    sig_stripped,
                    "Expected 'args' parameter in call signature",
                )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inductor_argument_unpacking(self):
        """
        Verify that Inductor unpacks arguments with `primals_N, ... = args`.

        The pythonify code must pass arguments in the correct order to match
        this unpacking pattern.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            unpacking_patterns = re.findall(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args",
                code,
            )
            self.assertGreater(
                len(unpacking_patterns),
                0,
                "Expected to find 'primals_1, primals_2 = args' pattern",
            )

            for pattern in unpacking_patterns:
                primals = re.findall(r"primals_(\d+)", pattern)
                for i, num in enumerate(primals, start=1):
                    self.assertEqual(
                        int(num),
                        i,
                        f"Expected sequential primals numbering, got {pattern}",
                    )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inductor_args_clear(self):
        """
        Verify that Inductor clears args after unpacking.

        This is a memory optimization pattern that Inductor uses.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "args.clear()",
                code,
                "Expected Inductor to clear args after unpacking",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inductor_returns_tuple(self):
        """
        Verify that Inductor's call() returns a tuple/list.

        The first element is the output, remaining elements are saved tensors.
        The pythonify code must extract result[0] for the user output.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            return_patterns = re.findall(r"return\s*\(([^)]+)\)", code)
            tuple_returns = [p for p in return_patterns if "," in p or "buf" in p]
            self.assertGreater(
                len(tuple_returns),
                0,
                "Expected Inductor to return a tuple (output, saved_tensors...)",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_argument_order_matches_inductor_expectation(self):
        """
        Verify that pythonify argument order matches Inductor's expectation.

        Inductor expects arguments in the order they appear in the FX graph,
        which is typically: user inputs first, then parameters, then buffers.

        The pythonify pipeline must build ArgumentExtractionNodes in this
        same order.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            unpacking_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(
                unpacking_match, "Expected to find primals unpacking"
            )

            unpacking_str = unpacking_match.group(1)
            num_primals = len(re.findall(r"primals_\d+", unpacking_str))

            expected_num_args = 2
            self.assertEqual(
                num_primals,
                expected_num_args,
                f"Expected {expected_num_args} arguments (1 input + 1 param), "
                f"got {num_primals}",
            )

            arg_extractions = re.findall(r"(arg\d+)\s*=", code)
            arg_numbers = [int(re.search(r"\d+", a).group()) for a in arg_extractions]
            if arg_numbers:
                self.assertEqual(
                    arg_numbers,
                    sorted(arg_numbers),
                    "Argument extraction should be in sequential order",
                )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_aot_autograd_forward_uses_list_args(self):
        """
        Verify that AOT Autograd's forward() calls compiled_fn(list(args)).

        When using the autograd wrapper (CompiledFunction), the forward()
        method must convert the *args to a list before calling compiled_fn.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "compiled_fn(list(args))",
                code,
                "Expected forward() to call compiled_fn(list(args))",
            )

        finally:
            os.unlink(path)


class TestAutogradFunctionInvocationPath(TestCase):
    """
    Tests that verify the autograd.Function invocation path is correct.

    The pythonify feature generates code that wraps Inductor kernels in a
    torch.autograd.Function for gradient tracking. This test class verifies:

    1. The autograd Function class (CompiledFunction) is defined correctly
    2. `callable = CompiledFunction.apply` is set up
    3. The invocation section calls `callable(arg1, arg2)` NOT `compiled_fn([...])` directly
    4. There is no double-invocation (both callable and compiled_fn called in invocation section)

    This addresses the root cause analysis in the requirements that identified
    a double-invocation bug where both `callable(...)` and `compiled_fn([...])`
    were being called.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_callable_is_defined_as_compiled_function_apply(self):
        """
        Verify that `callable = CompiledFunction.apply` is in the generated code.

        The autograd Function class must be set up so that calling `callable(...)`
        goes through the autograd machinery (forward/backward methods).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "callable = CompiledFunction.apply",
                code,
                "Expected 'callable = CompiledFunction.apply' to be defined",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_invocation_uses_callable_not_compiled_fn_directly(self):
        """
        Verify the invocation section calls callable() not compiled_fn() directly.

        When is_autograd_function=True, the generated invocation should be:
            result = callable(arg1, arg2)

        NOT:
            result = compiled_fn([arg1, arg2])

        The autograd Function's forward() method handles calling compiled_fn internally.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            # Find the "Invoke Compiled Callable" section
            invoke_section_match = re.search(
                r"# =+\n# Invoke Compiled Callable\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(
                invoke_section_match,
                "Expected to find 'Invoke Compiled Callable' section",
            )

            invoke_section = invoke_section_match.group(1)
            print(f"[DEBUG] Invoke section content:\n{invoke_section}")

            # The invocation should use callable, not compiled_fn directly
            # Pattern: result = callable(arg1, arg2, ...)
            callable_invocation = re.search(r"result\s*=\s*callable\(", invoke_section)
            self.assertIsNotNone(
                callable_invocation,
                f"Expected 'result = callable(...)' in invoke section, got:\n{invoke_section}",
            )

            # The invocation section should NOT have direct compiled_fn call
            # (compiled_fn is called inside CompiledFunction.forward, not here)
            direct_compiled_fn_call = re.search(
                r"(?<!_inductor_result = )compiled_fn\s*\(\s*\[", invoke_section
            )
            self.assertIsNone(
                direct_compiled_fn_call,
                f"Invoke section should not call compiled_fn directly, got:\n{invoke_section}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_no_double_invocation(self):
        """
        Verify there is no double-invocation bug.

        The requirements identified a bug where both paths were being called:
        1. callable(arg1, arg2) - through autograd Function
        2. compiled_fn([arg1, arg2]) - directly

        Only ONE path should exist in the invocation section.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            # Find the "Invoke Compiled Callable" section
            invoke_section_match = re.search(
                r"# =+\n# Invoke Compiled Callable\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(
                invoke_section_match,
                "Expected to find 'Invoke Compiled Callable' section",
            )

            invoke_section = invoke_section_match.group(1)

            # Count result assignments in invoke section
            # There should be exactly ONE assignment to result
            result_assignments = re.findall(r"^result\s*=", invoke_section, re.MULTILINE)
            self.assertEqual(
                len(result_assignments),
                1,
                f"Expected exactly 1 'result =' in invoke section, got {len(result_assignments)}:\n{invoke_section}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_callable_receives_positional_args_not_list(self):
        """
        Verify callable is called with positional args, not a list.

        When using the autograd Function wrapper:
            CORRECT: result = callable(arg1, arg2)
            WRONG:   result = callable([arg1, arg2])

        The autograd Function's forward() expects *args (positional), not a list.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            # Find the "Invoke Compiled Callable" section
            invoke_section_match = re.search(
                r"# =+\n# Invoke Compiled Callable\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(
                invoke_section_match,
                "Expected to find 'Invoke Compiled Callable' section",
            )

            invoke_section = invoke_section_match.group(1)

            # Should NOT have callable([...]) - that would pass a list as single arg
            list_arg_call = re.search(r"callable\s*\(\s*\[", invoke_section)
            self.assertIsNone(
                list_arg_call,
                f"callable should receive positional args, not a list:\n{invoke_section}",
            )

            # Should have callable(arg1, arg2, ...) - positional args
            positional_call = re.search(r"callable\s*\(\s*arg\d+", invoke_section)
            self.assertIsNotNone(
                positional_call,
                f"callable should be called with positional args like callable(arg1, arg2):\n{invoke_section}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_autograd_forward_internally_calls_compiled_fn(self):
        """
        Verify the autograd Function's forward() calls compiled_fn internally.

        The invocation goes:
        1. result = callable(arg1, arg2)   <- user-facing call
        2. CompiledFunction.forward(ctx, arg1, arg2)  <- autograd wrapper
        3. compiled_fn(list(args))  <- internal call to Inductor kernel

        This test verifies step 3 exists inside the forward() method.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            # Find the CompiledFunction class definition
            class_match = re.search(
                r"class CompiledFunction\(torch\.autograd\.Function\):(.*?)(?=\nclass |\ncallable =|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(
                class_match,
                "Expected to find CompiledFunction class definition",
            )

            class_body = class_match.group(1)

            # The forward method should call compiled_fn(list(args))
            forward_calls_compiled_fn = re.search(
                r"def forward\(.*?\):.*?compiled_fn\s*\(\s*list\s*\(\s*args\s*\)\s*\)",
                class_body,
                re.DOTALL,
            )
            self.assertIsNotNone(
                forward_calls_compiled_fn,
                f"Expected forward() to call compiled_fn(list(args)), class body:\n{class_body[:500]}...",
            )

        finally:
            os.unlink(path)


class TestObjectIdCapture(TestCase):
    """
    Tests for parameter and buffer access patterns in pythonify.

    With the golden path implementation, pythonify generates code that
    accesses parameters and buffers via module-based attribute traversal
    (e.g., model.W, model.running_mean) rather than ctypes-based object IDs.

    This test class verifies:
    1. Golden path uses model.X access patterns for parameters and buffers
    2. Generated code REQUIRES model in exec namespace
    3. OBJECT_ID fallback still works for non-module inputs
    4. Legacy fallback comments appear when OBJECT_ID path is used
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_obj_from_id_helper_is_generated(self):
        """
        Verify that the golden path uses module-based access instead of obj_from_id.

        With the golden path, parameters are accessed via model.W instead of
        obj_from_id(). The obj_from_id helper is only generated when the
        legacy fallback path is used (e.g., when model_name is not available).

        This test verifies that with a normal nn.Module, the golden path is used
        and the generated code contains model.W style access.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # With golden path, check for module-based access
            self.assertIn("GOLDEN PATH: Module-Based Parameter Access", code)
            self.assertIn("model.W", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_obj_from_id_is_used_for_parameters(self):
        """
        Verify that module-based access is used to extract model parameters (golden path).

        The generated code should have lines like:
            arg2 = model.W

        instead of:
            arg2 = obj_from_id(140234567890)  # legacy fallback

        The module-based access is the "golden path" for nn.Module pythonify.
        It provides process-portable code that doesn't depend on memory addresses.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Find argument extraction section
            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(
                extract_section_match,
                "Expected to find 'Argument Extraction' section",
            )

            extract_section = extract_section_match.group(1)

            # With golden path, should have model.W style extraction
            self.assertIn("model.W", extract_section)

            # Should NOT have obj_from_id calls for parameters (golden path avoids this)
            obj_from_id_calls = re.findall(r"arg\d+ = obj_from_id\(\d+\)", extract_section)
            self.assertEqual(
                len(obj_from_id_calls),
                0,
                f"Expected no obj_from_id calls for parameters with golden path, got:\n{extract_section}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_object_ids_are_valid(self):
        """
        Verify that golden path uses module-based access for parameters.

        With the golden path active, generated code should access parameters
        via model.W and model.bias instead of obj_from_id calls. This test
        confirms the golden path generates proper module-based access patterns.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn("model.W", code, "Expected model.W access (golden path)")
            self.assertIn("model.bias", code, "Expected model.bias access (golden path)")
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_object_ids_match_model_parameters(self):
        """
        Verify that golden path accesses parameters via model attribute names.

        The golden path generates code like model.W instead of obj_from_id(id).
        This test confirms parameters are accessed correctly through the model.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access pattern for parameter W
            self.assertIn("model.W", code, "Expected model.W access (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            # Execute with model in namespace and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with model in exec scope",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_object_ids_work_with_multiple_parameters(self):
        """
        Verify golden path works with models having multiple parameters.

        Each parameter should have its own model.X access pattern in the
        generated code.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                x = x.matmul(self.W1)
                x = x.matmul(self.W2)
                return x + self.bias

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify all 3 parameters use module-based access
            self.assertIn("model.W1", code, "Expected model.W1 access (golden path)")
            self.assertIn("model.W2", code, "Expected model.W2 access (golden path)")
            self.assertIn("model.bias", code, "Expected model.bias access (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with multiple parameters",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_exec_without_model_in_scope(self):
        """
        Verify generated code REQUIRES model in exec scope (golden path behavior).

        With the golden path, the generated code uses model.X access patterns
        which require the model variable to be in scope. This test verifies
        that a NameError is raised when model is missing from the namespace.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path is being used
            self.assertIn("model.W", code, "Expected model.W access (golden path)")

            # Execute WITHOUT model in namespace - should raise NameError
            frame = inspect.currentframe()
            namespace = {**frame.f_globals}
            namespace["x"] = x
            namespace["torch"] = torch
            # Explicitly do NOT include 'model' in namespace

            with self.assertRaises(NameError) as context:
                exec(code, namespace)

            self.assertIn(
                "model",
                str(context.exception),
                "NameError should mention missing 'model' variable",
            )

            # Verify it DOES work when model is provided
            namespace["model"] = model
            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match when model is in exec scope",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_object_ids_for_buffers(self):
        """
        Verify golden path works for registered buffers.

        Buffers (like running_mean in BatchNorm) should be accessed via
        model.buffer_name attribute traversal (golden path), not obj_from_id.
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.running_mean

        torch.manual_seed(0)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify both parameter and buffer use model.X access
            self.assertIn("model.W", code, "Expected model.W access (golden path)")
            self.assertIn("model.running_mean", code, "Expected model.running_mean access (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with buffer access",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_object_ids_are_valid(self):
        """
        Verify that buffers are accessed via golden path attribute traversal.

        With the golden path, buffer access uses model.buffer_name rather than
        object IDs. This test confirms the generated code works correctly and
        accesses the buffer through the model variable.
        """
        import re

        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("running_mean", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + self.running_mean

        torch.manual_seed(0)
        model = ModelWithBuffer(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify buffer uses model.X access
            self.assertIn("model.running_mean", code, "Expected model.running_mean access (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            # Execute and verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Outputs should match with buffer access",
            )
        finally:
            os.unlink(path)

    def test_legacy_fallback_comment_in_obj_from_id_helper(self):
        """
        Verify that the obj_from_id helper includes LEGACY FALLBACK comments.

        When the OBJECT_ID fallback path is used, the generated code should
        clearly indicate that this is a legacy fallback mechanism with
        appropriate warning comments.
        """
        from torch._dynamo.pythonify.pipeline import (
            RuntimeWrapperPipeline,
            CompilationArtifacts,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code

        # Create artifacts WITHOUT model_name to force OBJECT_ID fallback
        test_tensor = torch.randn(3, 4)

        artifacts = CompilationArtifacts(
            input_names=["x"],
            parameter_names=["W"],
            model_name="",  # Empty model_name forces OBJECT_ID fallback
            parameter_tensors={"W": test_tensor},
        )

        pipeline = RuntimeWrapperPipeline(artifacts)
        ir = pipeline.build()

        # Generate Python code
        code = generate_python_code(ir)

        # The obj_from_id helper should be present with LEGACY FALLBACK comments
        self.assertIn("def obj_from_id(obj_id):", code)
        self.assertIn("LEGACY FALLBACK", code)
        self.assertIn("obj_from_id helper", code)
        self.assertIn("ctypes-based approach is a fallback", code)

    def test_legacy_fallback_comment_at_call_sites(self):
        """
        Verify that LEGACY FALLBACK comments appear at obj_from_id call sites.

        When OBJECT_ID source is used for argument extraction, each call to
        obj_from_id() should be preceded by a comment indicating this is
        a legacy fallback path.
        """
        from torch._dynamo.pythonify.pipeline import (
            RuntimeWrapperPipeline,
            CompilationArtifacts,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code

        # Create artifacts WITHOUT model_name to force OBJECT_ID fallback
        test_tensor = torch.randn(3, 4)

        artifacts = CompilationArtifacts(
            input_names=["x"],
            parameter_names=["W"],
            model_name="",  # Empty model_name forces OBJECT_ID fallback
            parameter_tensors={"W": test_tensor},
        )

        pipeline = RuntimeWrapperPipeline(artifacts)
        ir = pipeline.build()

        # Generate Python code
        code = generate_python_code(ir)

        # Check that LEGACY FALLBACK comment appears before obj_from_id usage
        self.assertIn(
            "# LEGACY FALLBACK: Using object ID because module context unavailable",
            code,
        )

        # The obj_from_id call should follow the comment
        legacy_comment_idx = code.find(
            "LEGACY FALLBACK: Using object ID because module context unavailable"
        )
        obj_from_id_idx = code.find("obj_from_id(", legacy_comment_idx)
        self.assertGreater(
            obj_from_id_idx,
            legacy_comment_idx,
            "obj_from_id call should appear after the LEGACY FALLBACK comment",
        )

    def test_object_id_fallback_works_for_non_module_inputs(self):
        """
        Verify that OBJECT_ID fallback still works for non-module inputs.

        When there is no module context (model_name is empty), the pipeline uses
        the OBJECT_ID fallback path which relies on ctypes to retrieve tensors
        by their Python object ID. This is essential for cases like:
        - Raw callables (functions) that capture tensors as closure variables
        - Detached tensors not associated with any nn.Module
        - Ad-hoc compilation scenarios without a module context

        This test verifies the complete OBJECT_ID fallback flow:
        1. Pipeline creates ArgumentExtractionNode with source=OBJECT_ID
        2. gen_python.py generates obj_from_id() helper and calls
        3. The obj_from_id mechanism correctly retrieves tensors by memory address
        4. The extracted tensors are the exact same objects (by id)

        This is a critical regression test - the legacy path must continue working
        even as we default to the golden path for nn.Module cases.
        """
        from torch._dynamo.pythonify.pipeline import (
            RuntimeWrapperPipeline,
            CompilationArtifacts,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code
        import ctypes

        # Simulate a non-module scenario:
        # We have tensors that need to be accessed but no module context.
        # This is common for raw callables with captured tensor constants.
        weight_tensor = torch.randn(4, 4)
        bias_tensor = torch.randn(4)
        input_x = torch.randn(3, 4)

        # Create artifacts WITHOUT model_name to force OBJECT_ID fallback
        artifacts = CompilationArtifacts(
            input_names=["x"],
            parameter_names=["W", "bias"],
            model_name="",  # Empty model_name = no module context = OBJECT_ID fallback
            parameter_tensors={
                "W": weight_tensor,
                "bias": bias_tensor,
            },
        )

        pipeline = RuntimeWrapperPipeline(artifacts)
        ir = pipeline.build()

        # Verify the IR nodes use OBJECT_ID source (not PARAMETER)
        from torch._dynamo.pythonify.ir import ArgumentExtractionNode, ArgumentSource

        arg_nodes = [n for n in ir.nodes if isinstance(n, ArgumentExtractionNode)]
        param_nodes = [n for n in arg_nodes if n.access_path in ("W", "bias")]

        self.assertEqual(len(param_nodes), 2, "Should have nodes for W and bias")

        for node in param_nodes:
            self.assertEqual(
                node.source,
                ArgumentSource.OBJECT_ID,
                f"Node {node.name} should use OBJECT_ID source when model_name is empty. "
                f"Got {node.source} instead.",
            )
            self.assertIsNotNone(
                node.object_id,
                f"Node {node.name} should have an object_id set for OBJECT_ID source.",
            )

        # Generate Python code
        code = generate_python_code(ir)

        print(f"[DEBUG] Generated code for OBJECT_ID fallback test:\n{code[:2000]}...")

        # Verify the fallback path is being used
        self.assertIn(
            "def obj_from_id(obj_id):",
            code,
            "OBJECT_ID fallback should generate obj_from_id helper",
        )
        self.assertIn(
            "ctypes",
            code,
            "OBJECT_ID fallback should use ctypes for dereferencing",
        )
        self.assertIn(
            "LEGACY FALLBACK",
            code,
            "OBJECT_ID fallback should be labeled as LEGACY FALLBACK",
        )
        self.assertNotIn(
            "GOLDEN PATH",
            code,
            "OBJECT_ID fallback should NOT show GOLDEN PATH header",
        )

        # Verify obj_from_id is called with the actual tensor object IDs
        weight_id = id(weight_tensor)
        bias_id = id(bias_tensor)

        self.assertIn(
            f"obj_from_id({weight_id})",
            code,
            f"Should have obj_from_id call with weight tensor ID {weight_id}",
        )
        self.assertIn(
            f"obj_from_id({bias_id})",
            code,
            f"Should have obj_from_id call with bias tensor ID {bias_id}",
        )

        # Critically important: verify the ctypes obj_from_id mechanism works
        # This is the core of the OBJECT_ID fallback - if this doesn't work,
        # the generated code will fail at runtime.
        #
        # The obj_from_id function does:
        #   ctypes.cast(obj_id, ctypes.py_object).value
        #
        # This retrieves the Python object at the given memory address.

        extracted_W = ctypes.cast(weight_id, ctypes.py_object).value
        extracted_bias = ctypes.cast(bias_id, ctypes.py_object).value

        # Verify the extracted tensors are the EXACT SAME objects (same id)
        self.assertIs(
            extracted_W,
            weight_tensor,
            "Extracted W should be the exact same tensor object",
        )
        self.assertIs(
            extracted_bias,
            bias_tensor,
            "Extracted bias should be the exact same tensor object",
        )

        # Also verify values match (they're the same object, so this should always pass)
        self.assertTrue(
            torch.equal(extracted_W, weight_tensor),
            "Extracted W values should match original",
        )
        self.assertTrue(
            torch.equal(extracted_bias, bias_tensor),
            "Extracted bias values should match original",
        )

        print("[DEBUG] SUCCESS: OBJECT_ID fallback correctly retrieves tensors via ctypes!")

    def test_object_id_fallback_with_buffers(self):
        """
        Verify that OBJECT_ID fallback works for buffers as well as parameters.

        Buffers (registered via register_buffer) should also fall back to OBJECT_ID
        when there's no module context available.
        """
        from torch._dynamo.pythonify.pipeline import (
            RuntimeWrapperPipeline,
            CompilationArtifacts,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code
        from torch._dynamo.pythonify.ir import ArgumentExtractionNode, ArgumentSource
        import ctypes

        # Create tensors without module context
        weight_tensor = torch.randn(4, 4)
        scale_buffer = torch.tensor(2.0)

        artifacts = CompilationArtifacts(
            input_names=["x"],
            parameter_names=["W"],
            buffer_names=["scale"],
            model_name="",  # No module context
            parameter_tensors={"W": weight_tensor},
            buffer_tensors={"scale": scale_buffer},
        )

        pipeline = RuntimeWrapperPipeline(artifacts)
        ir = pipeline.build()

        # Find buffer node
        arg_nodes = [n for n in ir.nodes if isinstance(n, ArgumentExtractionNode)]
        buffer_nodes = [n for n in arg_nodes if n.access_path == "scale"]

        self.assertEqual(len(buffer_nodes), 1, "Should have node for scale buffer")
        buffer_node = buffer_nodes[0]

        self.assertEqual(
            buffer_node.source,
            ArgumentSource.OBJECT_ID,
            "Buffer should use OBJECT_ID source when model_name is empty",
        )

        buffer_id = id(scale_buffer)
        self.assertEqual(
            buffer_node.object_id,
            buffer_id,
            "Buffer node should have correct object_id",
        )

        # Generate and verify code contains the obj_from_id call
        code = generate_python_code(ir)

        self.assertIn(f"obj_from_id({buffer_id})", code)
        self.assertIn("def obj_from_id(obj_id):", code)
        self.assertIn("LEGACY FALLBACK", code)

        # Verify the ctypes mechanism correctly retrieves the buffer
        extracted_buffer = ctypes.cast(buffer_id, ctypes.py_object).value

        self.assertIs(
            extracted_buffer,
            scale_buffer,
            "Extracted buffer should be the exact same object",
        )
        self.assertEqual(
            extracted_buffer.item(),
            2.0,
            "Extracted buffer should have correct value",
        )

        print("[DEBUG] SUCCESS: OBJECT_ID fallback works for buffers!")


class TestGetModelReference(TestCase):
    """
    Tests that verify get_model_reference() returns the model during compilation.

    The pythonify feature relies on get_model_reference() returning the nn.Module
    being compiled so that parameter and buffer attributes can be accessed via
    the golden path (model.X attribute traversal). If get_model_reference()
    returns None, the code would fall back to object IDs.

    This test class verifies:
    1. get_model_reference() returns non-None during compilation
    2. The returned reference is the same model object
    3. The model reference is accessible when CompilationArtifacts are created
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_get_model_reference_returns_model_during_compilation(self):
        """
        Verify get_model_reference() returns the model during pythonify compilation.

        With golden path, the model reference enables module-based attribute access
        (model.W) instead of object ID fallback. We verify this by checking that
        the generated code uses model.X patterns.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify model-based attribute access pattern
            self.assertIn(
                "model.W",
                code,
                "get_model_reference() should have returned the model, enabling "
                "golden path module-based access (model.W pattern)",
            )
            # Should NOT use obj_from_id fallback
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_model_reference_is_same_object(self):
        """
        Verify the model reference enables correct attribute access at runtime.

        With golden path, we verify this by checking that the generated code
        correctly accesses model.W and produces correct numerical output.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path verification
            self.assertIn("model.W", code)
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify correctness
            namespace = {"torch": torch, "f_locals": {"x": x}}
            namespace["x"] = x
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            # The model reference should give us the same tensor
            self.assertTrue(
                torch.allclose(y1, y2),
                "Model reference should access the exact same parameter tensor",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_model_reference_parameters_accessible(self):
        """
        Verify that model.parameters() is accessible from the model reference.

        With golden path, we verify that ALL parameters are accessed via
        model.X patterns in the generated code.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: all parameters should be accessed via model.X
            self.assertIn(
                "model.W",
                code,
                "W parameter should be accessed via model.W",
            )
            self.assertIn(
                "model.bias",
                code,
                "bias parameter should be accessed via model.bias",
            )
            # Should NOT use obj_from_id fallback
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_model_reference_not_set_without_pythonify(self):
        """
        Verify get_model_reference() returns None when pythonify is not enabled.

        This confirms that the model reference is only set in pythonify context.
        """
        from torch._dynamo.pythonify import get_model_reference

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        torch._dynamo.reset()

        # Without pythonify, get_model_reference should return None
        ref_before = get_model_reference()
        self.assertIsNone(
            ref_before,
            "get_model_reference() should return None outside pythonify context",
        )

        # Compile without pythonify
        y = torch.compile(model)(x)

        # After normal compile, still None
        ref_after = get_model_reference()
        self.assertIsNone(
            ref_after,
            "get_model_reference() should return None after normal compile",
        )

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_set_model_reference_sets_correct_model(self):
        """
        Verify set_model_reference() correctly sets the model that get_model_reference() returns.

        This tests the fundamental contract of the set/get pair.
        """
        from torch._dynamo.pythonify import (
            get_model_reference,
            pythonify_context,
            set_model_reference,
        )

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        model = Model(4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            # Inside pythonify context, set_model_reference should work
            with pythonify_context(path):
                # Initially None
                ref_before = get_model_reference()
                self.assertIsNone(
                    ref_before,
                    "get_model_reference() should be None before set_model_reference()",
                )

                # Set the model
                set_model_reference(model)

                # Now should return the model
                ref_after = get_model_reference()
                self.assertIs(
                    ref_after,
                    model,
                    "get_model_reference() should return the model after set_model_reference()",
                )

            # Outside context, should be None again
            ref_outside = get_model_reference()
            self.assertIsNone(
                ref_outside,
                "get_model_reference() should be None outside pythonify context",
            )
        finally:
            if os.path.exists(path):
                os.unlink(path)


class TestArgumentOrderingValidation(TestCase):
    """
    Tests that validate argument ordering matches Inductor expectations.

    This test class provides comprehensive verification that the pythonify pipeline
    builds ArgumentExtractionNodes in the correct order to match what Inductor
    expects when unpacking arguments in its generated kernel code.

    The expected order is: user inputs first, then parameters, then buffers.
    This must match the Inductor kernel's `primals_1, primals_2, ... = args` line.

    With golden path, parameters are accessed via model.X attribute traversal.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_input_param_ordering_matches_inductor(self):
        """
        Verify that input comes before parameter in argument extraction.

        For a model like `x + x.matmul(self.W)`, the order should be:
        - arg1 = x (input from f_locals)
        - arg2 = model.W (parameter via golden path)

        And in Inductor:
        - primals_1 = x
        - primals_2 = W
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(extract_section_match)
            extract_section = extract_section_match.group(1)

            # Golden path: look for f_locals (input) and model.X (parameter)
            f_locals_match = re.search(r"arg(\d+)\s*=\s*f_locals", extract_section)
            model_match = re.search(r"arg(\d+)\s*=\s*model\.W", extract_section)

            self.assertIsNotNone(f_locals_match, "Expected f_locals extraction for input")
            self.assertIsNotNone(model_match, "Expected model.W extraction for parameter (golden path)")

            input_arg_num = int(f_locals_match.group(1))
            param_arg_num = int(model_match.group(1))

            self.assertLess(
                input_arg_num,
                param_arg_num,
                f"Input arg{input_arg_num} should come before parameter arg{param_arg_num}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_multiple_params_in_order(self):
        """
        Verify multiple parameters maintain their order.

        For a model with W1, W2, the argument order should match the order
        they appear in named_parameters() which is typically definition order.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W1) + x.matmul(self.W2)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            unpacking_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(unpacking_match, "Expected primals unpacking")

            unpacking_str = unpacking_match.group(1)
            num_primals = len(re.findall(r"primals_\d+", unpacking_str))

            self.assertEqual(
                num_primals,
                3,
                f"Expected 3 arguments (1 input + 2 params), got {num_primals}",
            )

            arg_extractions = re.findall(r"arg(\d+)\s*=", code)
            arg_numbers = [int(n) for n in arg_extractions]
            self.assertEqual(
                arg_numbers,
                sorted(arg_numbers),
                "Argument extraction should be in sequential order",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_params_before_buffers_ordering(self):
        """
        Verify that parameters come before buffers in argument ordering.

        The expected order is: inputs, parameters, buffers.
        This is critical for matching Inductor's expectations.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("bias", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            unpacking_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(unpacking_match, "Expected primals unpacking")

            unpacking_str = unpacking_match.group(1)
            num_primals = len(re.findall(r"primals_\d+", unpacking_str))

            self.assertEqual(
                num_primals,
                3,
                f"Expected 3 arguments (1 input + 1 param + 1 buffer), got {num_primals}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_invocation_args_match_extraction_order(self):
        """
        Verify that callable invocation uses args in the same order as extraction.

        The invocation should be `callable(arg1, arg2, ...)` where the args
        are in the same order as they were extracted.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            invoke_section_match = re.search(
                r"# =+\n# Invoke Compiled Callable\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(invoke_section_match, "Expected Invoke section")
            invoke_section = invoke_section_match.group(1)

            callable_match = re.search(
                r"result\s*=\s*callable\((arg\d+(?:,\s*arg\d+)*)\)",
                invoke_section,
            )
            self.assertIsNotNone(callable_match, "Expected callable(arg1, arg2, ...) in invocation")

            args_str = callable_match.group(1)
            arg_numbers = [int(m.group(1)) for m in re.finditer(r"arg(\d+)", args_str)]

            self.assertEqual(
                arg_numbers,
                sorted(arg_numbers),
                f"Invocation args should be in order: {arg_numbers}",
            )

            self.assertEqual(
                arg_numbers,
                [1, 2],
                f"Expected arg1, arg2 for model with 1 input + 1 param, got {arg_numbers}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_end_to_end_ordering_with_mixed_model(self):
        """
        End-to-end test that argument ordering works for a model with
        multiple inputs, parameters, and buffers.

        This is a comprehensive test that verifies the generated code
        actually executes correctly, proving the ordering is right.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(0.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return (x.matmul(self.W1) + x.matmul(self.W2)) * self.scale

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Outputs should match. Expected {y1}, got {y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_only_model_ordering(self):
        """
        Test argument ordering for a model with only buffers (no parameters).

        The order should be: inputs first, then buffers.

        Note: When there are no trainable parameters (inference mode), Inductor
        uses arg0_1, arg1_1 naming instead of primals_1, primals_2.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("W", torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Buffer-only model outputs should match. Expected {y1}, got {y2}",
            )

            unpacking_match = re.search(
                r"(\w+(?:,\s*\w+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(unpacking_match, "Expected args unpacking")

            unpacking_str = unpacking_match.group(1)
            num_args_in_unpacking = len(re.findall(r"\w+", unpacking_str))

            self.assertEqual(
                num_args_in_unpacking,
                2,
                f"Expected 2 arguments (1 input + 1 buffer), got {num_args_in_unpacking}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_arg_count_matches_primals_count(self):
        """
        Verify that the number of extracted arguments matches the number
        of primals that Inductor expects.

        This is a key invariant: len(extracted_args) == len(primals)
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))
                self.register_buffer("scale", torch.tensor(1.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return (x.matmul(self.W) + self.bias) * self.scale

        torch.manual_seed(0)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            unpacking_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(unpacking_match, "Expected primals unpacking")

            unpacking_str = unpacking_match.group(1)
            num_primals = len(re.findall(r"primals_\d+", unpacking_str))

            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(extract_section_match)
            extract_section = extract_section_match.group(1)

            arg_extractions = re.findall(r"arg\d+\s*=", extract_section)
            num_args = len(arg_extractions)

            self.assertEqual(
                num_args,
                num_primals,
                f"Number of extracted args ({num_args}) must match "
                f"number of primals ({num_primals})",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_ordering_with_nested_module(self):
        """
        Test argument ordering with nested modules (nn.Linear).

        KNOWN LIMITATION: For nn.Linear and similar ops, Inductor places
        parameters (weight, bias) BEFORE the input in the primals list.
        However, the pythonify pipeline currently uses a fixed order:
        inputs first, then parameters, then buffers.

        This test documents this known limitation by verifying:
        1. The generated code has the correct number of arguments
        2. Inductor expects parameters-first order for nn.Linear
        3. The pipeline generates inputs-first order

        The actual ordering mismatch is a known issue that would require
        parsing the Inductor-generated code to determine correct order.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            primals_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(
                primals_match, "Expected primals unpacking in Inductor code"
            )

            primals_str = primals_match.group(1)
            num_primals = len(re.findall(r"primals_\d+", primals_str))
            self.assertEqual(
                num_primals,
                3,
                f"Expected 3 primals (weight + bias + input), got {num_primals}",
            )

            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(extract_section_match)
            extract_section = extract_section_match.group(1)

            arg_extractions = re.findall(r"arg\d+\s*=", extract_section)
            num_extracted = len(arg_extractions)
            self.assertEqual(
                num_extracted,
                3,
                f"Expected 3 extracted args, got {num_extracted}",
            )

        finally:
            os.unlink(path)


class TestBackwardKernelNaming(TestCase):
    """
    Unit tests for the _rename_triton_functions_for_backward method.

    This test class verifies that the Triton function renaming logic works
    correctly to avoid name collisions between forward and backward kernels.
    When both forward and backward Inductor kernels define Triton functions
    with the same name (e.g., `triton_poi_fused_mul_0`), the backward kernel
    definition would overwrite the forward kernel when the generated file
    is exec'd.

    The _rename_triton_functions_for_backward method adds a `_bwd` suffix to
    all Triton function names in the backward source code. This includes:

    1. Variable assignments: `triton_xxx = async_compile.triton(...)`
       -> `triton_xxx_bwd = async_compile.triton(...)`

    2. String arguments: `async_compile.triton('triton_xxx', ...)`
       -> `async_compile.triton('triton_xxx_bwd', ...)`

    3. Function definitions: `def triton_xxx(...)`
       -> `def triton_xxx_bwd(...)`

    4. Function calls: `triton_xxx.run(...)`
       -> `triton_xxx_bwd.run(...)`

    5. Kernel name in metadata: `'kernel_name': 'triton_xxx'`
       -> `'kernel_name': 'triton_xxx_bwd'`
    """

    def _get_renaming_method(self):
        """Get the _rename_triton_functions_for_backward method for testing."""
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        visitor = PythonCodeGenVisitor()
        return visitor._rename_triton_functions_for_backward

    def test_function_definition_renamed(self):
        """
        Verify that `def triton_xxx(` is renamed to `def triton_xxx_bwd(`.
        """
        rename = self._get_renaming_method()

        source = """
def triton_poi_fused_add_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK: tl.constexpr):
    xoffset = tl.program_id(0) * XBLOCK
"""
        result = rename(source)

        self.assertIn("def triton_poi_fused_add_0_bwd(", result)
        self.assertNotIn("def triton_poi_fused_add_0(", result)

    def test_function_call_renamed(self):
        """
        Verify that `triton_xxx.run(` is renamed to `triton_xxx_bwd.run(`.
        """
        rename = self._get_renaming_method()

        source = """
def triton_poi_fused_mul_0(x, y, z):
    pass

triton_poi_fused_mul_0.run(arg1, arg2, arg3, grid=grid)
"""
        result = rename(source)

        self.assertIn("triton_poi_fused_mul_0_bwd.run(", result)
        self.assertNotIn("triton_poi_fused_mul_0.run(", result)

    def test_multiple_triton_functions_renamed(self):
        """
        Verify that multiple Triton functions are all renamed correctly.
        """
        rename = self._get_renaming_method()

        source = """
def triton_poi_fused_add_0(x):
    pass

def triton_poi_fused_mul_1(x):
    pass

triton_poi_fused_add_0.run(a)
triton_poi_fused_mul_1.run(b)
"""
        result = rename(source)

        self.assertIn("def triton_poi_fused_add_0_bwd(", result)
        self.assertIn("def triton_poi_fused_mul_1_bwd(", result)
        self.assertIn("triton_poi_fused_add_0_bwd.run(", result)
        self.assertIn("triton_poi_fused_mul_1_bwd.run(", result)

        self.assertNotIn("def triton_poi_fused_add_0(", result)
        self.assertNotIn("def triton_poi_fused_mul_1(", result)

    def test_no_triton_functions_unchanged(self):
        """
        Verify that source without Triton functions is returned unchanged.
        """
        rename = self._get_renaming_method()

        source = """
def call(args):
    primals_1, primals_2 = args
    return (buf0,)
"""
        result = rename(source)

        self.assertEqual(source, result)

    def test_non_triton_functions_not_renamed(self):
        """
        Verify that non-Triton functions (not starting with 'triton_') are not renamed.

        Only functions with names starting with 'triton_' should be renamed.
        Functions like 'call', 'helper_func', etc. should remain unchanged.
        """
        rename = self._get_renaming_method()

        source = """
def helper_func(x):
    pass

def call(args):
    return args
"""
        result = rename(source)

        self.assertNotIn("helper_func_bwd", result)
        self.assertNotIn("call_bwd", result)
        self.assertEqual(source, result)

    def test_forward_backward_kernel_uniqueness(self):
        """
        Verify that forward and backward kernels would have unique names.

        This test simulates the scenario where both forward and backward
        generate the same Triton function names, and verifies that after
        renaming, the names are unique.
        """
        rename = self._get_renaming_method()

        forward_source = """
def triton_poi_fused_mul_0(x, y, out):
    pass

def call(args):
    triton_poi_fused_mul_0.run(x, y, out)
    return (out,)
"""

        backward_source = """
def triton_poi_fused_mul_0(grad_out, x, grad_x):
    pass

def call(args):
    triton_poi_fused_mul_0.run(grad_out, x, grad_x)
    return (grad_x,)
"""

        renamed_backward = rename(backward_source)

        import re

        forward_funcs = set(re.findall(r"def (triton_\w+)\(", forward_source))
        backward_funcs = set(re.findall(r"def (triton_\w+)\(", renamed_backward))

        overlap = forward_funcs & backward_funcs
        self.assertEqual(
            len(overlap), 0,
            f"Forward and backward should have no overlapping function names, but found: {overlap}"
        )

        for func in forward_funcs:
            self.assertIn(
                f"{func}_bwd", backward_funcs,
                f"Expected {func}_bwd in backward functions"
            )

    def test_compiled_fn_and_compiled_fn_backward_distinct(self):
        """
        Verify that the generated code uses distinct names for forward and backward.

        The forward kernel should be assigned to `compiled_fn` and the backward
        kernel should be assigned to `compiled_fn_backward`, avoiding any name
        conflicts at the top-level variable binding.
        """
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor
        from torch._dynamo.pythonify.ir import (
            RuntimeWrapperIR,
            KernelLoadNode,
            KernelType,
        )

        forward_source = "def call(args):\n    return (args[0],)"
        backward_source = "def call(args):\n    return (args[0],)"

        visitor = PythonCodeGenVisitor()
        ir = RuntimeWrapperIR()

        forward_node = KernelLoadNode(
            kernel_id="forward_kernel",
            kernel_type=KernelType.INLINE,
            kernel_path=None,
            entry_point="call",
            variable_name="compiled_fn",
            inline_content=forward_source,
            metadata={"source": "inductor", "is_backward": False},
        )

        backward_node = KernelLoadNode(
            kernel_id="backward_kernel",
            kernel_type=KernelType.INLINE,
            kernel_path=None,
            entry_point="call",
            variable_name="compiled_fn_backward",
            inline_content=backward_source,
            metadata={"source": "inductor", "is_backward": True},
        )

        ir.add_node(forward_node)
        ir.add_node(backward_node)

        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn("compiled_fn = call", code)
        self.assertIn("compiled_fn_backward = call", code)


class TestInferenceEndToEnd(TestCase):
    """
    Comprehensive end-to-end tests for inference-only mode.

    These tests verify that the pythonify feature works correctly when
    requires_grad=False, following the exact pattern from the requirements
    specification. This is Phase 4 of the implementation plan.

    The key requirement is: Run end-to-end example with `requires_grad=False`,
    exec the generated code (with model in namespace for golden path), and
    verify `torch.allclose(y1, y2)` passes.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inference_end_to_end_basic(self):
        """
        Basic inference-only end-to-end test using golden path.

        This test follows the golden path pattern:
        1. Create a model with requires_grad=False input
        2. Compile with torch.compile(pythonify=path)
        3. Exec the generated Python file with model in namespace
        4. Verify torch.allclose(y1, y2) passes
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access pattern
            self.assertIn("model.W", code)
            self.assertIn("CompiledFunction", code)
            # Should NOT use obj_from_id fallback
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            torch.manual_seed(0)
            x2 = torch.randn(batch, features, requires_grad=False)

            namespace = {"torch": torch, "f_locals": {"x": x2}}
            namespace["x"] = x2
            namespace["model"] = model  # Golden path: model must be in namespace

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Inference outputs should match: y1={y1}, y2={y2}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inference_end_to_end_detached_comparison(self):
        """
        Test inference with .detach() comparison as shown in requirements.

        The requirements show: assert torch.allclose(y1.detach(), y2.detach())
        This test ensures the detach pattern works correctly with golden path.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            torch.manual_seed(0)
            x2 = torch.randn(batch, features, requires_grad=False)

            namespace = {"torch": torch, "f_locals": {"x": x2}}
            namespace["x"] = x2
            namespace["model"] = model  # Golden path: model must be in namespace

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Inference outputs should match when using .detach()",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inference_end_to_end_different_inputs(self):
        """
        Test inference with different input values using golden path.

        Verifies that the exec'd code produces correct results for
        inputs different from the ones used during compilation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_compile = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x_compile)

            with open(path) as f:
                code = f.read()

            x_test = torch.randn(batch, features, requires_grad=False)
            y_expected = model(x_test)

            namespace = {"torch": torch, "f_locals": {"x": x_test}}
            namespace["x"] = x_test
            namespace["model"] = model  # Golden path: model must be in namespace

            exec(code, namespace)

            y_actual = namespace["y"]

            self.assertTrue(
                torch.allclose(y_expected, y_actual, rtol=1e-4, atol=1e-4),
                "Inference should match eager execution for different inputs",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inference_end_to_end_requires_model_in_namespace(self):
        """
        Test that golden path REQUIRES model in exec namespace.

        With the golden path, parameters are accessed via model.X attribute
        traversal instead of ctypes-based object IDs. This means the generated
        code requires the model to be provided in the exec namespace.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path patterns (no obj_from_id)
            self.assertIn("model.W", code)
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            torch.manual_seed(0)
            x2 = torch.randn(batch, features, requires_grad=False)

            # Exec WITHOUT model in namespace - should raise NameError
            namespace = {"torch": torch, "f_locals": {"x": x2}}
            namespace["x"] = x2

            with self.assertRaises(NameError) as ctx:
                exec(code, namespace)

            self.assertIn("model", str(ctx.exception))

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_inference_end_to_end_with_multiple_parameters(self):
        """
        Test inference with a model that has multiple parameters using golden path.

        Verifies that module-based access works for all parameters (W1, W2, bias).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                h = x.matmul(self.W1)
                h = h.matmul(self.W2)
                return h + self.bias

        torch.manual_seed(123)
        features = 4
        batch = 2
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn("model.W1", code)
            self.assertIn("model.W2", code)
            self.assertIn("model.bias", code)
            # Should NOT use obj_from_id fallback
            self.assertNotIn("def obj_from_id", code)
            self.assertNotIn("obj_from_id(", code)

            namespace = {"torch": torch, "f_locals": {"x": x}}
            namespace["x"] = x
            namespace["model"] = model  # Golden path: model must be in namespace

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2, rtol=1e-4, atol=1e-4),
                "Inference with multiple parameters should match",
            )

        finally:
            os.unlink(path)


class TestGuardEvaluation(TestCase):
    """
    Tests for guard evaluation in pythonify generated code.

    These tests verify that:
    1. Generated code contains assert statements for shape, dtype, device checks
    2. Running with wrong-shaped inputs raises AssertionError
    3. Running with wrong dtype inputs raises AssertionError
    4. Dynamic guards are handled correctly (no assertion, just comment)
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_guards_are_generated_for_shape(self):
        """
        Test that shape guards are generated in the output code.

        The generated code should contain assert statements that verify
        the shape of input tensors matches what was used during compilation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Guard section should exist
            self.assertIn("Guard Evaluation", code)

            # Shape assertions should be present
            self.assertIn("assert arg1.shape[0] == 3", code)
            self.assertIn("assert arg1.shape[1] == 4", code)

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_guards_are_generated_for_dtype(self):
        """
        Test that dtype guards are generated in the output code.

        The generated code should contain assert statements that verify
        the dtype of input tensors matches what was used during compilation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # dtype assertions should be present
            self.assertIn("arg1.dtype == torch.float32", code)

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_wrong_shape_raises_assertion_error(self):
        """
        Test that running with wrong-shaped input raises AssertionError.

        When the generated code is exec'd with an input tensor of a
        different shape than what was compiled, the guard assertions
        should fail and raise AssertionError.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Try with different batch size (5 instead of 3)
            x_wrong_batch = torch.randn(5, features, requires_grad=False)

            namespace = {"torch": torch, "f_locals": {"x": x_wrong_batch}}
            namespace["x"] = x_wrong_batch
            namespace["model"] = model  # Golden path: model must be in namespace

            with self.assertRaises(AssertionError) as ctx:
                exec(code, namespace)

            # The error message should mention the shape mismatch
            self.assertIn("shape", str(ctx.exception).lower())

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_wrong_features_raises_assertion_error(self):
        """
        Test that running with wrong feature dimension raises AssertionError.

        When the generated code is exec'd with an input tensor of a
        different feature dimension than what was compiled, the guard
        assertions should fail.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Try with different feature size (8 instead of 4)
            x_wrong_features = torch.randn(batch, 8, requires_grad=False)

            namespace = {"torch": torch, "f_locals": {"x": x_wrong_features}}
            namespace["x"] = x_wrong_features
            namespace["model"] = model  # Golden path: model must be in namespace

            with self.assertRaises(AssertionError) as ctx:
                exec(code, namespace)

            # The error message should mention the shape mismatch
            self.assertIn("shape", str(ctx.exception).lower())

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_wrong_dtype_raises_assertion_error(self):
        """
        Test that running with wrong dtype raises AssertionError.

        When the generated code is exec'd with an input tensor of a
        different dtype than what was compiled, the guard assertions
        should fail.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Try with float64 instead of float32
            x_wrong_dtype = torch.randn(
                batch, features, requires_grad=False, dtype=torch.float64
            )

            namespace = {"torch": torch, "f_locals": {"x": x_wrong_dtype}}
            namespace["x"] = x_wrong_dtype
            namespace["model"] = model  # Golden path: model must be in namespace

            with self.assertRaises(AssertionError) as ctx:
                exec(code, namespace)

            # The error message should mention dtype
            self.assertIn("dtype", str(ctx.exception).lower())

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_correct_input_passes_guards(self):
        """
        Test that running with correct input passes all guards.

        When the generated code is exec'd with an input tensor that
        matches the shape and dtype used during compilation, the
        guard assertions should pass without error.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Use a different tensor with the SAME shape and dtype
            torch.manual_seed(42)
            x_correct = torch.randn(batch, features, requires_grad=False)

            namespace = {"torch": torch, "f_locals": {"x": x_correct}}
            namespace["x"] = x_correct
            namespace["model"] = model  # Golden path: model must be in namespace

            # Should NOT raise AssertionError
            exec(code, namespace)

            y2 = namespace["y"]

            # Verify we got a valid result
            self.assertEqual(y2.shape, (batch, features))
            self.assertEqual(y2.dtype, torch.float32)

        finally:
            os.unlink(path)

    def test_guard_ir_shape_guard_generation(self):
        """
        Test that GuardCheckNode generates correct shape assertions.

        This is a unit test for the code generation visitor that verifies
        shape guards produce valid Python assertions.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] == 8",
            expected_value=8,
            dimension=0,
            error_message="Expected arg1.shape[0] == 8",
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn('assert arg1.shape[0] == 8, "Expected arg1.shape[0] == 8"', code)

    def test_guard_ir_dtype_guard_generation(self):
        """
        Test that GuardCheckNode generates correct dtype assertions.

        This is a unit test for the code generation visitor that verifies
        dtype guards produce valid Python assertions.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.DTYPE,
            target_name="arg1",
            condition="arg1.dtype == torch.float32",
            expected_value="torch.float32",
            error_message="Expected arg1.dtype == torch.float32",
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn("arg1.dtype == torch.float32", code)

    def test_guard_ir_dynamic_guard_generation(self):
        """
        Test that dynamic guards produce comments instead of assertions.

        When is_dynamic=True, the code generator should emit a comment
        indicating the dimension is dynamic, not an assertion.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] == 8",
            expected_value=8,
            dimension=0,
            error_message="Expected arg1.shape[0] == 8",
            is_dynamic=True,
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Dynamic guards should produce comment, not assertion
        self.assertIn("# DYNAMIC: arg1.shape[0] can vary", code)
        # Should NOT have an assertion for the specific value
        self.assertNotIn('assert arg1.shape[0] == 8, "Expected arg1.shape[0] == 8"', code)

    def test_guard_ir_dynamic_guard_with_min_max(self):
        """
        Test that dynamic guards with min/max constraints produce assertions.

        Even for dynamic dimensions, we should assert min/max bounds if
        they are provided.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] >= 1 and arg1.shape[0] <= 100",
            expected_value=8,
            dimension=0,
            error_message="Expected 1 <= arg1.shape[0] <= 100",
            is_dynamic=True,
            min_value=1,
            max_value=100,
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Should have dynamic comment
        self.assertIn("# DYNAMIC: arg1.shape[0] can vary", code)
        # Should have min/max assertions
        self.assertIn("assert arg1.shape[0] >= 1", code)
        self.assertIn("assert arg1.shape[0] <= 100", code)

    def test_guard_valid_python_syntax(self):
        """
        Test that generated guard code is valid Python.

        The guard assertions should produce syntactically correct Python
        that can be compiled without errors.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] == 8",
            expected_value=8,
            dimension=0,
            error_message="Expected arg1.shape[0] == 8",
        ))
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.DTYPE,
            target_name="arg1",
            condition="arg1.dtype == torch.float32",
            expected_value="torch.float32",
            error_message="Expected arg1.dtype == torch.float32",
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Should be valid Python
        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated guard code has syntax error: {e}")


class TestSimpleModel(TestCase):
    """
    Tests for simple models including nn.Linear using the golden path.

    This test class verifies the pythonify feature with basic models using the
    golden path approach where parameters are accessed via model attribute
    traversal (model.W, model.linear.weight) instead of ctypes obj_from_id.

    All tests expect the golden path behavior:
    1. Generated code uses model.X pattern for parameter access
    2. No obj_from_id helper function in the output
    3. Model must be provided in the exec namespace
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_simple_matmul_model_works(self):
        """
        Test that simple matmul model works end-to-end using the golden path.

        This model uses x.matmul(W) which places input first, matching
        the pythonify argument ordering. The golden path accesses parameters
        via model.W attribute traversal instead of ctypes obj_from_id.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: parameters accessed via model.W, not obj_from_id
            self.assertNotIn("def obj_from_id", code)
            self.assertIn("model.W", code)
            self.assertIn("def call(", code)

            # Use the same input (cloned) for exec'd code.
            # Golden path: pass model in namespace for attribute traversal.
            x2 = x.clone()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Outputs should match: y1={y1}, y2={y2}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_simple_additive_model_works(self):
        """
        Test that x + x.matmul(W) model works end-to-end using the golden path.

        This model uses x + x.matmul(W) which places input first in the
        matmul operation. Inductor traces this with input-first ordering,
        which matches pythonify's argument ordering. The golden path accesses
        parameters via model.W attribute traversal.

        Note: A simple x + W model does NOT work because Inductor places
        the smaller-dimensional W (parameter) before x in the primals list
        due to broadcasting semantics. This is a documented limitation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Use the same input (cloned) for exec'd code.
            # Golden path: pass model in namespace for attribute traversal.
            x2 = x.clone()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Outputs should match: y1={y1}, y2={y2}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nn_linear_compilation_succeeds(self):
        """
        Test that nn.Linear model compilation succeeds using the golden path.

        The compilation produces valid Python code that uses module-based
        attribute access (model.linear.weight, model.linear.bias) instead of
        ctypes obj_from_id.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)

            with open(path) as f:
                code = f.read()

            # Golden path: no obj_from_id, uses model.linear.weight/bias
            self.assertNotIn("def obj_from_id", code)
            self.assertIn("model.linear", code)
            self.assertIn("CompiledFunction", code)
            self.assertIn("def call(", code)

            try:
                compile(code, "<test>", "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nn_linear_has_correct_arg_count(self):
        """
        Test that nn.Linear generates the correct number of arguments.

        nn.Linear(4, 4) has 2 parameters (weight, bias) + 1 input = 3 args.
        Both the Inductor code and pythonify extraction should have 3 args.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            primals_match = re.search(
                r"(primals_\d+(?:,\s*primals_\d+)*)\s*=\s*args", code
            )
            self.assertIsNotNone(primals_match, "Expected primals unpacking")
            num_primals = len(re.findall(r"primals_\d+", primals_match.group(1)))

            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(extract_section_match)
            extract_section = extract_section_match.group(1)
            num_args = len(re.findall(r"arg\d+\s*=", extract_section))

            self.assertEqual(
                num_args,
                3,
                f"Expected 3 extracted args (x + weight + bias), got {num_args}",
            )
            self.assertEqual(
                num_primals,
                3,
                f"Expected 3 primals, got {num_primals}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nn_linear_ordering_params_first(self):
        """
        Test that nn.Linear uses correct params-first ordering with golden path.

        This test verifies that:
        1. Inductor expects parameters FIRST for nn.Linear
        2. Pythonify correctly extracts parameters first (via ordered_arg_info)
        3. The ordering matches Inductor's expectations for correct execution
        4. Golden path uses model.linear.weight/bias instead of obj_from_id

        This ordering issue was previously a known limitation but has been fixed
        by using ordered_arg_info which preserves the iteration order from
        input_source_to_sizes_strides during compilation.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(42)
        features = 4
        model = Model(features)
        x = torch.randn(3, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            asserts = re.findall(r"assert_size_stride\(primals_(\d+),\s*\(([^)]+)\)", code)

            self.assertTrue(
                len(asserts) >= 3,
                f"Expected at least 3 assert_size_stride calls, got {len(asserts)}",
            )

            primals_1_shape = asserts[0][1] if asserts else ""
            primals_3_shape = asserts[2][1] if len(asserts) > 2 else ""

            self.assertIn(
                f"{features}, {features}",
                primals_1_shape,
                f"primals_1 should be weight (4, 4), got {primals_1_shape}",
            )
            self.assertIn(
                "3,",
                primals_3_shape,
                f"primals_3 should be input with batch=3, got {primals_3_shape}",
            )

            # Golden path: params accessed via model.linear.weight/bias, not obj_from_id
            self.assertNotIn("def obj_from_id", code)
            self.assertIn("model.linear.weight", code)
            self.assertIn("model.linear.bias", code)

            # Verify input is still accessed from f_locals
            input_match = re.search(
                r'arg\d+\s*=\s*f_locals\["x"\]',
                code,
            )
            self.assertIsNotNone(
                input_match,
                "Input x should be accessed from f_locals",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nn_linear_no_bias_compilation_succeeds(self):
        """
        Test that nn.Linear without bias compiles successfully using golden path.

        Pythonify correctly extracts weight before input, matching Inductor's
        expectations. This ordering is preserved via ordered_arg_info.
        Golden path uses model.linear.weight instead of obj_from_id.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.linear = nn.Linear(features, features, bias=False)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.linear(x)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)

            with open(path) as f:
                code = f.read()

            # Golden path: no obj_from_id, uses model.linear.weight
            self.assertNotIn("def obj_from_id", code)
            self.assertIn("model.linear.weight", code)
            self.assertIn("def call(", code)

            import re

            extract_section_match = re.search(
                r"# =+\n# Argument Extraction\n# =+\n(.*?)(?:# =+|\Z)",
                code,
                re.DOTALL,
            )
            self.assertIsNotNone(extract_section_match)
            extract_section = extract_section_match.group(1)
            num_args = len(re.findall(r"arg\d+\s*=", extract_section))

            self.assertEqual(
                num_args,
                2,
                f"Expected 2 extracted args (x + weight, no bias), got {num_args}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_simple_model_multiple_ops(self):
        """
        Test a simple model with multiple operations using golden path.

        This model uses x.matmul(W1).matmul(W2) + bias which should work
        because inputs appear first in operations. Golden path accesses
        parameters via model.W1, model.W2, model.bias attribute traversal.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.randn(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W1).matmul(self.W2) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Use the same input (cloned) for exec'd code.
            # Golden path: pass model in namespace for attribute traversal.
            x2 = x.clone()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Outputs should match: y1={y1}, y2={y2}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_simple_model_with_activation(self):
        """
        Test a simple model with an activation function using golden path.

        This model uses x.matmul(W) followed by ReLU, which should work
        end-to-end with pythonify. Golden path accesses parameters via
        model.W attribute traversal.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x.matmul(self.W))

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features, requires_grad=False)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Use the same input (cloned) for exec'd code.
            # Golden path: pass model in namespace for attribute traversal.
            x2 = x.clone()

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)

            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Outputs should match: y1={y1}, y2={y2}",
            )

        finally:
            os.unlink(path)


class TestBackwardPassEndToEnd(TestCase):
    """
    Comprehensive tests for backward pass end-to-end functionality.

    These tests verify that the backward pass through exec'd pythonify output
    produces correct gradients that match the original compiled model and
    eager mode execution.

    This test class addresses the TODO item:
    - Test backward pass works end-to-end
    - Verification: Run end-to-end example with requires_grad=True,
      call loss.backward(), verify gradients are computed correctly
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_end_to_end_basic(self):
        """
        Basic test that backward pass works end-to-end via exec'd pythonify output.

        This test follows the requirements:
        1. Create model with requires_grad=True input
        2. Compile with pythonify
        3. Execute forward and backward via exec'd code
        4. Verify gradients are computed and match
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()

            original_x_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("compiled_fn_backward", code)
            self.assertIn("def backward", code)

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertIsNotNone(x2.grad)
            self.assertTrue(
                torch.allclose(original_x_grad, x2.grad),
                f"Input gradients should match.\n"
                f"Expected: {original_x_grad}\n"
                f"Got: {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_gradients_match_compiled_model(self):
        """
        Test that gradients from exec'd code match the compiled model.

        This test verifies that:
        1. Compile model with pythonify
        2. Run forward + backward on compiled model
        3. Exec the generated code with same input
        4. Verify exec'd gradients match compiled model gradients

        Note: Uses mean() loss to match the basic test pattern.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            compiled_x_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            loss2 = y2.mean()
            loss2.backward()
            exec_x_grad = x2.grad

            self.assertTrue(
                torch.allclose(compiled_x_grad, exec_x_grad),
                f"Exec'd code gradients should match compiled model.\n"
                f"Compiled grad: {compiled_x_grad}\n"
                f"Exec grad: {exec_x_grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_with_sum_loss(self):
        """
        Test backward pass when compiled with sum() loss.

        This verifies that the backward kernel works with sum() loss,
        not just mean() loss.

        IMPORTANT: The backward kernel is specialized for the loss function
        used during compilation. The same loss function MUST be used at
        exec time as was used during compilation.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()

            original_x_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["model"] = model
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertIsNotNone(x2.grad)
            self.assertTrue(
                torch.allclose(original_x_grad, x2.grad, rtol=1e-4, atol=1e-6),
                f"Gradients should match.\n"
                f"Expected: {original_x_grad}\n"
                f"Got: {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_with_different_loss_functions(self):
        """
        Test backward pass with various loss functions.

        Verifies that the backward kernel works correctly with different
        ways of computing the loss (mean, sum, norm).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)

            loss1_mean = y1.mean()
            loss1_mean.backward(retain_graph=True)
            grad_mean = x1.grad.clone()
            x1.grad.zero_()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            loss2_mean = y2.mean()
            loss2_mean.backward()

            self.assertTrue(
                torch.allclose(grad_mean, x2.grad),
                "Gradients should match with mean() loss",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_multiple_invocations(self):
        """
        Test that backward can be called multiple times on different inputs.

        The exec'd code should support running forward/backward on different
        inputs (with the same shape/dtype) multiple times.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_initial = torch.compile(model, pythonify=path)(x_initial)
            y_initial.mean().backward()

            with open(path) as f:
                code = f.read()

            for i in range(3):
                x_eager = torch.randn(batch, features, requires_grad=True)
                y_eager = model(x_eager)
                y_eager.mean().backward()
                expected_grad = x_eager.grad.clone()

                x_exec = x_eager.detach().clone().requires_grad_(True)

                frame = inspect.currentframe()
                namespace = {**frame.f_globals, **frame.f_locals}
                namespace["x"] = x_exec
                namespace["torch"] = torch

                exec(code, namespace)
                y_exec = namespace["y"]

                y_exec.mean().backward()

                self.assertTrue(
                    torch.allclose(expected_grad, x_exec.grad),
                    f"Iteration {i}: gradients should match eager mode",
                )
        finally:
            os.unlink(path)


class TestSavedTensorsForBackward(TestCase):
    """
    Tests verifying that saved tensors are correctly passed to backward.

    This test class addresses the TODO item:
    - Verify saved tensors are correctly passed to backward
    - Verification: In generated code, `ctx.save_for_backward(*_inductor_result[1:])`
      should save tensors from forward. In backward, `ctx.saved_tensors` should
      provide these tensors.

    The pythonify feature uses Inductor's compiled kernels which return
    (output, saved_tensor1, saved_tensor2, ...). The forward() method saves
    the additional return values via ctx.save_for_backward(), and the backward()
    method retrieves them via ctx.saved_tensors.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_save_for_backward_is_generated(self):
        """
        Verify that the generated code contains ctx.save_for_backward() in forward.

        When a model requires gradients, the forward() method should save tensors
        needed for backward computation. The generated code should include:
        - ctx.save_for_backward(*_inductor_result[1:])
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "ctx.save_for_backward",
                code,
                "Expected ctx.save_for_backward() in generated forward() method",
            )

            self.assertIn(
                "_inductor_result[1:]",
                code,
                "Expected save_for_backward to use _inductor_result[1:] (tensors after output)",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_saved_tensors_retrieved_in_backward(self):
        """
        Verify that the generated backward() method retrieves saved_tensors.

        The backward() method should include:
        - saved = ctx.saved_tensors
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "ctx.saved_tensors",
                code,
                "Expected ctx.saved_tensors in generated backward() method",
            )

            self.assertIn(
                "saved = ctx.saved_tensors",
                code,
                "Expected 'saved = ctx.saved_tensors' pattern in backward()",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_inputs_use_saved_tensors(self):
        """
        Verify that backward inputs combine saved tensors with grad_outputs.

        The backward() method should construct backward_inputs from:
        - list(saved) + list(grad_outputs)
        This matches AOT Autograd's expectation of (*saved_tensors, *tangents).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "list(saved) + list(grad_outputs)",
                code,
                "Expected backward_inputs to combine saved tensors with grad_outputs",
            )

            self.assertIn(
                "backward_inputs = list(saved) + list(grad_outputs)",
                code,
                "Expected exact pattern: backward_inputs = list(saved) + list(grad_outputs)",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_saved_tensors_count_matches_inductor(self):
        """
        Verify that Inductor returns multiple values (output + saved tensors).

        The Inductor kernel's call() function returns a tuple where:
        - [0] is the output
        - [1:] are saved tensors for backward

        The generated code should check len(_inductor_result) > 1 before saving.
        """
        import re

        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "if len(_inductor_result) > 1:",
                code,
                "Expected check for saved tensors: if len(_inductor_result) > 1",
            )

            return_pattern = re.search(
                r"return\s*\(([^)]+)\)", code
            )
            self.assertIsNotNone(
                return_pattern,
                "Expected Inductor kernel to return a tuple",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_saved_tensors_flow_end_to_end(self):
        """
        Verify saved tensors flow correctly from forward to backward end-to-end.

        This test creates a model, runs forward+backward through exec'd code,
        and verifies that gradients are computed correctly (which requires
        saved tensors to be passed correctly).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            expected_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            loss2 = y2.mean()
            loss2.backward()

            self.assertIsNotNone(
                x2.grad,
                "Gradient should be computed (requires saved tensors in backward)",
            )
            self.assertTrue(
                torch.allclose(expected_grad, x2.grad),
                f"Gradient mismatch indicates saved tensors not passed correctly.\n"
                f"Expected: {expected_grad}\n"
                f"Got: {x2.grad}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_saved_tensors_with_multiple_outputs_model(self):
        """
        Test saved tensors with a model that produces multiple intermediate tensors.

        More complex operations may require saving multiple intermediate tensors
        for the backward pass. This test verifies the mechanism works for such cases.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.zeros(features, features))
                self.W2 = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                h1 = x.matmul(self.W1)
                h2 = h1.matmul(self.W2)
                return x + h2

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x1 = torch.randn(batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            expected_grad = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "ctx.save_for_backward",
                code,
                "Expected save_for_backward in multi-parameter model",
            )

            torch.manual_seed(42)
            x2 = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                "Forward outputs should match for multi-parameter model",
            )

            loss2 = y2.mean()
            loss2.backward()

            self.assertTrue(
                torch.allclose(expected_grad, x2.grad),
                "Gradient should match for multi-parameter model",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_num_inputs_saved_in_forward(self):
        """
        Verify that forward() saves num_inputs in context for backward.

        The forward method should store ctx.num_inputs so backward knows
        how many gradients to return.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "ctx.num_inputs",
                code,
                "Expected ctx.num_inputs to be set in forward()",
            )

            import re
            num_inputs_match = re.search(r"ctx\.num_inputs\s*=\s*(\d+)", code)
            self.assertIsNotNone(
                num_inputs_match,
                "Expected ctx.num_inputs = N pattern in forward()",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_backward_retrieves_num_inputs(self):
        """
        Verify that backward() retrieves num_inputs from context.

        The backward method should read ctx.num_inputs (though it's also stored
        as saved for internal context tracking).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        model = Model(4)
        x = torch.randn(3, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y = torch.compile(model, pythonify=path)(x)
            loss = y.mean()
            loss.backward()

            with open(path) as f:
                code = f.read()

            self.assertIn(
                "num_inputs = ctx.num_inputs",
                code,
                "Expected backward() to retrieve num_inputs from context",
            )
        finally:
            os.unlink(path)


class TestGradientCorrectnessAgainstEagerMode(TestCase):
    """
    Comprehensive tests for gradient correctness comparing pythonify against
    eager mode and torch.compile.

    This test class addresses the TODO item:
    - Test gradient correctness against eager mode
    - Verification: Compare gradients from pythonify exec'd code against
      torch.compile output and eager mode. All three should match within tolerance.

    The tests verify:
    1. Pythonify exec'd code gradients match eager mode gradients
    2. Pythonify exec'd code gradients match torch.compile gradients
    3. All three (eager, compiled, pythonify) produce identical gradients
    4. Gradients are correct for both inputs and parameters
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_input_gradients_match_eager_mode(self):
        """
        Test that pythonify exec'd code produces input gradients matching eager mode.

        Compares gradients on input tensor x between:
        1. Eager mode (model directly)
        2. Pythonify exec'd code
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                pythonify_x_grad,
                eager_x_grad,
                msg="Pythonify input gradients should match eager mode",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_input_gradients_match_compiled_mode(self):
        """
        Test that pythonify exec'd code produces input gradients matching torch.compile.

        Compares gradients on input tensor x between:
        1. torch.compile
        2. Pythonify exec'd code
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                pythonify_x_grad,
                compiled_x_grad,
                msg="Pythonify input gradients should match torch.compile",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_all_three_modes_match(self):
        """
        Test that eager mode, torch.compile, and pythonify all produce identical gradients.

        This is the comprehensive test that verifies all three execution modes
        produce the same gradients on input tensors.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                eager_x_grad,
                compiled_x_grad,
                msg="Eager and compiled gradients should match",
            )
            torch.testing.assert_close(
                compiled_x_grad,
                pythonify_x_grad,
                msg="Compiled and pythonify gradients should match",
            )
            torch.testing.assert_close(
                eager_x_grad,
                pythonify_x_grad,
                msg="Eager and pythonify gradients should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_forward_outputs_match_all_three_modes(self):
        """
        Test that forward outputs match across eager, compiled, and pythonify.

        Gradient correctness depends on forward correctness, so this test
        verifies that all three modes produce the same forward outputs first.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features)
        y_eager = model(x_eager)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features)
            y_compile = torch.compile(model, pythonify=path)(x_compile)

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]

            torch.testing.assert_close(
                y_eager,
                y_compile,
                msg="Eager and compiled forward outputs should match",
            )
            torch.testing.assert_close(
                y_compile,
                y_pythonify,
                msg="Compiled and pythonify forward outputs should match",
            )
            torch.testing.assert_close(
                y_eager,
                y_pythonify,
                msg="Eager and pythonify forward outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_gradients_with_multiple_parameters(self):
        """
        Test gradient correctness with a model that has multiple parameters.

        Verifies that gradients are correct for models with more than one
        parameter tensor.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.zeros(features, features))
                self.W2 = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W1) + x.matmul(self.W2)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                eager_x_grad,
                compiled_x_grad,
                msg="Multi-param model: eager and compiled gradients should match",
            )
            torch.testing.assert_close(
                compiled_x_grad,
                pythonify_x_grad,
                msg="Multi-param model: compiled and pythonify gradients should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_gradients_with_activation_function(self):
        """
        Test gradient correctness with a model that includes activation functions.

        Non-linear activations like ReLU affect gradient flow, so this test
        verifies that gradients are still correct.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return torch.relu(x + x.matmul(self.W))

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)

            torch.manual_seed(42)
            model_for_compile = Model(features)

            y_compile = torch.compile(model_for_compile, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                eager_x_grad,
                compiled_x_grad,
                msg="ReLU model: eager and compiled gradients should match",
            )
            torch.testing.assert_close(
                compiled_x_grad,
                pythonify_x_grad,
                msg="ReLU model: compiled and pythonify gradients should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_gradients_with_sum_loss(self):
        """
        Test gradient correctness when compiling AND executing with sum() loss.

        IMPORTANT: The backward kernel is specialized for the loss function
        used during compilation. The same loss function must be used at exec
        time. This test verifies that when BOTH compilation and execution use
        sum(), the gradients are correct.

        Note: If the loss function differs between compilation and execution,
        stride assertion errors may occur because the backward kernel is
        specialized for the grad_output stride pattern of the original loss.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                eager_x_grad,
                compiled_x_grad,
                msg="mean() loss: eager and compiled gradients should match",
            )
            torch.testing.assert_close(
                compiled_x_grad,
                pythonify_x_grad,
                msg="mean() loss: compiled and pythonify gradients should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_gradients_with_different_batch_sizes(self):
        """
        Test that gradients are still correct with different batch sizes.

        This test compiles with one batch size and verifies gradients are correct
        for the same batch size (since guards enforce shape constraints).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 5
        model = Model(features)

        torch.manual_seed(42)
        x_eager = torch.randn(batch, features, requires_grad=True)
        y_eager = model(x_eager)
        loss_eager = y_eager.mean()
        loss_eager.backward()
        eager_x_grad = x_eager.grad.clone()

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            torch.manual_seed(42)
            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()
            compiled_x_grad = x_compile.grad.clone()

            with open(path) as f:
                code = f.read()

            torch.manual_seed(42)
            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_x_grad = x_pythonify.grad

            torch.testing.assert_close(
                eager_x_grad,
                compiled_x_grad,
                msg="Different batch: eager and compiled gradients should match",
            )
            torch.testing.assert_close(
                compiled_x_grad,
                pythonify_x_grad,
                msg="Different batch: compiled and pythonify gradients should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_gradients_mathematical_correctness(self):
        """
        Test that gradients are mathematically correct.

        For the model y = x + x.matmul(W) where W is zeros,
        the gradient dy/dx should be I (identity matrix) which means
        dy/dx * grad_output = grad_output for any grad_output.

        When W=0: y = x, so dy/dx = I
        With mean() loss: grad_output = 1/(batch*features) for each element
        Expected gradient: all 1/(batch*features) = 1/12 for batch=3, features=4
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        features = 4
        batch = 3
        model = Model(features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            x_compile = torch.randn(batch, features, requires_grad=True)
            y_compile = torch.compile(model, pythonify=path)(x_compile)
            loss_compile = y_compile.mean()
            loss_compile.backward()

            with open(path) as f:
                code = f.read()

            x_pythonify = torch.randn(batch, features, requires_grad=True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x_pythonify
            namespace["torch"] = torch

            exec(code, namespace)
            y_pythonify = namespace["y"]
            loss_pythonify = y_pythonify.mean()
            loss_pythonify.backward()
            pythonify_grad = x_pythonify.grad

            expected_grad_value = 1.0 / (batch * features)
            expected_grad = torch.full((batch, features), expected_grad_value, device="cuda")

            torch.testing.assert_close(
                pythonify_grad,
                expected_grad,
                msg=f"With W=0, y=x so gradient should be 1/{batch*features} everywhere",
            )
        finally:
            os.unlink(path)


class TestNestedModulesSequential(TestCase):
    """
    Tests for pythonify with nested modules like nn.Sequential.

    These tests verify that parameter paths like `layer1.weight` are correctly
    resolved through the nested module hierarchy using the GOLDEN PATH approach.
    The generated code uses model-based attribute access (e.g., model.fn.0.weight)
    instead of ctypes-based object IDs.

    NOTE: Due to the known argument ordering limitation (pythonify uses a fixed
    order: inputs first, then parameters, then buffers), nn.Linear and similar
    ops may not work end-to-end. These tests document both working and
    non-working cases with nested modules.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_sequential_compiles_successfully(self):
        """
        Test that nn.Sequential models compile without errors.

        The compilation step should succeed and produce generated code that uses
        the golden path (model-based attribute access) for the nested parameters.
        """
        torch.set_default_device("cuda")

        model = nn.Sequential(
            nn.Linear(4, 8),
            nn.ReLU(),
            nn.Linear(8, 4),
        )

        torch.manual_seed(42)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")
            self.assertIn("CompiledFunction", code, "Should have CompiledFunction class")
            self.assertIn("import torch", code, "Should import torch")

            self.assertIsNotNone(y1)
            self.assertEqual(y1.shape, (3, 4))
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_sequential_parameter_count(self):
        """
        Test that all parameters from nn.Sequential are captured.

        An nn.Sequential with two nn.Linear layers has 4 parameters:
        - layer 0: weight (4, 8) and bias (8,)
        - layer 2: weight (8, 4) and bias (4,)

        The generated code should have model-based access patterns for all parameters.
        """
        torch.set_default_device("cuda")

        model = nn.Sequential(
            nn.Linear(4, 8),
            nn.ReLU(),
            nn.Linear(8, 4),
        )

        torch.manual_seed(42)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            param_patterns = [p for p in model_access_patterns if ".weight" in p or ".bias" in p]
            self.assertGreaterEqual(
                len(param_patterns),
                4,
                f"Expected at least 4 model-based parameter accesses for 4 parameters, got {len(param_patterns)}: {param_patterns}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_module_with_matmul_pattern(self):
        """
        Test a nested module structure that uses the x.matmul(W) pattern.

        This test verifies that nested modules compile and generate code
        correctly using the golden path (model-based attribute access).
        The test documents that:
        - Compilation succeeds
        - model-based access is used for all nested parameters
        - Generated code has correct structure
        """
        torch.set_default_device("cuda")

        class NestedMatmul(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.transform = nn.ModuleDict({
                    "layer1": self._make_layer(features),
                    "layer2": self._make_layer(features),
                })

            def _make_layer(self, features: int):
                layer = nn.Module()
                layer.W = nn.Parameter(torch.randn(features, features))
                return layer

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                x = x.matmul(self.transform["layer1"].W)
                x = x.matmul(self.transform["layer2"].W)
                return x

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = NestedMatmul(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)
            self.assertEqual(y1.shape, (batch, features))

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for nested parameters")
            self.assertIn("CompiledFunction", code, "Should have CompiledFunction class")

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            w_patterns = [p for p in model_access_patterns if ".W" in p]
            self.assertGreaterEqual(
                len(w_patterns),
                2,
                f"Should have model-based access for the 2 nested W parameters, got: {w_patterns}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_deeply_nested_module(self):
        """
        Test a deeply nested module structure (3+ levels).

        Verifies that parameter path traversal works for deeply nested
        parameters like `model.middle.inner.W` using the golden path.
        """
        torch.set_default_device("cuda")

        class Inner(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        class Middle(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.inner = Inner(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.inner(x)

        class Outer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.middle = Middle(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + self.middle(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Outer(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Deeply nested model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_sequential_with_custom_module(self):
        """
        Test nn.Sequential containing custom modules that use matmul pattern.

        This test verifies compilation and code generation for nn.Sequential
        with custom modules using the golden path (model-based attribute access).
        """
        torch.set_default_device("cuda")

        class MatmulLayer(nn.Module):
            def __init__(self, in_features: int, out_features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(in_features, out_features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        model = nn.Sequential(
            MatmulLayer(4, 8),
            nn.ReLU(),
            MatmulLayer(8, 4),
        )

        torch.manual_seed(42)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)
            self.assertEqual(y1.shape, (3, 4))

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")
            self.assertIn("CompiledFunction", code)

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            w_patterns = [p for p in model_access_patterns if ".W" in p]
            self.assertGreaterEqual(len(w_patterns), 2, f"Should have model-based access for 2 W params, got: {w_patterns}")
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_module_list_with_matmul(self):
        """
        Test nn.ModuleList with modules using matmul pattern.

        This test verifies that ModuleList parameters are correctly captured
        via the golden path (model-based attribute access).
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int, num_layers: int):
                super().__init__()
                self.layers = nn.ModuleList([
                    nn.Module() for _ in range(num_layers)
                ])
                for layer in self.layers:
                    layer.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                for layer in self.layers:
                    x = x.matmul(layer.W)
                return x

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features, num_layers=3)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)
            self.assertEqual(y1.shape, (batch, features))

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")
            self.assertIn("CompiledFunction", code)

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            w_patterns = [p for p in model_access_patterns if ".W" in p]
            self.assertGreaterEqual(
                len(w_patterns),
                3,
                f"Should have model-based access for all 3 layer W parameters, got: {w_patterns}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_module_access_patterns_are_valid(self):
        """
        Test that model-based access patterns for nested modules are correct.

        Verify that the generated code accesses nested module parameters via
        the correct attribute paths (e.g., model.layer1.W, model.layer2.W).
        """
        torch.set_default_device("cuda")

        class Nested(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layer1 = nn.Module()
                self.layer1.W = nn.Parameter(torch.randn(features, features))
                self.layer2 = nn.Module()
                self.layer2.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.layer1.W).matmul(self.layer2.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Nested(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            w_patterns = [p for p in model_access_patterns if ".W" in p]

            self.assertGreaterEqual(len(w_patterns), 2, f"Should have at least 2 model-based W accesses, got: {w_patterns}")

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Nested module model-based access should produce correct output",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_sequential_nn_linear_ordering_limitation(self):
        """
        Document the known limitation with nn.Linear in nn.Sequential.

        nn.Linear places weight before input in Inductor's primals ordering,
        which causes a mismatch with pythonify's fixed ordering. This test
        documents that compilation succeeds and the golden path generates
        model-based access patterns.
        """
        torch.set_default_device("cuda")

        model = nn.Sequential(
            nn.Linear(4, 4),
        )

        torch.manual_seed(42)
        x = torch.randn(3, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            self.assertIsNotNone(y1)
            self.assertEqual(y1.shape, (3, 4))

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")
            self.assertIn("CompiledFunction", code)

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_buffers_in_submodules(self):
        """
        Test that buffers in nested submodules are correctly captured.

        Similar to parameters, buffers in nested modules should be captured
        via the golden path with correct model-based attribute access.
        """
        torch.set_default_device("cuda")

        class Inner(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("scale", torch.tensor(2.0))

        class Outer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.inner = Inner(features)
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.inner.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Outer(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            import re

            model_access_patterns = re.findall(r"model\.[A-Za-z0-9_.]+", code)
            w_patterns = [p for p in model_access_patterns if ".W" in p]
            scale_patterns = [p for p in model_access_patterns if ".scale" in p]
            self.assertGreaterEqual(
                len(w_patterns) + len(scale_patterns),
                2,
                f"Should have model-based access for W parameter and scale buffer, got: W={w_patterns}, scale={scale_patterns}",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Nested buffers should work correctly",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_sequential_backward_with_matmul_pattern(self):
        """
        Test that backward pass code is generated for nested sequential modules.

        This test verifies that:
        - Compilation with requires_grad=True succeeds
        - Both forward and backward kernels are generated
        - compiled_fn_backward is present in generated code
        - The golden path (model-based access) is used
        """
        torch.set_default_device("cuda")

        class MatmulLayer(nn.Module):
            def __init__(self, in_features: int, out_features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(in_features, out_features) * 0.1)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        class SequentialMatmul(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.layers = nn.Sequential(
                    MatmulLayer(features, features),
                    MatmulLayer(features, features),
                )

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + self.layers(x)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = SequentialMatmul(features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            x1 = torch.randn(batch, features, requires_grad=True)
            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            grad1 = x1.grad.clone()

            self.assertIsNotNone(grad1)
            self.assertEqual(grad1.shape, (batch, features))

            with open(path) as f:
                code = f.read()

            self.assertIn("model.", code, "Should use model-based access for parameters")
            self.assertIn("CompiledFunction", code, "Should have CompiledFunction class")
            self.assertIn("backward", code, "Should have backward method")
            self.assertIn("compiled_fn_backward", code, "Should have backward kernel")
        finally:
            os.unlink(path)


class TestBuffersWithBatchNorm(TestCase):
    """
    Tests for pythonify with BatchNorm-like running_mean/running_var buffers.

    BatchNorm layers have registered buffers (running_mean, running_var, num_batches_tracked)
    that are different from parameters. This test class verifies that these buffers are
    correctly accessed via the golden path (module-based attribute traversal):
    1. Parameters and buffers accessed via model.X pattern (e.g., model.W, model.running_mean)
    2. Generated code does NOT use obj_from_id for parameter/buffer extraction
    3. Model must be passed in the exec namespace for the code to work

    Note: Due to the known argument ordering limitation in pythonify
    (pythonify uses fixed order: inputs, parameters, buffers, while Inductor
    may use different ordering for some operations), tests use the
    `x.matmul(W)` pattern which is known to work.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_manual_running_mean_buffer(self):
        """
        Test pythonify with a model that has running_mean-like buffers.

        This simulates the core behavior of BatchNorm's running_mean buffer:
        a buffer that is used in computation but not trained via gradients.
        Uses the x.matmul(W) pattern which works with pythonify.
        Verifies golden path: model.W, model.running_mean, model.running_var patterns.
        """
        torch.set_default_device("cuda")

        class ModelWithRunningMean(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("running_var", torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                # Use matmul pattern that works, then apply buffer scaling
                result = x.matmul(self.W)
                # Add buffers in a simple way that doesn't break ordering
                return result + self.running_mean + self.running_var

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithRunningMean(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns are used
            self.assertIn(
                "model.W",
                code,
                "Expected model.W access for parameter (golden path)",
            )
            self.assertIn(
                "model.running_mean",
                code,
                "Expected model.running_mean access for buffer (golden path)",
            )
            self.assertIn(
                "model.running_var",
                code,
                "Expected model.running_var access for buffer (golden path)",
            )
            # Verify obj_from_id is NOT used for parameter extraction
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Running mean model outputs should match. "
                f"Expected:\n{y1}\nGot:\n{y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_golden_path_access(self):
        """
        Verify that generated code uses module-based access for buffers.

        With the golden path, pythonify generates model.buffer_name patterns
        instead of obj_from_id() calls. This test verifies that buffers are
        accessed via attribute traversal on the model.
        """
        torch.set_default_device("cuda")

        class ModelWithBuffers(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("running_var", torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.running_mean + self.running_var

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithBuffers(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn(
                "model.W",
                code,
                "Expected model.W access for parameter (golden path)",
            )
            self.assertIn(
                "model.running_mean",
                code,
                "Expected model.running_mean access for buffer (golden path)",
            )
            self.assertIn(
                "model.running_var",
                code,
                "Expected model.running_var access for buffer (golden path)",
            )
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Buffer model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_multiple_buffer_types(self):
        """
        Test with multiple buffer types (scalar, 1D, 2D) similar to BatchNorm.

        BatchNorm has:
        - running_mean: 1D buffer
        - running_var: 1D buffer
        - num_batches_tracked: scalar buffer

        This test verifies all these buffer types are accessed via golden path.
        """
        torch.set_default_device("cuda")

        class ModelWithMultipleBufferTypes(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("running_var", torch.ones(features))
                self.register_buffer("num_batches_tracked", torch.tensor(1.0))
                self.register_buffer("scale_matrix", torch.eye(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                result = result.matmul(self.scale_matrix)
                result = result + self.running_mean + self.running_var
                return result * self.num_batches_tracked

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithMultipleBufferTypes(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns for all buffers
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn("model.running_mean", code, "Expected model.running_mean (golden path)")
            self.assertIn("model.running_var", code, "Expected model.running_var (golden path)")
            self.assertIn("model.num_batches_tracked", code, "Expected model.num_batches_tracked (golden path)")
            self.assertIn("model.scale_matrix", code, "Expected model.scale_matrix (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                f"Multiple buffer types model outputs should match. "
                f"Expected:\n{y1}\nGot:\n{y2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_used_as_scale(self):
        """
        Test buffers used as scale factors (like BatchNorm's weight/scale).

        This tests a common pattern where buffers are used to scale the output.
        Verifies golden path: model.W, model.scale, model.running_mean patterns.
        """
        torch.set_default_device("cuda")

        class ModelWithScaleBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))
                self.register_buffer("running_mean", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                return (result + self.running_mean) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithScaleBuffer(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn("model.scale", code, "Expected model.scale (golden path)")
            self.assertIn("model.running_mean", code, "Expected model.running_mean (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Scale buffer model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_with_different_dtypes(self):
        """
        Test buffers with different dtypes.

        BatchNorm uses float32 for running_mean/var and int64 for num_batches_tracked.
        Verifies golden path works correctly with mixed dtype buffers.
        """
        torch.set_default_device("cuda")

        class ModelWithMixedDtypeBuffers(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer(
                    "running_mean", torch.zeros(features, dtype=torch.float32)
                )
                self.register_buffer("scale", torch.tensor(2.0, dtype=torch.float32))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                return (result + self.running_mean) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithMixedDtypeBuffers(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn("model.running_mean", code, "Expected model.running_mean (golden path)")
            self.assertIn("model.scale", code, "Expected model.scale (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Mixed dtype buffer model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_backward_pass(self):
        """
        Test backward pass with buffers involved in computation.

        Buffers don't receive gradients but should work correctly
        during backward pass for input gradients. Uses golden path.
        """
        torch.set_default_device("cuda")

        class ModelWithBufferBackward(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features) * 0.1)
                self.register_buffer("scale", torch.tensor(0.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithBufferBackward(features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            x1 = torch.randn(batch, features, requires_grad=True)
            x1_data = x1.data.clone()

            y1 = torch.compile(model, pythonify=path)(x1)
            loss1 = y1.mean()
            loss1.backward()
            grad1 = x1.grad.clone()

            with open(path) as f:
                code = f.read()

            self.assertIn("backward", code, "Should have backward code for training")
            self.assertIn(
                "compiled_fn_backward", code, "Should have backward kernel"
            )
            # Golden path: verify module-based access
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn("model.scale", code, "Expected model.scale (golden path)")

            x2 = x1_data.clone().requires_grad_(True)

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x2
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            loss2 = y2.mean()
            loss2.backward()
            grad2 = x2.grad.clone()

            self.assertTrue(
                torch.allclose(y1.detach(), y2.detach()),
                f"Forward outputs should match.\nExpected:\n{y1}\nGot:\n{y2}",
            )

            self.assertTrue(
                torch.allclose(grad1, grad2, atol=1e-5),
                f"Gradients should match.\nExpected:\n{grad1}\nGot:\n{grad2}",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_nested_module_with_buffers(self):
        """
        Test nested modules with buffers (simulating nested BatchNorm-like structures).

        Verifies that buffers in nested submodules are correctly accessed via golden path.
        With nested modules, the access pattern should be model.norm.running_mean etc.
        """
        torch.set_default_device("cuda")

        class InnerWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("scale", torch.tensor(2.0))

        class OuterModel(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.norm = InnerWithBuffer(features)
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                return (result + self.norm.running_mean) * self.norm.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = OuterModel(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns for nested buffers
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn(
                "model.norm.running_mean",
                code,
                "Expected model.norm.running_mean for nested buffer (golden path)",
            )
            self.assertIn(
                "model.norm.scale",
                code,
                "Expected model.norm.scale for nested buffer (golden path)",
            )
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Nested module with buffers outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_attribute_access_in_generated_code(self):
        """
        Verify that generated code uses attribute access for buffers.

        With the golden path, buffer access patterns like model.running_mean
        and model.running_var should appear in the generated code instead of
        obj_from_id() calls with hardcoded IDs.
        """
        torch.set_default_device("cuda")

        class ModelWithBuffers(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("running_var", torch.ones(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                return result + self.running_mean + self.running_var

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithBuffers(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify both buffer attribute access patterns are present
            self.assertIn(
                "model.running_mean",
                code,
                "Expected model.running_mean access pattern in generated code",
            )
            self.assertIn(
                "model.running_var",
                code,
                "Expected model.running_var access pattern in generated code",
            )
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_only_no_params(self):
        """
        Test a model with only buffers (no trainable parameters).

        This is an edge case where the model has buffers but no nn.Parameters.
        Verifies golden path works for buffer-only models.
        """
        torch.set_default_device("cuda")

        class BufferOnlyModel(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.register_buffer("W", torch.randn(features, features))
                self.register_buffer("running_mean", torch.zeros(features))
                self.register_buffer("scale", torch.tensor(0.5))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                result = x.matmul(self.W)
                return (result + self.running_mean) * self.scale

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = BufferOnlyModel(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access for all buffers
            self.assertIn("model.W", code, "Expected model.W (golden path)")
            self.assertIn("model.running_mean", code, "Expected model.running_mean (golden path)")
            self.assertIn("model.scale", code, "Expected model.scale (golden path)")
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Buffer-only model outputs should match",
            )
        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_buffer_ordering_with_param(self):
        """
        Test that buffer ordering is correct when mixed with parameters.

        The expected order is: inputs, parameters, buffers.
        With the golden path, both parameters and buffers are accessed via
        model.X patterns, so ordering is handled by the model structure.
        """
        torch.set_default_device("cuda")

        class ModelWithMixedOrder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("bias", torch.zeros(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = ModelWithMixedOrder(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: verify module-based access patterns
            self.assertIn(
                "model.W",
                code,
                "Expected model.W for parameter (golden path)",
            )
            self.assertIn(
                "model.bias",
                code,
                "Expected model.bias for buffer (golden path)",
            )
            # Verify obj_from_id is NOT used
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path should NOT use obj_from_id helper",
            )

            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["x"] = x
            namespace["torch"] = torch
            namespace["model"] = model

            exec(code, namespace)
            y2 = namespace["y"]

            self.assertTrue(
                torch.allclose(y1, y2),
                "Mixed parameter/buffer model outputs should match",
            )
        finally:
            os.unlink(path)


class TestDynamicShapes(TestCase):
    """
    Tests for pythonify with dynamic shapes.

    When torch.compile is called with dynamic=True, certain dimensions are marked
    as dynamic and should not have strict shape guards asserted. Instead, the
    generated code should:
    1. Emit comments indicating which dimensions are dynamic
    2. Optionally include min/max bound assertions for dynamic dimensions
    3. Allow the exec'd code to work with different batch sizes

    With the golden path, parameters are accessed via model.X attribute traversal.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_generates_dynamic_comments(self):
        """
        Test that dynamic=True produces dynamic comments in generated code.

        When compiling with dynamic=True, the generated code should include
        comments like '# DYNAMIC: arg1.shape[0] can vary' instead of assertions.
        Uses golden path for parameter access.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path, dynamic=True)(x)

            with open(path) as f:
                code = f.read()

            # Golden path: should use model.X access, NOT obj_from_id
            self.assertIn("model.W", code, "Should use golden path model.W access")
            self.assertIn("CompiledFunction", code, "Should have autograd Function")
            self.assertNotIn("def obj_from_id", code, "Should NOT use obj_from_id fallback")
            self.assertNotIn("obj_from_id(", code, "Should NOT use obj_from_id calls")

            # The dynamic=True flag should result in dynamic dimension handling.
            # Either dynamic comments OR no shape assertions for dynamic dims.
            # The key test is that execution works with different batch sizes.

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_compilation_succeeds(self):
        """
        Test that compilation with dynamic=True succeeds and produces valid code.

        KNOWN LIMITATION: When dynamic=True is used, Inductor generates kernels
        that may expect additional symbolic shape arguments (e.g., primals_3 for
        the dynamic batch size expression). The pythonify exec path currently
        doesn't handle this additional argument, causing a mismatch.

        This test verifies:
        1. Compilation with dynamic=True succeeds
        2. The generated code has valid Python syntax
        3. The compiled model produces correct results

        Uses golden path for parameter access.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        compile_batch = 3
        model = Model(features)

        x_compile = torch.randn(compile_batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path, dynamic=True)(x_compile)

            with open(path) as f:
                code = f.read()

            # Golden path: should use model.X access, NOT obj_from_id
            self.assertIn("model.W", code, "Should use golden path model.W access")
            self.assertIn("CompiledFunction", code, "Should have autograd Function")
            self.assertIn("compiled_fn", code, "Should have compiled function")
            self.assertNotIn("def obj_from_id", code, "Should NOT use obj_from_id fallback")
            self.assertNotIn("obj_from_id(", code, "Should NOT use obj_from_id calls")

            # Verify Python syntax is valid
            compile(code, "<string>", "exec")

            # Verify the forward pass result is correct
            y_expected = model(x_compile)
            self.assertTrue(
                torch.allclose(y1, y_expected),
                "Compiled model should produce same result as eager mode",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_ir_node_is_dynamic_flag(self):
        """
        Test that GuardCheckNode is_dynamic flag is set correctly.

        When dynamic=True, the pipeline should set is_dynamic=True on GuardCheckNode
        for the dynamic dimensions.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        # Create an IR with a dynamic guard
        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] == 8",
            expected_value=8,
            dimension=0,
            error_message="Expected arg1.shape[0] == 8",
            is_dynamic=True,
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Dynamic guards should produce a DYNAMIC comment
        self.assertIn("# DYNAMIC:", code, "Should have DYNAMIC comment")
        self.assertIn("can vary", code, "Should indicate dimension can vary")
        # Should NOT have assertion for the exact value
        self.assertNotIn('assert arg1.shape[0] == 8, "Expected arg1.shape[0] == 8"', code)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_with_min_max_bounds(self):
        """
        Test that dynamic shapes with min/max bounds generate appropriate assertions.

        Even for dynamic dimensions, min/max bound assertions should be generated
        if the bounds are specified.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] >= 1 and arg1.shape[0] <= 256",
            expected_value=32,
            dimension=0,
            error_message="Expected 1 <= arg1.shape[0] <= 256",
            is_dynamic=True,
            min_value=1,
            max_value=256,
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Should have dynamic comment
        self.assertIn("# DYNAMIC:", code)
        # Should have min/max assertions
        self.assertIn("assert arg1.shape[0] >= 1", code)
        self.assertIn("assert arg1.shape[0] <= 256", code)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_static_dims_still_checked(self):
        """
        Test that static dimensions are still checked even with dynamic=True.

        When dynamic=True is used, only the first dimension (batch) is typically
        dynamic. The feature dimensions should still have strict assertions.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.zeros(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(0)
        features = 4
        batch = 3
        model = Model(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            torch.compile(model, pythonify=path, dynamic=True)(x)

            with open(path) as f:
                code = f.read()

            # The feature dimension (dim 1) should still be checked.
            # Check that there are shape assertions in the code for non-dynamic dims.
            # The exact assertion depends on which dims are marked dynamic.
            self.assertIn("shape", code, "Should have some shape references")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_with_backward_pass(self):
        """
        Test that dynamic shapes work correctly with backward pass.

        The autograd function should handle dynamic batch sizes in both
        forward and backward passes.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        compile_batch = 3
        model = Model(features)

        x_compile = torch.randn(compile_batch, features, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y1 = torch.compile(model, pythonify=path, dynamic=True)(x_compile)
            loss1 = y1.mean()
            loss1.backward()

            with open(path) as f:
                code = f.read()

            # Verify backward kernel exists
            self.assertIn("compiled_fn_backward", code, "Should have backward kernel")
            self.assertIn("backward", code, "Should have backward method")

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_pipeline_is_dynamic_guard_method(self):
        """
        Test the _is_dynamic_guard method in RuntimeWrapperPipeline.

        This method determines if a guard should be marked as dynamic based on
        the dynamic_dims configuration in CompilationArtifacts.
        """
        from torch._dynamo.pythonify.ir import GuardType
        from torch._dynamo.pythonify.pipeline import (
            CompilationArtifacts,
            RuntimeWrapperPipeline,
        )

        # Create artifacts with dynamic dims for "x" dimension 0
        artifacts = CompilationArtifacts(
            input_names=["x"],
            dynamic_dims={"x": [0]},  # x.shape[0] is dynamic
        )

        pipeline = RuntimeWrapperPipeline(artifacts)

        # Check that dimension 0 is dynamic
        self.assertTrue(
            pipeline._is_dynamic_guard("x", 0, GuardType.SHAPE),
            "x.shape[0] should be dynamic",
        )

        # Check that dimension 1 is NOT dynamic
        self.assertFalse(
            pipeline._is_dynamic_guard("x", 1, GuardType.SHAPE),
            "x.shape[1] should be static",
        )

        # Check that dtype guards are never dynamic
        self.assertFalse(
            pipeline._is_dynamic_guard("x", None, GuardType.DTYPE),
            "dtype guards should never be dynamic",
        )

        # Check that unspecified inputs have no dynamic dims
        self.assertFalse(
            pipeline._is_dynamic_guard("y", 0, GuardType.SHAPE),
            "y.shape[0] should be static (not in dynamic_dims)",
        )

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_compilation_artifacts_dynamic_dims(self):
        """
        Test that CompilationArtifacts correctly stores dynamic_dims.

        The dynamic_dims dict should map input names to lists of dynamic
        dimension indices.
        """
        from torch._dynamo.pythonify.pipeline import CompilationArtifacts

        artifacts = CompilationArtifacts(
            input_names=["x", "y"],
            dynamic_dims={
                "x": [0],      # x has dynamic batch dim
                "y": [0, 1],   # y has dynamic batch and seq dims
            },
        )

        self.assertEqual(artifacts.dynamic_dims["x"], [0])
        self.assertEqual(artifacts.dynamic_dims["y"], [0, 1])
        self.assertNotIn("z", artifacts.dynamic_dims)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_guard_node_fields(self):
        """
        Test that GuardCheckNode has proper fields for dynamic shape support.

        The node should have is_dynamic, min_value, and max_value fields.
        """
        from torch._dynamo.pythonify.ir import GuardCheckNode, GuardType

        # Test default values
        node = GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="x",
            condition="x.shape[0] == 8",
            expected_value=8,
        )
        self.assertFalse(node.is_dynamic, "Default is_dynamic should be False")
        self.assertIsNone(node.min_value, "Default min_value should be None")
        self.assertIsNone(node.max_value, "Default max_value should be None")

        # Test with dynamic values
        dynamic_node = GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="x",
            condition="x.shape[0] >= 1",
            expected_value=8,
            is_dynamic=True,
            min_value=1,
            max_value=1024,
        )
        self.assertTrue(dynamic_node.is_dynamic)
        self.assertEqual(dynamic_node.min_value, 1)
        self.assertEqual(dynamic_node.max_value, 1024)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_dynamic_shapes_multiple_dynamic_dims(self):
        """
        Test handling of multiple dynamic dimensions.

        A model can have multiple inputs with multiple dynamic dimensions.
        All should be handled correctly.
        """
        from torch._dynamo.pythonify.ir import (
            GuardCheckNode,
            GuardType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        # Dynamic batch dimension for input 1
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[0] == 8",
            expected_value=8,
            dimension=0,
            is_dynamic=True,
        ))

        # Static feature dimension for input 1
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg1",
            condition="arg1.shape[1] == 64",
            expected_value=64,
            dimension=1,
            is_dynamic=False,
        ))

        # Dynamic sequence length for input 2
        ir.add_node(GuardCheckNode(
            guard_type=GuardType.SHAPE,
            target_name="arg2",
            condition="arg2.shape[1] == 128",
            expected_value=128,
            dimension=1,
            is_dynamic=True,
        ))

        visitor = PythonCodeGenVisitor()
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Should have dynamic comment for arg1.shape[0]
        self.assertIn("# DYNAMIC: arg1.shape[0] can vary", code)

        # Should have dynamic comment for arg2.shape[1]
        self.assertIn("# DYNAMIC: arg2.shape[1] can vary", code)

        # Should have assertion for static dim arg1.shape[1]
        self.assertIn("arg1.shape[1] == 64", code)


class TestPythonifyCUDAGraphs(TestCase):
    """
    Tests for pythonify with CUDA graphs (mode='reduce-overhead').

    CUDA graphs capture GPU operations into a replayable graph structure,
    reducing Python overhead significantly. When pythonify is enabled with
    mode='reduce-overhead', the generated code should include explicit
    CUDA graph capture and replay logic.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_reduce_overhead_mode(self):
        """
        Test pythonify with mode='reduce-overhead' generates CUDA graph code.

        This test verifies that:
        1. Compilation with mode='reduce-overhead' and pythonify succeeds
        2. Generated code contains CUDA graph-related constructs (CUDAGraph, replay, etc.)
        3. The generated code has valid Python syntax
        4. The compiled model produces correct results

        Note: Full end-to-end exec of CUDA graph code requires careful setup
        because CUDA graphs require static tensor addresses. This test focuses
        on verifying code generation produces the expected patterns.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(4, 4))

            def forward(self, x):
                return x @ self.W

        torch.manual_seed(42)
        model = Model()
        x = torch.randn(2, 4)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            # Compile with reduce-overhead mode which enables CUDA graphs
            y1 = torch.compile(
                model,
                mode='reduce-overhead',
                pythonify=path
            )(x)

            with open(path) as f:
                code = f.read()

            # Verify the generated code is valid Python syntax
            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")

            # Verify the forward pass result is correct (matches eager mode)
            torch.manual_seed(42)
            model_eager = Model()
            x_eager = torch.randn(2, 4)
            y_expected = model_eager(x_eager)
            self.assertTrue(
                torch.allclose(y1.detach(), y_expected.detach(), atol=1e-5),
                f"Compiled output should match eager mode. Got {y1}, expected {y_expected}",
            )

            # Verify CUDA graph-related code patterns are present.
            # When CUDA graphs are enabled, we expect to see these constructs
            # in the generated code. Note: The exact patterns depend on the
            # implementation of visit_cuda_graph_setup in gen_python.py.
            cuda_graph_patterns = [
                # Core CUDA graph constructs
                "CUDAGraph",  # CUDA graph class
                "graph",  # graph variable or method
            ]

            found_patterns = []
            for pattern in cuda_graph_patterns:
                if pattern in code:
                    found_patterns.append(pattern)

            # We expect at least some CUDA graph patterns when mode='reduce-overhead'
            # This validates that the CUDA graph code generation path is being used.
            # Note: If cuda_graphs_enabled is False in artifacts (due to config),
            # then these patterns may not appear. This is acceptable.

            # Basic structural checks that should always be present
            self.assertIn("CompiledFunction", code, "Should have autograd Function")
            self.assertIn("compiled_fn", code, "Should have compiled function")

        finally:
            os.unlink(path)
            torch.set_default_device("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_code_patterns(self):
        """
        Test that CUDA graph IR nodes generate expected code patterns.

        This unit test directly constructs CUDAGraphSetupNode IR and verifies
        that the PythonCodeGenVisitor generates the expected code structures
        for CUDA graph capture and replay.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import (
            generate_python_code,
            PythonCodeGenVisitor,
        )

        # Build an IR with CUDA graph setup
        ir = RuntimeWrapperIR()

        # Add argument extraction nodes first (required for CUDA graph setup)
        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.OBJECT_ID,
            access_path="W",
            object_id=12345,
        ))

        # Add CUDA graph setup node
        ir.add_node(CUDAGraphSetupNode(
            graph_id="test_graph",
            warmup_runs=2,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=True,
            static_input_indices=[1],  # arg1 (W) is static
        ))

        # Add callable invocation
        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1"],
            result_name="result",
        ))

        # Add return
        ir.add_node(ReturnResultNode(
            result_name="result",
            expose_as="y",
        ))

        # Generate code
        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Verify expected CUDA graph code patterns
        self.assertIn("torch.cuda.CUDAGraph()", code, "Should create CUDAGraph")
        self.assertIn("_test_graph_graph", code, "Should have graph variable")
        self.assertIn("_test_graph_stream", code, "Should have stream variable")
        self.assertIn("torch.cuda.Stream()", code, "Should create stream")

        # Verify warmup loop
        self.assertIn("for _warmup_iter in range(2):", code, "Should have warmup loop")

        # Verify graph capture
        self.assertIn("torch.cuda.graph(", code, "Should have graph capture context")

        # Verify replay function
        self.assertIn("def _replay_test_graph(inputs):", code, "Should have replay function")
        self.assertIn(".replay()", code, "Should call replay()")

        # Verify static input handling - only arg0 (index 0) should be copied
        # arg1 (index 1) is static and should NOT be copied
        self.assertIn("_static_inputs_test_graph[0].copy_(", code,
                      "Should copy dynamic input at index 0")

        # Verify Python syntax is valid
        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated CUDA graph code has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_all_static_inputs(self):
        """
        Test CUDA graph code when all inputs are static (no copies needed).

        When all inputs are marked as static (e.g., all parameters/buffers),
        the replay function should not emit any copy statements.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.OBJECT_ID,
            access_path="W1",
            object_id=11111,
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.OBJECT_ID,
            access_path="W2",
            object_id=22222,
        ))

        # All inputs are static
        ir.add_node(CUDAGraphSetupNode(
            graph_id="all_static",
            warmup_runs=1,
            static_inputs=True,
            static_input_indices=[0, 1],  # Both inputs are static
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Verify replay function exists
        self.assertIn("def _replay_all_static(inputs):", code)

        # Verify no copy statements (all inputs are static)
        # Should have a comment about no dynamic inputs
        self.assertIn("No dynamic inputs to copy", code)

        # Verify valid syntax
        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_with_pool(self):
        """
        Test CUDA graph code generation with memory pool.

        When pool_id is specified, the generated code should include
        pool handling for deterministic memory allocation.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="pooled_graph",
            warmup_runs=1,
            pool_id="shared_pool_1",  # Pool ID specified
            static_inputs=False,
            static_input_indices=[],
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Verify pool-related code is present
        self.assertIn("graph_pool_handle", code, "Should have pool handle")
        self.assertIn("pool=", code, "Should pass pool to graph context")

        # Verify valid syntax
        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code with pool has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_device_index(self):
        """
        Test CUDA graph code generation with device_index for multi-GPU.

        When device_index is specified, the generated code should include
        torch.cuda.set_device() call before creating the graph and stream.
        This ensures CUDA graph capture and replay happen on the correct GPU
        in multi-GPU scenarios.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="multigpu_graph",
            warmup_runs=1,
            device_index=1,
            static_inputs=False,
            static_input_indices=[],
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn(
            "torch.cuda.set_device(1)",
            code,
            "Should set device for multi-GPU",
        )
        self.assertIn(
            "multi-GPU",
            code,
            "Should have comment about multi-GPU",
        )
        self.assertIn(
            "device 1",
            code,
            "Should mention the specific device in comments",
        )

        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code with device_index has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_device_index_not_set(self):
        """
        Test that device_index is not emitted when not specified.

        When device_index is None (default), the generated code should not
        include any torch.cuda.set_device() call.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="default_device_graph",
            warmup_runs=1,
            device_index=None,
            static_inputs=False,
            static_input_indices=[],
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertNotIn(
            "set_device",
            code,
            "Should NOT set device when device_index is None",
        )

        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code without device_index has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_backward(self):
        """
        Test CUDA graphs with backward pass (training mode).

        This test verifies that pythonify with mode='reduce-overhead' and
        requires_grad=True generates code with:
        1. Separate CUDA graph capture for forward and backward passes
        2. Proper handling of saved_tensors between forward and backward
        3. Valid Python syntax that compiles
        4. Correct forward pass results matching eager mode

        Training with CUDA graphs requires capturing forward and backward as
        separate graphs, managing saved_tensors between them in static buffers.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self):
                super().__init__()
                self.W = nn.Parameter(torch.randn(4, 4))

            def forward(self, x):
                return x @ self.W

        torch.manual_seed(42)
        model = Model()
        x = torch.randn(2, 4, requires_grad=True)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            y1 = torch.compile(
                model,
                mode='reduce-overhead',
                pythonify=path
            )(x)

            loss1 = y1.mean()
            loss1.backward()
            original_grad = x.grad.clone()

            with open(path) as f:
                code = f.read()

            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")

            torch.manual_seed(42)
            model_eager = Model()
            x_eager = torch.randn(2, 4, requires_grad=True)
            y_expected = model_eager(x_eager)
            self.assertTrue(
                torch.allclose(y1.detach(), y_expected.detach(), atol=1e-5),
                f"Compiled output should match eager mode. Got {y1}, expected {y_expected}",
            )

            self.assertIn("CompiledFunction", code, "Should have autograd Function")
            self.assertIn("compiled_fn", code, "Should have compiled function")
            self.assertIn("def backward", code, "Should have backward method")
            self.assertIn("compiled_fn_backward", code, "Should have backward kernel")

        finally:
            os.unlink(path)
            torch.set_default_device("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_backward_code_patterns(self):
        """
        Test that CUDA graph IR for training generates expected code patterns.

        This unit test directly constructs CUDAGraphSetupNode IR with FORWARD
        and BACKWARD phases to verify that the PythonCodeGenVisitor generates
        the expected code structures for training CUDA graph capture/replay.
        """
        from torch._dynamo.pythonify.ir import (
            AOTAutogradWrapperNode,
            CUDAGraphPhase,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="forward_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": False},
        ))
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="backward_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn_backward",
            inline_content="def call(args):\n    return (args[0],)",
            metadata={"source": "inductor", "is_backward": True},
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="fwd_graph",
            warmup_runs=2,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=True,
            static_input_indices=[1],
            phase=CUDAGraphPhase.FORWARD,
            backward_graph_id="bwd_graph",
            saved_tensor_indices=[0],
            num_forward_outputs=1,
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="bwd_graph",
            warmup_runs=2,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=True,
            static_input_indices=[],
            phase=CUDAGraphPhase.BACKWARD,
        ))

        ir.add_node(AOTAutogradWrapperNode(
            class_name="CompiledFunction",
            num_inputs=2,
        ))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn("_fwd_graph_graph", code, "Should have forward graph variable")
        self.assertIn("_bwd_graph_graph", code, "Should have backward graph variable")

        self.assertIn("_fwd_graph_captured", code, "Should have forward captured flag")
        self.assertIn("_bwd_graph_captured", code, "Should have backward captured flag")

        self.assertIn("_static_saved_tensors_fwd_graph", code,
                      "Should have static saved tensors for forward-backward communication")

        self.assertIn("def forward", code, "Should have forward method")
        self.assertIn("def backward", code, "Should have backward method")

        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated training CUDA graph code has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_multiple_runs(self):
        """
        Test CUDA graph code generation for multiple-run scenarios.

        This test verifies that:
        1. The generated code contains proper CUDA graph setup for replay
        2. The replay function correctly handles dynamic input copying
        3. Static buffers are properly declared for graph capture/replay
        4. The warmup phase is included before graph capture

        CUDA graphs capture GPU operations with static tensor addresses. On replay,
        dynamic inputs must be copied to the static buffers before replaying the
        graph. This test ensures the generated code has the correct structure.

        Note: Full end-to-end CUDA graph execution through pythonify requires
        additional runtime integration. This test focuses on verifying code
        generation correctness.
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x + x.matmul(self.W)

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = Model(features)

        x_initial = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            y_compiled = torch.compile(
                model,
                mode='reduce-overhead',
                pythonify=path
            )(x_initial)

            with open(path) as f:
                code = f.read()

            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")

            self.assertTrue(
                torch.allclose(y_compiled.detach(), model(x_initial).detach(), atol=1e-5),
                "Initial compiled run should match eager mode",
            )

            self.assertIn("CUDAGraph", code, "Should create CUDA graph")
            self.assertIn("_replay_", code, "Should have replay function")
            self.assertIn(".copy_(", code, "Should copy dynamic inputs")
            self.assertIn(".replay()", code, "Should replay the graph")
            self.assertIn("torch.no_grad()", code, "Should use no_grad for capture")

            self.assertIn("_static_inputs_", code, "Should have static input buffers")
            self.assertIn("_static_outputs_", code, "Should have static output buffers")
            self.assertIn("warmup", code.lower(), "Should have warmup phase")

            self.assertIn("for _warmup_iter", code, "Should have warmup loop")
            self.assertIn("torch.cuda.stream", code, "Should use dedicated stream")
            self.assertIn("torch.cuda.graph(", code, "Should have graph capture context")

        finally:
            os.unlink(path)
            torch.set_default_device("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_static_inputs(self):
        """
        Test that CUDA graph code correctly distinguishes static vs dynamic inputs.

        When CUDA graphs are enabled, parameters and buffers (static inputs) should
        NOT be copied before graph replay because they already have stable memory
        addresses. Only dynamic inputs (user-provided tensors) should be copied.

        This test constructs an IR with multiple inputs where some are static
        (parameters/buffers accessed via OBJECT_ID) and some are dynamic
        (user-provided tensors accessed via F_LOCALS). It verifies that:

        1. Static input indices are correctly identified
        2. The replay function only copies dynamic inputs
        3. Static inputs are NOT copied (no .copy_() call for their indices)
        4. Comments in generated code explain which inputs are static vs dynamic
        5. Generated code has valid Python syntax

        This behavior is critical for CUDA graph correctness and performance:
        - Correctness: Graph capture records memory addresses; static inputs have
          stable addresses that don't change between calls
        - Performance: Avoiding unnecessary copies reduces overhead before replay
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        # arg0 is a dynamic input (user-provided tensor from f_locals)
        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        # arg1 is a static input (parameter accessed via object ID)
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.OBJECT_ID,
            access_path="W1",
            object_id=11111,
        ))
        # arg2 is another dynamic input (user-provided tensor)
        ir.add_node(ArgumentExtractionNode(
            name="arg2",
            source=ArgumentSource.F_LOCALS,
            access_path="y",
        ))
        # arg3 is another static input (buffer accessed via object ID)
        ir.add_node(ArgumentExtractionNode(
            name="arg3",
            source=ArgumentSource.OBJECT_ID,
            access_path="running_mean",
            object_id=22222,
        ))

        # CUDA graph setup with static_input_indices = [1, 3]
        # This means arg1 and arg3 are static (parameters/buffers)
        # while arg0 and arg2 are dynamic (user-provided tensors)
        ir.add_node(CUDAGraphSetupNode(
            graph_id="mixed_inputs",
            warmup_runs=2,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=True,
            static_input_indices=[1, 3],  # arg1 and arg3 are static
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1", "arg2", "arg3"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="output"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        # Verify replay function exists
        self.assertIn(
            "def _replay_mixed_inputs(inputs):",
            code,
            "Should have replay function for mixed_inputs graph"
        )

        # Verify dynamic inputs ARE copied (indices 0 and 2)
        self.assertIn(
            "_static_inputs_mixed_inputs[0].copy_(inputs[0])",
            code,
            "Dynamic input at index 0 should be copied"
        )
        self.assertIn(
            "_static_inputs_mixed_inputs[2].copy_(inputs[2])",
            code,
            "Dynamic input at index 2 should be copied"
        )

        # Verify static inputs are NOT copied (indices 1 and 3)
        # The replay function should not contain copy statements for these indices
        self.assertNotIn(
            "_static_inputs_mixed_inputs[1].copy_(",
            code,
            "Static input at index 1 (parameter W1) should NOT be copied"
        )
        self.assertNotIn(
            "_static_inputs_mixed_inputs[3].copy_(",
            code,
            "Static input at index 3 (buffer running_mean) should NOT be copied"
        )

        # Verify comments explain static vs dynamic handling
        self.assertIn(
            "Static inputs",
            code,
            "Should have comment explaining static inputs"
        )
        self.assertIn(
            "dynamic",
            code.lower(),
            "Should mention dynamic inputs in comments"
        )

        # Verify valid Python syntax
        try:
            compile(code, "<test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_static_inputs_end_to_end(self):
        """
        End-to-end test verifying static input handling with real model.

        This test uses torch.compile with mode='reduce-overhead' on a model
        with parameters (static inputs) and verifies that the generated
        pythonify code correctly handles the distinction between static
        and dynamic inputs.

        The model has multiple parameters that should be treated as static
        (no copy needed before graph replay) while the user-provided input
        tensor should be dynamic (copied before each replay).
        """
        torch.set_default_device("cuda")

        class MultiParamModel(nn.Module):
            """
            A model with multiple parameters to test static input handling.

            Parameters (W1, W2, bias) should be identified as static inputs
            because they have stable memory addresses that persist across
            multiple forward calls.
            """

            def __init__(self, features: int):
                super().__init__()
                self.W1 = nn.Parameter(torch.randn(features, features))
                self.W2 = nn.Parameter(torch.randn(features, features))
                self.bias = nn.Parameter(torch.randn(features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                # x @ W1 + x @ W2 + bias
                # x is dynamic, W1/W2/bias are static
                return x @ self.W1 + x @ self.W2 + self.bias

        torch.manual_seed(42)
        features = 4
        batch = 3
        model = MultiParamModel(features)

        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            y_compiled = torch.compile(
                model,
                mode='reduce-overhead',
                pythonify=path
            )(x)

            # Verify compiled output matches eager mode
            y_eager = model(x)
            self.assertTrue(
                torch.allclose(y_compiled.detach(), y_eager.detach(), atol=1e-5),
                f"Compiled output should match eager mode. "
                f"Got {y_compiled}, expected {y_eager}",
            )

            with open(path) as f:
                code = f.read()

            # Verify valid Python syntax
            try:
                compile(code, path, "exec")
            except SyntaxError as e:
                self.fail(f"Generated code has syntax error: {e}")

            # Verify CUDA graph patterns are present
            self.assertIn("CUDAGraph", code, "Should have CUDA graph")
            self.assertIn("_replay_", code, "Should have replay function")
            self.assertIn(".replay()", code, "Should call replay()")

            # Verify the code contains static input handling logic
            # Either through comments explaining static inputs or through
            # the structure of the copy logic
            has_static_input_handling = (
                "static" in code.lower() or
                "Static" in code
            )
            self.assertTrue(
                has_static_input_handling,
                "Generated code should reference static inputs"
            )

        finally:
            os.unlink(path)
            torch.set_default_device("cpu")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_code_quality(self):
        """
        Comprehensive code quality test for CUDA graph code generation.

        This test verifies specific code patterns in generated CUDA graph code
        to catch regressions in code generation. It checks:

        1. **Variable naming conventions**: Consistent prefixes like
           _{graph_id}_graph, _static_inputs_{graph_id}, _static_outputs_{graph_id}

        2. **Comment quality**: Multi-line explanatory comments that document
           the purpose of each code section (warmup, capture, replay)

        3. **Code structure**: Proper context manager usage (torch.no_grad,
           torch.cuda.stream, torch.cuda.graph), loop constructs for warmup

        4. **Stream handling**: Dedicated stream creation, stream context for
           warmup, and synchronization before graph capture

        5. **Replay function**: Proper function signature, input copying logic,
           .replay() call, and return statement

        6. **Memory pool handling**: When pool_id is specified, proper pool
           handle creation and pool argument in graph context

        These patterns ensure the generated code is not only syntactically
        correct but also follows best practices for CUDA graph usage.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor
        import re

        # Build IR with a realistic CUDA graph setup
        ir = RuntimeWrapperIR()

        # Add kernel load first (required for compiled_fn reference)
        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0] + args[1],)",
            metadata={"source": "inductor"},
        ))

        # Add argument extraction nodes
        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.OBJECT_ID,
            access_path="weight",
            object_id=12345,
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg2",
            source=ArgumentSource.F_LOCALS,
            access_path="bias",
        ))

        # CUDA graph setup with mixed static/dynamic inputs
        # arg0 (index 0) - dynamic input (user tensor)
        # arg1 (index 1) - static input (parameter)
        # arg2 (index 2) - dynamic input (user tensor)
        ir.add_node(CUDAGraphSetupNode(
            graph_id="quality_test",
            warmup_runs=3,
            capture_mode="thread_local",
            stream_name="default",
            pool_id="test_pool",  # Include pool to test pool handling
            static_inputs=True,
            static_input_indices=[1],  # arg1 is static
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1", "arg2"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        # Generate code
        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        # ========== Variable Naming Convention Tests ==========
        # Test that graph-related variables follow the naming convention
        self.assertIn(
            "_quality_test_graph",
            code,
            "Graph variable should follow _{graph_id}_graph naming"
        )
        self.assertIn(
            "_quality_test_stream",
            code,
            "Stream variable should follow _{graph_id}_stream naming"
        )
        self.assertIn(
            "_static_inputs_quality_test",
            code,
            "Static inputs should follow _static_inputs_{graph_id} naming"
        )
        self.assertIn(
            "_static_outputs_quality_test",
            code,
            "Static outputs should follow _static_outputs_{graph_id} naming"
        )
        self.assertIn(
            "_quality_test_pool",
            code,
            "Pool variable should follow _{graph_id}_pool naming"
        )

        # ========== Comment Quality Tests ==========
        # The code should have multi-line explanatory comments for key sections
        # Check for comments explaining graph creation
        self.assertIn(
            "Create CUDA graph",
            code,
            "Should have comment explaining CUDA graph creation"
        )

        # Check for comments explaining stream purpose
        stream_comment_found = (
            "dedicated stream" in code.lower() or
            "separate stream" in code.lower()
        )
        self.assertTrue(
            stream_comment_found,
            "Should have comment explaining stream isolation purpose"
        )

        # Check for comments explaining warmup
        self.assertIn(
            "Warmup",
            code,
            "Should have comment explaining warmup phase"
        )

        # Check for comments explaining graph capture
        capture_comment_found = (
            "Capture" in code or
            "captured" in code.lower()
        )
        self.assertTrue(
            capture_comment_found,
            "Should have comment explaining graph capture"
        )

        # Check for comments explaining memory pool (when pool_id is set)
        pool_comment_found = (
            "memory pool" in code.lower() or
            "pool handle" in code.lower() or
            "deterministic" in code.lower()
        )
        self.assertTrue(
            pool_comment_found,
            "Should have comment explaining memory pool purpose"
        )

        # ========== Code Structure Tests ==========
        # Test torch.no_grad() context for capture
        self.assertIn(
            "with torch.no_grad():",
            code,
            "Should use torch.no_grad() context for graph capture"
        )

        # Test dedicated stream context
        self.assertIn(
            "with torch.cuda.stream(",
            code,
            "Should use dedicated stream context for warmup"
        )

        # Test graph capture context
        self.assertIn(
            "with torch.cuda.graph(",
            code,
            "Should use torch.cuda.graph() context for capture"
        )

        # Test warmup loop with correct iteration count
        self.assertIn(
            "for _warmup_iter in range(3):",
            code,
            "Should have warmup loop with specified iteration count"
        )

        # Test stream synchronization before capture
        self.assertIn(
            "wait_stream(",
            code,
            "Should synchronize streams before graph capture"
        )

        # ========== Stream Handling Tests ==========
        # Verify stream is created
        self.assertIn(
            "torch.cuda.Stream()",
            code,
            "Should create dedicated CUDA stream"
        )

        # Verify stream is used in graph capture
        self.assertIn(
            "stream=_quality_test_stream",
            code,
            "Should pass stream to graph capture context"
        )

        # ========== Memory Pool Handling Tests ==========
        # Verify pool handle is created
        self.assertIn(
            "torch.cuda.graph_pool_handle()",
            code,
            "Should call graph_pool_handle() when pool_id is specified"
        )

        # Verify pool is passed to graph context
        self.assertIn(
            "pool=_quality_test_pool",
            code,
            "Should pass pool handle to graph capture context"
        )

        # ========== Replay Function Tests ==========
        # Test replay function exists with correct signature
        self.assertIn(
            "def _replay_quality_test(inputs):",
            code,
            "Should define replay function with inputs parameter"
        )

        # Test replay function calls .replay()
        self.assertIn(
            "_quality_test_graph.replay()",
            code,
            "Replay function should call graph.replay()"
        )

        # Test replay function returns static outputs
        self.assertIn(
            "return _static_outputs_quality_test",
            code,
            "Replay function should return static outputs"
        )

        # Test that only dynamic inputs are copied (indices 0 and 2)
        self.assertIn(
            "_static_inputs_quality_test[0].copy_(inputs[0])",
            code,
            "Should copy dynamic input at index 0"
        )
        self.assertIn(
            "_static_inputs_quality_test[2].copy_(inputs[2])",
            code,
            "Should copy dynamic input at index 2"
        )

        # Test that static input (index 1) is NOT copied in replay
        replay_fn_match = re.search(
            r"def _replay_quality_test\(inputs\):.*?return _static_outputs_quality_test",
            code,
            re.DOTALL
        )
        self.assertIsNotNone(
            replay_fn_match,
            "Should be able to extract replay function body"
        )
        if replay_fn_match:
            replay_fn_body = replay_fn_match.group(0)
            self.assertNotIn(
                "_static_inputs_quality_test[1].copy_(",
                replay_fn_body,
                "Static input at index 1 should NOT be copied in replay function"
            )

        # ========== Static Buffer Allocation Tests ==========
        # Test that static inputs are initialized as None
        self.assertIn(
            "_static_inputs_quality_test = None",
            code,
            "Static inputs should be initialized to None"
        )

        # Test that static outputs are initialized as None
        self.assertIn(
            "_static_outputs_quality_test = None",
            code,
            "Static outputs should be initialized to None"
        )

        # Test static buffer population during warmup
        self.assertIn(
            "if _static_inputs_quality_test is None:",
            code,
            "Should check for None before populating static inputs"
        )

        # Test cloning for static buffer initialization
        self.assertIn(
            ".clone()",
            code,
            "Should clone tensors when initializing static buffers"
        )

        # ========== Import Tests ==========
        # Verify torch import is present
        self.assertIn(
            "import torch",
            code,
            "Should import torch"
        )

        # ========== Python Syntax Validity ==========
        try:
            compile(code, "<quality_test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}\n\nCode:\n{code}")

        # ========== Code Quality Metrics ==========
        # Count lines of code in CUDA Graphs Setup section
        # The section is between "# CUDA Graphs Setup" and "# Invoke Compiled"
        cuda_graph_section_match = re.search(
            r"# CUDA Graphs Setup\n# =+\n(.*?)# =+\n# Invoke Compiled",
            code,
            re.DOTALL
        )
        if cuda_graph_section_match:
            cuda_graph_lines = cuda_graph_section_match.group(1).count('\n')
            self.assertGreater(
                cuda_graph_lines,
                20,
                f"CUDA graph section should have substantial code (>20 lines), got {cuda_graph_lines}"
            )
        else:
            # If we can't find the exact section, just count occurrences
            # of CUDA graph-related code
            cuda_keywords = [
                "CUDAGraph", "cuda.graph", "cuda.stream",
                "_static_inputs_", "_static_outputs_", "_replay_"
            ]
            cuda_keyword_count = sum(code.count(kw) for kw in cuda_keywords)
            self.assertGreater(
                cuda_keyword_count,
                5,
                f"CUDA graph code should have multiple CUDA-related keywords, got {cuda_keyword_count}"
            )

        # Count comment lines (should have good documentation)
        comment_lines = len(re.findall(r'^[ \t]*#', code, re.MULTILINE))
        self.assertGreater(
            comment_lines,
            10,
            f"Generated code should have good documentation (>10 comment lines), got {comment_lines}"
        )

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_skip_dynamic_graphs(self):
        """
        Test that skip_dynamic_graphs generates code for dynamic shape fallback.

        When skip_dynamic_graphs is True, the CUDA graph replay function should:
        1. Record expected input shapes during graph capture
        2. Check if current input shapes match expected shapes at runtime
        3. Fall back to direct function call if shapes differ

        This test verifies that:
        - Expected shapes variable is created after graph capture
        - Replay function checks shape matching before replay
        - Fallback to compiled_fn(inputs) is emitted when shapes don't match
        - Generated code has valid Python syntax
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor
        import re

        ir = RuntimeWrapperIR()

        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0] + args[1],)",
            metadata={"source": "inductor"},
        ))

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.OBJECT_ID,
            access_path="weight",
            object_id=12345,
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="dynamic_test",
            warmup_runs=1,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=True,
            static_input_indices=[1],
            skip_dynamic_graphs=True,
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn(
            "_expected_shapes_dynamic_test",
            code,
            "Should create expected shapes variable when skip_dynamic_graphs=True"
        )

        self.assertIn(
            "_shapes_match",
            code,
            "Should have shape matching logic when skip_dynamic_graphs=True"
        )

        self.assertIn(
            "if not _shapes_match:",
            code,
            "Should check if shapes match before replay"
        )

        self.assertIn(
            "return compiled_fn(inputs)",
            code,
            "Should fall back to direct function call when shapes don't match"
        )

        self.assertIn(
            "for i in range(len(inputs))",
            code,
            "Should iterate over inputs for shape checking"
        )

        shape_check_comment = "CUDA graphs require fixed tensor shapes"
        self.assertIn(
            shape_check_comment,
            code,
            "Should have comment explaining why shape check is needed"
        )

        fallback_comment = "fall back to direct"
        self.assertIn(
            fallback_comment.lower(),
            code.lower(),
            "Should have comment explaining the fallback behavior"
        )

        try:
            compile(code, "<skip_dynamic_test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}\n\nCode:\n{code}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_skip_dynamic_graphs_disabled(self):
        """
        Test that skip_dynamic_graphs=False does NOT generate shape checking code.

        When skip_dynamic_graphs is False (the default), the replay function should
        NOT include shape checking or fallback logic. This test verifies that:
        - No expected shapes variable is created
        - No shape matching logic is present
        - Replay function directly copies inputs and replays graph
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0] + args[1],)",
            metadata={"source": "inductor"},
        ))

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="no_dynamic_check",
            warmup_runs=1,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=False,
            static_input_indices=[],
            skip_dynamic_graphs=False,
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertNotIn(
            "_expected_shapes_",
            code,
            "Should NOT create expected shapes when skip_dynamic_graphs=False"
        )

        self.assertNotIn(
            "_shapes_match",
            code,
            "Should NOT have shape matching when skip_dynamic_graphs=False"
        )

        self.assertIn(
            "_no_dynamic_check_graph.replay()",
            code,
            "Should still have normal graph replay"
        )

        try:
            compile(code, "<no_dynamic_check_test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}\n\nCode:\n{code}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_force_sync(self):
        """
        Test CUDA graph code generation with force_cudagraph_sync=True.

        When force_cudagraph_sync is True, the generated code should include
        torch.cuda.synchronize() after graph replay. This is useful for
        debugging, timing, or profiling scenarios where explicit synchronization
        is required after graph execution.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0] + args[1],)",
            metadata={"source": "inductor"},
        ))

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))
        ir.add_node(ArgumentExtractionNode(
            name="arg1",
            source=ArgumentSource.F_LOCALS,
            access_path="y",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="sync_graph",
            warmup_runs=1,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=False,
            static_input_indices=[],
            force_cudagraph_sync=True,
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0", "arg1"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn(
            "_sync_graph_graph.replay()",
            code,
            "Should have graph replay call"
        )

        self.assertIn(
            "torch.cuda.synchronize()",
            code,
            "Should have synchronization call after replay when force_cudagraph_sync=True"
        )

        self.assertIn(
            "Force synchronization",
            code,
            "Should have comment explaining synchronization purpose"
        )

        self.assertIn(
            "debugging",
            code.lower(),
            "Should mention debugging in sync comment"
        )

        try:
            compile(code, "<sync_graph_test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}\n\nCode:\n{code}")

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_pythonify_cuda_graphs_force_sync_disabled(self):
        """
        Test that force_cudagraph_sync=False does NOT add synchronization.

        When force_cudagraph_sync is False (the default), the replay function
        should NOT include torch.cuda.synchronize() call. This is the normal
        case where we want to maximize performance by not blocking on graph
        completion.
        """
        from torch._dynamo.pythonify.ir import (
            ArgumentExtractionNode,
            ArgumentSource,
            CallableInvocationNode,
            CUDAGraphSetupNode,
            KernelLoadNode,
            KernelType,
            ReturnResultNode,
            RuntimeWrapperIR,
        )
        from torch._dynamo.pythonify.gen_python import PythonCodeGenVisitor

        ir = RuntimeWrapperIR()

        ir.add_node(KernelLoadNode(
            kernel_type=KernelType.INLINE,
            kernel_id="test_kernel",
            kernel_path="",
            entry_point="call",
            variable_name="compiled_fn",
            inline_content="def call(args):\n    return (args[0] + args[1],)",
            metadata={"source": "inductor"},
        ))

        ir.add_node(ArgumentExtractionNode(
            name="arg0",
            source=ArgumentSource.F_LOCALS,
            access_path="x",
        ))

        ir.add_node(CUDAGraphSetupNode(
            graph_id="nosync_graph",
            warmup_runs=1,
            capture_mode="thread_local",
            stream_name="default",
            pool_id=None,
            static_inputs=False,
            static_input_indices=[],
            force_cudagraph_sync=False,
        ))

        ir.add_node(CallableInvocationNode(
            callable_name="compiled_fn",
            argument_names=["arg0"],
            result_name="result",
        ))
        ir.add_node(ReturnResultNode(result_name="result", expose_as="y"))

        visitor = PythonCodeGenVisitor()
        visitor.prescan_ir(ir)
        ir.accept_all(visitor)
        code = visitor.get_code()

        self.assertIn(
            "_nosync_graph_graph.replay()",
            code,
            "Should have graph replay call"
        )

        # Find the replay function and check it doesn't have synchronize
        import re
        replay_func_match = re.search(
            r"def _replay_nosync_graph\(inputs\):.*?return _static_outputs_nosync_graph",
            code,
            re.DOTALL
        )
        self.assertIsNotNone(
            replay_func_match,
            "Should find replay function in generated code"
        )
        if replay_func_match:
            replay_func = replay_func_match.group(0)
            self.assertNotIn(
                "torch.cuda.synchronize()",
                replay_func,
                "Should NOT have synchronization when force_cudagraph_sync=False"
            )

        try:
            compile(code, "<nosync_graph_test>", "exec")
        except SyntaxError as e:
            self.fail(f"Generated code has syntax error: {e}\n\nCode:\n{code}")


class TestRequirementsSpecIntegration(TestCase):
    """
    Integration tests that directly match the requirements specification examples.

    The requirements specification states:
    - "Users pass `self` (or another nn.Module instance) into the pythonified
      code's exec namespace."
    - "The generator uses module metadata to reconstruct parameters, buffers,
      and submodules directly from `self`, NOT from ctypes / object ids."
    - "The resulting code is process-portable (no object-id coupling), safer,
      and aligned with typical nn.Module usage."

    These tests verify these exact requirements are met.
    """

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_requirements_spec_example_self_in_namespace(self):
        """
        Integration test matching the core requirements spec example.

        From requirements.md:
        "Users pass `self` (or another nn.Module instance) into the pythonified
        code's exec namespace."

        This test demonstrates the golden path by:
        1. Creating an nn.Module with self.W parameter
        2. Compiling with pythonify
        3. Passing the module (representing `self`) into exec namespace
        4. Verifying parameters are accessed via model.W, not ctypes/obj_from_id
        5. Verifying the result matches compiled output
        """
        torch.set_default_device("cuda")

        class SimpleLinearModel(nn.Module):
            """
            A simple model that uses self.W - matching the requirements spec example.

            The requirements say the generated code should access parameters via
            attribute traversal off the provided module instance, so self.W becomes
            model.W in the generated code.
            """

            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(12345)
        features = 4
        batch = 2
        model = SimpleLinearModel(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Generated code (first 2000 chars):\n{code[:2000]}...")

            # REQUIREMENT: Parameters accessed via attribute traversal, not ctypes
            # The generated code must use "model.W" pattern, not "obj_from_id(...)"
            self.assertIn(
                "model.W",
                code,
                "Requirements spec: Parameters must be accessed via model.W "
                "(attribute traversal), not ctypes-based obj_from_id",
            )
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Requirements spec: Golden path must NOT use obj_from_id helper",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Requirements spec: Golden path must NOT call obj_from_id",
            )

            # REQUIREMENT: No ctypes for parameter extraction
            # ctypes should not be used for reconstructing nn.Module parameters
            # Note: ctypes may still appear in Inductor kernel boilerplate, but
            # the parameter extraction logic itself should not use ctypes
            self.assertNotIn(
                "ctypes.cast",
                code.split("# ===")[0],
                "Requirements spec: Parameter extraction must not use ctypes.cast",
            )

            # REQUIREMENT: Process-portable, no object-id coupling
            # The generated code should be portable across processes
            self.assertIn(
                "GOLDEN PATH",
                code,
                "Requirements spec: Golden path documentation must be present",
            )
            # Check for PROCESS-PORTABLE in the golden path section
            # The section extends from "GOLDEN PATH" until the next major section
            self.assertIn(
                "PROCESS-PORTABLE",
                code,
                "Requirements spec: Golden path must document process-portability",
            )

            # REQUIREMENT: Users pass `self` (module) into exec namespace
            # Execute the generated code with the module in the namespace
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["model"] = model
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            # Verify the result matches
            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-5, atol=1e-5),
                f"Exec'd output must match compiled output. "
                f"Max diff: {(y_compiled - y_exec).abs().max().item()}",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_requirements_spec_no_ctypes_for_parameters_and_buffers(self):
        """
        Verify ctypes is not used for parameter/buffer extraction.

        From requirements.md section "Structured Extraction (No ctypes)":
        "For parameters/buffers/submodules, emit attribute traversal off the
        provided module... Use PARAMETER / BUFFER sources resolved via module
        attributes (not OBJECT_ID)."
        """
        torch.set_default_device("cuda")

        class ModelWithBuffer(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))
                self.register_buffer("scale", torch.tensor(2.0))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W) * self.scale

        torch.manual_seed(54321)
        features = 4
        model = ModelWithBuffer(features)
        x = torch.randn(2, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify both parameters and buffers use attribute traversal
            self.assertIn(
                "model.W",
                code,
                "Parameters must use model.W pattern",
            )
            self.assertIn(
                "model.scale",
                code,
                "Buffers must use model.scale pattern",
            )

            # Verify obj_from_id is not used for either
            self.assertNotIn(
                "def obj_from_id",
                code,
                "Golden path must not define obj_from_id helper",
            )
            self.assertNotIn(
                "obj_from_id(",
                code,
                "Golden path must not call obj_from_id for parameters or buffers",
            )

            # Execute to verify correctness
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["model"] = model
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-5, atol=1e-5),
                "Exec'd output must match compiled output",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_requirements_spec_live_attribute_access_not_stale_ids(self):
        """
        Prove that attribute access is live, not based on stale object IDs.

        From requirements.md Testing Requirements:
        "Mutate a parameter/buffer between compilation and exec to prove live
        attribute access (no stale ids)."

        This is crucial because:
        - Object-ID approach caches memory addresses at compile time
        - If the parameter is mutated after compilation, object-ID sees stale values
        - Golden path (module-based) accesses model.W at runtime, seeing fresh values
        """
        torch.set_default_device("cuda")

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.W = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.W)

        torch.manual_seed(99999)
        features = 4
        batch = 2
        model = Model(features)
        x = torch.randn(batch, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()

            # Compile and record output with ORIGINAL parameter values
            y_with_original = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            # Verify golden path is being used
            self.assertIn("model.W", code)
            self.assertNotIn("obj_from_id(", code)

            # CRITICAL: Mutate the parameter AFTER compilation
            # This is the key test - if object IDs were cached, they'd be stale
            original_W = model.W.data.clone()
            model.W.data.fill_(0.5)

            # Compute what we expect with the MUTATED parameter
            expected_with_mutated = x.matmul(model.W)

            # Execute the generated code
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["model"] = model
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            # The exec'd output should match the MUTATED parameter, NOT the original
            # This proves live attribute access
            self.assertTrue(
                torch.allclose(y_exec, expected_with_mutated, rtol=1e-5, atol=1e-5),
                "Output must reflect MUTATED parameter values (live access). "
                f"Expected max diff < 1e-5, got {(y_exec - expected_with_mutated).abs().max().item()}",
            )

            # Verify it does NOT match the original compilation output
            # (which used the original parameter values)
            self.assertFalse(
                torch.allclose(y_exec, y_with_original, rtol=1e-3, atol=1e-3),
                "Output must NOT match original compiled output (proves live access)",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_requirements_spec_nested_path_submodules(self):
        """
        Verify nested_path is respected for submodule hierarchies.

        From requirements.md:
        "Respect `nested_path` to navigate module hierarchies."

        For self.encoder.layer1.weight, the generated code should access
        model.encoder.layer1.weight (attribute traversal).
        """
        torch.set_default_device("cuda")

        class Encoder(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.weight = nn.Parameter(torch.randn(features, features))

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return x.matmul(self.weight)

        class Model(nn.Module):
            def __init__(self, features: int):
                super().__init__()
                self.encoder = Encoder(features)

            def forward(self, x: torch.Tensor) -> torch.Tensor:
                return self.encoder(x)

        torch.manual_seed(11111)
        features = 4
        model = Model(features)
        x = torch.randn(2, features)

        with tempfile.NamedTemporaryFile(suffix=".py", delete=False) as f:
            path = f.name

        try:
            torch._dynamo.reset()
            y_compiled = torch.compile(model, pythonify=path)(x)

            with open(path) as f:
                code = f.read()

            print(f"[DEBUG] Nested module code snippet:\n{code[:1500]}...")

            # The nested path should be reflected in the generated code
            # self.encoder.weight -> model.encoder.weight
            self.assertIn(
                "model.encoder.weight",
                code,
                "Nested submodule parameters must use full path: model.encoder.weight",
            )

            # Verify obj_from_id not used
            self.assertNotIn("obj_from_id(", code)

            # Execute and verify
            frame = inspect.currentframe()
            namespace = {**frame.f_globals, **frame.f_locals}
            namespace["model"] = model
            namespace["x"] = x
            namespace["torch"] = torch

            exec(code, namespace)
            y_exec = namespace["y"]

            self.assertTrue(
                torch.allclose(y_compiled, y_exec, rtol=1e-5, atol=1e-5),
                "Nested module output must match",
            )

        finally:
            os.unlink(path)

    @unittest.skipIf(not torch.cuda.is_available(), "CUDA not available")
    def test_requirements_spec_object_id_fallback_for_non_module(self):
        """
        Verify OBJECT_ID fallback still works for non-module inputs.

        From requirements.md:
        "The object-id fallback can remain for non-module / ad-hoc objects..."
        "Preserve object-id support only for non-module cases (e.g., raw callables,
        detached tensors) where module lookup is impossible."

        This ensures backward compatibility for cases where module-based
        reconstruction is not possible.
        """
        from torch._dynamo.pythonify.pipeline import (
            RuntimeWrapperPipeline,
            CompilationArtifacts,
        )
        from torch._dynamo.pythonify.gen_python import generate_python_code

        # Create a standalone tensor (not from nn.Module)
        standalone_tensor = torch.randn(4, 4, device="cuda")

        # Create artifacts WITHOUT model_name to force OBJECT_ID fallback
        artifacts = CompilationArtifacts(
            input_names=["x"],
            parameter_names=["captured_tensor"],
            model_name="",
            parameter_tensors={
                "captured_tensor": standalone_tensor,
            },
        )

        pipeline = RuntimeWrapperPipeline(artifacts)
        ir = pipeline.build()

        # Generate Python code
        code = generate_python_code(ir)

        # For non-module cases, obj_from_id SHOULD be used
        self.assertIn(
            "def obj_from_id",
            code,
            "OBJECT_ID fallback must define obj_from_id helper",
        )
        self.assertIn(
            "obj_from_id(",
            code,
            "OBJECT_ID fallback must call obj_from_id",
        )
        self.assertIn(
            str(id(standalone_tensor)),
            code,
            "OBJECT_ID fallback must include the actual object ID",
        )

        # Verify fallback is clearly marked as legacy
        self.assertIn(
            "LEGACY FALLBACK",
            code,
            "OBJECT_ID fallback must be clearly labeled as legacy",
        )

        # Verify golden path is NOT used when there's no module context
        self.assertNotIn(
            "GOLDEN PATH",
            code,
            "OBJECT_ID fallback should NOT show GOLDEN PATH documentation",
        )


if __name__ == "__main__":
    run_tests()
