# Configuration for linkchecker (https://github.com/linkchecker/linkchecker)
# Reference: https://linkchecker.github.io/linkchecker/man/linkcheckerrc.html
#
# Checks internal links and anchors in the built HTML documentation.
# Run after building docs:
#   cd docs && make html && cd ..
#   linkchecker --config=.linkcheckerrc docs/build/html/index.html
#
# Or use:  make -C docs linkcheck-local

[output]

[text]
colorwarning=blue

# Enable anchor checking — verifies that #fragment targets resolve to
# an actual id in the target page.
# Note: anchor checking fails on GitHub .md pages because they use JS
# to create anchor ids, but it works well on normal HTML tags.
[AnchorCheck]

[filtering]
ignorewarnings=http-redirected,http-moved-permanent

# Only check internal links by default. External URLs are already
# handled by scripts/lint_urls.sh in the _link_check.yml workflow.
#
# Also don't recurse into /generated/ API stubs — links TO those pages
# (and their anchors) are still checked, but we skip crawling the
# ~2500 stubs for outgoing links since those are auto-generated by
# Sphinx and reliable.  This cuts runtime significantly.
nofollow=
    https?://.*
    .*/generated/.*

# Skip source views, static assets, and all external URLs.
# External URLs are already validated by scripts/lint_urls.sh.
# GitHub uses JS to generate anchor IDs (line numbers, headings in
# .md files, etc.) so linkchecker will always report false "anchor
# not found" errors on them since it doesn't execute JS.
#
# Also skip images, videos, and other binary files since we only care
# about link integrity in documentation pages, not asset validation.
ignore=
    ^https?://
    _modules/
    _sources/
    _static/
    genindex\.html
    py-modindex\.html
    search\.html
    \.png$
    \.jpg$
    \.jpeg$
    \.gif$
    \.svg$
    \.ico$
    \.webp$
    \.pdf$
    \.zip$
    \.tar\.gz$
    \.whl$
    ^mailto:

[checking]
# Default threads - can be overridden via CLI: linkchecker -t <num>
# CI workflows should use: -t $(nproc) to match available CPU cores
threads=10
timeout=30
# Limit recursion depth to avoid crawling the entire site.
# Depth 3 covers: index -> section pages -> detail pages -> linked pages
# This catches most broken anchors while keeping runtime reasonable (~10-20 min).
recursionlevel=3
