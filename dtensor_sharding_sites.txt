# DTensor Sharding Sites with Potential Data-Dependent Errors

This document identifies locations in PyTorch's DTensor implementation where data-dependent errors could occur during sharding propagation decisions based on input tensor shapes.

## Utility Functions (torch/distributed/tensor/_ops/utils.py)

### is_tensor_shardable
Location: https://github.com/pytorch/pytorch/blob/fa560e1158b496d1c37ca2a2443a0b7f5a08dad5/torch/distributed/tensor/_ops/utils.py#L175

**Purpose:** Filters sharding strategies generally (expand_to_full_mesh_op_strategy) and for matmul operations specifically.

**Behavior:** Returns False if sharded dimension < number of shards.

**Considerations:**
- Discussion in https://github.com/pytorch/pytorch/issues/165034 suggests that zero-size shards may be acceptable, making it potentially safe to default to True (shardable).
- When encountering a sharded dimension with a large upper bound (e.g., u0: [0, 4096]) and relatively small mesh dimension (e.g., 8), automatically filtering out the strategy appears overly conservative to avoid zero-dimension shards in edge cases like u0: [0, 7].

### is_tensor_evenly_shardable
Location: https://github.com/pytorch/pytorch/blob/fa560e1158b496d1c37ca2a2443a0b7f5a08dad5/torch/distributed/tensor/_ops/utils.py#L181

**Purpose:** Validates whether AVG reduction across shards is feasible.

**Usage contexts:**
1. AVG reduction feasibility check:
   https://github.com/pytorch/pytorch/blob/fa560e1158b496d1c37ca2a2443a0b7f5a08dad5/torch/distributed/tensor/_ops/_math_ops.py#L291
   - Using statically_known_true appears appropriate unless recompiling to enable AVG reduction is highly desirable.
   - Note: Weighted average reduction is not supported.

2. new_empty_strided validation (related to last shard's striding):
   https://github.com/pytorch/pytorch/blob/fa560e1158b496d1c37ca2a2443a0b7f5a08dad5/torch/distributed/tensor/_ops/_tensor_ops.py#L229

### infer_broadcast_dims_map
Location: https://github.com/pytorch/pytorch/blob/fa560e1158b496d1c37ca2a2443a0b7f5a08dad5/torch/distributed/tensor/_ops/utils.py#L232

**Purpose:** Determines placement mapping after a tensor has been broadcasted or unsqueezed in the front dimensions.

**Dynamic shapes considerations:**
- For the dynamic case, direct symbol comparisons using statically_known_true are likely preferred.
- Potential issues with plain == comparisons:
  * May guard on equality for different backed symbols with identical hints
  * May trigger data-dependent errors on different unbacked symbols with no established relations

## Operator Sharding Rules

Many operator sharding rules require checking statically known properties.

### scatter_add
Location: https://github.com/pytorch/pytorch/blob/main/torch/distributed/tensor/_ops/_tensor_ops.py#L618

**Rule:** If dimension d is equal for input and index tensors, and d is not the scatter dimension, then data parallelism (DP) is possible and sharding on d is allowed.

### gather
Location: https://github.com/pytorch/pytorch/blob/d1a6e006e090b1365e6653853982f175cca452e8/torch/distributed/tensor/_ops/_tensor_ops.py#L645-L655

**Rule:** MaskPartial placement is an option when the index tensor has size 1 in the indexing dimension.

**Note:** Further investigation needed to fully understand the implications.

### split (with split_size)
Location: https://github.com/pytorch/pytorch/blob/d1a6e006e090b1365e6653853982f175cca452e8/torch/distributed/tensor/_ops/_tensor_ops.py#L1169

**Behavior:** Returns an extra chunk if the tensor is not evenly divisible by split_size.

**Dynamic shapes considerations:**
- For unbacked dimensions: Splitting would likely trigger data-dependent errors during fake tensor propagation.
- For backed dimensions: Recompilation on this condition is feasible.

### Pointwise operations
Location: https://github.com/pytorch/pytorch/blob/fc540cefd498f1001a5fc7a4f187080628b27839/torch/distributed/tensor/_ops/_common_rules.py#L254

**Rule:** Uses == 1 check for annotating size-1 dimensions.

### propagate_shape_and_sharding
Location: https://github.com/pytorch/pytorch/blob/fc540cefd498f1001a5fc7a4f187080628b27839/torch/distributed/tensor/_ops/_view_ops.py#L586-L635

**Rule:** Enforces divisibility requirements by mesh dimension size.

**Note:** Further investigation needed to fully understand the implications.
