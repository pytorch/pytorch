I1117 22:09:56.512000 212882 torch/_inductor/config.py:998] compile_threads set to 32
I1117 22:09:57.819000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.819000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_mm_plus_mm'', 'device_type=None', 'op_name=None'
I1117 22:09:57.820000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::bmm'', 'device_type=None', 'op_name=None'
I1117 22:09:57.820000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_int_mm'', 'device_type=None', 'op_name=None'
I1117 22:09:57.820000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::_scaled_mm'', 'device_type=None', 'op_name=None'
I1117 22:09:57.821000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm_dtype'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.821000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenConfigHeuristics for 'template_name='aten::mm'', 'device_type=None', 'op_name=None'
I1117 22:09:57.822000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::baddbmm'', 'device_type=None', 'op_name='baddbmm''
I1117 22:09:57.822000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenAddMMConfigHeuristics for 'template_name='aten::addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:09:57.822000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: ATenBiasAddMMConfigHeuristics for 'template_name='aten::bias_addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:09:57.823000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_addmm'', 'device_type=None', 'op_name='addmm''
I1117 22:09:57.823000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyContiguousMMConfigHeuristics for 'template_name='contiguous_mm'', 'device_type=None', 'op_name='mm''
I1117 22:09:57.824000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: EmptyDecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type=None', 'op_name='mm''
I1117 22:09:57.824000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: DecomposeKConfigHeuristics for 'template_name='decompose_k'', 'device_type='cuda'', 'op_name='mm''
I1117 22:09:57.828000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.829000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.829000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cuda'', 'op_name='baddbmm''
I1117 22:09:57.829000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:09:57.830000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMAHTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='mm-ah''
I1117 22:09:57.830000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.831000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.831000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:09:57.831000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDABlackwellAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='addmm''
I1117 22:09:57.832000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:09:57.832000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAEpilogueScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_epilogue_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:09:57.833000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledTMAMainLoopScalingTemplateConfigHeuristic for 'template_name='triton::scaled_mm_device_tma_main_loop_scaling'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:09:57.833000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAScaledBlackwellTMATemplateConfigHeuristic for 'template_name='triton::blackwell_ws_persistent_device_tma'', 'device_type='cuda'', 'op_name='scaled_mm''
I1117 22:09:57.833000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cuda'', 'op_name=None'
I1117 22:09:57.834000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CUDAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cuda'', 'op_name='int_mm''
I1117 22:09:57.834000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name=None'
I1117 22:09:57.835000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name=None'
I1117 22:09:57.835000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='cpu'', 'op_name='baddbmm''
I1117 22:09:57.835000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='addmm''
I1117 22:09:57.836000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='scaled_mm''
I1117 22:09:57.836000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='cpu'', 'op_name='int_mm''
I1117 22:09:57.837000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: CPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='cpu'', 'op_name=None'
I1117 22:09:57.837000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name=None'
I1117 22:09:57.837000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name=None'
I1117 22:09:57.838000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='xpu'', 'op_name='baddbmm''
I1117 22:09:57.838000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='addmm''
I1117 22:09:57.839000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name=None'
I1117 22:09:57.839000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUAddmmPersistentTMATemplateConfigHeuristic for 'template_name='triton::mm_persistent_tma'', 'device_type='xpu'', 'op_name='addmm''
I1117 22:09:57.839000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='scaled_mm''
I1117 22:09:57.840000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='xpu'', 'op_name='int_mm''
I1117 22:09:57.840000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: XPUMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='xpu'', 'op_name=None'
I1117 22:09:57.840000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name=None'
I1117 22:09:57.841000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name=None'
I1117 22:09:57.841000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::bmm'', 'device_type='mtia'', 'op_name='baddbmm''
I1117 22:09:57.842000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAAddMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='addmm''
I1117 22:09:57.842000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAScaledMMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='scaled_mm''
I1117 22:09:57.842000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAInt8MMTemplateConfigHeuristic for 'template_name='triton::mm'', 'device_type='mtia'', 'op_name='int_mm''
I1117 22:09:57.843000 212882 torch/_inductor/template_heuristics/registry.py:65] Registered template heuristic: MTIAMMPlusMMTemplateConfigHeuristic for 'template_name='triton::mm_plus_mm'', 'device_type='mtia'', 'op_name=None'
I1117 22:09:58.575000 212882 torch/_inductor/async_compile.py:258] [0/0] Creating 'subprocess' pool with 32 workers
I1117 22:09:58.673000 212882 torch/_inductor/compile_worker/subproc_pool.py:170] [0/0] Suppressing compile worker output due to config
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] TRACED GRAPH
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  ===== BEFORE PRE GRAD =====
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]     def forward(self, L_x_: "f32[2, 256, 128][32768, 128, 1]cuda:0", L_weight_: "f32[128][1]cuda:0"):
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_x_ = L_x_
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         l_weight_ = L_weight_
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:110 in test_fn, code: return dynamic_range_op(x, weight)
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         dynamic_range_140101602740000_default: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_140101602740000.default(l_x_, l_weight_);  l_x_ = l_weight_ = None
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         return (dynamic_range_140101602740000_default,)
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs]         
V1117 22:09:58.676000 212882 torch/_inductor/compile_fx.py:2411] [0/0] [__pre_grad_graphs] 
V1117 22:09:59.316000 212882 torch/_inductor/compile_fx.py:892] [0/0] FX cache status: use_cache=True, local=True, remote=False, aot_mode=False, force_disable_caches=False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] FX graph cache hash details for key f4huti2hmhrdc7hx5jhiziohpnajfbwh6gm6iwybpde35qgh7vla:
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [7jqaag6tp4ornvornb6zwaddh5hivkjuyzi24u5x4p7ke23q37q] gm: <lambda>()
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] def forward(self, arg0_1, arg1_1):
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0]     dynamic_range_140101602740000 = torch.ops.test_lib.dynamic_range_140101602740000.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0]     return (dynamic_range_140101602740000,)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0]     
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] # To see more debug info, please use `graph_module.print_readable()`
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ffvbi42jvc443loukbs4l7f2ktyctumpl3p52q36kphvdbcz745] example_inputs[0]: TensorMetadata(dtype=torch.float32, shape=torch.Size([2, 256, 128]), stride=(32768, 128, 1), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [hhafgaghh5icfiiv2s3gtezjixz3usyfz36wro23umj3t2dfcyw] example_inputs[1]: TensorMetadata(dtype=torch.float32, shape=torch.Size([128]), stride=(1,), device=device(type='cuda', index=0), layout=torch.strided, memory_format=torch.contiguous_format, storage_offset=0, storage_bytes=None, requires_grad=False, is_quantized=False, is_conj=False, is_neg=False, is_inference=False, is_sparse=False, is_coalesced=None, dense_dim=None, sparse_dim=None)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] cache_key_tag: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [lmglpn4zi7vob56n34r2j2rk7flv5xfgrcvmo7xcpirqsitygqx] fx_kwargs[boxed_forward_device_index]: BoxedDeviceIndex(value=None)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[cpp_wrapper]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [xq2hdkbfkbcuye6rgtypayrkhqf4cntij2dsd24rei3lsknakkf] fx_kwargs[cudagraphs]: BoxedBool(value=False)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[extern_node_serializer]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[fx_wrapper]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] fx_kwargs[is_backward]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] fx_kwargs[is_inference]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] fx_kwargs[layout_opt]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] fx_kwargs[static_input_idxs]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inputs_to_check[0]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inputs_to_check[1]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [du4vyrfyozrfxcf6kk6ma7oqwatapifazeelfsawmsiu6gjdtxp] deterministic_algorithms_settings: (False, False, True)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [pyawus3dzq5k52f53obyevhjmttghvob2hr5d7g4uml5s7av6wb] cuda_matmul_settings: ('none', True, True)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [mg64ekekr3poks6x2x3ywwrxpdd6tk4fb4fciybz7gmmssguevi] torch_version: ï¿½ï¿½e*ï¿½ï¿½ï¿½ï¿½Ï™ï¿½ï¿½ï¿½n ï¿½ï¿½Ñ˜ï¿½ï¿½$	ï¿½l
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [poglqjwowp4gnkmehjby2lvdjrwuo5tbxa2gayd6smgasl2hgsd] system_info[device]: {'name': 'NVIDIA H100'}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [clxrcah4tnsqpyo7ofmbwy5i3t3bwzajak4ct2aqrncz35rgmqk] system_info[version]: {'triton': '3.5.00355c3e3452fa4500122244b8fff8864f22bc05c417a3d6f5dada9f2e92f8040-c4d799c32eb6a3b92ad36ad7ea5645efc5b2fc6cd5c73dadc3a678d57dd2ccf4-c37149275a03d063fc1c339cd76a19da1c17e3d555410372e6cad29eedef6202-23d635e690d670bf61798e1259674b78c0ed5ba222ab6a455f329f27a758fc2d-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-03bffcc16c9d6be5ea8fde645caf3a6e3c0ac892eec49051a79d4cc99b66b9c7-e68505098eef3e7c0b050cb91da2587ab6c10d455ca0b941ff06fa8068e16305-318dbf7101b6ea9ebccfc57046fd8d963fe1d837c487005b37edf471a3207a9d-25cb0bee9547488335de2d495af738298ba6d4c20f1d37941dc17751c57a211e-e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855-3e3d33fbab70c7c05fc70e2e2fdd763dca0b847f62150592b99f8886a419ee64-83ef58f2371da7ad8e01822c2afc82f9a2d6516c2249ee0bbd8873bb20616be0-01a5893c8aec83f997d0e4452de7dbac0f06883c4f92e5e97ec7b03dc9f8457a-5c1281b67c0d949da34ccf1c3b68804a2caa2665953984d837d753e91af611fe-5d15c5bebef8d7aa51b21fd187e5faa95eba4a213254355bc69e0648013599f7-30106ed84518c6ca7aca08e2c0ee188755f512cc0cb2d7da8914cc48c1ad6dcc-adb54e71d0ffc3bdac437fbc97929769fbbe4ebd03e6361c6357f2a24f7c5954-27b2a5d1e8db008bacefe6019f63922bbd65926de90bb1b527ee597477d2f365-a610dc5c215589aab7a784e1c07acef3e16d53ef00f08de793899964956f4e2a-18572e33e474a820799036f2b2f8c3e54d8a526386356716cccf2bf32a832376-2cdca74c4297804dcf499a7e9d4315ab87edfe2d72f536a8fdc02f28a3e7dacd-b53abe93473eb37d88bc378692065c9a8b1bf54b6417cb1911a13d10918c6d20-f60c2bb2d8eebe1c191f4b8b819844414dd1bce243645635a094f9f92665a58e-08abee21ce6230a873ed0831f70f9570b7ce39969dbf9b2f28ae1a1992ee1cc7-8e4b8599f819f32bcabae6fd118dbbccfbec0ba9e1909224d39c5fe32fbb491f-3db4bee9427c7eb0e2105aff484bdacc819357d298e8f6e89c372ae9c3625bdf-59cf295f3aab4fa62b96a627aa9fec1302950133750de59e542c7b4c9e5b80b6-5305890c3b133def44e2f3d3405e0fb1fd6ce78d0a28b2127670a195bbe11c66', 'cuda': '12.4'}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [l3afxu4ycjwiasp557ikgya4pkhjmtmvrnsf4k3hgasyemhl747] system_info[hash]: 72df87843923244f5dfe78006e89da8f3b9c95b753d87107ae12f932f99f8dba
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[TYPE_CHECKING]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_padding]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[can_inplace_pad_graph_input]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_auto_functionalized_v2]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_progress]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[verbose_progress]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[worker_log_path]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [mxibia26nanvqq4lqvdfub66benrqh5fqtsyzzj2qnwy7srv2s3] inductor_config[precompilation_timeout_seconds]: 3600
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[remote_gemm_autotune_cache]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bundle_triton_into_fx_graph_cache]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[non_blocking_remote_cache_write]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bundled_autotune_remote_cache]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_skip_cache_dynamic_shape_guards]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[unsafe_marked_cacheable_functions]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[sleep_sec_TESTING_ONLY]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [pikr7bbcoixfzftsazp5ggufhdklj24babfry77bl4nuvyrrcp4] inductor_config[triton_kernel_default_layout_constraint]: needs_fixed_stride_order
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp_wrapper_build_separate]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fx_wrapper]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp_cache_precompile_headers]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[online_softmax]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[dce]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[static_weight_shapes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[size_asserts]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[nan_asserts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_triton_nan_asserts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[scalar_asserts]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[alignment_asserts]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pick_loop_orders]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[inplace_buffers]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[allow_buffer_reuse]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[memory_planning]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[use_fast_math]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bfloat16_atomic_adds_enabled]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [x75won4jmsgeb63pcvwr2y4eteyzzdhmf5rv6xhjppie4hx2yu5] inductor_config[memory_pool]: intermediates
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_harness]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[epilogue_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[prologue_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[epilogue_fusion_first]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[pattern_matcher]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[b2b_gemm_pass]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[custom_partitioner_fn]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[pre_grad_custom_pass]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[_post_fusion_custom_pass]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_cat_fx_passes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[efficient_conv_bn_eval_fx_passes]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[is_predispatch]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[group_fusion]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[batch_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[pre_grad_fusion_options]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[post_grad_fusion_options]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_locality]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[dynamic_scale_rblock]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_fuse_int_mm_with_mul]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_mixed_mm]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [zwmmbkdkarexuhbigurz5lfnhx64tht7fznecjkrvznh6rzivbv] inductor_config[fx_passes_numeric_check]: {'pre_grad': False, 'precision': 0.0001, 'num_iterations': 1, 'requires_optimizer': True}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v2td5s4lnsvyxvaevy4chx6kc5h3mm2axazbgwimqule5zrzao7] inductor_config[mixed_mm_choice]: heuristic
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_compute_comm_overlap]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[reorder_for_compute_comm_overlap_passes]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[reorder_prefetch_limit]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[reorder_for_peak_memory]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[reorder_for_peak_memory_debug]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[size_threshold_for_succ_based_strategy]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_all_gathers_fx]: none
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_all_gathers_fx_bucket_size_determinator]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [t3u4yj5mzijyfjvypyqngc4gf3wv6433necbugezv54jsexzrfp] inductor_config[bucket_reduce_scatters_fx]: none
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[bucket_reduce_scatters_fx_bucket_size_determinator]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [lxxtoqhcoepwfokeiibd575gnxo3uzwiv4hmpomlwkpzqz3qzsh] inductor_config[estimate_op_runtime]: default
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[runtime_estimations_mms_benchmark]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yezuzjtg4h3jjur4jwtwiehbyixa7eonq4tqsqmwqve2lvvmrem] inductor_config[intra_node_bw]: 300
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[inter_node_bw]: 25
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_experimental_benchmarker]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[distributed_max_autotune_gemm]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_pointwise]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_gemm]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[autotune_num_choices_displayed]: 10
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_report_choices_stats]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[max_autotune_prune_choices_based_on_shared_mem]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton_disable_device_detection]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[graph_partition]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[custom_should_partition_ops]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[max_autotune_allow_flexible_layouts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_same_precision]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[multi_kernel_hints]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [2y7luesktjrque3nr7qtxnum2mkbeegzdrsvkm3rvdlhqboajhx] inductor_config[max_autotune_gemm_backends]: ATEN,TRITON,CPP
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [uqlsbif4zxd75vt522p52txyuguieipi2lwz5g5awt56lccqk7s] inductor_config[max_autotune_conv_backends]: ATEN,TRITON
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_gemm_search_space]: DEFAULT
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[max_autotune_flex_search_space]: DEFAULT
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_by_default]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[selective_decompose]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_dce]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_pre_grad_passes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_joint_graph_passes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_post_grad_passes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cutedsl_enable_autotuning]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_fallback_to_aten]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [wft6ljqsfr3x4m7fa5zuyb7cwknky4irrxz4bjr6uzr2yiopxqj] inductor_config[unbacked_symint_fallback]: 8192
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[search_autotune_cache]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[save_args]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_in_subproc]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [iglov24t7x5ruci344aer2tm6nqshi4veuw4wxlssxtu46cx76m] inductor_config[max_autotune_subproc_result_timeout_seconds]: 60.0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_graceful_timeout_seconds]: 0.0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [zslw6pp37dzmhi5lhweftlhhdttfjade3t5j3y3vfk3ouze7nhw] inductor_config[max_autotune_subproc_terminate_timeout_seconds]: 0.0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[autotune_multi_device]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_tuning]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[coordinate_descent_check_all_directions]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[coordinate_descent_search_radius]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[autoheuristic_collect]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jwbrgxes7vjqumngs5hyj6gn5nytv2whnppnzngvaagfmawhkkd] inductor_config[autoheuristic_use]: mixed_mm
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[run_jit_post_compile_hook]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jvchmi66fvqzlemhr5fcqorz5trfdtdalzfagtj2aolmimwqhdq] inductor_config[autoheuristic_log_path]: DEFAULT
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4p2fdjlvxrcw7c7fvzm5huhtqxnro4kvkx56f7p5zyrxqkwooov] inductor_config[layout_opt_default]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[layout_optimization]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_layout_optimization]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[keep_output_stride]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[warn_mix_layout]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [lkkae3meylaixfif4thncru4hjqeaislawjoghffrbwuscaagei] inductor_config[realize_reads_threshold]: 4
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [rr5m5hsocoyodldz7vcvaizdwvm2rt34evmqdxvng7wz3tufvo6] inductor_config[realize_opcount_threshold]: 30
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[realize_acc_reads_threshold]: 8
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[realize_acc_reads_size_threshold]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_random]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[fallback_embedding_bag_byte_unpack]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[implicit_fallbacks]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_unaligned_fallback_output]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[inductor_choices_class]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aggressive_fusion]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_fusion]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_fusion]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[enabled_metric_tables]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_ordering_after_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[loop_index_inversion_in_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[score_fusion_memory_threshold]: 10
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[benchmark_epilogue_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[max_epilogue_benchmarked_choices]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_size]: 64
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jykiys6ynafs3zdylwa5ggq6j655mxeh42d6mtdi22gffkrmiac] inductor_config[max_fusion_buffer_group_pairwise_attempts]: 64
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[max_fusion_unique_io_buffers]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[max_pointwise_cat_inputs]: 8
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_pointwise_cat]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[unroll_reductions_threshold]: 8
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[comment_origin]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[conv_1x1_as_mm]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[split_reductions]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[deterministic]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[min_num_split]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_kernel]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[constant_and_index_propagation]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[always_keep_tensor_constants]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[assert_indirect_indexing]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[compute_all_bounds]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[combo_kernels]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[benchmark_combo_kernel]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernels_autotune]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[combo_kernel_allow_mixed_sizes]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[combo_kernel_foreach_dynamic_shapes]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [7c6i6h6bfell5u33q6rcv25lpgmk4jah3uhjjx6bjevvjnshoim] inductor_config[combo_kernel_max_num_args]: 250
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[joint_graph_constant_folding]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_index_asserts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_precision_casts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[emulate_divison_rounding]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[is_nightly_or_source]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[developer_warnings]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[optimize_scatter_upon_const_tensor]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[add_pre_grad_passes]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[remove_pre_grad_passes]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [eyt4i73byifiidlcgmugo4juf3sqznr7bv6k2xujnk6hfzd7vcn] inductor_config[small_memory_access_threshold]: 16777216
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[worker_suppress_logging]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[log_tlparse]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_fuse_ddp_communication]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [5fxczt3ciyxitdhizb7sfsgn7fhpczcqsngttnt5ot2wyctk7co] inductor_config[_fuse_ddp_bucket_size]: 25
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_micro_pipeline_tp]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_collective.auto_select]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4vdewewvaarnygruqwzavmkvu4lqggolypo2tq5ohtx2kcelkky] inductor_config[_collective.one_shot_all_reduce_threshold_bytes]: 131072
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aten_distributed_optimizations.enable_overlap_scheduling]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.collective_bucketing]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.insert_overlap_deps]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.max_compute_pre_fetch]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aten_distributed_optimizations.custom_runtime_estimation]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [y5k6t5wy5da5t7gcf4bkajyleeovwikw6pyjiaf6ieqev62zxrj] inductor_config[aten_distributed_optimizations.collective_estimator]: analytical
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[quiesce_async_compile_pool]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [smfbtbb3fuwobubxbj6g3jio7u6ufdkgz35qhppjgt3yxptkxha] inductor_config[quiesce_async_compile_time]: 60
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[use_static_cuda_launcher]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[static_launch_user_defined_triton_kernels]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[strict_static_cuda_launcher]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[global_cache_dir]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[kernel_name_max_ops]: 10
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[shape_padding]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[comprehensive_padding]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_channels_last]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_dynamic_shapes]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[disable_padding_cpu]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[expand_dimension_for_pointwise_nodes]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[padding_alignment_bytes]: 128
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [dnnw5ks3yxrp7mwvihb2hh4tqx35ye637xt33x64kw4fvz2nyzg] inductor_config[padding_stride_threshold]: 1024
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[pad_outputs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[bw_outputs_user_visible]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[force_shape_pad]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[permute_fusion]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profiler_mark_wrapper_call]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[generate_intermediate_hooks]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[debug_ir_traceback]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[_raise_error_for_testing]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[_profile_var]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[profile_bandwidth_regex]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[profile_bandwidth_output]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[profile_bandwidth_with_do_bench_using_profiling]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[disable_cpp_codegen]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[freezing_discard_parameters]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[decompose_mem_bound_mm]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[assume_aligned_inputs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[unsafe_ignore_unsupported_triton_autotune_args]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[check_stack_no_cycles_TESTING_ONLY]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_linear_binary_folding]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[annotate_training]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[enable_caching_generated_triton_templates]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[autotune_lookup_table]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [c2gaopsp6oqinpzvhlaz4gsqnq3shl5e54b7j6dsgwjadh5uatx] inductor_config[file_lock_timeout]: 600
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[enable_autograd_for_aot]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[torchinductor_worker_logpath]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [sz3im5ogc6asp7g4uqocnovype63tkdexzfrniv6hn2oank3biu] inductor_config[cpp.threads]: -1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.no_redundant_loops]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.dynamic_threads]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.simdlen]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [xgnfe6mw7nii5zpxhlblgsehzrcqmjqpqswcwvf5adwbhz7aj2h] inductor_config[cpp.min_chunk_size]: 512
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [c7zj4qytmety6keurs3hsh5wn7foxp3dqx4kym2ucszzcb2ngrf] inductor_config[cpp.cxx]: (None, 'g++')
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_kernel_profile]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.weight_prepack]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_relu_bug_TESTING_ONLY]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.inject_log1p_bug_TESTING_ONLY]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.vec_isa_ok]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[cpp.descriptive_names]: original_aten
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[cpp.max_horizontal_fusion_size]: 16
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.fallback_scatter_reduce_sum]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_unsafe_math_opt_flag]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ijs44lspkinjvhcs7uff7n3noc53jvsp4yfljjh22mafhb7khxe] inductor_config[cpp.enable_floating_point_contract_flag]: off
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_tiling_heuristics]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_grouped_gemm_template]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cpp.gemm_max_k_slices]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_cache_blocking]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cpp.gemm_thread_factors]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.enable_loop_tail_vec]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.enable_concat_linear]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_decompose_tanh]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.use_small_dequant_buffer]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cpp.force_inline_kernel]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cpp.use_constexpr_for_int_array]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraphs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_trees]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_skip_dynamic_graphs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.cudagraph_capture_sizes]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.slow_path_cudagraph_asserts]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_trees_history_recording]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.cudagraph_support_input_mutation]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ljdqgtysl3vdf7j6attlz5gmjg2ncihnveojfyubosplmkrjgra] inductor_config[triton.cudagraph_unexpected_rerecord_limit]: 128
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yttmfmxblgcbsvbokguzowcorrcxz5uunxtcvsbe6nijgcx45he] inductor_config[triton.cudagraph_dynamic_shape_warn_limit]: 8
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraph_sync]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cudagraphs_warmup]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cudagraph_or_error]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.reorder_for_reducing_graph_partitions]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.fast_path_cudagraph_asserts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_cudagraph_warmup]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_graph]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.debug_sync_kernel]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.dense_indexing]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.coalesce_tiling_analysis]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.max_tiles]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.prefer_nd_tiling]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_pointwise]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.autotune_cublasLt]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.autotune_at_compile_time]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.autotune_with_sample_inputs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.tile_reductions]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.native_matmul]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_pointwise_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.tiling_prevents_reduction_fusion]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.unique_kernel_names]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.unique_user_kernel_names]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [yrty22bseefglnysuoec4ji7j2rnaggdj3g33zzj7avogwfmgdw] inductor_config[triton.descriptive_names]: original_aten
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.persistent_reductions]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.cooperative_reductions]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.force_cooperative_reductions]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [vrl5ktomgtzox5xucd3np6vug3vyj6hwwzahqijuwpmamlv7ohi] inductor_config[triton.multi_kernel]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.divisible_by_16]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [fv6slhtedtydps5s5u2etitscliblzcidyitqf7krsv4e23fzk6] inductor_config[triton.min_split_scan_rblock]: 256
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.store_cubin]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[triton.spill_threshold]: 16
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_block_ptr]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.use_tensor_descriptor]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.inject_relu_bug_TESTING_ONLY]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.codegen_upcast_to_fp32]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_persistent_tma_matmul]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_template_tma_store]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.enable_epilogue_subtiling]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.skip_l1_cache]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.disallow_failing_autotune_kernels_TESTING_ONLY]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [j6c55jha5r2sdys2rwq7uqhtleea5dgjcye7nicfgft36v7xfvp] inductor_config[triton.num_decompose_k_splits]: 10
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [jffvide67gguonizth6bla7qwy6egn73yfn66335sv5b7i2rx3p] inductor_config[triton.decompose_k_threshold]: 32
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.enable_pdl]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[triton.mix_order_reduction]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[triton.mix_order_reduction_initial_xblock]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[triton.mix_order_reduction_split_size]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[triton.mix_order_reduction_autotune_split_size]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.output_path]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_compile]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.debug_symbols]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [6fxyf5ymh244xdypwkhtsbszab4nnfsgmul2kmyqmw422i5h54e] inductor_config[aot_inductor.compile_wrapper_opt_level]: O1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[aot_inductor.debug_intermediate_value_printer]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.filtered_kernel_names]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_in_spec]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[aot_inductor.serialized_out_spec]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_runtime_constant_folding]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.force_mmap_weights]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.use_consts_asm_build]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.package]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_cpp_only]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.dynamic_linkage]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.metadata]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.raise_error_on_ignored_optimization]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.dump_aoti_minifier]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [pr5nr4a7dthirgd2ljo3d2xakc63ywxugusu6mkmr6gmpeliyib] inductor_config[aot_inductor.repro_level]: 2
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.presets]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.allow_stack_allocation]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.use_minimal_arrayref_interface]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.weight_use_caching_allocator]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.package_constants_in_so]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.package_constants_on_disk_format]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.precompile_headers]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.embed_kernel_binary]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.emit_multi_arch_kernel]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.model_name_for_generated_files]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [4bryyl4ahh5whyg3zwqebpwmjnx6w77nqgqbdjlowju6lkqtn7w] inductor_config[aot_inductor.custom_ops_to_c_shims]: {}
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.custom_op_libs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor.enable_lto]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[aot_inductor.link_libtorch]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.cross_target_platform]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[aot_inductor.aoti_shim_library_path]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[aot_inductor_mode.compile_standalone]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.arch]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.version]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tvyftmtdmezlejo2xllu7awzv4pzc4vm4fub4b3gpl5jptjkosi] inductor_config[cuda.compile_opt_level]: -O1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_cuda_lto]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_ptxas_info]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.enable_debug_info]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.use_fast_math]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_max_profiling_configs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ty4d7ntvjwumcgotd4j6w7bwokf5njhzmtvqvxa32jjub6k2ty2] inductor_config[cuda.cutlass_max_profiling_swizzle_options]: [1, 2, 4, 8]
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_epilogue_fusion_enabled]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_tma_only]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cuda_cxx]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [aghvyrrgwvxijco2pk5wzc3cgmmthrbmgxitiibxuuscxdwrjd3] inductor_config[cuda.cutlass_backend_min_gemm_size]: 1
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.generate_test_runner]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_allowlist_regex]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[cuda.cutlass_op_denylist_regex]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ngkkx5e6z7erl6da23zb2cmsctz4yvaqyameyg5hbqln4wrhh7x] inductor_config[cuda.cutlass_instantiation_level]: 0
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.cutlass_hash_with_compile_cmd]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.cutlass_prescreening]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ly46nlihymo3siersryfadlchkmxk6ohljz4l7vognsjg2qurpp] inductor_config[cuda.cutlass_enabled_ops]: all
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.use_binary_remote_cache]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.upload_to_binary_remote_cache]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[cuda.binary_remote_cache_force_write]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[cuda.enable_caching_codegen]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[rocm.arch]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [gzctoy3drvth5kwqmdxb4tjn2picfdjsdu33nbniulhx5hsi3lv] inductor_config[rocm.ck_supported_arch]: ['gfx90a', 'gfx942', 'gfx950']
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [klfqjprnpfhcdurgvuikvc4rpd5ynkpk77toousr5h3u5roty6p] inductor_config[rocm.compile_opt_level]: -O2
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.is_debug]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.save_temps]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.use_fast_math]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[rocm.flush_denormals]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.print_kernel_resource_usage]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.rocm_home]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_dir]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.generate_test_runner]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.n_max_profiling_configs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_max_profiling_configs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.ck_tile_max_profiling_configs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[rocm.use_preselected_instances]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[rocm.kBatch_sweep]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.split_k_threshold]: 16
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ebt2ncs4f5y7dn7btzi76mnouepvzad474tmp5iju4wiuumjl4s] inductor_config[rocm.contiguous_threshold]: 16
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [bsvfcwwoczx2rlkdz2eta6doujsymyihmi46hhwk6clrrvwcb6m] inductor_config[cpu_backend]: cpp
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[cuda_backend]: triton
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [caw4ly2z672k6kjfahoxwpajp5idhhtrpgf3ma2clylcp7c7aid] inductor_config[xpu_backend]: triton
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [ljhgflgihidopsfsdcbqynv27nceykby3nutyd5jlcpq7n6e7l4] inductor_config[halide.cpu_target]: host
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [wx7vmsmrdpk5ue2txlywp3lj3faqmdjphs5fgg2ehzsyno7uovg] inductor_config[halide.gpu_target]: host-cuda
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [svgytlua5wcyeia7wq7e6zgh5tsueikrnzchmdmouvmkpfsc2zq] inductor_config[halide.scheduler_cuda]: Anderson2021
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [k5ogk6345jvklsnu7g2njqstiz2g6pm5wmqpgg3kasrmuqwjvl6] inductor_config[halide.scheduler_cpu]: Adams2019
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.asserts]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.debug]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[halide.scan_kernels]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [zwewsbwzgzypcnzixgl7ybbc4tk5kq36yeo267m422vyiuhdyiv] inductor_config[_save_config_ignore]: ['trace.upload_tar', 'joint_custom_pre_pass', 'joint_custom_post_pass', 'pre_grad_custom_pass', 'aot_inductor.repro_level', 'aot_inductor.dump_aoti_minifier', 'post_grad_custom_pre_pass', 'post_grad_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass']
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [6trwnwm4voevl4joplmkcssruwgd46kgqfejamut6kq662kstpd] inductor_config[_cache_config_ignore_prefix]: ['trace', 'cuda.cutlass_dir', 'worker_start_method', 'compile_threads', 'post_grad_custom_post_pass', 'post_grad_custom_pre_pass', 'joint_custom_pre_pass', 'joint_custom_post_pass', '_fuse_ddp_communication_passes', '_pre_fusion_custom_pass', 'always_complex_memory_overlap_TESTING_ONLY', 'fx_graph_cache', 'fx_graph_remote_cache', 'autotune_local_cache', 'autotune_remote_cache']
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [h25wqx6vliw4j5rtzzbv6latydxyei3deyg6v7wzvnzryfktuki] inductor_config[external_matmul]: []
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[write_are_deterministic_algorithms_enabled]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[lookup_table.table]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[lookup_table.check_src_hash]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_extern_kernel_in_multi_template]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.max_mm_configs]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_dtype_assert]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.runtime_triton_shape_assert]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.static_cpp_dtype_assert]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_name_regex]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.autotune_choice_desc_regex]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.graphsafe_rng_func_ignores_fallback_random]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] inductor_config[test_configs.track_memory_lifecycle]: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.use_libtorch]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [cev5uo2jlwdhw2uyzcm7vr6cl23azjfw437f5r5lskm7spucos6] inductor_config[test_configs.assume_bucketing_reduces_latency]: True
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.force_filter_reduction_configs]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [v3hzzlv4tjgvp3pyhmzagjd25orl6n7nynoa7svlhhwk73b7u3c] inductor_config[test_configs.distort_benchmarking_result]: 
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_pre_grad_graph]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] inductor_config[test_configs.bisect_keep_custom_backend_for_inductor]: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_pre_pass: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [esstihe2nyydk4mhzpvox3qkajyu5y5t23hk3fi2me7jn75xi3o] precompile_enabled: False
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] post_grad_custom_post_pass: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_pre_pass: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] joint_custom_post_pass: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _pre_fusion_custom_pass: None
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [nk3qjerriqqc77fquy5nbegbf4gnlzzbxbtxwvyxvcdzt65xl2a] _fuse_ddp_communication_passes[0]: fuse_ddp_with_concat_op
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [t46i2lzpuxqpmemjedva3sub75arja6fqed4duz4kp2bb7d3sgc] _fuse_ddp_communication_passes[1]: schedule_comm_wait
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [74x2jtykapblkbwkh24fsfbwq4iejjkibyckoc2bmgj6llnf57s] custom_backend_passes: (None, None, None, None, None)
V1117 22:09:59.322000 212882 torch/_inductor/codecache.py:983] [0/0] [tquy2we2efmowuj4wuqzcfcfdcrkzkzmwdae6hprj7fa64jpusq] _custom_partitioner_fn: None
V1117 22:09:59.322000 212882 torch/_inductor/compile_fx.py:928] [0/0] FX cache key generated: f4huti2hmhrdc7hx5jhiziohpnajfbwh6gm6iwybpde35qgh7vla
I1117 22:09:59.323000 212882 torch/_inductor/codecache.py:1613] [0/0] fx graph cache miss for key f4huti2hmhrdc7hx5jhiziohpnajfbwh6gm6iwybpde35qgh7vla
V1117 22:09:59.323000 212882 torch/_inductor/compile_fx.py:997] [0/0] FX cache miss, compiling and saving to cache
V1117 22:09:59.324000 212882 torch/_inductor/triton_bundler.py:140] [0/0] TritonBundler.begin_compile is called
I1117 22:09:59.324000 212882 torch/_inductor/compile_fx.py:1230] [0/0] Step 3: torchinductor compiling FORWARDS graph 0
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] TRACED GRAPH
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  ===== AFTER POST GRAD =====
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]  /data/users/tianren/pytorch/torch/fx/_lazy_graph_module.py class <lambda>(torch.nn.Module):
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]     def forward(self, arg0_1: "f32[2, 256, 128][32768, 128, 1]cuda:0", arg1_1: "f32[128][1]cuda:0"):
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         # File: /data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py:110 in test_fn, code: return dynamic_range_op(x, weight)
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         dynamic_range_140101602740000: "f32[2, 256, 128][32768, 128, 1]cuda:0" = torch.ops.test_lib.dynamic_range_140101602740000.default(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         return (dynamic_range_140101602740000,)
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs]         
V1117 22:09:59.342000 212882 torch/_inductor/compile_fx.py:1310] [0/0] [__post_grad_graphs] 
V1117 22:09:59.343000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.344000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.345000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %dynamic_range_140101602740000 : [num_users=1] = call_function[target=torch.ops.test_lib.dynamic_range_140101602740000.default](args = (%arg0_1, %arg1_1), kwargs = {}) 
V1117 22:09:59.345000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function test_lib::dynamic_range_140101602740000 at 0x7f69d502e480>
I1117 22:09:59.346000 212882 torch/_inductor/kernel/custom_op.py:747] [0/0] === Range-based Autotuning for dynamic_range_autotuned ===
I1117 22:09:59.347000 212882 torch/_inductor/kernel/custom_op.py:748] [0/0] Dispatch on: x[1], Ranges: [(1, 512), (513, 2048), (2049, inf)]
V1117 22:09:59.369000 212882 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:09:59.370000 212882 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 22:09:59.371000 212882 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 22:09:59.372000 212882 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:09:59.373000 212882 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 22:09:59.374000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.375000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.375000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 22:09:59.376000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.377000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 22:09:59.377000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.379000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 22:09:59.379000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.380000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 22:09:59.380000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.381000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 22:09:59.382000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.401000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:09:59.401000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf0,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:09:59.402000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:09:59.403000 212882 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: dynamic_range_autotuned_range_1_512_short_sequence_impl_0 (name=dynamic_range_autotuned_range_1_512)
V1117 22:09:59.403000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 22:09:59.404000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.405000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 22:09:59.405000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.406000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 22:09:59.407000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.408000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 22:09:59.408000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.409000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 22:09:59.409000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
I1117 22:09:59.411000 212882 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [1, 512]: Selected dynamic_range_autotuned_range_1_512_short_sequence_impl_0
V1117 22:09:59.426000 212882 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:09:59.427000 212882 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 22:09:59.428000 212882 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 22:09:59.428000 212882 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:09:59.429000 212882 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 22:09:59.430000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.431000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.432000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 22:09:59.432000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f6be422a520>
V1117 22:09:59.433000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 22:09:59.433000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.434000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 22:09:59.435000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f6be4286de0>
V1117 22:09:59.436000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 22:09:59.436000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf2,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:09:59.437000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:09:59.438000 212882 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4 (name=dynamic_range_autotuned_range_513_2048)
V1117 22:09:59.439000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 22:09:59.439000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f6be422a520>
V1117 22:09:59.440000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 22:09:59.440000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.442000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 22:09:59.442000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f6be4286de0>
I1117 22:09:59.444000 212882 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [513, 2048]: Selected dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4
V1117 22:09:59.458000 212882 torch/_inductor/select_algorithm.py:2731] [0/0] Max autotune selects from 4 choices.
V1117 22:09:59.459000 212882 torch/_inductor/select_algorithm.py:3098] [0/0] Starting precompilation
V1117 22:09:59.460000 212882 torch/_inductor/select_algorithm.py:3134] [0/0] Found all 4 timings in cache, returning no_op
V1117 22:09:59.460000 212882 torch/_inductor/select_algorithm.py:2946] [0/0] Precompilation elapsed time: 0.00s
V1117 22:09:59.461000 212882 torch/_inductor/select_algorithm.py:3033] [0/0] Autotuning elapsed time: 0.00s
V1117 22:09:59.462000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.463000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.463000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 22:09:59.464000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f6be422a520>
V1117 22:09:59.465000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 22:09:59.465000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.466000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 22:09:59.467000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f6be4286de0>
V1117 22:09:59.468000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 22:09:59.469000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] Autotuning selected choice: TensorBox(StorageBox(
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   SubgraphBuffer(
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     name=buf4,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     inputs=[TensorBox(StorageBox(
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     )), TensorBox(StorageBox(
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ))],
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     constant_args=(),
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwargs={},
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     output_view=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     python_kernel_name=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     cpp_kernel_name=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     op_overload=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     arg_properties=[{}, {}],
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     allarg_properties={},
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     kwarg_properties=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     unbacked_bindings={},
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     mutation_outputs=[],
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origin_node=None,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     stack_traces = {,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]       File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]         return dynamic_range_op(x, weight),
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     ,
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]     }
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0]   )
V1117 22:09:59.469000 212882 torch/_inductor/select_algorithm.py:2848] [0/0] ))
V1117 22:09:59.470000 212882 torch/_inductor/kernel/custom_op.py:677] [0/0] Inlining winning choice: dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7 (name=dynamic_range_autotuned_range_2049_inf)
V1117 22:09:59.471000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 22:09:59.471000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f6be422a520>
V1117 22:09:59.472000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 22:09:59.472000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.474000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 22:09:59.474000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f6be4286de0>
I1117 22:09:59.476000 212882 torch/_inductor/kernel/custom_op.py:792] [0/0] Range [2049, inf]: Selected dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7
V1117 22:09:59.477000 212882 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'dynamic_range_autotuned_range_1_512_short_sequence_impl_0' to decomposition[0] 'short_sequence_impl'
V1117 22:09:59.478000 212882 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'dynamic_range_autotuned_range_513_2048_medium_sequence_impl_4' to decomposition[1] 'medium_sequence_impl'
V1117 22:09:59.479000 212882 torch/_inductor/kernel/custom_op.py:461] [0/0] Matched choice 'dynamic_range_autotuned_range_2049_inf_medium_sequence_impl_7' to decomposition[1] 'medium_sequence_impl'
I1117 22:09:59.480000 212882 torch/_inductor/kernel/custom_op.py:819] [0/0] âœ“ Completed autotuning for 3 ranges
V1117 22:09:59.481000 212882 torch/_inductor/kernel/custom_op.py:356] [0/0] Merging range [2049, inf] into [513, inf] (same impl: medium_sequence_impl)
I1117 22:09:59.481000 212882 torch/_inductor/kernel/custom_op.py:377] [0/0] Range merging: reduced from 3 ranges to 2 ranges
I1117 22:09:59.482000 212882 torch/_inductor/kernel/custom_op.py:824] [0/0] After merging: 2 unique implementations across 2 ranges
I1117 22:09:59.483000 212882 torch/_inductor/kernel/custom_op.py:880] [0/0] âœ“ Generated dispatch function saved to: /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py
I1117 22:09:59.485000 212882 torch/_inductor/kernel/custom_op.py:885] [0/0] Creating runtime dispatch using make_fx tracing
V1117 22:09:59.486000 212882 torch/_inductor/kernel/custom_op.py:931] [0/0] Tracing dispatch function with make_fx...
V1117 22:09:59.492000 212882 torch/_inductor/kernel/custom_op.py:943] [0/0] GraphModule created with 8 nodes
I1117 22:09:59.493000 212882 torch/_inductor/kernel/custom_op.py:952] [0/0] Creating SubgraphBuffer with multi-range dispatch capability...
V1117 22:09:59.494000 212882 torch/_inductor/kernel/custom_op.py:964] [0/0]   Compiling range [1, 512]: short_sequence_impl
V1117 22:09:59.499000 212882 torch/_inductor/kernel/custom_op.py:985] [0/0]     â†’ Generated GraphModule with 8 nodes
V1117 22:09:59.500000 212882 torch/_inductor/kernel/custom_op.py:964] [0/0]   Compiling range [513, inf]: medium_sequence_impl
V1117 22:09:59.504000 212882 torch/_inductor/kernel/custom_op.py:985] [0/0]     â†’ Generated GraphModule with 6 nodes
I1117 22:09:59.506000 212882 torch/_inductor/kernel/custom_op.py:992] [0/0] âœ“ Compiled 2 range implementations
V1117 22:09:59.509000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.510000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.511000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%arg0_1, [0, 1, 2]), kwargs = {}) 
V1117 22:09:59.512000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.512000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%arg1_1, 1), kwargs = {}) 
V1117 22:09:59.513000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.514000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %unsqueeze_1 : [num_users=1] = call_function[target=torch.ops.aten.unsqueeze.default](args = (%unsqueeze, 2), kwargs = {}) 
V1117 22:09:59.514000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function unsqueeze at 0x7f6be4237100>
V1117 22:09:59.516000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %permute_1 : [num_users=1] = call_function[target=torch.ops.aten.permute.default](args = (%unsqueeze_1, [1, 2, 0]), kwargs = {}) 
V1117 22:09:59.516000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function permute at 0x7f6be4235440>
V1117 22:09:59.517000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%permute, %permute_1), kwargs = {}) 
V1117 22:09:59.517000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.519000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return mul 
V1117 22:09:59.520000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
V1117 22:09:59.521000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg0_1 : [num_users=1] = placeholder[target=arg0_1] 
V1117 22:09:59.521000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %arg1_1 : [num_users=1] = placeholder[target=arg1_1] 
V1117 22:09:59.522000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %alias : [num_users=1] = call_function[target=torch.ops.aten.alias.default](args = (%arg0_1,), kwargs = {}) 
V1117 22:09:59.523000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function nop at 0x7f6be422a520>
V1117 22:09:59.524000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %mul : [num_users=1] = call_function[target=torch.ops.aten.mul.Tensor](args = (%alias, %arg1_1), kwargs = {}) 
V1117 22:09:59.524000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function mul at 0x7f6be429e980>
V1117 22:09:59.525000 212882 torch/_inductor/graph.py:1602] [0/0] lowering %clone : [num_users=1] = call_function[target=torch.ops.aten.clone.default](args = (%mul,), kwargs = {}) 
V1117 22:09:59.526000 212882 torch/_inductor/graph.py:1291] [0/0]   via <function clone at 0x7f6be4286de0>
V1117 22:09:59.527000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return clone 
V1117 22:09:59.528000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id -1
I1117 22:09:59.528000 212882 torch/_inductor/kernel/custom_op.py:1016] [0/0] âœ“ Created SubgraphBuffer with multi-range dispatch (2 ranges)
V1117 22:09:59.530000 212882 torch/_inductor/graph.py:1602] [0/0] lowering return (dynamic_range_140101602740000,) 
V1117 22:09:59.531000 212882 torch/_inductor/graph.py:1498] [0/0] Force channels last inputs for 0 conv for the current graph with id 0
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf0,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.543000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000, mul, permu...,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.544000 212882 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf2,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.546000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf3', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000, clone, mul]),
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.547000 212882 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf4,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.548000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='buf5', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(arg1_1, i2)
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000, clone, mul]),
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.549000 212882 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf6,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=dynamic_range_140101602740000,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.551000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf7,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.552000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling SubgraphBuffer(
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   name=buf8,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   layout=FixedLayout('cuda:0', torch.float32, size=torch.Size([2, 256, 128]), stride=(32768, 128, 1)),
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   inputs=[TensorBox(StorageBox(
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg0_1', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]))
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   )), TensorBox(StorageBox(
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]     InputBuffer(name='arg1_1', layout=FixedLayout('cuda:0', torch.float32, size=[128], stride=[1]))
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ))],
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   constant_args=(),
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwargs={},
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   output_view=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   python_kernel_name=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   cpp_kernel_name=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ordered_kwargs_for_cpp_kernel=(),
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   op_overload=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   arg_properties=[{}, {}],
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   allarg_properties={},
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   kwarg_properties=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   unbacked_bindings={},
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   mutation_outputs=[],
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=None,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000]),
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.553000 212882 torch/_inductor/scheduler.py:2988] [0/0] )
V1117 22:09:59.554000 212882 torch/_inductor/scheduler.py:3059] [0/0] scheduling output buf6
V1117 22:09:59.555000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf8
V1117 22:09:59.555000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op8
V1117 22:09:59.556000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf7
V1117 22:09:59.556000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op7
V1117 22:09:59.557000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf5
V1117 22:09:59.557000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op5
V1117 22:09:59.558000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf4
V1117 22:09:59.558000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op4
V1117 22:09:59.558000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf3
V1117 22:09:59.559000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op3
V1117 22:09:59.559000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf2
V1117 22:09:59.560000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op2
V1117 22:09:59.560000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf1
V1117 22:09:59.561000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op1
V1117 22:09:59.561000 212882 torch/_inductor/scheduler.py:3211] [0/0] removed dead buffer: buf0
V1117 22:09:59.561000 212882 torch/_inductor/scheduler.py:3222] [0/0] removed dead operation: op0
I1117 22:09:59.563000 212882 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:09:59.563000 212882 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:09:59.564000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:09:59.565000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:09:59.566000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:09:59.579000 212882 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node op6 with estimated runtime 0.000214
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_range_1_512_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_range_1_512_arg1_1, i2)
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=mul,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000, mul, permu...,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.582000 212882 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:09:59.583000 212882 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_range_1_512_buf0
I1117 22:09:59.591000 212882 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:09:59.592000 212882 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:09:59.592000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:09:59.593000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:09:59.594000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:09:59.596000 212882 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_range_1_512_op0 with estimated runtime 0.000214
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_arg0_1, %get_index), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_arg1_1, %get_index_1), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_range_1_512_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:09:59.604000 212882 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:09:59.608000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.609000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.611000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.617000 212882 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140101602740000_0
V1117 22:09:59.619000 212882 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0] scheduling ComputedBuffer(name='dynamic_range_autotuned_autotuned_range_513_inf_buf0', layout=FixedLayout('cuda:0', torch.float32, size=[2, 256, 128], stride=[32768, 128, 1]), data=Pointwise(
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   'cuda',
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   torch.float32,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   def inner_fn(index):
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       i0, i1, i2 = index
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp0 = ops.load(dynamic_range_autotuned_autotuned_range_513_inf_arg0_1, i2 + 128 * i1 + 32768 * i0)
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp1 = ops.load(dynamic_range_autotuned_autotuned_range_513_inf_arg1_1, i2)
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       tmp2 = tmp0 * tmp1
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return tmp2
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ranges=[2, 256, 128],
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origin_node=clone,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   origins=OrderedSet([dynamic_range_140101602740000, clone, mul]),
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   stack_traces = {,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]     File "/data/users/tianren/pytorch/test/inductor/test_dynamic_range_standalone.py", line 110, in test_fn,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]       return dynamic_range_op(x, weight),
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   ,
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0]   }
V1117 22:09:59.623000 212882 torch/_inductor/scheduler.py:2988] [0/0] ), _split_size=None, _original_inner_fn=None, _original_ranges=None, _original_reduction_ranges=None)
V1117 22:09:59.624000 212882 torch/_inductor/scheduler.py:3059] [0/0] scheduling output dynamic_range_autotuned_autotuned_range_513_inf_buf0
I1117 22:09:59.629000 212882 torch/_inductor/memory.py:894] [0/0] Reordering for peak memory -- 1 nodes
I1117 22:09:59.629000 212882 torch/_inductor/memory.py:928] [0/0] Baseline peak memory: 0
I1117 22:09:59.630000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_lpmf peak memory: 0
I1117 22:09:59.631000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_bfs peak memory: 0
I1117 22:09:59.631000 212882 torch/_inductor/memory.py:946] [0/0] topological_sort_dfs peak memory: 0
V1117 22:09:59.634000 212882 torch/_inductor/scheduler.py:5996] [0/0] Generating code for node dynamic_range_autotuned_autotuned_range_513_inf_op0 with estimated runtime 0.000214
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0] get_bounds:
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0] graph():
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %ops : [num_users=4] = placeholder[target=ops]
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %load : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_513_inf_arg0_1, %get_index), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index_1 : [num_users=1] = call_module[target=get_index](args = (index1,), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %load_1 : [num_users=1] = call_method[target=load](args = (%ops, dynamic_range_autotuned_autotuned_range_513_inf_arg1_1, %get_index_1), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %mul : [num_users=1] = call_method[target=mul](args = (%ops, %load, %load_1), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %get_index_2 : [num_users=1] = call_module[target=get_index](args = (index0,), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     %store : [num_users=1] = call_method[target=store](args = (%ops, dynamic_range_autotuned_autotuned_range_513_inf_buf0, %get_index_2, %mul, None), kwargs = {})
V1117 22:09:59.637000 212882 torch/_inductor/bounds.py:81] [0/0]     return store
V1117 22:09:59.639000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.640000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.641000 212882 torch/_inductor/codegen/triton.py:2092] [0/0] Cannot use TMA descriptor for load / store since:  Requires triton>=3.4.0, a CUDA device with cc>=9.0 and `use_tensor_descriptor` and `assume_aligned_inputs` options enabled
V1117 22:09:59.642000 212882 torch/_inductor/codegen/simd.py:1873] [0/0] Generating kernel code with kernel_name: triton_poi_fused_dynamic_range_140101602740000_0
V1117 22:09:59.643000 212882 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:09:59.644000 212882 torch/_inductor/graph.py:2325] [0/0] Finished codegen for all nodes. The list of kernel names available: OrderedSet([])
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] Output code: 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] # AOT ID: ['0_inference']
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from ctypes import c_void_p, c_long, c_int
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import torch
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import math
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import random
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import os
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import tempfile
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from math import inf, nan
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from cmath import nanj
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.hooks import run_intermediate_hooks
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.utils import maybe_profile
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.codegen.memory_planning import _align as align
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch import device, empty_strided
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.async_compile import AsyncCompile
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.select_algorithm import extern_kernels
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_heuristics import start_graph, end_graph
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] aten = torch.ops.aten
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] inductor_ops = torch.ops.inductor
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] _quantized = torch.ops._quantized
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_size_stride = torch._C._dynamo.guards.assert_size_stride
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] assert_alignment = torch._C._dynamo.guards.assert_alignment
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu = torch._C._dynamo.guards._empty_strided_cpu
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cpu_pinned = torch._C._dynamo.guards._empty_strided_cpu_pinned
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_cuda = torch._C._dynamo.guards._empty_strided_cuda
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_xpu = torch._C._dynamo.guards._empty_strided_xpu
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_mtia = torch._C._dynamo.guards._empty_strided_mtia
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] reinterpret_tensor = torch._C._dynamo.guards._reinterpret_tensor
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] alloc_from_pool = torch.ops.inductor._alloc_from_pool
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile = AsyncCompile()
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] empty_strided_p2p = torch._C._distributed_c10d._SymmetricMemory.empty_strided_p2p
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] # kernel path: /tmp/torchinductor_tianren/fg/cfglh2aht54gsv6ibje3tmxlfeu6bridfjeuqc7ajoimvzjb3skd.py
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Unsorted Source Nodes: [dynamic_range_140101602740000_default], Original ATen: [test_lib.dynamic_range_140101602740000]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] # Source node to ATen node mapping:
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] #   dynamic_range_140101602740000_default => dynamic_range_140101602740000
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_poi_fused_dynamic_range_140101602740000_0 = async_compile.triton('triton_poi_fused_dynamic_range_140101602740000_0', '''
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] import triton.language as tl
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime import triton_helpers, triton_heuristics
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.triton_helpers import libdevice, math as tl_math
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._inductor.runtime.hints import AutotuneHint, ReductionHint, TileHint, DeviceProperties
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] triton_helpers.set_driver_to_gpu()
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton_heuristics.pointwise(
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     size_hints={'x': 65536}, 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     filename=__file__,
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     triton_meta={'signature': {'in_ptr0': '*fp32', 'in_ptr1': '*fp32', 'out_ptr0': '*fp32', 'xnumel': 'i32', 'XBLOCK': 'constexpr'}, 'device': DeviceProperties(type='cuda', index=0, multi_processor_count=132, cc=90, major=9, regs_per_multiprocessor=65536, max_threads_per_multi_processor=2048, warp_size=32), 'constants': {}, 'native_matmul': False, 'configs': [{(0,): [['tt.divisibility', 16]], (1,): [['tt.divisibility', 16]], (2,): [['tt.divisibility', 16]], (3,): [['tt.divisibility', 16]]}], 'enable_fp_fusion': True},
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     inductor_meta={'grid_type': 'Grid1D', 'autotune_hints': set(), 'kernel_name': 'triton_poi_fused_dynamic_range_140101602740000_0', 'mutated_arg_names': [], 'optimize_mem': True, 'no_x_dim': False, 'atomic_add_found': False, 'num_load': 2, 'num_store': 1, 'num_reduction': 0, 'backend_hash': 'BDCD04EBA777A11071614C3A47955F25C1ECF1ACA0FACD1C9BC97535638A01EB', 'assert_indirect_indexing': True, 'autotune_local_cache': True, 'autotune_pointwise': True, 'autotune_remote_cache': None, 'force_disable_caches': False, 'dynamic_scale_rblock': True, 'max_autotune': False, 'max_autotune_pointwise': False, 'min_split_scan_rblock': 256, 'spill_threshold': 16, 'store_cubin': False, 'deterministic': False, 'force_filter_reduction_configs': False, 'are_deterministic_algorithms_enabled': False, 'tiling_scores': {'x': 786944}},
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     min_elem_per_thread=0
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] )
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] @triton.jit
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] def triton_poi_fused_dynamic_range_140101602740000_0(in_ptr0, in_ptr1, out_ptr0, xnumel, XBLOCK : tl.constexpr):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xnumel = 65536
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xoffset = tl.program_id(0) * XBLOCK
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xindex = xoffset + tl.arange(0, XBLOCK)[:]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     xmask = tl.full([XBLOCK], True, tl.int1)[:]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x2 = xindex
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     x0 = (xindex % 128)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp0 = tl.load(in_ptr0 + (x2), None)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp1 = tl.load(in_ptr1 + (x0), None, eviction_policy='evict_last')
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tmp2 = tmp0 * tmp1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     tl.store(out_ptr0 + (x2), tmp2, None)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] ''', device_str='cuda')
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned_range_1_512(args):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_range_1_512_arg0_1, dynamic_range_autotuned_autotuned_range_1_512_arg1_1 = args
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_1_512_arg1_1, (128, ), (1, ))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_range_1_512_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_140101602740000_default], Original ATen: [test_lib.dynamic_range_140101602740000]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_140101602740000_0.run(dynamic_range_autotuned_autotuned_range_1_512_arg0_1, dynamic_range_autotuned_autotuned_range_1_512_arg1_1, dynamic_range_autotuned_autotuned_range_1_512_buf0, 65536, stream=stream0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_1_512_arg0_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_1_512_arg1_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_range_1_512_buf0, )
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] from torch._C import _cuda_getCurrentRawStream as get_raw_stream
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] def dynamic_range_autotuned_autotuned_range_513_inf(args):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     dynamic_range_autotuned_autotuned_range_513_inf_arg0_1, dynamic_range_autotuned_autotuned_range_513_inf_arg1_1 = args
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     args.clear()
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_513_inf_arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     assert_size_stride(dynamic_range_autotuned_autotuned_range_513_inf_arg1_1, (128, ), (1, ))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     with torch.cuda._DeviceGuard(0):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         torch.cuda.set_device(0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         dynamic_range_autotuned_autotuned_range_513_inf_buf0 = empty_strided_cuda((2, 256, 128), (32768, 128, 1), torch.float32)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         # Unsorted Source Nodes: [dynamic_range_140101602740000_default], Original ATen: [test_lib.dynamic_range_140101602740000]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         stream0 = get_raw_stream(0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         triton_poi_fused_dynamic_range_140101602740000_0.run(dynamic_range_autotuned_autotuned_range_513_inf_arg0_1, dynamic_range_autotuned_autotuned_range_513_inf_arg1_1, dynamic_range_autotuned_autotuned_range_513_inf_buf0, 65536, stream=stream0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_513_inf_arg0_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         del dynamic_range_autotuned_autotuned_range_513_inf_arg1_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return (dynamic_range_autotuned_autotuned_range_513_inf_buf0, )
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] async_compile.wait(globals())
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] del async_compile
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] class Runner:
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def __init__(self, partitions):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = partitions
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def recursively_apply_fns(self, fns):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         new_callables = []
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         for fn, c in zip(fns, self.partitions):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             new_callables.append(fn(c))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         self.partitions = new_callables
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     def call(self, args):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         arg0_1, arg1_1 = args
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         args.clear()
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg0_1, (2, 256, 128), (32768, 128, 1))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         assert_size_stride(arg1_1, (128, ), (1, ))
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         with torch.cuda._DeviceGuard(0):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             torch.cuda.set_device(0)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned_range_1_512
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_range_1_512_args = [arg0_1, arg1_1]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf7,) = dynamic_range_autotuned_autotuned_range_1_512(dynamic_range_autotuned_autotuned_range_1_512_args)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             # subgraph: dynamic_range_autotuned_autotuned_range_513_inf
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             dynamic_range_autotuned_autotuned_range_513_inf_args = [arg0_1, arg1_1]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf8,) = dynamic_range_autotuned_autotuned_range_513_inf(dynamic_range_autotuned_autotuned_range_513_inf_args)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             def buf6_runtime_dispatch(args):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 arg0, arg1 = args
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 dispatch_size = arg0.size(1)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 if dispatch_size <= 512:
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                     return dynamic_range_autotuned_autotuned_range_1_512(args)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                 else:
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]                     return dynamic_range_autotuned_autotuned_range_513_inf(args)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             buf6_runtime_dispatch_args = [arg0_1, arg1_1]
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             (buf6,) = buf6_runtime_dispatch(buf6_runtime_dispatch_args)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg0_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]             del arg1_1
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]         return (buf6, )
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] runner = Runner(partitions=[])
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] call = runner.call
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] recursively_apply_fns = runner.recursively_apply_fns
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] def benchmark_compiled_module(times=10, repeat=10):
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._dynamo.testing import rand_strided
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.utils import print_performance
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg0_1 = rand_strided((2, 256, 128), (32768, 128, 1), device='cuda:0', dtype=torch.float32)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     arg1_1 = rand_strided((128, ), (1, ), device='cuda:0', dtype=torch.float32)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     fn = lambda: call([arg0_1, arg1_1])
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     return print_performance(fn, times=times, repeat=repeat)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] if __name__ == "__main__":
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     from torch._inductor.wrapper_benchmark import compiled_module_main
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code]     compiled_module_main('None', benchmark_compiled_module)
V1117 22:09:59.645000 212882 torch/_inductor/graph.py:2431] [0/0] [__output_code] 
V1117 22:09:59.646000 212882 torch/_inductor/graph.py:2442] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/aq/caq52wovivpsqtlfddgvlhvlmy2hdc533phd4bv55a6iz2rsd5v2.py
V1117 22:09:59.653000 212882 torch/_inductor/runtime/triton_heuristics.py:316] [0/0] CachingAutotuner gets 2 configs for triton_poi_fused_dynamic_range_140101602740000_0
V1117 22:09:59.654000 212882 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 22:09:59.654000 212882 torch/_inductor/runtime/triton_heuristics.py:322] [0/0] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None
V1117 22:09:59.655000 212882 torch/_inductor/runtime/triton_heuristics.py:331] [0/0] Triton cache dir: /tmp/torchinductor_tianren/triton/0
V1117 22:09:59.745000 212882 torch/_inductor/graph.py:2401] [0/0] Output code written to: /tmp/torchinductor_tianren/aq/caq52wovivpsqtlfddgvlhvlmy2hdc533phd4bv55a6iz2rsd5v2.py
I1117 22:09:59.746000 212882 torch/_inductor/graph.py:2402] [0/0] [__output_code] Output code written to: /tmp/torchinductor_tianren/aq/caq52wovivpsqtlfddgvlhvlmy2hdc533phd4bv55a6iz2rsd5v2.py
I1117 22:09:59.748000 212882 torch/_inductor/triton_bundler.py:197] [0/0] Saving 1 statically launchable CachingAutotuners
V1117 22:09:59.748000 212882 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 22:09:59.749000 212882 torch/_inductor/triton_bundler.py:151] [0/0] TritonBundler.end_compile is called
V1117 22:09:59.749000 212882 torch/_inductor/compile_fx.py:1024] [0/0] Saving compiled graph to FX cache with key: f4huti2hmhrdc7hx5jhiziohpnajfbwh6gm6iwybpde35qgh7vla
V1117 22:09:59.751000 212882 torch/_inductor/compile_fx.py:1093] [0/0] FX codegen and compilation took 0.435s
I1117 22:09:59.751000 212882 torch/_inductor/compile_fx.py:1120] [0/0] Overview info of inductor aten mms: 
I1117 22:09:59.752000 212882 torch/_inductor/compile_fx.py:1121] [0/0] Name                           | B                    | M                    | N                    | K                    | Count               
I1117 22:09:59.752000 212882 torch/_inductor/compile_fx.py:1126] [0/0] ----------------------------------------------------------------------------------------------------------------------------------
I1117 22:09:59.753000 212882 torch/_inductor/compile_fx.py:1136] [0/0] Step 3: torchinductor done compiling FORWARDS graph 0
V1117 22:09:59.797000 212882 torch/_inductor/runtime/triton_heuristics.py:1076] Benchmark all input configs for triton_poi_fused_dynamic_range_140101602740000_0, get:
V1117 22:09:59.798000 212882 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 512, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005664, nreg 22, nspill 0, #shared-mem 0
V1117 22:09:59.798000 212882 torch/_inductor/runtime/triton_heuristics.py:1078] XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 22:09:59.799000 212882 torch/_inductor/runtime/triton_heuristics.py:1114] Best config for triton_poi_fused_dynamic_range_140101602740000_0: XBLOCK: 256, num_warps: 4, num_ctas: 1, num_stages: 1, maxnreg: None: 0.005600, nreg 12, nspill 0, #shared-mem 0
V1117 22:09:59.799000 212882 torch/_inductor/runtime/autotune_cache.py:310] Save heuristic tuning result to /tmp/torchinductor_tianren/fg/a6dd2214968f5fb980a12b708f4def648f605c1a6cb4f1f3bc84a2ba7d930b79.best_config
Running test on device: cuda

=== Verifying all implementations produce equivalent results ===
  âœ“ short_impl correct for seq_len=256
  âœ“ medium_impl correct for seq_len=256
  âœ“ long_impl correct for seq_len=256
  âœ“ short_impl correct for seq_len=1024
  âœ“ medium_impl correct for seq_len=1024
  âœ“ long_impl correct for seq_len=1024
  âœ“ short_impl correct for seq_len=4096
  âœ“ medium_impl correct for seq_len=4096
  âœ“ long_impl correct for seq_len=4096

=== Testing autotuning with compilation ===
  âœ“ Compiled version produces correct results
  âœ“ Dispatch function generated with torch.cond at /tmp/torch_inductor_range_dispatch/dynamic_range_autotuned_dispatch.py

âœ… All tests passed!
I1117 22:10:02.234000 212882 torch/_inductor/remote_cache.py:432] Cache Metrics:
I1117 22:10:02.234000 212882 torch/_inductor/remote_cache.py:432]   LocalAutotuneCache: {hit: 0, miss: 1, put: 1, exception: 0}
I1117 22:10:02.234000 212882 torch/_inductor/remote_cache.py:432]   backend:_LocalAutotuneCacheBackend: {hit: 0, miss: 1, put: 1, exception: 0}
I1117 22:10:02.234000 212882 torch/_inductor/remote_cache.py:432] 
